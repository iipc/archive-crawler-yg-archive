{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":164438524,"authorName":"Lars Clausen","from":"Lars Clausen &lt;lc@...&gt;","profile":"lrclause","replyTo":"LIST","senderId":"Yqx7xjkfLoTu2m9mG-3vEY1zzqZ4piXpM8DFh22rTFtqHK0Oxbbjtl1fW3MeUGEMAH5k5ZwadWUGu7QcQhBhQjN4ayQBPdP6VBeVKQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Compression of ARC files","postDate":"1089982034","msgId":660,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEwODk5ODIwMzMuMTkzMy4xMzguY2FtZWxAcGM3NzAuc2Iuc3RhdHNiaWJsaW90ZWtldC5kaz4=","inReplyToHeader":"PDE2NjMxLjQ5NjU5LjI1NDQzNy45NzM1NDNAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","referencesHeader":"PDIwMDQwNzE0MDQwMi5pNkU0MmhaMjA3NjVAcG9sbHV4LnN0YXRzYmlibGlvdGVrZXQuZGs+IDwxMDg5NzkwODExLjE5MzMuMjEuY2FtZWxAcGM3NzAuc2Iuc3RhdHNiaWJsaW90ZWtldC5kaz4gPDQwRjU1RkFCLjUwNjA5MDZAYXJjaGl2ZS5vcmc+IDwxMDg5ODczOTUwLjE5MzMuNTQuY2FtZWxAcGM3NzAuc2Iuc3RhdHNiaWJsaW90ZWtldC5kaz4gPDQwRjZDMjA1LjkwNDAzMDJAYXJjaGl2ZS5vcmc+IDwxMDg5OTY3MDI3LjE5MzMuODYuY2FtZWxAcGM3NzAuc2Iuc3RhdHNiaWJsaW90ZWtldC5kaz4gPDE2NjMxLjQ5NjU5LjI1NDQzNy45NzM1NDNAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4="},"prevInTopic":659,"nextInTopic":661,"prevInTime":659,"nextInTime":661,"topicId":629,"numMessagesInTopic":20,"msgSnippet":"... Well, I ve discussed it locally without getting to a conclusion, and would like the arguments for and against to be out there where other people can find","rawEmail":"Return-Path: &lt;lc@...&gt;\r\nX-Sender: lc@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 80598 invoked from network); 16 Jul 2004 12:51:01 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m13.grp.scd.yahoo.com with QMQP; 16 Jul 2004 12:51:01 -0000\r\nReceived: from unknown (HELO luna.statsbiblioteket.dk) (130.225.24.87)\n  by mta4.grp.scd.yahoo.com with SMTP; 16 Jul 2004 12:51:01 -0000\r\nReceived: from pc770.sb.statsbiblioteket.dk\n (pc770.sb.statsbiblioteket.dk [130.225.24.181]) by luna.statsbiblioteket.dk\n (iPlanet Messaging Server 5.2 HotFix 1.16 (built May 14 2003))\n with ESMTP id &lt;0I0Y007A03IQ4T@...&gt; for\n archive-crawler@yahoogroups.com; Fri, 16 Jul 2004 14:47:14 +0200 (MEST)\r\nDate: Fri, 16 Jul 2004 14:47:14 +0200\r\nIn-reply-to: &lt;16631.49659.254437.973543@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-id: &lt;1089982033.1933.138.camel@...&gt;\r\nOrganization: Statsbiblioteket\r\nMIME-version: 1.0\r\nX-Mailer: Ximian Evolution 1.4.5 (1.4.5-1)\r\nContent-type: text/plain\r\nContent-transfer-encoding: 7BIT\r\nReferences: &lt;200407140402.i6E42hZ20765@...&gt;\n &lt;1089790811.1933.21.camel@...&gt;\n &lt;40F55FAB.5060906@...&gt;\n &lt;1089873950.1933.54.camel@...&gt;\n &lt;40F6C205.9040302@...&gt;\n &lt;1089967027.1933.86.camel@...&gt;\n &lt;16631.49659.254437.973543@...&gt;\r\nX-eGroups-Remote-IP: 130.225.24.87\r\nFrom: Lars Clausen &lt;lc@...&gt;\r\nSubject: Re: [archive-crawler] Compression of ARC files\r\nX-Yahoo-Group-Post: member; u=164438524\r\nX-Yahoo-Profile: lrclause\r\n\r\nOn Fri, 2004-07-16 at 13:54, Tom Emerson wrote:\n&gt; Lars Clausen writes:\n&gt; &gt; I&#39;m not saying that zlib is any worse or better than other compression\n&gt; &gt; algorithms in comprehensibility.  It&#39;s that if you just get the file,\n&gt; &gt; it&#39;s binary junk, not the immediately comprehensible ASCII-based ARC\n&gt; &gt; format that had me convinced would be readable 100 years from now.\n&gt; \n&gt; So configure your crawls to not compress the ARC files and you get the\n&gt; results you want.\n\nWell, I&#39;ve discussed it locally without getting to a conclusion, and\nwould like the arguments for and against to be out there where other\npeople can find them.\n\n&gt; Remember too that the data stored in the arc file, except for the URL\n&gt; headers in each member, can be binary junk depending on the content\n&gt; type. What guarantee will you have that PDF files or ShockWave movies\n&gt; will be readable in 100 years?\n\nThat is a separate problem that has to be dealt with in other ways. \nAbsolutely not easy, either.  But the many pages that we now get as HTML\nwould be readable if the file contents are readable.  And at least for\nweb crawling, HTML is by far the most common.\n\n&gt; Anyway, if you want to look at the raw contents of your compressed\n&gt; ARCs, just use zmore:\n&gt; \n&gt; (0) tree% zmore ARN-20040708211401-00000.arc.gz\n\nYes, now, if you happen to have zmore.  Solaris doesn&#39;t come with it,\nfor instance (and BTW, I prefer zless:).\n\nBut if you read RFC 1952 (the gzip standard), you&#39;ll notice that &quot;The\ndata format defined by this specification does not attempt to provide\nrandom access to compressed data...&quot;, which is essentially what we&#39;re\ntrying to make it do.  That some tools work with that is just luck, some\ntools won&#39;t.  In fact, .arc.gz files are not gzip files according to the\nstandard, but a concatenation of multiple gzip files (not zlib streams).\n\n&gt; &gt; One could argue that the same holds for hardware or file system\n&gt; &gt; compression, but I would say that in those cases, if you can read the\n&gt; &gt; files, you get it out of compressed mode.  Only if a tape/disk was found\n&gt; &gt; so long from now that none of the original hardware is available and\n&gt; &gt; some kind of other scan is set up would the compression be a problem.\n&gt; \n&gt; And I would argue that it is far more likely that the hardware would\n&gt; have deteriorated and be unusable. For that matter if we have media\n&gt; now (besides good old paper) that exists in 100 years we&#39;ll be lucky:\n&gt; it won&#39;t matter at all whether the data is compressed or not if it\n&gt; cannot be read.\n\nThat is true.  However, most archives are well aware of the need for\nmigrating across hardware.  But if you carefully migrate a file across\nseveral generations of hardware and OSes only to find that you&#39;ve lost\nthe info on how to understand the file, you&#39;re still in trouble.  A\ndigital archive is no better than its weakest link.  Both hardware\ncompression and compressing file systems are merely things you migrate\nyour data through.  That is different from having the data itself be\ncompressed.\n\n&gt; &gt; One way to make compression a little more workable would be to not\n&gt; &gt; compress the meatdata lines.  That way, there wouldn&#39;t be any confusion\n&gt; &gt; about the length of the block, and it would be possible to understand\n&gt; &gt; the structure of the file without using zlib.  It could (and should) be\n&gt; &gt; indicated in the header what compression style is used.  \n&gt; \n&gt; There is no confusion about the length of the block now, since each\n&gt; compressed member in the ARC file is a complete gzip stream: you just\n&gt; decode it until done. The noise related to overrunning the end offset\n&gt; is a side-effect of the Java implementation of zlib, not the\n&gt; compression format itself.\n\nWhat you can&#39;t do right now is skip past blocks without decompressing\nthem, as you don&#39;t know their compressed lengths.  That means that\nextracting CDX files is more time-consuming, as you&#39;ll have to read and\nuncompress about 100 times as much material.  \n\nOf course, the proposed idea of having the metadata\n\n&gt; On thing you could do is take a page from the PDF format: a\n&gt; cross-reference table exists at the end of the file that gives\n&gt; information about the sections in the rest of the file. The same could\n&gt; be done for ARCs, giving the start offsets of each member in the\n&gt; ARC. That way a reader could quickly scan the offset table and not\n&gt; need to worry about the overruns.\n\nThat would violate one of the requirements set forth in\nhttp://www.archive.org/web/researcher/ArcFileFormat.php:\n\n    The file must be &quot;stream able&quot;: it must be possible to concatenate\nmultiple archive files in a data stream.\n\nI&#39;d also like to point out the requirement that says\n\n    Once written, a record must be viable: the integrity of the file\n    must not depend on subsequent creation of an in-file index of the\n    contents.\n\nTo understand a gzipped ARC file, you either have to have an index or\nyou must parse and uncompress it from the beginning.  There&#39;s no going\ninto the middle of it and finding the headers.  While it&#39;s not the\nintegrity of the file that is destroyed, it does make index-less work on\nthe file a lot harder.\n\n-Lars\n\n\n"}}