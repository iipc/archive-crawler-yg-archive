{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"QnXhkfWBhUJlVV8OpsBkVjuvQUcizutxMifaaeBeUve7t_KRee0u3KnrT3Y4DP-Ww9e8XZgqITKJwL2bYAbboA6QfqAJbEbi","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Parallelizing crawler","postDate":"1153152741","msgId":3065,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkJCNkU1LjIwNzAyMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ0QkIzRTU3LjEwNTA2QGFvbC5jb20+","referencesHeader":"PGU5OGlxZCtpMXMzQGVHcm91cHMuY29tPiA8NDRCN0RGMzUuMzA0MDgwMUBhcmNoaXZlLm9yZz4gPDQ0QkIzRTU3LjEwNTA2QGFvbC5jb20+"},"prevInTopic":3060,"nextInTopic":3071,"prevInTime":3064,"nextInTime":3066,"topicId":3043,"numMessagesInTopic":16,"msgSnippet":"... Sounds sweet. + Did you use the ubicrawler ConsistentHashFunction implementation that is in Heritrix in the dsi.unimi.it jar or did you use something else?","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 69346 invoked from network); 17 Jul 2006 16:10:57 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m39.grp.scd.yahoo.com with QMQP; 17 Jul 2006 16:10:57 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta2.grp.scd.yahoo.com with SMTP; 17 Jul 2006 16:10:56 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k6HEsdk05209;\n\tMon, 17 Jul 2006 07:54:40 -0700\r\nMessage-ID: &lt;44BBB6E5.2070208@...&gt;\r\nDate: Mon, 17 Jul 2006 09:12:21 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8.0.4) Gecko/20060516 SeaMonkey/1.0.2\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e98iqd+i1s3@...&gt; &lt;44B7DF35.3040801@...&gt; &lt;44BB3E57.10506@...&gt;\r\nIn-Reply-To: &lt;44BB3E57.10506@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: Parallelizing crawler\r\nX-Yahoo-Group-Post: member; u=168599281; y=R0uItY_Vg4Q8JtA-uda2XLSPmzPxRlGg7rLNPU7LxbZTO7ZnjWmqba83\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nSharad Agarwal wrote:\n&gt;\n&gt; We had the requirement of scaling heritrix. We have done it by employing\n&gt; Consistent hashing scheme. The hashing scheme splits the url space based\n&gt; on domain (so that politness is ensured) on which each heritrix instance\n&gt; work on. The whole set up is controller less; each node has the hashing\n&gt; function. Whenever a out of node&#39;s scope url is found, it is sent to the\n&gt; node owning it. These urls are collected and sent in batches to the\n&gt; owning nodes via JMX by a separate thread.\n&gt; The architecture looks to be scalable in the sense that there is no\n&gt; controller bottleneck. All heritrix node are identical. The whole scheme\n&gt; is working pretty fine for us. We have gone quite a few millions with\n&gt; 3-4 heritrix instance nodes.\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\nSounds sweet.\n\n+ Did you use the ubicrawler ConsistentHashFunction implementation that \nis in Heritrix in the dsi.unimi.it jar or did you use something else?\n+ If, lets say, a hostmaster complained you were hitting her servers too \nhard, how did you figure which of the crawlers the domain had landed \non?  (Was it a problem for you that its effectively opaque which domain \nlanded on which server?)\n+ Do you have anything to say about how well the load was balanced \nacross the crawlers in practise.  I understand that in theory the load \nshould be evenly distributed but I wonder how the practise was.  Could \nyou adjust how much load a machine was carrying post startup?\n+ Is the code proprietary and if not, would you mind sharing it?\n\nGood stuff,\nSt.Ack\n\n\n&gt;\n&gt; - Sharad\n&gt;\n&gt; stack@... &lt;mailto:stack%40archive.org&gt; wrote:\n&gt;\n&gt; &gt;molzbh wrote:\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;&gt;Cool. Anyways my thoughts on splitting the architecture were to split\n&gt; &gt;&gt;the Processors across various machines. Have a Statistics server\n&gt; &gt;&gt;maintain statistics, and a single frontier.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;Here&#39;s a few items you&#39;d have to contend with if you split Heritrix:\n&gt; &gt;\n&gt; &gt;+ You&#39;ll have to spend alot of resources serializing and deserializing\n&gt; &gt;passing rich URLs and configurations.\n&gt; &gt;+ Certain processors, as written, expect to find on local disk the\n&gt; &gt;downloaded resources (You could share them using NFS. NFS won&#39;t take\n&gt; &gt;too kindly to the crawler&#39;s heavy I/O).\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;&gt;My idea was to have\n&gt; &gt;&gt;frontier emit a batch of URLS to the processore boxes, hence instead\n&gt; &gt;&gt;of next() you have nextBatch(). Anyways, I guess this involves a lot\n&gt; &gt;&gt;of work, hence I am going in for a Peer2Peer setup, with fully loaded\n&gt; &gt;&gt;agents doing the URL splits and emmitting batches to the others. I am\n&gt; &gt;&gt;wondering however on the efficiency of JMX with RMI connectors to do\n&gt; &gt;&gt;this Job. Do you think going the plain RMI way would be faster?\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;We haven&#39;t measured. JMX is heavyweight but works.\n&gt; &gt;\n&gt; &gt;One suggestion has been to use JGroups. Friends of Heritrix were\n&gt; &gt;experimenting making a &#39;cluster&#39; subclass of Frontier. On\n&gt; &gt;initialization, it joined a JGroups group and listened in for URL\n&gt; &gt;broadcasts. Those that fit its portion of the URL space, it picked off\n&gt; &gt;the bus and added to the local Frontier. Those URLs meant for another\n&gt; &gt;crawler, were broadcast (They&#39;ve said they&#39;ll write the list if findings\n&gt; &gt;prove promising).\n&gt; &gt;\n&gt; &gt;But depends on your requirements. Joe Hungs&#39; group didn&#39;t bother\n&gt; &gt;swapping URLs across the cluster. Their experience was that the\n&gt; &gt;crawlers had sufficient work without exchanging URLs and figured that\n&gt; &gt;each crawler would discover its URL-segment important pages anyways\n&gt; &gt;without having to have injection from peers (I don&#39;t know if they\n&gt; &gt;&#39;proved&#39; this assertion).\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;&gt;Sorry\n&gt; &gt;&gt;for pounding you with questions but I am still in the design phase of\n&gt; &gt;&gt;the system looking for a billion plus crawl, and since it won&#39;t be a\n&gt; &gt;&gt;one time thing I am also looking at scalability and agent join/leave\n&gt; &gt;&gt;mechanisms.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;Can we collaborate? This is a problem we need to solve ourselves.\n&gt; &gt;\n&gt; &gt;St.Ack\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;&gt;--- In archive-crawler@yahoogroups.com \n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt; &gt;&gt;&lt;mailto:archive-crawler%40yahoogroups.com&gt;, Michael Stack &lt;stack@...&gt;\n&gt; &gt;&gt;wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt;Anmol Bhasin wrote:\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;Thanks! I am wondering however if we leave the central frontier\n&gt; &gt;&gt;&gt;&gt;machine concept out for a bit, would there be anybenfit to split URL\n&gt; &gt;&gt;&gt;&gt;space in place of Split Architechture.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;I am trying to do quick big crawl, hence wondering which approaches\n&gt; &gt;&gt;&gt;&gt;are the best ways to get moving.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;If you&#39;re in a hurry, split URL space (and throw hardware at it). Some\n&gt; &gt;&gt;&gt;fairly large crawls have been achieved using this technique both by us\n&gt; &gt;&gt;&gt;-- 200million plus -- and by others (See the testimonial cited in the\n&gt; &gt;&gt;&gt;previous where Joe Hung and his compaï¿½eros did a 1Billion+ pages).\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;What were you thinking regards splitting the architecture?\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;Yours,\n&gt; &gt;&gt;&gt;St.Ack\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;  \n\n\n"}}