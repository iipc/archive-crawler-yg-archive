{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163922992,"authorName":"johnerikhalse","from":"&quot;johnerikhalse&quot; &lt;johnh@...&gt;","profile":"johnerikhalse","replyTo":"LIST","senderId":"00TRU4me7o_ZsUD2FYNLqFfqUuep4SqO4mpkptPmEV6LYog1X3_HVAao6MOYRC6-BeEvXnK8814CuFYRiMN6EGTq8Wo57zW__WU","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Suggestions for per host settings","postDate":"1069790582","msgId":184,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGJxMGNobStrN3ZkQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":183,"nextInTime":185,"topicId":184,"numMessagesInTopic":1,"msgSnippet":"SUGGESTIONS FOR PER HOST SETTINGS This document aims to describe a solution for the per host settings as required by the Memorandum of understanding between","rawEmail":"Return-Path: &lt;johnh@...&gt;\r\nX-Sender: johnh@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 18120 invoked from network); 25 Nov 2003 20:05:18 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m14.grp.scd.yahoo.com with QMQP; 25 Nov 2003 20:05:18 -0000\r\nReceived: from unknown (HELO n9.grp.scd.yahoo.com) (66.218.66.93)\n  by mta4.grp.scd.yahoo.com with SMTP; 25 Nov 2003 20:05:17 -0000\r\nReceived: from [66.218.66.141] by n9.grp.scd.yahoo.com with NNFMP; 25 Nov 2003 20:03:02 -0000\r\nDate: Tue, 25 Nov 2003 20:03:02 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nSubject: Suggestions for per host settings\r\nMessage-ID: &lt;bq0chm+k7vd@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 6133\r\nX-Mailer: Yahoo Groups Message Poster\r\nFrom: &quot;johnerikhalse&quot; &lt;johnh@...&gt;\r\nX-Yahoo-Group-Post: member; u=163922992\r\nX-Yahoo-Profile: johnerikhalse\r\n\r\nSUGGESTIONS FOR PER HOST SETTINGS\n\nThis document aims to describe a solution for the per host settings as\nrequired by the &quot;Memorandum of understanding&quot; between the Internet\nArchive and the Nordic National Libraries. The requirement is to be\nable to specify settings for each site, domain defaults and global\ndefault. This document does not address the possibility to set\nconfiguration settings on a per document basis.\n\n\nHIERARCHY OF SETTINGS\n\nSettings should be settable in a hierarchy with the default or global\nsettings on top. Then there should be possible to set settings on a\nper domain basis and on a per host basis. The latter should always has\nprecedence over the former.\n\nIn addition to this we might want to have two sets of hierarchies so\nthat a set of settings could be shared among different crawls. And\nanother that would eventually override the first set. The use for this\ncould be that there is a set of configurations shared by different\ninstitutions doing similar crawls for example different national\nlibraries doing almost the same crawl but for different domains. The\nsettings hierarchy will then look something like this for a host A in\nthe domain B, we call the first configuration hierarchy for common and\nthe second for local:\n\n1. read the order file which contains the default settings for the\n   crawl and also points to the common and local configurations.\n2. override if there are global settings in common\n3. override if there are global settings in local\n4. override if there are domain B settings in common\n5. override if there are domain B settings in local\n6. override if there are host A settings in common\n7. override if there are host A settings in local\n\nAll settings except for the initial order file should only contain\nthose fields which are to be overridden.\n\n\nSETTINGS THAT SHOULD BE MODIFIABLE PER HOST\n\nNot all settings should be settable on a per domain or per host\nbasis. Especially this is the case with certain settings as the\nnumber of threads and which directories is to be used for log and arc\nfiles. This list shows some of the settings that should be\noverrideable by domain or host settings.\n\n* Scope settings\n  - max link hops\n  - max trans hops\n* Behavior settings\n  - robots honoring policy\n  - user-agent and from settings\n* Politeness settings\n  - delay factor\n  - min delay\n  - max delay\n* Processor settings\n  - max file size to fetch\n  - which filters for inclusion and exclusion to apply\n\nIn addition we might want to change which processors should handle URIs\nfrom a specific host.\n\n\nCONFIGURATION FILES\n\nProbably the best way of extending how the configuration is to be read\nis by adding a two new fields in the order file which points to\ndirectories containing the common and the local configuration\nhierarchies. These directories has three different kind of files which\nis all optional. One global file with the global settings. One or more\ndomain files with settings for one domain each. One or more host files\nwith settings for one host each.\n\n\nIMPLEMENTATION\n\n* FRONTIER\n\nTo achieve these goals we need a place where the frontier should get the\nconfiguration settings for each host.\n\nIn the current implementation there is a CrawlServer class which keeps\ntrack of the robots.txt for a host and also makes sure that we only\nhave one fetch in progress from a certain host. I think this class\nmight be a good candidate for keeping the per host configuration as\nwell. The CrawlServer should at instantiation ask the CrawlController\nfor its configuration by issuing its host name. Then it is up to the\ncontroller to figure out if it should stick with the default settings\nor if some or all settings should be overridden for this host.\n\nThe frontier should be altered to associate a CrawlServer with the URI\nas soon as it is fetched from the pending queue and turned into a\nCrawlURI. This way the URI could be asked for the configuration\nsettings it should use.\n\nThe big implication this has on the design is that the different\nmodules that currently extends the XMLConfig class for getting\nits own configuration settings should instead ask other components for\nits settings so that they are more dynamic. This way the module will\nnot be as dependent of the structure of the order file as it is\ntoday. The configuration will be a data structure on its own and most\nmodules should not extend the XMLConfig class.\n\n\n* CONTROLLER\n\nThe second issue to be considered is how the controller keeps track of\nthe configurations for each host. One way of doing this is to compile\nthe hierarchy of settings for a host into one order object which is then\ndelivered to the CrawlServer class upon request. Another approach is to\nkeep the hierarchy of settings in memory and let the CrawlServer get\nthe configuration object with the fines granularity for a certain\nhost. When a setting is requested the request will be thrown up the\nhierarchy until it finds something to respond. Both solutions can be\nimplemented with a late-initialization approach so that the actual\nconfiguration is not read into memory before it is needed.\n\nThe main problem is that with a large broad crawl there might be a lot\nof CrawlServers active and that will demand a lot of memory to be\nallocated. Several things could be done to reduce that problem.\n\nThe frontier could be constrained to only work with a limited set of\nhosts at a time. When it thinks it has finished a host, either by\nconstraints given to it or there is no more pending URIs, it should\nthrow the CrawlServer away and start working on new ones. If the host\nis to be revisited, that is that other hosts pointing to links inside\nthis host which haven&#39;t been visited, then the configuration has to be\nreconstructed.\n\nA variant of the above is to keep track of when a CrawlServer object\nwas last accessed and dispose the object after a certain time.\n\nAnother optimization would be to not use the XML DOM as the underlying\ndata structure for configuration settings. The DOM creates a lot of\nunnecessary objects. By building a custom data structure created by a\nSAX parser the burden both in terms of memory and processing, could be\nreduced.\n\n\n\n"}}