{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264138474,"authorName":"Kenji Nagahashi","from":"Kenji Nagahashi &lt;knagahashi@...&gt;","profile":"kenznag","replyTo":"LIST","senderId":"j0Z-XTcx9Fv9B6-Rt2ej7sOSQNqa_fIS5SsY-CQy5ZGBIrlsU496V-KcG2BOCQGZzuTOrAhMoBS3xN0FPyNnk9Qk2RlAH9d7fgqd2Tc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix Frontier","postDate":"1297361673","msgId":7012,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENTQyQjA5LjkwMjA0MDRAZ21haWwuY29tPg==","inReplyToHeader":"PDRENTFEODI4LjkwNzA5MDJAYXJjaGl2ZS5vcmc+","referencesHeader":"PEM5Nzc0OTQwLjIxMUUlbXVyYWxpa3BAYW1hem9uLmNvbT4gPDRENTFEODI4LjkwNzA5MDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":7008,"nextInTopic":7013,"prevInTime":7011,"nextInTime":7013,"topicId":7005,"numMessagesInTopic":11,"msgSnippet":"Hi Krishna, I m an engineer working on web-wide crawl at Internet Archive. As Gordon said, We re strongly interested in the architecture you described, and in","rawEmail":"Return-Path: &lt;knagahashi@...&gt;\r\nX-Sender: knagahashi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 64188 invoked from network); 10 Feb 2011 18:14:42 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m16.grp.re1.yahoo.com with QMQP; 10 Feb 2011 18:14:42 -0000\r\nX-Received: from unknown (HELO mail-iy0-f170.google.com) (209.85.210.170)\n  by mta3.grp.re1.yahoo.com with SMTP; 10 Feb 2011 18:14:42 -0000\r\nX-Received: by iym14 with SMTP id 14so1487554iym.1\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 10 Feb 2011 10:14:40 -0800 (PST)\r\nX-Received: by 10.42.180.200 with SMTP id bv8mr23774073icb.514.1297361680825;\n        Thu, 10 Feb 2011 10:14:40 -0800 (PST)\r\nReturn-Path: &lt;knagahashi@...&gt;\r\nX-Received: from Kenjis-MacBook-Pro.local (router300.sf.archive.org [208.70.27.190])\n        by mx.google.com with ESMTPS id g4sm147843ick.11.2011.02.10.10.14.36\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Thu, 10 Feb 2011 10:14:38 -0800 (PST)\r\nMessage-ID: &lt;4D542B09.9020404@...&gt;\r\nDate: Thu, 10 Feb 2011 10:14:33 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; ja-JP-mac; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;C9774940.211E%muralikp@...&gt; &lt;4D51D828.9070902@...&gt;\r\nIn-Reply-To: &lt;4D51D828.9070902@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix Frontier\r\nX-Yahoo-Group-Post: member; u=264138474; y=68rEEYJQAI89L6asOIEJMYP4j2swfddY87K7SfFJCpRykQ\r\nX-Yahoo-Profile: kenznag\r\n\r\nHi Krishna,\n\nI&#39;m an engineer working on web-wide crawl at Internet Archive.\nAs Gordon said, We&#39;re strongly interested in the architecture you \ndescribed, and in fact developing a Heritrix-3 based prototype right \nnow. For your information, current approach is:\n\n- a simple central control server, backed by distributed storage, that \ndoes &quot;seen&quot; check, de-deplication and URL scheduling\n- Heritrix 3 configured with custom UniqUriFilter and a disposition \nprocessor talking with the central server over HTTP\n\ncurrently it requires small modification to WorkQueueFrontier that \nallows it to efficiently &quot;pull&quot; more URIs from the central server when \nit run out of &quot;ready&quot; queues. So it&#39;s unlikely that it will be \ncompatible with the next release of Heritrix 3, although I&#39;d be happy to \nmake it a part of future releases.\n\nI really welcome discussions on this topic.\n\nThanks,\nKenji Nagahashi\n\n(2/8/11 3:56 PM), Gordon Mohr wrote:\n&gt; On 2/8/11 5:55 AM, Krishna, Murali wrote:\n&gt;  &gt; Thanks Gordon for the detailed response.\n&gt;  &gt; We don&#39;t have all the urls upfront, it is a continuous stream of urls\n&gt;  &gt; and we don&#39;t want to wait for the previous heritrix jobs to finish.\n&gt;  &gt; Essentially, we want to schedule them as and when is possible (honoring\n&gt;  &gt; politeness). Also some of the urls have higher priority for crawling.\n&gt;\n&gt; If you want your prioritization to also take effect within the standard\n&gt; Heritrix frontier queues, there&#39;s a (seldom-used) pair of features,\n&gt; queue and URI &#39;precedence&#39; values, which affect (1) how inactive queues\n&gt; are sorted while waiting for their chance to be active; (2) where URIs\n&gt; are inserted into individual queues (if all the other factors which\n&gt; affect whether they are pushed-near-the-top or queued-to-the-back are\n&gt; equal).\n&gt;\n&gt;  &gt; The problem with BDB frontier is that it is tied to the box and is a\n&gt;  &gt; reliability concern if the machine goes down. We are thinking of\n&gt; having the\n&gt;  &gt; urls in reliable queue service in a different cluster and make the\n&gt; heritrix\n&gt;  &gt; read from that queue. This makes heritrix instance stateless (the crawled\n&gt;  &gt; content goes to another cluster) and easy to replace with another box. Of\n&gt;  &gt; course this calls for a new checkpointing mechanism outside the box.\n&gt;\n&gt; We&#39;ve considered possibilities for this as well, and some sort of\n&gt; remote/distributed frontier is likely in the future for Heritrix, though\n&gt; nothing is definitively scheduled/prioritized for an upcoming release.\n&gt;\n&gt; Some of the range of possibilities could include:\n&gt;\n&gt; - Replacing the current BDB-JE binary key-value store in the standard\n&gt; frontier with a remote/distributed alternative. This might be most\n&gt; straightforward, though a few issues that don&#39;t come up with the local\n&gt; store would have to be addressed, including: (1) new latencies/overhead;\n&gt; (2) multiple crawl processes sharing the same store, (3) what recovery\n&gt; from arbitrary crashes might be possible.\n&gt;\n&gt; - Still using the traditional default frontier locally, but pull URIs in\n&gt; batches from the shared/remote store. (This might not require any\n&gt; rewriting of the local frontier; just some other module that reports\n&gt; results up, and pulls the right batches of new URIs down.) The shared\n&gt; URI-queues/history info could be very different in its representations,\n&gt; checkpointing, etc.\n&gt;\n&gt; - Replacing the usual local frontier with an alternative which operates\n&gt; in a whole new way with the remote queues/store, just implementing the\n&gt; minimal expected frontier behavior. We want the code to support this\n&gt; possibility, but there are almost certainly hidden assumptions in other\n&gt; parts of the code that the frontier behaves like our standard one, which\n&gt; would need some clean up when tested by this new approach. And, much of\n&gt; the existing timing/politeness/ordering behavior would have to be\n&gt; reimplementing in broadly-similar ways... though perhaps much of the\n&gt; existing design could be mirrored albeit with remote queues/sets.\n&gt;\n&gt;  &gt; I understand it is an overkill to use heritrix, but In future, we might\n&gt;  &gt; need depth crawl which we can easily implement by scheduling the newly\n&gt;  &gt; detected urls back into the reliable queue service. We are just trying to\n&gt;  &gt; leverage the politeness, threading and pluggable processor frameworks of\n&gt;  &gt; heritrix.\n&gt;  &gt;\n&gt;  &gt; Thoughts?\n&gt;\n&gt; I better understand your motivations and they seem reasonable. I&#39;ll be\n&gt; very interested to hear any architectural directions you take, and can\n&gt; discuss. If either the code, or simply the changes that make Heritrix\n&gt; better able to rely on remote frontier functionality, can be contributed\n&gt; back, it will likely be of interest to the IA and other Heritrix users\n&gt; as well!\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt;  &gt; Thanks,\n&gt;  &gt; Murali\n&gt;  &gt;\n&gt;  &gt; On 2/8/11 1:56 PM, &quot;Gordon Mohr&quot;&lt;gojomo@...\n&gt; &lt;mailto:gojomo%40archive.org&gt;&gt; wrote:\n&gt;  &gt;\n&gt;  &gt;&gt; On 2/7/11 3:17 AM, Krishna, Murali wrote:\n&gt;  &gt;&gt;&gt;\n&gt;  &gt;&gt;&gt;\n&gt;  &gt;&gt;&gt; Hi all,\n&gt;  &gt;&gt;&gt; We have a list of urls to be crawled, essentially just a fetch and some\n&gt;  &gt;&gt;&gt; processing. Assume that the list can be huge and run into billions. So,\n&gt;  &gt;&gt;&gt; we are thinking of writing a new Frontier which will accomplish this.\n&gt;  &gt;&gt;&gt; Will have multiple heritrix worker boxes, each of the workerï¿½s frontier\n&gt;  &gt;&gt;&gt; will get one portion of the centralized url repository (distributed\n&gt;  &gt;&gt;&gt; storage) and schedule them for crawling.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; You probably won&#39;t need a new Frontier for this; the default frontier\n&gt;  &gt;&gt; (BdbFrontier) should work for tens to even hundreds of millions of\n&gt;  &gt;&gt; queued URIs per node.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; I have more confidence in H3 for loading millions of seed URIs at\n&gt;  &gt;&gt; startup (which should be even more efficient in H3 SVN TRUNK and the\n&gt;  &gt;&gt; next H3 release), though you could also feed them in smaller batches via\n&gt;  &gt;&gt; the H1 JMX interface, or in batches via the &#39;action&#39; directory mechanism\n&gt;  &gt;&gt; in H3.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; If you simply have a large static list of URIs to crawl -- and don&#39;t\n&gt;  &gt;&gt; need link-extraction and any other running analysis/reporting --\n&gt;  &gt;&gt; Heritrix may be overkill for your purposes.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;&gt; 1. Can we achieve this by extending the WorkQueueFrontier ? I couldnï¿½t\n&gt;  &gt;&gt;&gt; find much documentation on how WQF handles politeness. I am thinking of\n&gt;  &gt;&gt;&gt; grouping the urls into workqueue based on politeness requirement, will\n&gt;  &gt;&gt;&gt; it automatically take care of politeness if I group correctly? Can I\n&gt;  &gt;&gt;&gt; configure crawl-delay per WorkQueue?\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; You can control what is crawled -- whether outlinks from your starting\n&gt;  &gt;&gt; URIs are followed, for example, to get inline resources or other linked\n&gt;  &gt;&gt; pages -- by customizing the scoping rules. If grouping URIs by hostname\n&gt;  &gt;&gt; into queues is insufficient, you can implement a new\n&gt;  &gt;&gt; QueueAssignmentPolicy. In H3, politeness delays per URI -- affecting the\n&gt;  &gt;&gt; queue from which the URI came -- are configured outside the frontier, in\n&gt;  &gt;&gt; the DispositionProcessor. So lots of behavioral customization doesn&#39;t\n&gt;  &gt;&gt; require reimplementing or specializing the frontier.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; The queues are the units of politeness: by default, only one URI from a\n&gt;  &gt;&gt; queue will be in-process at a time. When a URI finishes, a configurable\n&gt;  &gt;&gt; pause (see the minDelay, maxDelay, delayFactor, and\n&gt;  &gt;&gt; respectCrawlDelayUpToSeconds settings on DispositionProcessor in H3) is\n&gt;  &gt;&gt; applied to that queue before any other URIs are tried. Note that URI\n&gt;  &gt;&gt; domain-lookup/connectivity failures cause the same URI to be pushed back\n&gt;  &gt;&gt; atop the queue, a longer (frontier retryDelaySeconds) pause to be taken,\n&gt;  &gt;&gt; and multiple (frontier maxRetries) attempts to be made, before the next\n&gt;  &gt;&gt; URI is tried. This means you usually do not want URIs on different\n&gt;  &gt;&gt; hosts, where one host might be unreachable, mixed in the same queue --\n&gt;  &gt;&gt; one failure will delay them all -- unless you also knock the\n&gt;  &gt;&gt; retries/retryDelay way down.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; You can use the settings &#39;sheet overlay&#39; (aka &#39;overrides&#39; in H1) to set\n&gt;  &gt;&gt; different politeness values for different URIs by host or other\n&gt;  &gt;&gt; patterns; the queue then takes on the delay of the URI that was just\n&gt;  &gt;&gt; offered/completed.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; You should also look at previous list traffic about HashCrawlMapper for\n&gt;  &gt;&gt; ideas on splitting the URI space, and the BloomUriUniqFilter as an\n&gt;  &gt;&gt; option for an all in-memory URI-already-seen filter that may be\n&gt;  &gt;&gt; appropriate for larger crawls.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;&gt; 2. What are inactive queues, retired queues and getURIList here?\n&gt; (sorry,\n&gt;  &gt;&gt;&gt; couldnï¿½t find doc)\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; &#39;inactive&#39; queues are those that are not yet being considered to keep a\n&gt;  &gt;&gt; thread busy. All those queues that are &#39;active&#39; round-robin to provide\n&gt;  &gt;&gt; URIs to available threads, until the queue exhausts its &#39;session&#39;\n&gt;  &gt;&gt; budget; then it goes to the back of all &#39;inactive&#39; queues. If a thread\n&gt;  &gt;&gt; can&#39;t be kept busy with an available &#39;active&#39; queue, then the top\n&gt;  &gt;&gt; &#39;inactive&#39; queue is activated. The intent is for the crawler to\n&gt;  &gt;&gt; intensely focus on some queues for a while -- hoping to finish them, or\n&gt;  &gt;&gt; at least get a big batch of URIs with as little time-skew as possible ï¿½ï¿½\n&gt;  &gt;&gt; but then rotate others into activity eventually. (The &#39;budgeting&#39; and\n&gt;  &gt;&gt; URI &#39;cost&#39; parameters affect this cycle.)\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; &#39;retired&#39; queues have already offered up URIs whose total &#39;cost&#39; exceeds\n&gt;  &gt;&gt; their &#39;totalBudget&#39;, and so they continue to collect newly-discovered\n&gt;  &gt;&gt; URIs, but will never b considered for &#39;active&#39; rotation (unless you\n&gt;  &gt;&gt; raise their &#39;totalBudget&#39;). You probably don&#39;t need this feature, and\n&gt;  &gt;&gt; won&#39;t notice any &#39;retired&#39; queues unless you set a &#39;totalBudget&#39; and\n&gt;  &gt;&gt; nonzero URI cost policy.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; I don&#39;t know what you mean by &quot;getURIList&quot;.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;&gt; 3. How does checkpointing work, I want to restart from the last crawled\n&gt;  &gt;&gt;&gt; state. Is there a callback to do the frequent checkpointing for\n&gt;  &gt;&gt;&gt; WorkQueueFrontierï¿½s implementations.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; Checkpointing tries to save the whole crawl state at requested points.\n&gt;  &gt;&gt; You can set it to automatically checkpoint at a certain interval. In H1,\n&gt;  &gt;&gt; it&#39;s via a system property; see Checkpointer.initialize(). In H3, it&#39;s\n&gt;  &gt;&gt; CheckpointService&#39;s checkpointIntervalMinutes property. In H1, the crawl\n&gt;  &gt;&gt; must reach a full pause for a checkpoint to occur; with long downloads\n&gt;  &gt;&gt; and connection timeouts, this can mean a slowdown in the tens of\n&gt;  &gt;&gt; minutes. In H3, a checkpoint requires a much narrower lock, so may only\n&gt;  &gt;&gt; take a few seconds or a minute or two, and long network fetches may\n&gt;  &gt;&gt; continue during the checkpoint.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; In both cases, you need to retain the &#39;state&#39; directory files\n&gt;  &gt;&gt; (.JDB/.DEL) corresponding to the checkpoints from which you might want\n&gt;  &gt;&gt; to resume. (If not needed for a checkpoint, you can freely delete the\n&gt;  &gt;&gt; .DELs.) Resuming from an earlier checkpoint may foul any other\n&gt;  &gt;&gt; subsequent-but-unused checkpoints (though future checkpoints should be\n&gt;  &gt;&gt; fine).\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; The checkpointing system has always been a bit rough but I have more\n&gt;  &gt;&gt; confidence in its flexibility and speed in H3. I would not yet count on\n&gt;  &gt;&gt; it for perfect resumability in a large crawl without more experience\n&gt;  &gt;&gt; using it. If not using checkpoints, or checkpoints fail for some reason,\n&gt;  &gt;&gt; an approximation of a frontier&#39;s state at the time of a crash can be\n&gt;  &gt;&gt; recreated from the &#39;frontier recovery log&#39; also kept by the crawler.\n&gt;  &gt;&gt; (Not all running stats/state can be reconstructed, but essentially all\n&gt;  &gt;&gt; the same pending URIs will be reenqueued.)\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; I believe there&#39;s a JMX call in H1 to request a checkpoint, and in H3\n&gt;  &gt;&gt; it&#39;s just one of the web UI/web service calls easy to trigger by a\n&gt; web hit:\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Herit\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Herit&gt;\n&gt;  &gt;&gt; rix3.0APIGuide-CheckpointJob\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; Hope this helps!\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; - Gordon @ IA\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; ------------------------------------\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; Yahoo! Groups Links\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; ------------------------------------\n&gt;  &gt;\n&gt;  &gt; Yahoo! Groups Links\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;\n&gt; \n\n\n"}}