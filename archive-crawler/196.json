{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163922992,"authorName":"John Erik Halse","from":"John Erik Halse &lt;johnh@...&gt;","profile":"johnerikhalse","replyTo":"LIST","senderId":"X3wG2aKKJ-UQ1UnfVYBRTg3db7F664qmty_UMlutdTOOhCFRVB-S8snEMB3JSes2eeKEioOy0YV5_MiXfqWXYDpE1-ifX-hUi8A","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: [Fwd: Second draft of per host settings\tdocument]","postDate":"1071096022","msgId":196,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEwNzEwOTYwMjIuMTA5MDQuNjUuY2FtZWxAYjExNi1keW4tMzcuYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDNGRDc4OEJELjYwNzA2MDFAYXJjaGl2ZS5vcmc+","referencesHeader":"PDNGRDc4OEJELjYwNzA2MDFAYXJjaGl2ZS5vcmc+"},"prevInTopic":195,"nextInTopic":197,"prevInTime":195,"nextInTime":197,"topicId":195,"numMessagesInTopic":5,"msgSnippet":"Thanks, a lot of good points here. ... It is, with the current code, possible to change the order file during a crawl. This is done trough the UI and the","rawEmail":"Return-Path: &lt;johnh@...&gt;\r\nX-Sender: johnh@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 37559 invoked from network); 10 Dec 2003 22:43:37 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m3.grp.scd.yahoo.com with QMQP; 10 Dec 2003 22:43:37 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 10 Dec 2003 22:43:37 -0000\r\nReceived: (qmail 4192 invoked by uid 100); 10 Dec 2003 22:42:56 -0000\r\nReceived: from b116-dyn-37.archive.org (johnh@...@209.237.240.37)\n  by mail-dev.archive.org with RC4-MD5 encrypted SMTP; 10 Dec 2003 22:42:56 -0000\r\nSubject: Re: [archive-crawler] Re: [Fwd: Second draft of per host settings\n\tdocument]\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nIn-Reply-To: &lt;3FD788BD.6070601@...&gt;\r\nReferences: &lt;3FD788BD.6070601@...&gt;\r\nContent-Type: text/plain\r\nMessage-Id: &lt;1071096022.10904.65.camel@...&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Ximian Evolution 1.4.5 \r\nDate: Wed, 10 Dec 2003 14:40:22 -0800\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-6.3 required=6.0\n\ttests=AWL,BAYES_20,EMAIL_ATTRIBUTION,IN_REP_TO,QUOTED_EMAIL_TEXT,\n\t      REFERENCES,REPLY_WITH_QUOTES,TONER,USER_AGENT_XIMIAN\n\tautolearn=ham version=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: John Erik Halse &lt;johnh@...&gt;\r\nX-Yahoo-Group-Post: member; u=163922992\r\nX-Yahoo-Profile: johnerikhalse\r\n\r\nThanks, a lot of good points here.\n\nOn Wed, 2003-12-10 at 12:57, Michael Stack wrote:\n&gt; Proposal looks good to me.  Model reminds of me of apache &#39;.htaccess&#39; \n&gt; file scheme where you can insert directory specific config. to override \n&gt; the core httpd.conf.  I like it.  Below are some general \n&gt; comments/questions:\n&gt; \n&gt; + Does the order file change as you do a crawl?  Where is state kept?  \n&gt; What if you want to tweak a setting during a crawl.  Where would that be \n&gt; done?  Would you change the order file or would you instead change UI \n&gt; and its value would be save to a state file subsequently used crawling?  \n&gt; Would such a change be one of the &#39;dynamic settings&#39; from the global \n&gt; configuration file saved at the head of the configuration dir \n&gt; (&#39;overrides&#39; for the order file) mentioned in your doc.?\nIt is, with the current code, possible to change the order file during a\ncrawl. This is done trough the UI and the changes are stored to disk.\n\n&gt; + Why can&#39;t I set any value?  What if during a crawl I want to up the \n&gt; number of running threads or change the logging levels or the way in \n&gt; which statistics are being gathered (e.g. move from &#39;pedestrian&#39; crawl \n&gt; to debug mode) or even instantiate new processors and change the order \n&gt; in which the processor chain works.  Some settings won&#39;t be changeable \n&gt; mid-crawl and these might fail w/ a polite message (e..g. changing heap \n&gt; size mid-run) but otherwise, I&#39;d suggest no constraint on what can be \n&gt; changed in configuration (Keep in mind I don&#39;t know much about crawler \n&gt; deploys).\nYes, most things should be changeable during a crawl. The distinction\nbetween the order file and the configuration hierarchy is that settings\nthat should be possible to override on a per host basis goes to the\ncrawl configuration, while settings that only makes sense for the\ncrawler as a whole goes to the order file. The names of these two kind\nconfigurations should be changed to better reflect this.\n\n&gt; + I&#39;d imagine in the scheme of things, the reading of a new \n&gt; configuration file into memory, whether per host or per domain, a rare \n&gt; event.  Do you agree?  At what time are the domain/host configuration \n&gt; files read?  On initially crossing into a new domain or on first seeing \n&gt; a host?  Will we ever refresh what we have in mem?\nMy thought is that we read the configuration file when crossing into a\nnew domain. However it is possible to imagine an arbitrary large number\nof per host configuration files. Then it might be possible that we\ncannot keep them all in memory at all times.\n\n&gt; + What are the plans for having multiple crawler instances working off \n&gt; the one Frontier instance?  If the frontier makes a CrawlServer to \n&gt; associate w/ a URI, how will we ensure that one crawler instance does \n&gt; one server only (Maybe frontier can do distribution between crawlers).\nIf you by multiple crawler instances mean a crawler spanning multiple\nmachines (or VMs), then this has not been addressed yet.\n\n&gt; + Won&#39;t there only be one instance of the configuration instance in \n&gt; memory? It won&#39;t be duplicated per CrawlerServer -- just a reference to \n&gt; the single instance?  Why then are we worried about XML DOM cost?  Won&#39;t \n&gt; the configuration just be read into an object tree w/ an xml serializer \n&gt; used creating the objects w/ periodic visits to the file on disk to \n&gt; check mod time?  The serializer would do something like, if a value \n&gt; exists at a certain node in the configuration hierarchy, a getter that \n&gt; returns the value at that location in the hierarchy is returned, else, \n&gt; we call super.  Or are we talking of doing per host an aggregation of \n&gt; all xml snippets to write a new configuration file to feed the crawler \n&gt; accessing a particular host?\nWhen I started writing the document the DOM was used as the internal\ndata structure and XPaths was used to get the different settings. This\nis costly if the values should be looked up for every URI. These days\nthe settings are kept in a HashMap after the initial lookup which\ngreatly decreases the cost. But if we are working with an internal data\nstructure distinct from the DOM, it shouldn&#39;t by much harder to use SAX\ninstead of the DOM to build it. Then as mentioned before there could\npossibly be thousands of per host configuration files and then\nperformance becomes an issue. I&#39;d like the Configuration objects to\nrepresent a file on disk. If there is a configuration for a host the\nCrawlServer points to this. If there isn&#39;t, the CrawlServer will point\nto the configuration for its domain if present, or to the global\nconfiguration. When a module asks for a setting it will ask the\nconfiguration object which the CrawlServer references. The configuration\nobjects knows its parent, so if the setting isn&#39;t there it will push the\nrequest up to that.\n\n&gt; + Author, date and section numbering make it easier to refer to a \n&gt; document and its sections.\nAgree.\n\n&gt; + Later we might use an ldap server implementation behind your \n&gt; &#39;Configuration&#39; reader class to store config.  The hierarchical nature \n&gt; of the configuration w/ a rare write would make it a natural match.\nThat makes sense. You could even keep it in an RDBMS, but I think the\ndefault implementation shouldn&#39;t need external servers to be set up, so\nthe concept of a hierarchy of XML files should probably be the default\nimplementation.\n\n&gt; \n&gt; Other comments:\n&gt; \n&gt; + Is CrawlServer class to do w/ crawl accessing a particular server?  \n&gt; Maybe rename it Server?\n&gt; + I have comments on the xml -- no schema nor dtd and extensive use of \n&gt; element attributes -- but they can go elsewhere.\n&gt; \n&gt; St.Ack\n&gt; \n&gt; \n&gt; \n&gt; To unsubscribe from this group, send an email to:\n&gt; archive-crawler-unsubscribe@yahoogroups.com\n&gt; \n&gt;  \n&gt; \n&gt; Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n&gt; \n&gt; \n\n\n"}}