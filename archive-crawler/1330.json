{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"2viKAzbxWSjZxfbloGG6PQBxJC96AkDbqCfVrixkxyERtgnQj8w4f1LSANOqXeQol0aLuIh18tqleFYXCW_lvg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Links Structure","postDate":"1105063006","msgId":1330,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxRERFQzVFLjQwNzA1MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIwMDUwMTA3MDAyNzI3Ljk0MTMyLnFtYWlsQHdlYjYwNTA0Lm1haWwueWFob28uY29tPg==","referencesHeader":"PDIwMDUwMTA3MDAyNzI3Ljk0MTMyLnFtYWlsQHdlYjYwNTA0Lm1haWwueWFob28uY29tPg=="},"prevInTopic":1329,"nextInTopic":1335,"prevInTime":1329,"nextInTime":1331,"topicId":1319,"numMessagesInTopic":9,"msgSnippet":"Very sweet.  Thanks for the below. St.Ack","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 31720 invoked from network); 7 Jan 2005 02:04:26 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m11.grp.scd.yahoo.com with QMQP; 7 Jan 2005 02:04:26 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta1.grp.scd.yahoo.com with SMTP; 7 Jan 2005 02:04:26 -0000\r\nReceived: (qmail 24844 invoked by uid 100); 7 Jan 2005 01:49:25 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 7 Jan 2005 01:49:25 -0000\r\nMessage-ID: &lt;41DDEC5E.4070502@...&gt;\r\nDate: Thu, 06 Jan 2005 17:56:46 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.3) Gecko/20041007 Debian/1.7.3-5\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;20050107002727.94132.qmail@...&gt;\r\nIn-Reply-To: &lt;20050107002727.94132.qmail@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.8 required=6.5 tests=AWL autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Links Structure\r\nX-Yahoo-Group-Post: member; u=168599281\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nVery sweet.  Thanks for the below.\nSt.Ack\n\n\nNiti Witthayawiroj wrote:\n\n&gt; Hi,\n&gt;  \n&gt; Thank so much for your suggestion. Now i can get the informations of \n&gt; link structure from Heritrix. My supervisor(Chistian) help me to write \n&gt; a new java class and add it to the org.archive.crawler.writer package. \n&gt; It will write information of link pages to heritrix_out.log file.\n&gt;  \n&gt; The code of the new class:\n&gt;  \n&gt; package org.archive.crawler.writer;\n&gt; import java.io.*;\n&gt; import java.util.*;\n&gt; import org.archive.crawler.datamodel.*;\n&gt; import org.archive.crawler.framework.Processor;\n&gt; import java.util.logging.Logger;\n&gt; import org.archive.crawler.datamodel.CoreAttributeConstants;\n&gt; import javax.management.AttributeNotFoundException;\n&gt; import javax.management.MBeanException;\n&gt; import javax.management.ReflectionException;\n&gt; import org.archive.crawler.settings.Type;\n&gt; import org.archive.crawler.settings.SimpleType;\n&gt; import org.apache.commons.httpclient.URIException;\n&gt; public class LinkStructureProcessor extends Processor\n&gt; {\n&gt;     /**\n&gt;      * Key to use asking settings for arc path value.\n&gt;      */\n&gt;     public static final String ATTR_PATH =&quot;path&quot;;\n&gt;     /**\n&gt;      * Logger.\n&gt;      */\n&gt;     private static final Logger logger =\n&gt;         Logger.getLogger(LinkStructureProcessor.class.getName());\n&gt;     /**\n&gt;      * Where to drop link files.\n&gt;      */\n&gt;     private File outputDir;\n&gt;  private PrintWriter writer = null;\n&gt;  public LinkStructureProcessor(String name) {\n&gt;         super(name, &quot;LinkStructure processor&quot;);\n&gt;         Type e = addElementToDefinition(\n&gt;             new SimpleType(ATTR_PATH, &quot;Where to store link files&quot;, \n&gt; &quot;links&quot;));\n&gt;         e.setOverrideable(false);\n&gt;     }\n&gt;  protected void readConfiguration()\n&gt;         throws AttributeNotFoundException, MBeanException, \n&gt; ReflectionException {\n&gt;         // set up output directory\n&gt;         setOutputDir((String) getAttribute(ATTR_PATH));\n&gt;     }\n&gt;     public synchronized void initialTasks() {\n&gt;         // ReadConfiguration populates settings used creating ARCWriter.\n&gt;         try {\n&gt;             readConfiguration();\n&gt;         } catch (MBeanException e) {\n&gt;             logger.warning(e.getLocalizedMessage());\n&gt;         } catch (ReflectionException e) {\n&gt;             logger.warning(e.getLocalizedMessage());\n&gt;         } catch (AttributeNotFoundException e) {\n&gt;             logger.warning(e.getLocalizedMessage());\n&gt;         }\n&gt;     }\n&gt;  public File getOutputDir() {\n&gt;         return this.outputDir;\n&gt;     }\n&gt;     public void setOutputDir(String buffer) {\n&gt;         this.outputDir = new File(buffer);\n&gt;         if (!this.outputDir.isAbsolute()) {\n&gt;             // OutputDir should be relative to &quot;disk&quot;\n&gt;             this.outputDir = new File(getController().getDisk(), buffer);\n&gt;         }\n&gt;         if (!this.outputDir.exists()) {\n&gt;             try {\n&gt;                 this.outputDir.mkdirs();\n&gt;             } catch (Exception e) {\n&gt;                 e.printStackTrace();\n&gt;             }\n&gt;         }\n&gt;     }\n&gt;    /**\n&gt;      * Takes a CrawlURI and writes its accompaining link structure to disk\n&gt;      *\n&gt;      * @param curi CrawlURI to process.\n&gt;      */\n&gt;     protected void innerProcess(CrawlURI curi) {\n&gt;         // If failure, or we haven&#39;t fetched the resource yet, return\n&gt;         if (curi.getFetchStatus() &lt;= 0) {\n&gt;             return;\n&gt;         }\n&gt;         Set links = (Set)curi.getAList().\n&gt;             getObject(CoreAttributeConstants.A_HTML_LINKS);\n&gt;   if(links != null) {\n&gt;    if(outputDir == null) {\n&gt;     setOutputDir(&quot;links&quot;);\n&gt;    }\n&gt;    if(writer == null) {\n&gt;     try\n&gt;     {\n&gt;      writer = new PrintWriter(new FileWriter(new File(outputDir, \n&gt; getName())), true);\n&gt;     }\n&gt;     catch (IOException e)\n&gt;     {\n&gt;      e.printStackTrace();\n&gt;     }\n&gt;    }\n&gt;    StringWriter sw = new StringWriter();\n&gt;    PrintWriter writer = new PrintWriter(sw);\n&gt;    writer.print(&quot;from &quot;);\n&gt;    writer.println(curi.getURIString());\n&gt;    UURI uu = curi.getUURI();\n&gt;    for (Iterator i = links.iterator(); i.hasNext();) {\n&gt;     String link = (String)i.next();\n&gt;     writer.print(&quot; to &quot;);\n&gt;     try\n&gt;     {\n&gt;      UURI uuto = uu.resolve(link);\n&gt;      writer.println(uuto);\n&gt;     }\n&gt;     catch (URIException e)\n&gt;     {\n&gt;      writer.println(link);\n&gt;     }\n&gt;    }\n&gt;    System.out.println(sw.getBuffer());\n&gt;   }\n&gt;     }\n&gt;  public void crawlEnding(String sExitMessage) {\n&gt;   // sExitMessage is unused.\n&gt;   if(writer != null) {\n&gt;    this.writer.close();\n&gt;   }\n&gt;  }\n&gt; }\n&gt;  \n&gt; we added this class to the conf/modules/processors.options and add it \n&gt; to the writers processors module when create a new crawl job.\n&gt;  \n&gt; Regards,\n&gt; Niti\n&gt;\n&gt; */Igor Ranitovic &lt;igor@...&gt;/* wrote:\n&gt;\n&gt;     Hi Niti,\n&gt;\n&gt;     Yes, the commandline will output links only one hop away.\n&gt;     Heritrix does not use PageRank therefore we don&#39;t have that info\n&gt;     as part of Heritrix reports.\n&gt;     However, crawl.log has information that you need but it will\n&gt;     require some work. Take a look at\n&gt;     hoppath.pl -- that might give you some ideas on how to approach\n&gt;     this problem.\n&gt;\n&gt;     For example:\n&gt;     $HERITRIX_HOME/bin/hoppath.pl crawl.log\n&gt;     http://www.nyrock.com/images/jr_thumb.gif\n&gt;\n&gt;     2004-11-30-23-31-36 - http://www.nyrock.com/movies/200cigs.htm\n&gt;     2004-11-30-23-31-34  P http://www.nyrock.com/robots.txt\n&gt;     2004-11-30-23-31-49   R http://www.nyrock.com/\n&gt;     2004-11-30-23-46-54    L http://www.nyrock.com/gallery/index.asp\n&gt;     2004-12-01-01-37-13     L http://www.nyrock.com/gallery2.htm\n&gt;     2004-12-01-01-37-19      E http://www.nyrock.com/images/jr_thumb.gif\n&gt;\n&gt;     Let us know if we can help more.\n&gt;\n&gt;     Take care,\n&gt;     i.\n&gt;\n&gt;     P.S. It is great that you are comparing Heritrix against other\n&gt;     crawlers. We will like to learn more\n&gt;     about you findings.\n&gt;\n&gt;\n&gt;     &gt; Thank you so much for your answer. I used the\n&gt;     &gt; commandline that you give me, but i think the output\n&gt;     &gt; from commandline are not correct with my requirement.\n&gt;     &gt; I would like to get link structure of the crawled URIs\n&gt;     &gt; to use for &quot;link analysis&quot;. I need this informations\n&gt;     &gt; because i would like to compare with other crawlers.\n&gt;     &gt;\n&gt;     &gt; For example with nutch carwler(http://www.nutch.org/).\n&gt;     &lt;http://www.nutch.org/%29.&gt;\n&gt;     &gt; I can get informations about links from\n&gt;     &gt; nutch(net.nutch.db.WebDBReader).\n&gt;     &gt; The informations about crawled pages are:\n&gt;     &gt;\n&gt;     &gt; Page 1: Version: 4\n&gt;     &gt; URL:\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/jena/com/hp/hpl/jena/vocabulary/RDFS.Nodes.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/jena/com/hp/hpl/jena/vocabulary/RDFS.Nodes.html&gt;\n&gt;     &gt; ID: 000964db178cd1c7af9b20f21c62e612\n&gt;     &gt; Next fetch: Mon Dec 27 05:24:50 CET 2004\n&gt;     &gt; Retries since fetch: 0\n&gt;     &gt; Retry interval: 30 days\n&gt;     &gt; Num outlinks: 14\n&gt;     &gt; Score: 1.0\n&gt;     &gt; NextScore: 1.0\n&gt;     &gt;\n&gt;     &gt; Page 2: Version: 4\n&gt;     &gt; URL:\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/j2sdk1.4.2/java/rmi/Naming.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/j2sdk1.4.2/java/rmi/Naming.html&gt;\n&gt;     &gt; ID: 000ac1fa6dac5cfb0c7f1b682c40ea82\n&gt;     &gt; Next fetch: Mon Dec 27 05:24:50 CET 2004\n&gt;     &gt; Retries since fetch: 0\n&gt;     &gt; Retry interval: 30 days\n&gt;     &gt; Num outlinks: 22\n&gt;     &gt; Score: 1.0\n&gt;     &gt; NextScore: 1.0\n&gt;     &gt;\n&gt;     &gt; Page 3: Version: 4\n&gt;     &gt; URL:\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/j2sdk1.4.2/javax/swing/text/Utilities.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/j2sdk1.4.2/javax/swing/text/Utilities.html&gt;\n&gt;     &gt; ID: 0012c54788ae24548b6671bf6479eaf3\n&gt;     &gt; Next fetch: Mon Dec 27 05:24:50 CET 2004\n&gt;     &gt; Retries since fetch: 0\n&gt;     &gt; Retry interval: 30 days\n&gt;     &gt; Num outlinks: 20\n&gt;     &gt; Score: 1.0\n&gt;     &gt; NextScore: 1.0\n&gt;     &gt; ....\n&gt;     &gt; and the information about link structure:\n&gt;     &gt;\n&gt;     &gt; from\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/jena/com/hp/hpl/jena/vocabulary/RDFS.Nodes.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/jena/com/hp/hpl/jena/vocabulary/RDFS.Nodes.html&gt;\n&gt;     &gt;  to\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/j2sdk1.4.2/java/lang/Object.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/j2sdk1.4.2/java/lang/Object.html&gt;\n&gt;     &gt;  to ... #show all 14 outlinks\n&gt;     &gt;\n&gt;     &gt; from\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/j2sdk1.4.2/java/rmi/Naming.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/j2sdk1.4.2/java/rmi/Naming.html&gt;\n&gt;     &gt;  to\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/j2sdk1.4.2/allclasses-noframe.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/j2sdk1.4.2/allclasses-noframe.html&gt;\n&gt;     &gt;  to ... #show all 22 outlinks\n&gt;     &gt;\n&gt;     &gt; from\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/j2sdk1.4.2/javax/swing/text/Utilities.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/j2sdk1.4.2/javax/swing/text/Utilities.html&gt;\n&gt;     &gt;  to\n&gt;     &gt;\n&gt;     http://www.learninglab.de/~brunkhor/javadoc/j2sdk1.4.2/allclasses-noframe.html\n&gt;     &lt;http://www.learninglab.de/%7Ebrunkhor/javadoc/j2sdk1.4.2/allclasses-noframe.html&gt;\n&gt;     &gt;  to ... #show all 20 outlinks\n&gt;     &gt; ....\n&gt;     &gt;\n&gt;     &gt; I need the information like that for analyzing\n&gt;     &gt; links.Can you suggest me how i can get some\n&gt;     &gt; information like that from Heritrix(maybe from\n&gt;     &gt; crawl.log or other way).\n&gt;     &gt;\n&gt;     &gt; Regards and Thank you so much,\n&gt;     &gt; Niti\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt; --- Igor Ranitovic &lt;igor@...&gt; wrote:\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;&gt; From crawl.log you can get this information with a\n&gt;     &gt;&gt;bit of perl and unix commandline.\n&gt;     &gt;&gt;For example:\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;cat crawl.log | tr -s &quot; &quot; | cut -f4,6 -d &quot; &quot; | sort\n&gt;     &gt;&gt;-k2,2 | perl -ane &#39;BEGIN{$uri=&quot;&quot;;}; if (!$uri) {\n&gt;     &gt;&gt;print &quot;from: $F[1]&#92;n to: $F[0]&#92;n&quot;;}; if ($uri &&\n&gt;     &gt;&gt;$uri !~ $F[1]){print &quot;from: $F[1]&#92;n to: $F[0]&#92;n&quot;}\n&gt;     &gt;&gt;else {print &quot; to $F[0]&#92;n&quot;;}; $uri =$F[1];&#39;\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;Lets us know if this is what you are looking for.\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;i.\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;Niti Witthayawiroj wrote:\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;Hallo All,\n&gt;     &gt;&gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;I used Heritrix to crawl some hosts and my crawl\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;job\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;is finished. I would like to get the links\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;structure\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;of pages that are carwled. Can you tell me about\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;the\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;format of the links structure and How i can get\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;it.\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;For example: the format of links structure(or\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;other\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;format, may be links with pageId)\n&gt;     &gt;&gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;from http://a.com/a.html\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;to http://a.com/b.html\n&gt;     &gt;&gt;&gt;&gt;to http://a.com/c.html\n&gt;     &gt;&gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;from http://a.com/b.html\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;to http://a.com/a.html\n&gt;     &gt;&gt;&gt;&gt;to http://a.com/c.html\n&gt;     &gt;&gt;&gt;&gt;...\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;We don&#39;t have publically-available tools currently\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;that will allow you\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;to extract document links post crawl.  We have to\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;add them. We currently\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;use proprietary tools here at the Archive for\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;doing this but intend to\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;move off them with time.\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;We have various ideas for how to implement the\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;link extraction tools --\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;usually based off some rerunning of the Heritrix\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;extractors against the\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;downloaded ARCs -- but we just haven&#39;t gotten\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;around to it yet.  There&#39;s\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;also a feature request to drop links found in a\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;page as we crawl.\n&gt;     &gt;&gt;\n&gt;     &gt;&gt;&gt;The crawl.log will list how a page was discovered.\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;St.Ack\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;Thank so much\n&gt;     &gt;&gt;&gt;&gt;Niti\n&gt;     &gt;&gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;\n&gt;     &gt;&gt;&gt;&gt;         \n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; Do you Yahoo!?\n&gt; Yahoo! Mail \n&gt; &lt;http://us.rd.yahoo.com/mail_us/taglines/virus/*http://promotions.yahoo.com/new_mail/static/protection.html&gt; \n&gt; - Helps protect you from nasty viruses.\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}