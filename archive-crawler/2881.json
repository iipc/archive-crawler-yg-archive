{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"Ikajttv4-UyKByF03E_gDMsHpy_K2Kb-isht9dWFf2y0wO6baBci-28uahy42N25JWvu6tv4gIi-2vX3L8gLomUMgh_MOlG4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Carrying the already-seen list between crawls","postDate":"1148688922","msgId":2881,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0Nzc5QTFBLjkwMDA5MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGI4MTI1OWEwMDYwNTI0MTUzNnRmMGRiMDkxc2VlYzEwNWU4MDRkMzczMjdAbWFpbC5nbWFpbC5jb20+","referencesHeader":"PGI4MTI1OWEwMDYwNDE4MTgwN2w3NThiNmJiM3NiZWJhMWNkNWQ2MjhlMDFiQG1haWwuZ21haWwuY29tPgkgPDQ0NDVEOENBLjMwMjA1MDdAYXJjaGl2ZS5vcmc+CSA8YjgxMjU5YTAwNjA1MTkxMDA4dTIyOWNkN2Zka2ZmMjgyZmZmZmY3ZmVjMTVAbWFpbC5nbWFpbC5jb20+CSA8NDQ2RTA3QjYuODA1MDgwM0BhcmNoaXZlLm9yZz4gPGI4MTI1OWEwMDYwNTI0MTUzNnRmMGRiMDkxc2VlYzEwNWU4MDRkMzczMjdAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":2880,"nextInTopic":3109,"prevInTime":2880,"nextInTime":2882,"topicId":2791,"numMessagesInTopic":7,"msgSnippet":"... Tell me more about fails.  I m interested.  They seem to work pretty reliably for us. ... Does this happen often for you? ... I d have thought that users","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 68649 invoked from network); 27 May 2006 00:14:49 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m31.grp.scd.yahoo.com with QMQP; 27 May 2006 00:14:49 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta2.grp.scd.yahoo.com with SMTP; 27 May 2006 00:14:45 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k4QN0sw07342;\n\tFri, 26 May 2006 16:00:54 -0700\r\nMessage-ID: &lt;44779A1A.9000907@...&gt;\r\nDate: Fri, 26 May 2006 17:15:22 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8.0.1) Gecko/20060127 SeaMonkey/1.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;b81259a00604181807l758b6bb3sbeba1cd5d628e01b@...&gt;\t &lt;4445D8CA.3020507@...&gt;\t &lt;b81259a00605191008u229cd7fdkff282fffff7fec15@...&gt;\t &lt;446E07B6.8050803@...&gt; &lt;b81259a00605241536tf0db091seec105e804d37327@...&gt;\r\nIn-Reply-To: &lt;b81259a00605241536tf0db091seec105e804d37327@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Carrying the already-seen list between crawls\r\nX-Yahoo-Group-Post: member; u=168599281; y=P15cwlx1j10ZKIRCKdm5gEc-ARto4b7nLfQuj1pJR3x1pcclrcuLl0kz\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nGreg Kempe wrote:\n&gt; On 19/05/06, Michael Stack &lt;stack@...&gt; wrote:\n&gt; &gt; Greg Kempe wrote:\n&gt; &gt; &gt; ...\n&gt; &gt; &gt; On a related note, how possible is it for a BdbFrontier to recover\n&gt; &gt; &gt; using only the bdb environment? That is, how important is the\n&gt; &gt; &gt; org.archive.crawler.frontier.BdbFrontier.serialized file created\n&gt; &gt; &gt; during a checkpoint (or any of the other .serialized files, for that\n&gt; &gt; &gt; matter)? If the crawler could resume using only the bdb environment,\n&gt; &gt; &gt; explicit checkpoints could be optional. In such a case things like\n&gt; &gt; &gt; statistics may be lost, but at least the already-seen list and\n&gt; &gt; &gt; frontier would be recoverable. The alternative -- using recover.gz --\n&gt; &gt; &gt; loses the same data but is more cumbersome to resume from.\n&gt; &gt; &gt;\n&gt; &gt; Do you think such an option would be of general use?  Running\n&gt; &gt; checkpointing costs little if you use the &#39;Fast checkpointing&#39; mode.  At\n&gt; &gt; least that is our impression.  Is yours different?  Run it often.  With\n&gt; &gt; checkpointing you can be &#39;certain&#39; you&#39;re resuming from a wholesome \n&gt; state.\n&gt;\n&gt; That&#39;s a fair point. My primary usecase is to recover from unclean\n&gt; shutdowns when a checkpoint either failed, \nTell me more about fails.  I&#39;m interested.  They seem to work pretty \nreliably for us.\n\n&gt; or isn&#39;t possible because\n&gt; heritrix is stuck doing other things.\nDoes this happen often for you?\n\n&gt;\n&gt; For instance, I found that checkpoints fail mid-way if the crawl is a\n&gt; recovery and the recovery thread is still reading frontier entries.\n&gt; The recovery frontier reader modifies one of the maps and a concurrent\n&gt; modification exception results (see log snippet below).  Since the\n&gt; crawler starts after it has a small part of the froniter, the\n&gt; situation isn&#39;t that much of an edge case.\n\nI&#39;d have thought that users would pick either recover log or \ncheckpointing.  Running with both is going to make for interesting  \nsituations like the one you cite below where we&#39;re getting a CME because \nwe&#39;re checkpointing a crawler that hasn&#39;t really started up crawling.\n\n\n&gt;\n&gt; 05/23/2006 23:18:26 +0000 FINE\n&gt; org.archive.crawler.framework.CrawlController sendCheckpointEvent Sent\n&gt; CHECKPOINTING\n&gt; 05/23/2006 23:18:26 +0000 FINE\n&gt; org.archive.crawler.framework.CrawlController checkpoint Rotating log\n&gt; files.\n&gt; 05/23/2006 23:18:26 +0000 FINE\n&gt; org.archive.crawler.framework.CrawlController checkpoint BigMaps.\n&gt; java.util.ConcurrentModificationException\n&gt;         at java.util.HashMap$HashIterator.nextEntry(HashMap.java:787)\n&gt;         at java.util.HashMap$KeyIterator.next(HashMap.java:823)\n&gt;         at org.archive.util.CachedBdbMap.sync(CachedBdbMap.java:496)\n&gt;         at \n&gt; org.archive.crawler.framework.CrawlController.checkpointBigMaps(CrawlController.java:1966)\n&gt;         at \n&gt; org.archive.crawler.framework.CrawlController.checkpoint(CrawlController.java:1181)\n&gt;         at \n&gt; org.archive.crawler.framework.Checkpointer$CheckpointingThread.run(Checkpointer.java:194)\n&gt; 05/23/2006 23:18:50 +0000 INFO\n&gt; org.archive.crawler.framework.Checkpointer$CheckpointingThread run\n&gt; Finished\n&gt;\n&gt; &gt; One idea we&#39;ve kicked around is having the recover.gz log start over at\n&gt; &gt; each checkpoint.  After a checkpoint recovery, optionally you could\n&gt; &gt; replay the recovery.gz to bring the crawler even closer to the crash \n&gt; point.\n&gt;\n&gt; That would be useful. If the jmx importUris method used the same logic\n&gt; to read a recover log as the frontier recovery code does, you could\n&gt; use it to add both frontier uris and already-seen uris.\nIt does already (If I understand you properly).  You can pass recovery \nlog files to importUris (or crawl logs or just a URI per line).  You \njust have to be sure to specify the correct &#39;style&#39;.  See the javadoc: \nhttp://crawler.archive.org/apidocs/index.html.\n\nSt.Ack\n\n"}}