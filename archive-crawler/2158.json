{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"YF8mQshidspGPjGn-dRPXVQv8jrPiFiggOv8WTBCRpHUfR6WH1_JI1g3Q-o8EpUBu2ERmMyQ6kND3ruVlkAq_Q","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Best approach to 7M seeds","postDate":"1125704646","msgId":2158,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzMThFM0M2LjkwODAzMDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDlkMWU0NTI1MDUwOTAyMTYxNjMxOWEzQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PDlkMWU0NTI1MDUwODE3MTAyMzQ4Y2Q2MmE4QG1haWwuZ21haWwuY29tPgkgPDQzMDM4NTI3LjgwMjAxMDFAYXJjaGl2ZS5vcmc+CSA8OWQxZTQ1MjUwNTA4MjIxMzQ3NzI2Y2RjZGRAbWFpbC5nbWFpbC5jb20+CSA8OWQxZTQ1MjUwNTA5MDIxMjU4NDlmZDM3M2FAbWFpbC5nbWFpbC5jb20+CSA8MTcxNzYuNDUxNzUuOTI5MzA3LjkzODkxN0B0aXBoYXJlcy5iYXNpc3RlY2gubmV0PgkgPDlkMWU0NTI1MDUwOTAyMTMyMjU2NzgzNGI3QG1haWwuZ21haWwuY29tPgkgPDQzMThDMTBBLjEwMzA1MDVAYXJjaGl2ZS5vcmc+CSA8OWQxZTQ1MjUwNTA5MDIxNTA0NzU2YjRkNDVAbWFpbC5nbWFpbC5jb20+CSA8NDMxOEQ1NjAuMTAyMDYwNUBhcmNoaXZlLm9yZz4gPDlkMWU0NTI1MDUwOTAyMTYxNjMxOWEzQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2157,"nextInTopic":2159,"prevInTime":2157,"nextInTime":2159,"topicId":2116,"numMessagesInTopic":25,"msgSnippet":"... It d be empty if no extractors.  You need them. ... I d doubt the running of the extractors your memory problem.  The extractors have upper-bounds on the","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 37104 invoked from network); 2 Sep 2005 23:53:12 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m25.grp.scd.yahoo.com with QMQP; 2 Sep 2005 23:53:12 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta6.grp.scd.yahoo.com with SMTP; 2 Sep 2005 23:53:12 -0000\r\nReceived: (qmail 17681 invoked by uid 100); 2 Sep 2005 23:53:01 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.8?) (stack@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 2 Sep 2005 23:53:01 -0000\r\nMessage-ID: &lt;4318E3C6.9080309@...&gt;\r\nDate: Fri, 02 Sep 2005 16:44:06 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.8) Gecko/20050513 Debian/1.7.8-1\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;9d1e4525050817102348cd62a8@...&gt;\t &lt;43038527.8020101@...&gt;\t &lt;9d1e45250508221347726cdcdd@...&gt;\t &lt;9d1e4525050902125849fd373a@...&gt;\t &lt;17176.45175.929307.938917@...&gt;\t &lt;9d1e45250509021322567834b7@...&gt;\t &lt;4318C10A.1030505@...&gt;\t &lt;9d1e45250509021504756b4d45@...&gt;\t &lt;4318D560.1020605@...&gt; &lt;9d1e45250509021616319a3@...&gt;\r\nIn-Reply-To: &lt;9d1e45250509021616319a3@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-91.2 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Best approach to 7M seeds\r\nX-Yahoo-Group-Post: member; u=168599281; y=wejlqlmwJi55Jve5RzEVeu0vVDeCTIxgJx7tJ3gK0rbRxJ07UeWqopPT\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nMatt Ittigson wrote:\n\n&gt; On 9/2/05, Igor Ranitovic &lt;igor@...&gt; wrote:\n&gt; &gt; Hi Matt,\n&gt; &gt;\n&gt; &gt; Stack just brought up that you need to turn off the extractors.\n&gt;\n&gt; Part of our application is to grab the links found in each URL and\n&gt; dump them out into a separate file.  Without the extractors, would\n&gt; curi.getOutLinks be empty without the extractors?\n\nIt&#39;d be empty if no extractors.  You need them.\n\n&gt;\n&gt; &gt; If you want embeds (images, frames, etc.) than leave extractors on, \n&gt; setup max-hops to 1 and enable\n&gt; &gt; additionalScopeFocus and set max-embed-hops.\n&gt;\n&gt; Would there be a huge difference in possible memory usage with or\n&gt; without the extractors (I might need them anyways, from above)?\n&gt;\nI&#39;d doubt the running of the extractors your memory problem.  The \nextractors have upper-bounds on the number of links they&#39;ll pull from a \npage to guard against running out of memory processing whacky pages of \ntens of thousands of links.  There could be an issue in one of the \nlibraries we&#39;re using but these are usually easy to spot: The OOME \noriginates in the lib. and recrawling the single suspect page causes a \nspike or an OOME.\n\n&gt; &gt; Also, if you have 7M URLs from ~7M different hosts that would \n&gt; require a lot of memory since for each\n&gt; &gt; host we need to create a separate queue. Therefore, we suggest that \n&gt; you change default\n&gt; &gt; QueueAssignmentPolicy to something like this\n&gt; &gt; \n&gt; http://crawler.archive.org/apidocs/org/archive/crawler/frontier/BucketQueueAssignmentPolicy.html\n&gt; &gt;\n&gt; &gt; Let us know if this is working out for you.\n&gt;\nI was going to suggest you try removing FrontierScheduler from the \npostprocessing chain so extracted links do not get scheduled but this \nwill block scheduling of robots and dns so that won&#39;t work.\n\nChatting w/ Igor, try a DecidingScope that by default accepts everything \nas first rule.  Second rule would be something like \n&#39;rejectIfTooManyHops&#39; (set to &#39;1&#39;) or use the &#39;. \nHopsPathMatchesRegExpDecideRule&#39;.  Final rule should be \nPrerequisiteAcceptDecideRule.\n\nIf you set in heritrix.properties the following:\n\norg.archive.crawler.deciderules.DecideRuleSequence.level=FINE\n\n... you can watch the URLs as they go through the scope.  Will help you \ndebug.\n\n(You&#39;ll also likely need the aforementioned alternate queue assignment \npolicy).\nGood luck,\nSt.Ack\n\n&gt; Thanks.  I should have some preliminary results for you soon.  I\n&gt; appreciate all the help from everyone.\n&gt;\n&gt; -matt\n&gt;\n&gt; &gt; &gt;&gt;Hi Matt,\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;If you are planning to fetch only 7M URLs and not continue with \n&gt; traversing newly discover urls,\n&gt; &gt; &gt;&gt;you can use broad scope and import URIs as non seeds directly to \n&gt; frontier.\n&gt; &gt; &gt;&gt;Would that work?\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;ll try that first and get back to everyone.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Thanks for the suggestion.\n&gt; &gt; &gt;\n&gt; &gt; &gt; -matt\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;&gt;&gt;On 9/2/05, Tom Emerson &lt;Tree@...&gt; wrote:\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt;If you are interested in only getting those 7M seeds wouldn&#39;t it be\n&gt; &gt; &gt;&gt;&gt;&gt;easier to just write a script in Perl to spin over them and \n&gt; fetch the\n&gt; &gt; &gt;&gt;&gt;&gt;content?\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;Good question.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;I really like the Heritrix GUI.  The JMX integration is a great \n&gt; way to\n&gt; &gt; &gt;&gt;&gt;track the progress of the spider from other processes.  The coming\n&gt; &gt; &gt;&gt;&gt;clustered spidering is a way to go from, say, 7M seeds to 70M seeds.\n&gt; &gt; &gt;&gt;&gt;We use the ARC files on the backend and have written a module that\n&gt; &gt; &gt;&gt;&gt;pulls links out of Heritrix in the way we need &#39;em.  And we&#39;ve used\n&gt; &gt; &gt;&gt;&gt;Heritrix to get to the point that we had 7M seeds.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;Not that all of those things couldn&#39;t be replicated in a simpler\n&gt; &gt; &gt;&gt;&gt;system than Heritrix ... and perhaps should be if our biggest problem\n&gt; &gt; &gt;&gt;&gt;is figuring out a way to spider 7M seeds.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;-matt\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;Yahoo! Groups Links\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;Yahoo! Groups Links\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}