{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":420868483,"authorName":"Vitalii Tymchyshyn","from":"Vitalii Tymchyshyn &lt;tivv00@...&gt;","replyTo":"LIST","senderId":"VDG0lVQaWYh97fcGQhRlWN1mNxRGBDPkeOAXAvvYN_VicZFsHk0nnU84oSmGLZZjxK5-vZFsfFgNqizcHyapNcQdlQ6WF3mf02m64w","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Freeze after Preparing Seeds","postDate":"1291801813","msgId":6875,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDRkY1NEQ1LjgwODA1MDRAZ21haWwuY29tPg==","inReplyToHeader":"PDkwMDExMEQxRDEyNTQxMENCMDA1OEM3M0Q5MzY5RkIxQGRhdGFjbGlwLmNvbT4=","referencesHeader":"PDkwMDExMEQxRDEyNTQxMENCMDA1OEM3M0Q5MzY5RkIxQGRhdGFjbGlwLmNvbT4="},"prevInTopic":6872,"nextInTopic":0,"prevInTime":6874,"nextInTime":6876,"topicId":6869,"numMessagesInTopic":3,"msgSnippet":"... The thing that works well for me is to lower BDB cache size. By default AFAIR it is ~70 of the heap, but for large crawl I prefer 30%. Best regards,","rawEmail":"Return-Path: &lt;tivv00@...&gt;\r\nX-Sender: tivv00@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 21793 invoked from network); 8 Dec 2010 09:50:01 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m13.grp.re1.yahoo.com with QMQP; 8 Dec 2010 09:50:01 -0000\r\nX-Received: from unknown (HELO mail-bw0-f50.google.com) (209.85.214.50)\n  by mta1.grp.re1.yahoo.com with SMTP; 8 Dec 2010 09:50:01 -0000\r\nX-Received: by bwg12 with SMTP id 12so1157231bwg.23\n        for &lt;archive-crawler@yahoogroups.com&gt;; Wed, 08 Dec 2010 01:50:00 -0800 (PST)\r\nX-Received: by 10.204.79.204 with SMTP id q12mr1548126bkk.157.1291801800032;\n        Wed, 08 Dec 2010 01:50:00 -0800 (PST)\r\nReturn-Path: &lt;tivv00@...&gt;\r\nX-Received: from [10.1.1.57] (skuns.zoral.com.ua [91.193.166.194])\n        by mx.google.com with ESMTPS id a17sm204051bku.11.2010.12.08.01.49.58\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Wed, 08 Dec 2010 01:49:58 -0800 (PST)\r\nMessage-ID: &lt;4CFF54D5.8080504@...&gt;\r\nDate: Wed, 08 Dec 2010 11:50:13 +0200\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.1.15) Gecko/20101027 Thunderbird/3.0.10\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Zach Bailey &lt;zach.bailey@...&gt;\r\nReferences: &lt;900110D1D125410CB0058C73D9369FB1@...&gt;\r\nIn-Reply-To: &lt;900110D1D125410CB0058C73D9369FB1@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------070001000109070405020306&quot;\r\nFrom: Vitalii Tymchyshyn &lt;tivv00@...&gt;\r\nSubject: Re: [archive-crawler] Freeze after Preparing Seeds\r\nX-Yahoo-Group-Post: member; u=420868483\r\n\r\n\r\n--------------070001000109070405020306\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\n\r\n07.12.10 23:22, Zach Bailey написав(ла):\n&gt;\n&gt; After attempting to load a seed file with 1 million seeds Heritrix \n&gt; becomes completely unresponsive and looking at the looks I get an \n&gt; error message about &quot;GC overhead limit exceeded&quot;.\n&gt;\n&gt; I was under the impression that the purpose of the BDB frontier was to \n&gt; &quot;page out&quot; data from the JVM heap when encountering GC pressure.\n&gt;\n&gt; I am using fairly vanilla crawl settings aside from including the \n&gt; BloomUriUniqFilter (which shouldn&#39;t even be holding any data at this \n&gt; point, since this is before unpausing the crawl to start it). JVM heap \n&gt; is set to 5 gig.\n&gt;\nThe thing that works well for me is to lower BDB cache size. By default \nAFAIR it is ~70 of the heap, but for large crawl I prefer 30%.\n\nBest regards, Vitalii Tymchyshyn\n\r\n--------------070001000109070405020306\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: 8bit\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta content=&quot;text/html; charset=UTF-8&quot; http-equiv=&quot;Content-Type&quot;&gt;\n&lt;/head&gt;\n&lt;body bgcolor=&quot;#ffffff&quot; text=&quot;#000000&quot;&gt;\n07.12.10 23:22, Zach Bailey написав(ла):\n&lt;blockquote cite=&quot;mid:900110D1D125410CB0058C73D9369FB1@...&quot;\n type=&quot;cite&quot;&gt;&lt;span style=&quot;display: none;&quot;&gt; &lt;/span&gt;\n\n  &lt;div id=&quot;ygrp-text&quot;&gt;\n  &lt;p&gt; &lt;/p&gt;\n  &lt;div&gt; After attempting to load a seed file with 1 million seeds\nHeritrix becomes completely unresponsive and looking at the looks I get\nan error message about &quot;GC overhead limit exceeded&quot;.&lt;span\n id=&quot;goog_1368997585&quot;&gt;&lt;/span&gt;&lt;span id=&quot;goog_1368997586&quot;&gt;&lt;/span&gt;&lt;/div&gt;\n  &lt;div&gt;&lt;span id=&quot;goog_1368997587&quot;&gt;&lt;/span&gt;&lt;span id=&quot;goog_1368997588&quot;&gt;&lt;/span&gt;&lt;br&gt;\n  &lt;/div&gt;\n  &lt;div&gt;I was under the impression that the purpose of the BDB frontier\nwas to &quot;page out&quot; data from the JVM heap when encountering GC pressure.&lt;span\n id=&quot;goog_1368997589&quot;&gt;&lt;/span&gt;&lt;span id=&quot;goog_1368997590&quot;&gt;&lt;/span&gt;&lt;/div&gt;\n  &lt;div&gt;&lt;span id=&quot;goog_1368997591&quot;&gt;&lt;/span&gt;&lt;span id=&quot;goog_1368997592&quot;&gt;&lt;/span&gt;&lt;br&gt;\n  &lt;/div&gt;\n  &lt;div&gt;I am using fairly vanilla crawl settings aside from including\nthe BloomUriUniqFilter (which shouldn&#39;t even be holding any data at\nthis point, since this is before unpausing the crawl to start it). JVM\nheap is set to 5 gig. &lt;/div&gt;\n  &lt;br&gt;\n  &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/blockquote&gt;\nThe thing that works well for me is to lower BDB cache size. By default\nAFAIR it is ~70 of the heap, but for large crawl I prefer 30%.&lt;br&gt;\n&lt;br&gt;\nBest regards, Vitalii Tymchyshyn&lt;br&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------070001000109070405020306--\r\n\n"}}