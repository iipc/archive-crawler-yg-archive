{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"F9kpnZ1Mu7XxjMCye03q5BJRnb7CGSdBcKTUANQdPihNYtHgOWWCi9P0P_MEFkdWIoKW5AC_ngHBOkJHpgs7Jwm_jHGXhas","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] &quot;Ignore&quot; as &quot;robots-honoring-policy&quot; , yet &quot;robots.txt&quot; downloaded","postDate":"1149705674","msgId":2926,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0ODcxRENBLjQwMzA0MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGU2NzU4ZytsN2NrQGVHcm91cHMuY29tPg==","referencesHeader":"PGU2NzU4ZytsN2NrQGVHcm91cHMuY29tPg=="},"prevInTopic":2925,"nextInTopic":0,"prevInTime":2925,"nextInTime":2927,"topicId":2925,"numMessagesInTopic":2,"msgSnippet":"... Yes; you should see fetches of URLs that would otherwise be precluded in your logs. ... The code which insists on robots.txt being fetched before any other","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 77075 invoked from network); 7 Jun 2006 18:43:08 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m35.grp.scd.yahoo.com with QMQP; 7 Jun 2006 18:43:08 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta1.grp.scd.yahoo.com with SMTP; 7 Jun 2006 18:43:08 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id A8DC31415623C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  7 Jun 2006 11:40:46 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 25272-10-83 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 7 Jun 2006 11:40:46 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 54B0F1415622C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  7 Jun 2006 11:40:46 -0700 (PDT)\r\nMessage-ID: &lt;44871DCA.4030408@...&gt;\r\nDate: Wed, 07 Jun 2006 11:41:14 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e6758g+l7ck@...&gt;\r\nIn-Reply-To: &lt;e6758g+l7ck@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] &quot;Ignore&quot; as &quot;robots-honoring-policy&quot; , yet\n &quot;robots.txt&quot; downloaded\r\nX-Yahoo-Group-Post: member; u=137285340; y=0vJVURAay_gD33AL5tNuwOkKzQhjQkVD1MiZ5rswyDa6\r\nX-Yahoo-Profile: gojomo\r\n\r\njcr2102 wrote:\n&gt; I have set my &quot;robots-honoring-policy&quot; to &quot;Ignore&quot;. However, in the\n&gt; crawl log, it still appears that robots.txt files are being downloaded\n&gt; (despite the lack of any explicit link to them on the pages listed as\n&gt; referers). \n&gt; \n&gt; Is robots.txt indeed being ignored?\n\nYes; you should see fetches of URLs that would otherwise be precluded in \nyour logs.\n\n&gt; Is there a way to stop robots.txt from being downloaded in the first\n&gt; place?\n\nThe code which insists on robots.txt being fetched before any other URLs \non a host is independent from the code that interprets and applies the \nrobots rules, and there is no separate setting for disabling that attempt.\n\nFurther, if you blocked the robots.txt URL fetch attempts in another \nway, say by defining a scope that rejected those URLs, or \nprocessor-specific filters to skip fetching of such URLs, I believe the \nother non-robots URLs would fail too, as not having had their \nprerequisites fetched. It would take a code patch to change this behavior.\n\nIn our uses, even for those occasional cases where we have permission to \nignore robots, we want a record of what the robots.txt said for future \nreference.\n\n- Gordon @ IA\n\n"}}