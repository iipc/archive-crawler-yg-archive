{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"paA1HL7DL_1xSm-RUFblgB93DGpy9dNZV1yXAX5ChXbtnf0DJgidC_iQ7SabfrIFG7_U8p2sGuf18PyUpUuu-jp_gsfgVNk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Freeze after Preparing Seeds","postDate":"1291763922","msgId":6872,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDRkVDMEQyLjgwOTAyMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDkwMDExMEQxRDEyNTQxMENCMDA1OEM3M0Q5MzY5RkIxQGRhdGFjbGlwLmNvbT4=","referencesHeader":"PDkwMDExMEQxRDEyNTQxMENCMDA1OEM3M0Q5MzY5RkIxQGRhdGFjbGlwLmNvbT4="},"prevInTopic":6869,"nextInTopic":6875,"prevInTime":6871,"nextInTime":6873,"topicId":6869,"numMessagesInTopic":3,"msgSnippet":"... An interesting thing about the GC overhead limit OutOfMemoryError is that it doesn t mean the JVM is actually out-of-memory, just that it s spending too","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 90605 invoked from network); 7 Dec 2010 23:18:44 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m10.grp.re1.yahoo.com with QMQP; 7 Dec 2010 23:18:44 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.re1.yahoo.com with SMTP; 7 Dec 2010 23:18:44 -0000\r\nX-Received: (qmail 23052 invoked by uid 0); 7 Dec 2010 23:18:42 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 7 Dec 2010 23:18:42 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4CFEC0D2.8090201@...&gt;\r\nDate: Tue, 07 Dec 2010 15:18:42 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.12) Gecko/20101027 Thunderbird/3.1.6\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Zach Bailey &lt;zach.bailey@...&gt;\r\nReferences: &lt;900110D1D125410CB0058C73D9369FB1@...&gt;\r\nIn-Reply-To: &lt;900110D1D125410CB0058C73D9369FB1@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Freeze after Preparing Seeds\r\nX-Yahoo-Group-Post: member; u=137285340; y=hNrKelqcCdiVse9EXyf1j_evgQCHCp1En-OLbBz5-rh_\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 12/7/10 1:22 PM, Zach Bailey wrote:\n&gt; After attempting to load a seed file with 1 million seeds Heritrix\n&gt; becomes completely unresponsive and looking at the looks I get an error\n&gt; message about &quot;GC overhead limit exceeded&quot;.\n&gt;\n&gt; I was under the impression that the purpose of the BDB frontier was to\n&gt; &quot;page out&quot; data from the JVM heap when encountering GC pressure.\n\nAn interesting thing about the &#39;GC overhead limit&#39; OutOfMemoryError is \nthat it doesn&#39;t mean the JVM is actually out-of-memory, just that it&#39;s \nspending &#39;too much&#39; time in GC (default definition of &#39;too much&#39;: over \n98%), and thinks something is wrong.\n\nWhile our goal with H3 was to allow seed lists of any size, \nunfortunately it turns out that bulk seed loads uniquely stress some our \nBDB-backed object caches, which rely on soft-references and finalization \ntricks to try to make their operation transparent.\n\nSpecifically: during this load, almost all of the heap is taken by these \nsoft-cached objects. As a result, every time a little more memory is \nneeded, just a few are collected, leaving the heap at nearly-full. \n(Assuming as is typical all your seeds are different hosts, it wouldn&#39;t \nhurt to collect many many more -- they won&#39;t be needed in memory again \nfor a while. But the soft references mean, hold until absolutely \nneeded). In a big multi-GB, mostly-full heap, normal GCs (before taking \nsoft references) take the maximum amount of time, while freeing almost \nno memory. Hence, the &#39;overhead limit&#39; gets hit.\n\nThat &#39;overhead limit&#39; can be turned off via JVM options -- then your \nseed-load would complete without the OOME, but still very slowly because \nof all the worst-case GCing happening.\n\nOnce real crawling begins, with plenty of normal hard-reference objects \nbeing created and collected, and the caches staying hot around the \nhostnames-in-progress (rather than a cycle through *all*), behavior is \nnormal (or at least, non-problematic).\n\nI&#39;m testing some changes to discard much of the soft/finalization magic \nto avoid this (and potentially other GC-stressor) problems, which should \narrive in trunk in the next day or two. But even then, you might not \nwant to jump to the cutting edge code; there are a number of other \nrecent optimizations not fully shaken out yet.\n\nSome potential workarounds short of making or waiting for a code-change:\n\n- disable the overhead limit, as mentioned above, and be very patient\n\n- if your scope is not seed-derived, so that you could reasonably feed \nthe seeds in multiple smaller batches after crawling on some sites has \nbegin, do that (via the &#39;action&#39; directory)\n\n- this *might* be one of the ironic cases where a smaller object heap \nmeans less chance of OOME. (Other examples are when a larger object heap \nmeans native memory is depleted, or when a larger object heap means \nfinalization occurs less frequently causing native buffers to linger.)\n\n- The &#39;-XX:SoftRefLRUPolicyMSPerMB=&lt;value&gt;&#39; JVM setting might help, if \nset lower than the default 1000, to bias the soft-references to be \ncleared faster.\n\n- Trying an alternate GC at JVM launch from whatever you&#39;re already \nusing might help (or hurt) the relative expression of the problem. For \nexample, the the G1GC available in the latest 1.6.0 JVMs.\n\n(I haven&#39;t tried any of those but each or a combination might help.)\n\nIf delving into code, either a custom build or via the UI&#39;s Scripting \nConsole:\n\n- You might be able to simulate a smaller heap during seed-loading by \nallocating one giant dummy array to hold most memory; this would mean \nGCs during the seed-load touch far fewer objects (go quickly) before \ndeciding to reclaim the soft-references. For example, before loading \nseeds via &#39;action&#39; directory, use scripting console to assign the dummy \narray to a global like a system property, then clear when loading is done.\n\n- Forcing &#39;System.runFinalization()&#39; or even explicit &#39;System.gc()&#39; \nrequests intermittently during seed-load might accelerate soft-reference \ncollection. (There&#39;s a commit to H3 trunk on November 5 to \nTextSeedModule that tries triggering finalization every 20K seeds to see \nif it helps.)\n\nThis issue should be gone by the next official release but some mix of \nthese may get you over the import hump in the meantime.\n\n&gt; I am using fairly vanilla crawl settings aside from including the\n&gt; BloomUriUniqFilter (which shouldn&#39;t even be holding any data at this\n&gt; point, since this is before unpausing the crawl to start it). JVM heap\n&gt; is set to 5 gig.\n\nNote that the Bloom Filter reaches its full size as soon as it is built, \nand never grows or shrinks with use. Its default size is about 500MB; \nyou could tweak it to be larger given your heap for an even lower error \nrate.\n\n(If using the default 500MB or larger Bloom in a heap of 3GB or smaller, \nyou&#39;d likely want to decrease the BdbModule &#39;cachePercentage&#39; of heap \ndedicated to BDB to something less than 60%.)\n\n- Gordon @ IA\n\n"}}