{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":189125050,"authorName":"KFranks@lbl.gov","from":"KFranks@...","profile":"juvenate","replyTo":"LIST","senderId":"j4tsfbkSbOBGx0lqQYZIPTel68A_CeI21XWdGrpajJL1WcyvTKk6ghKcWxWE8n3mcKQjWp12","spamInfo":{"isSpam":false,"reason":"12"},"subject":"4k seed urls - Hums along for a few hours then stalls","postDate":"1115190550","msgId":1781,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU3M2YyNTYxNWUuNTYxNWU1NzNmMkBsYmwuZ292Pg=="},"prevInTopic":0,"nextInTopic":1782,"prevInTime":1780,"nextInTime":1782,"topicId":1781,"numMessagesInTopic":3,"msgSnippet":"Hi, I d like to use Heritrix but I cannot seem to get around this issue: I have 100k seed urls. These have been split into 4k subsets. A Heritrix process is","rawEmail":"Return-Path: &lt;KFranks@...&gt;\r\nX-Sender: KFranks@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 54914 invoked from network); 4 May 2005 07:09:16 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m23.grp.scd.yahoo.com with QMQP; 4 May 2005 07:09:16 -0000\r\nReceived: from unknown (HELO mta1.lbl.gov) (128.3.41.24)\n  by mta2.grp.scd.yahoo.com with SMTP; 4 May 2005 07:09:16 -0000\r\nReceived: from mta1.lbl.gov (localhost [127.0.0.1])\n\tby mta1.lbl.gov (8.12.10/8.12.10) with ESMTP id j4479An3027417\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 4 May 2005 00:09:11 -0700 (PDT)\r\nReceived: from lbl.gov (imape.lbl.gov [128.3.41.49])\n\tby mta1.lbl.gov (8.12.10/8.12.10) with ESMTP id j4479AP8027414\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 4 May 2005 00:09:10 -0700 (PDT)\r\nReceived: from [24.7.86.118] by imape.lbl.gov (mshttpd); Wed, 04 May\n 2005 00:09:10 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;573f25615e.5615e573f2@...&gt;\r\nDate: Wed, 04 May 2005 00:09:10 -0700\r\nX-Mailer: iPlanet Messenger Express 5.2 HotFix 2.03 (built Jan 11 2005)\r\nMIME-Version: 1.0\r\nContent-Language: en\r\nX-Accept-Language: en\r\nPriority: normal\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Disposition: inline\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: KFranks@...\r\nSubject: 4k seed urls - Hums along for a few hours then stalls\r\nX-Yahoo-Group-Post: member; u=189125050\r\nX-Yahoo-Profile: juvenate\r\n\r\nHi,\n\nI&#39;d like to use Heritrix but I cannot seem to get around this issue:\n\n\n\n\nI have 100k seed urls. These have been split into 4k subsets. A \nHeritrix process is assigned a 4k seed url set, an ip address and \nport. For Heritrix 1.0.4 AND 1.2.0 (yes I tried 1.4.0 first - with \n1.4.0 I ran into some threading issues with jdk1.5.0) versions I had \ninitial success, however after modifying timeouts and bringing the toe \nthread count down to 5 from 100 I have not yet been able to execute an \nextended crawl beyond just a few hours??? Every Heritrix process just \nsits there, hanging.\n\nPlease review the order.xml config file I made below and send your \npointers at will as My team would like to continue to use Heritrix for \na web wide vertical biomedical search engine but I have to resolve \nthis issue quickly now as I&#39;ve spent days trying to work around it so \nfar with little success. Thank You.\n\nIn a nutshell, why is Heritrix hanging without downloading only after \na few hours given the order.xml file below?\n\n  - JVM: SUNWappserver jdk1.4\n  - RedHat 2.4\n  - Crawling is autolaunched\n  - Crawling happens before the hang, approx 30k docs downloaded.\n  - what a java thread dump (generated with SIGQUIT) shows\n  \nA cut and paste of the console:\n===\n\nAdministrator Console  \n     Status of crawler as of May. 4, 2005 05:02:57 GMT     Alerts: no \nalerts  \nCrawler is running  Current job: Auto launched  \n0 jobs pending, 0 completed    Downloaded 36896 documents in 6 h., 17 \nmin. and 31 sec.  \n \n    \n \n  Console    Jobs    Profiles    Logs    Reports    About    Help   \n \n \nCrawler statusCrawler running:   Yes  \nCurrent job:   Auto launched  \nJobs pending:   0  \nJobs completed:   0  \n Used memory:   369408 KB  \nHeap size:   474024 KB  \nMax heap size:   520256 KB  \nAlerts:  0 (0 new)  \n \n \nStatus:   Running  \nProcessed docs/sec:   0.0 (1.62)     KB/sec: 0 (16)  \nRun time:   6 h., 17 min. and 31 sec.  \n  \nActive thread count:   0 of 0  \nTotal data received:   372 MB  \n \n \nDOWNLOADED/QUEUED DOCUMENT RATIO\n 24%     (36896 of 151750) \n \n \n  \nStop crawling pending jobs | Terminate current job | Pause current job \n| Refresh  \n  \n\n  \n \nShut down Heritrix software | Logout  \n \n===\n\n\n\nThe order.xml:\n\n===\n\n\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;crawl-order \nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; \nxsi:noNamespaceSchemaLocation=&quot;heritrix_settings.xsd&quot;&gt;\n  &lt;meta&gt;\n    &lt;name&gt;jupiter-1-1242457757&lt;/name&gt;\n    &lt;description&gt;Profile: jupiter-1-1242457757&lt;/description&gt;\n    &lt;operator&gt;Admin&lt;/operator&gt;\n    &lt;organization&gt;&lt;/organization&gt;\n    &lt;audience&gt;&lt;/audience&gt;\n    &lt;date&gt;20041202152639&lt;/date&gt;\n  &lt;/meta&gt;\n  &lt;controller&gt;\n    &lt;string name=&quot;settings-directory&quot;&gt;settings&lt;/string&gt;\n    &lt;string name=&quot;disk-path&quot;&gt;&lt;/string&gt;\n    &lt;string name=&quot;logs-path&quot;&gt;logs&lt;/string&gt;\n    &lt;string name=&quot;checkpoints-path&quot;&gt;checkpoints&lt;/string&gt;\n    &lt;string name=&quot;state-path&quot;&gt;state&lt;/string&gt;\n    &lt;string name=&quot;scratch-path&quot;&gt;scratch&lt;/string&gt;\n    &lt;long name=&quot;max-bytes-download&quot;&gt;0&lt;/long&gt;\n    &lt;long name=&quot;max-document-download&quot;&gt;0&lt;/long&gt;\n    &lt;long name=&quot;max-time-sec&quot;&gt;0&lt;/long&gt;\n    &lt;integer name=&quot;max-toe-threads&quot;&gt;5&lt;/integer&gt;\n    &lt;newObject name=&quot;scope&quot; \nclass=&quot;org.archive.crawler.scope.DomainScope&quot;&gt;\n      &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n      &lt;string name=&quot;seedsfile&quot;&gt;seeds.txt&lt;/string&gt;\n      &lt;integer name=&quot;max-link-hops&quot;&gt;20&lt;/integer&gt;\n      &lt;integer name=&quot;max-trans-hops&quot;&gt;25&lt;/integer&gt;\n      &lt;newObject name=&quot;exclude-filter&quot; \nclass=&quot;org.archive.crawler.filter.OrFilter&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;boolean name=&quot;if-matches-return&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n          &lt;newObject name=&quot;pathdepth&quot; \nclass=&quot;org.archive.crawler.filter.PathDepthFilter&quot;&gt;\n            &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n            &lt;integer name=&quot;max-path-depth&quot;&gt;20&lt;/integer&gt;\n            &lt;boolean name=&quot;path-less-or-equal-return&quot;&gt;false&lt;/boolean&gt;\n          &lt;/newObject&gt;\n          &lt;newObject name=&quot;pathologicalpath&quot; \nclass=&quot;org.archive.crawler.filter.PathologicalPathFilter&quot;&gt;\n            &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n            &lt;integer name=&quot;repetitions&quot;&gt;3&lt;/integer&gt;\n          &lt;/newObject&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;additionalScopeFocus&quot; \nclass=&quot;org.archive.crawler.filter.FilePatternFilter&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;boolean name=&quot;if-match-return&quot;&gt;true&lt;/boolean&gt;\n        &lt;string name=&quot;use-default-patterns&quot;&gt;All&lt;/string&gt;\n        &lt;string name=&quot;regexp&quot;&gt;&lt;/string&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;transitiveFilter&quot; \nclass=&quot;org.archive.crawler.filter.TransclusionFilter&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;integer name=&quot;max-speculative-hops&quot;&gt;1&lt;/integer&gt;\n        &lt;integer name=&quot;max-referral-hops&quot;&gt;2147483647&lt;/integer&gt;\n        &lt;integer name=&quot;max-embed-hops&quot;&gt;2147483647&lt;/integer&gt;\n      &lt;/newObject&gt;\n    &lt;/newObject&gt;\n    &lt;map name=&quot;http-headers&quot;&gt;\n      &lt;string name=&quot;user-agent&quot;&gt;Mozilla/5.0 (compatible; \nheritrix/1.0.4 +http://www.healthdash.com)&lt;/string&gt;\n      &lt;string name=&quot;from&quot;&gt;info@...&lt;/string&gt;\n    &lt;/map&gt;\n    &lt;newObject name=&quot;robots-honoring-policy&quot; \nclass=&quot;org.archive.crawler.datamodel.RobotsHonoringPolicy&quot;&gt;\n      &lt;string name=&quot;type&quot;&gt;classic&lt;/string&gt;\n      &lt;boolean name=&quot;masquerade&quot;&gt;false&lt;/boolean&gt;\n      &lt;text name=&quot;custom-robots&quot;&gt;&lt;/text&gt;\n      &lt;stringList name=&quot;user-agents&quot;&gt;\n      &lt;/stringList&gt;\n    &lt;/newObject&gt;\n    &lt;newObject name=&quot;frontier&quot; \nclass=&quot;org.archive.crawler.frontier.Frontier&quot;&gt;\n      &lt;float name=&quot;delay-factor&quot;&gt;5.0&lt;/float&gt;\n      &lt;integer name=&quot;max-delay-ms&quot;&gt;5000&lt;/integer&gt;\n      &lt;integer name=&quot;min-delay-ms&quot;&gt;500&lt;/integer&gt;\n      &lt;integer name=&quot;max-retries&quot;&gt;3&lt;/integer&gt;\n      &lt;long name=&quot;retry-delay-seconds&quot;&gt;9&lt;/long&gt;\n      &lt;boolean name=&quot;hold-queues&quot;&gt;false&lt;/boolean&gt;\n      &lt;integer name=&quot;preference-embed-hops&quot;&gt;1&lt;/integer&gt;\n      &lt;integer name=&quot;host-valence&quot;&gt;1&lt;/integer&gt;\n      &lt;integer name=&quot;total-bandwidth-usage-KB-sec&quot;&gt;0&lt;/integer&gt;\n      &lt;integer name=&quot;max-per-host-bandwidth-usage-KB-sec&quot;&gt;0&lt;/integer&gt;\n      &lt;integer name=&quot;host-queues-memory-capacity&quot;&gt;200&lt;/integer&gt;\n    &lt;/newObject&gt;\n    &lt;map name=&quot;pre-fetch-processors&quot;&gt;\n      &lt;newObject name=&quot;Preselector&quot; \nclass=&quot;org.archive.crawler.prefetch.Preselector&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n        &lt;boolean name=&quot;recheck-scope&quot;&gt;true&lt;/boolean&gt;\n        &lt;boolean name=&quot;block-all&quot;&gt;false&lt;/boolean&gt;\n        &lt;string name=&quot;block-by-regexp&quot;&gt;&lt;/string&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;Preprocessor&quot; \nclass=&quot;org.archive.crawler.prefetch.PreconditionEnforcer&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n        &lt;integer name=&quot;ip-validity-duration-seconds&quot;&gt;21600&lt;/integer&gt;\n        &lt;integer name=&quot;robot-validity-duration-seconds&quot;&gt;86400&lt;/integer&gt;\n      &lt;/newObject&gt;\n    &lt;/map&gt;\n    &lt;map name=&quot;fetch-processors&quot;&gt;\n      &lt;newObject name=&quot;DNS&quot; \nclass=&quot;org.archive.crawler.fetcher.FetchDNS&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;HTTP&quot; \nclass=&quot;org.archive.crawler.fetcher.FetchHTTP&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n          &lt;newObject name=&quot;MIME-filter&quot; \nclass=&quot;org.archive.crawler.filter.URIRegExpFilter&quot;&gt;\n            &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n            &lt;boolean name=&quot;if-match-return&quot;&gt;false&lt;/boolean&gt;\n            &lt;string name=&quot;regexp&quot;&gt;^.*(?i)&#92;.\n(a|ai|aif|aifc|aiff|asc|au|avi|bcpio|bin|bmp|bz2|c|cdf|cgi|cgm|class|cp\nio|cpp?\n|cpt|csh|css|cxx|dcr|dif|dir|djv|djvu|dll|dmg|dms|doc|dtd|dv|dvi|dxr|ep\ns|etx|exe|ez|gif|gram|grxml|gtar|h|hdf|hqx|ice|ico|ics|ief|ifb|iges|igs\n|iso|jnlp|jp2|jpe|jpeg|jpg|js|kar|latex|lha|lzh|m3u|mac|man|mathml|me|m\nesh|mid|midi|mif|mov|movie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|nc|\no|oda|ogg|pbm|pct|pdb|pdf|pgm|pgn|pic|pict|pl|png|pnm|pnt|pntg|ppm|ppt|\nps|py|qt|qti|qtif|ra|ram|ras|rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|sgml|sh|\nshar|silo|sit|skd|skm|skp|skt|smi|smil|snd|so|spl|src|srpm|sv4cpio|sv4c\nrc|svg|swf|t|tar|tcl|tex|texi|texinfo|tgz|tif|tiff|tr|tsv|ustar|vcd|vrm\nl|vxml|wav|wbmp|wbxml|wml|wmlc|wmls|wmlsc|wrl|xbm|xht|xhtml|xls|xml|xpm\n|xsl|xslt|xwd|xyz|z|zip)$&lt;/string&gt;\n          &lt;/newObject&gt;\n        &lt;/map&gt;\n        &lt;integer name=&quot;timeout-seconds&quot;&gt;1200&lt;/integer&gt;\n        &lt;integer name=&quot;sotimeout-ms&quot;&gt;20000&lt;/integer&gt;\n        &lt;long name=&quot;max-length-bytes&quot;&gt;9223372036854775807&lt;/long&gt;\n        &lt;string name=&quot;load-cookies-from-file&quot;&gt;&lt;/string&gt;\n        &lt;string name=&quot;save-cookies-to-file&quot;&gt;&lt;/string&gt;\n        &lt;string name=&quot;trust-level&quot;&gt;open&lt;/string&gt;\n        &lt;stringList name=&quot;accept-headers&quot;&gt;\n        &lt;/stringList&gt;\n        &lt;string name=&quot;http-proxy-host&quot;&gt;&lt;/string&gt;\n        &lt;string name=&quot;http-proxy-port&quot;&gt;&lt;/string&gt;\n        &lt;string name=&quot;default-encoding&quot;&gt;ISO-8859-1&lt;/string&gt;\n        &lt;boolean name=&quot;sha1-content&quot;&gt;true&lt;/boolean&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;Archiver&quot; \nclass=&quot;org.archive.crawler.writer.ARCWriterProcessor&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n        &lt;boolean name=&quot;compress&quot;&gt;true&lt;/boolean&gt;\n        &lt;string name=&quot;prefix&quot;&gt;IAH&lt;/string&gt;\n        &lt;string name=&quot;suffix&quot;&gt;unprocessed&lt;/string&gt;\n        &lt;integer name=&quot;max-size-bytes&quot;&gt;100000000&lt;/integer&gt;\n        &lt;string \nname=&quot;path&quot;&gt;/home/jboss/geneva/GenevaServer/GenevaUtils/data/arcs&lt;/stri\nng&gt;\n        &lt;integer name=&quot;pool-max-active&quot;&gt;5&lt;/integer&gt;\n        &lt;integer name=&quot;pool-max-wait&quot;&gt;300000&lt;/integer&gt;\n      &lt;/newObject&gt;\n    &lt;/map&gt;\n    &lt;map name=&quot;extract-processors&quot;&gt;\n      &lt;newObject name=&quot;ExtractorHTTP&quot; \nclass=&quot;org.archive.crawler.extractor.ExtractorHTTP&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;ExtractorHTML&quot; \nclass=&quot;org.archive.crawler.extractor.ExtractorHTML&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;ExtractorCSS&quot; \nclass=&quot;org.archive.crawler.extractor.ExtractorCSS&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;ExtractorJS&quot; \nclass=&quot;org.archive.crawler.extractor.ExtractorJS&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;ExtractorSWF&quot; \nclass=&quot;org.archive.crawler.extractor.ExtractorSWF&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n    &lt;/map&gt;\n    &lt;map name=&quot;write-processors&quot;&gt;\n      &lt;newObject name=&quot;Archiver&quot; \nclass=&quot;org.archive.crawler.writer.ARCWriterProcessor&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n        &lt;boolean name=&quot;compress&quot;&gt;true&lt;/boolean&gt;\n        &lt;string name=&quot;prefix&quot;&gt;IAH&lt;/string&gt;\n        &lt;string name=&quot;suffix&quot;&gt;${HOSTNAME}&lt;/string&gt;\n        &lt;integer name=&quot;max-size-bytes&quot;&gt;100000000&lt;/integer&gt;\n        &lt;string \nname=&quot;path&quot;&gt;/home/jboss/geneva/GenevaServer/GenevaUtils/data/arcs&lt;/stri\nng&gt;\n        &lt;integer name=&quot;pool-max-active&quot;&gt;5&lt;/integer&gt;\n        &lt;integer name=&quot;pool-max-wait&quot;&gt;300000&lt;/integer&gt;\n      &lt;/newObject&gt;\n    &lt;/map&gt;\n    &lt;map name=&quot;post-processors&quot;&gt;\n      &lt;newObject name=&quot;Updater&quot; \nclass=&quot;org.archive.crawler.postprocessor.CrawlStateUpdater&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n      &lt;newObject name=&quot;Postselector&quot; \nclass=&quot;org.archive.crawler.postprocessor.Postselector&quot;&gt;\n        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;map name=&quot;filters&quot;&gt;\n        &lt;/map&gt;\n        &lt;boolean name=&quot;seed-redirects-new-seed&quot;&gt;true&lt;/boolean&gt;\n      &lt;/newObject&gt;\n    &lt;/map&gt;\n    &lt;map name=&quot;loggers&quot;&gt;\n      &lt;newObject name=&quot;crawl-statistics&quot; \nclass=&quot;org.archive.crawler.admin.StatisticsTracker&quot;&gt;\n        &lt;integer name=&quot;interval-seconds&quot;&gt;20&lt;/integer&gt;\n      &lt;/newObject&gt;\n    &lt;/map&gt;\n    &lt;string name=&quot;recover-path&quot;&gt;&lt;/string&gt;\n    &lt;newObject name=&quot;credential-store&quot; \nclass=&quot;org.archive.crawler.datamodel.CredentialStore&quot;&gt;\n      &lt;map name=&quot;credentials&quot;&gt;\n      &lt;/map&gt;\n    &lt;/newObject&gt;\n  &lt;/controller&gt;\n&lt;/crawl-order&gt;\n\n===\n\n\nWhen I click on &quot;Reports&quot; this is what I get:\n\n\nAn error occured\njava.util.NoSuchElementException\njava.util.NoSuchElementException\n\tat java.util.TreeMap.key(TreeMap.java:431)\n\tat java.util.TreeMap.firstKey(TreeMap.java:286)\n\tat java.util.TreeSet.first(TreeSet.java:404)\n\tat org.archive.crawler.framework.ToePool.oneLineReport\n(ToePool.java:111)\n\tat \norg.archive.crawler.framework.CrawlController.oneLineReportThreads\n(CrawlController.java:1015)\n\tat org.archive.crawler.admin.CrawlJobHandler.getThreadOneLine\n(CrawlJobHandler.java:952)\n\tat org.archive.crawler.jspc.admin.reports_jsp._jspService\n(Unknown Source)\n\tat org.apache.jasper.runtime.HttpJspBase.service\n(HttpJspBase.java:137)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:853)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle\n(ServletHolder.java:358)\n\tat \norg.mortbay.jetty.servlet.WebApplicationHandler$Chain.doFilter\n(WebApplicationHandler.java:342)\n\tat org.archive.crawler.admin.ui.RootFilter.doFilter\n(RootFilter.java:67)\n\tat \norg.mortbay.jetty.servlet.WebApplicationHandler$Chain.doFilter\n(WebApplicationHandler.java:334)\n\tat org.mortbay.jetty.servlet.WebApplicationHandler.dispatch\n(WebApplicationHandler.java:286)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle\n(ServletHandler.java:567)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1807)\n\tat org.mortbay.jetty.servlet.WebApplicationContext.handle\n(WebApplicationContext.java:525)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1757)\n\tat org.mortbay.http.HttpServer.service(HttpServer.java:879)\n\tat org.mortbay.http.HttpConnection.service\n(HttpConnection.java:790)\n\tat org.mortbay.http.HttpConnection.handleNext\n(HttpConnection.java:961)\n\tat org.mortbay.http.HttpConnection.handle\n(HttpConnection.java:807)\n\tat org.mortbay.http.SocketListener.handleConnection\n(SocketListener.java:197)\n\tat org.mortbay.util.ThreadedServer.handle\n(ThreadedServer.java:276)\n\tat org.mortbay.util.ThreadPool$PoolThread.run\n(ThreadPool.java:511)\n\nYou may be able to recover by going back \n\n\n===\n\na tail of the &quot;local-errors.log&quot; file:\n\nlocal-errors.log for Auto launched  Displaying: 0.0% of 18 MB     \n \n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:437)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:324)\n\tat org.archive.crawler.fetcher.FetchHTTP.innerProcess\n(FetchHTTP.java:306)\n\tat org.archive.crawler.framework.Processor.process\n(Processor.java:102)\n\tat org.archive.crawler.framework.ToeThread.processCrawlUri\n(ToeThread.java:255)\n\tat org.archive.crawler.framework.ToeThread.run\n(ToeThread.java:131)\n20050504013235893    -2          - http://www.stats4me.com/robots.txt \nXP http://www.stats4me.com/ no-type #003 - - -\n java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:305)\n\tat java.net.PlainSocketImpl.connectToAddress\n(PlainSocketImpl.java:171)\n\tat java.net.PlainSocketImpl.connect(PlainSocketImpl.java:158)\n\tat java.net.Socket.connect(Socket.java:452)\n\tat \norg.archive.crawler.fetcher.HeritrixProtocolSocketFactory.createSocket\n(HeritrixProtocolSocketFactory.java:172)\n\tat org.apache.commons.httpclient.HttpConnection.open\n(HttpConnection.java:669)\n\tat \norg.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpCo\nnnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1328)\n\tat \norg.apache.commons.httpclient.HttpMethodDirector.executeWithRetry\n(HttpMethodDirector.java:369)\n\tat \norg.apache.commons.httpclient.HttpMethodDirector.executeMethod\n(HttpMethodDirector.java:178)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:437)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:324)\n\tat org.archive.crawler.fetcher.FetchHTTP.innerProcess\n(FetchHTTP.java:306)\n\tat org.archive.crawler.framework.Processor.process\n(Processor.java:102)\n\tat org.archive.crawler.framework.ToeThread.processCrawlUri\n(ToeThread.java:255)\n\tat org.archive.crawler.framework.ToeThread.run\n(ToeThread.java:131)\n20050504013251576    -2          - http://i49.netscape.com/robots.txt \nEP http://i49.netscape.com/c.cgi?B3745349$2554149 no-type #001 - - -\n java.net.SocketTimeoutException: connect timed out: timeout set at \n20000ms.\n\tat \norg.archive.crawler.fetcher.HeritrixProtocolSocketFactory.createSocket\n(HeritrixProtocolSocketFactory.java:175)\n\tat org.apache.commons.httpclient.HttpConnection.open\n(HttpConnection.java:669)\n\tat \norg.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpCo\nnnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1328)\n\tat \norg.apache.commons.httpclient.HttpMethodDirector.executeWithRetry\n(HttpMethodDirector.java:369)\n\tat \norg.apache.commons.httpclient.HttpMethodDirector.executeMethod\n(HttpMethodDirector.java:178)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:437)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:324)\n\tat org.archive.crawler.fetcher.FetchHTTP.innerProcess\n(FetchHTTP.java:306)\n\tat org.archive.crawler.framework.Processor.process\n(Processor.java:102)\n\tat org.archive.crawler.framework.ToeThread.processCrawlUri\n(ToeThread.java:255)\n\tat org.archive.crawler.framework.ToeThread.run\n(ToeThread.java:131)\n20050504013251926    -2          - \nhttp://orders.biomedcentral.com/robots.txt XP \nhttp://orders.biomedcentral.com/default.asp?jou_id=3003 no-type #004 -\n - -\n java.net.SocketTimeoutException: connect timed out: timeout set at \n20000ms.\n\tat \norg.archive.crawler.fetcher.HeritrixProtocolSocketFactory.createSocket\n(HeritrixProtocolSocketFactory.java:175)\n\tat org.apache.commons.httpclient.HttpConnection.open\n(HttpConnection.java:669)\n\tat \norg.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpCo\nnnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1328)\n\tat \norg.apache.commons.httpclient.HttpMethodDirector.executeWithRetry\n(HttpMethodDirector.java:369)\n\tat \norg.apache.commons.httpclient.HttpMethodDirector.executeMethod\n(HttpMethodDirector.java:178)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:437)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod\n(HttpClient.java:324)\n\tat org.archive.crawler.fetcher.FetchHTTP.innerProcess\n(FetchHTTP.java:306)\n\tat org.archive.crawler.framework.Processor.process\n(Processor.java:102)\n\tat org.archive.crawler.framework.ToeThread.processCrawlUri\n(ToeThread.java:255)\n\tat org.archive.crawler.framework.ToeThread.run\n(ToeThread.java:131)\n\n \nThats all folks!\n\nThanks in advance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- Kasian Franks\n\nLife Sciences Division (Mail Stop 83-0101)\nhttp://geneva.lbl.gov/GenoPharm\nLand:    KFranks@...\nSea:    510 393 6221\nLawrence Berkeley National Laboratory     \n1 Cyclotron Road                          \nBerkeley, CA 94720-8265\n\n\n\n\n"}}