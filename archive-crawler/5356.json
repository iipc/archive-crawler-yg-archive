{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":352493089,"authorName":"haidong.pan","from":"&quot;haidong.pan&quot; &lt;haidong.pan@...&gt;","profile":"haidong.pan","replyTo":"LIST","senderId":"SSJ7K3661nDRbEKgyobTcIqGiu0dEp5BCQGCUwr5cSLgonTqbR87odIVk-DtKtNADI0oTloV7eg7IPJefwi8kXB2cuOIQfzpPtVqUg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Newbie, Only download html pages in seeds","postDate":"1215903895","msgId":5356,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PGc1YmRhbis1cWxvQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ4NzdDM0VDLjcwNTA5MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":5355,"nextInTopic":5357,"prevInTime":5355,"nextInTime":5357,"topicId":5353,"numMessagesInTopic":6,"msgSnippet":"Hi Thank you very much for your help. Yes, I only want fetch seed URIS, no any other resources. And i follow second advice, only 1 URI in sheet:","rawEmail":"Return-Path: &lt;haidong.pan@...&gt;\r\nReceived: (qmail 23977 invoked from network); 13 Jul 2008 07:16:13 -0000\r\nReceived: from unknown (66.218.67.95)\n  by m49.grp.scd.yahoo.com with QMQP; 13 Jul 2008 07:16:13 -0000\r\nReceived: from unknown (HELO n40d.bullet.mail.sp1.yahoo.com) (66.163.169.146)\n  by mta16.grp.scd.yahoo.com with SMTP; 13 Jul 2008 07:16:13 -0000\r\nReceived: from [216.252.122.219] by n40.bullet.mail.sp1.yahoo.com with NNFMP; 13 Jul 2008 07:16:13 -0000\r\nReceived: from [209.73.164.83] by t4.bullet.sp1.yahoo.com with NNFMP; 13 Jul 2008 07:16:13 -0000\r\nReceived: from [66.218.66.90] by t7.bullet.scd.yahoo.com with NNFMP; 13 Jul 2008 07:16:13 -0000\r\nX-Sender: haidong.pan@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 89550 invoked from network); 12 Jul 2008 23:04:55 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m42.grp.scd.yahoo.com with QMQP; 12 Jul 2008 23:04:55 -0000\r\nX-Received: from unknown (HELO n50a.bullet.mail.sp1.yahoo.com) (66.163.168.144)\n  by mta15.grp.scd.yahoo.com with SMTP; 12 Jul 2008 23:04:55 -0000\r\nX-Received: from [216.252.122.216] by n50.bullet.mail.sp1.yahoo.com with NNFMP; 12 Jul 2008 23:04:55 -0000\r\nX-Received: from [66.218.69.2] by t1.bullet.sp1.yahoo.com with NNFMP; 12 Jul 2008 23:04:55 -0000\r\nX-Received: from [66.218.66.73] by t2.bullet.scd.yahoo.com with NNFMP; 12 Jul 2008 23:04:55 -0000\r\nDate: Sat, 12 Jul 2008 23:04:55 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g5bdan+5qlo@...&gt;\r\nIn-Reply-To: &lt;4877C3EC.7050907@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;haidong.pan&quot; &lt;haidong.pan@...&gt;\r\nSubject: Re: Newbie, Only download html pages in seeds\r\nX-Yahoo-Group-Post: member; u=352493089; y=sT1zWYU0MY4glIpbjNvQXtIRoi2bxb7RmcxsT6EC0PTD-sLGG0w\r\nX-Yahoo-Profile: haidong.pan\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 13 Jul 2008 07:16:10 -0000\r\n\r\nHi\n\nThank you very much for your help.\nYes, I only want fetch seed URIS, no=\r\n any other resources.\nAnd i follow second advice, only 1 URI in sheet: http=\r\n://localhost\nThe result is:\n2008-07-12T22:58:37.632Z   -50          - http:=\r\n//localhost/ - -\nunknown #005 - - - 30t\n\nThis is my full config:\nglobal \tro=\r\not \tmap \tjava.lang.Object\nglobal \troot:metadata \tprimary \norg.archive.modul=\r\nes.writer.DefaultMetadataProvider\nglobal \troot:metadata:description \tstring=\r\n \tBasic seeds sites crawl.\nglobal \troot:metadata:operator-contact-url \tstri=\r\nng \thttp://www.aol.com\nglobal \troot:metadata:robots-honoring-policy \tprimar=\r\ny \norg.archive.modules.net.RobotsHonoringPolicy\nglobal \troot:metadata:robot=\r\ns-honoring-policy:user-agents \tlist \njava.lang.String\nglobal \troot:loggerMo=\r\ndule \tprimary \norg.archive.crawler.framework.CrawlerLoggerModule\nglobal \tro=\r\not:seeds \tprimary \torg.archive.modules.seeds.SeedModuleImpl\nglobal \troot:sc=\r\nope \tobject \norg.archive.modules.deciderules.DecideRuleSequence\nglobal \troo=\r\nt:scope:rules \tlist \torg.archive.modules.deciderules.DecideRule\nglobal \troo=\r\nt:scope:rules:0 \tobject \norg.archive.modules.deciderules.RejectDecideRule\ng=\r\nlobal \troot:scope:rules:1 \tobject \norg.archive.modules.deciderules.surt.Sur=\r\ntPrefixedDecideRule\nglobal \troot:scope:rules:2 \tobject \norg.archive.modules=\r\n.deciderules.TooManyHopsDecideRule\nglobal \troot:scope:rules:3 \tobject \norg.=\r\narchive.modules.deciderules.TransclusionDecideRule\nglobal \troot:scope:rules=\r\n:4 \tobject \norg.archive.modules.deciderules.PathologicalPathDecideRule\nglob=\r\nal \troot:scope:rules:5 \tobject \norg.archive.modules.deciderules.TooManyPath=\r\nSegmentsDecideRule\nglobal \troot:scope:rules:6 \tobject \norg.archive.modules.=\r\ndeciderules.PrerequisiteAcceptDecideRule\nglobal \troot:uriUniqFilter \tprimar=\r\ny \norg.archive.crawler.util.BdbUriUniqFilter\nglobal \troot:queue-assignment-=\r\npolicy \tprimary \norg.archive.crawler.frontier.SurtAuthorityQueueAssignmentP=\r\nolicy\nglobal \troot:server-cache \tprimary \torg.archive.modules.net.BdbServer=\r\nCache\nglobal \troot:credential-store \tprimary \norg.archive.modules.credentia=\r\nl.CredentialStore\nglobal \troot:controller \tprimary \norg.archive.crawler.fra=\r\nmework.CrawlControllerImpl\nglobal \troot:controller:frontier \tprimary \norg.a=\r\nrchive.crawler.frontier.BdbFrontier\nglobal \troot:controller:frontier:rules =\r\n\tlist \norg.archive.modules.canonicalize.CanonicalizationRule\nglobal \troot:c=\r\nontroller:frontier:rules:0 \tobject \norg.archive.modules.canonicalize.Lowerc=\r\naseRule\nglobal \troot:controller:frontier:rules:1 \tobject \norg.archive.modul=\r\nes.canonicalize.StripUserinfoRule\nglobal \troot:controller:frontier:rules:2 =\r\n\tobject \norg.archive.modules.canonicalize.StripWWWNRule\nglobal \troot:contro=\r\nller:frontier:rules:3 \tobject \norg.archive.modules.canonicalize.StripSessio=\r\nnIDs\nglobal \troot:controller:frontier:rules:4 \tobject \norg.archive.modules.=\r\ncanonicalize.StripSessionCFIDs\nglobal \troot:controller:frontier:rules:5 \tob=\r\nject \norg.archive.modules.canonicalize.FixupQueryStr\nglobal \troot:controlle=\r\nr:frontier:scope \treference \troot:scope\nglobal \troot:controller:processors =\r\n\tmap \torg.archive.modules.Processor\nglobal \troot:controller:processors:Pres=\r\nelector \tobject \norg.archive.crawler.prefetch.Preselector\nglobal \troot:cont=\r\nroller:processors:Preselector:scope \treference \nroot:scope\nglobal \troot:con=\r\ntroller:processors:Preprocessor \tobject \norg.archive.crawler.prefetch.Preco=\r\nnditionEnforcer\nglobal \troot:controller:processors:DNS \tobject \norg.archive=\r\n.modules.fetcher.FetchDNS\nglobal \troot:controller:processors:HTTP \tobject \n=\r\norg.archive.modules.fetcher.FetchHTTP\nglobal \troot:controller:processors:HT=\r\nTP:accept-headers \tlist \njava.lang.String\nglobal \troot:controller:processor=\r\ns:HTTP:midfetch-rules \tobject \norg.archive.modules.deciderules.DecideRuleSe=\r\nquence\nglobal \troot:controller:processors:ExtractorHTTP \tobject \norg.archiv=\r\ne.modules.extractor.ExtractorHTTP\nglobal \troot:controller:processors:Extrac=\r\ntorHTML \tobject \norg.archive.modules.extractor.ExtractorHTML\nglobal \troot:c=\r\nontroller:processors:ExtractorCSS \tobject \norg.archive.modules.extractor.Ex=\r\ntractorCSS\nglobal \troot:controller:processors:ExtractorJS \tobject \norg.arch=\r\nive.modules.extractor.ExtractorJS\nglobal \troot:controller:processors:Extrac=\r\ntorSWF \tobject \norg.archive.modules.extractor.ExtractorSWF\nglobal \troot:con=\r\ntroller:processors:Archiver \tobject \norg.archive.modules.writer.ARCWriterPr=\r\nocessor\nglobal \troot:controller:processors:Updater \tobject \norg.archive.cra=\r\nwler.postprocessor.CrawlStateUpdater\nglobal \troot:controller:processors:Lin=\r\nksScoper \tobject \norg.archive.crawler.postprocessor.LinksScoper\nglobal \nroo=\r\nt:controller:processors:LinksScoper:log-rejects-rules:rules \tlist \norg.arch=\r\nive.modules.deciderules.DecideRule\nglobal \nroot:controller:processors:Links=\r\nScoper:log-rejects-rules:rules:0 \nobject \torg.archive.modules.deciderules.R=\r\nejectDecideRule\nglobal \nroot:controller:processors:LinksScoper:log-rejects-=\r\nrules:rules:1 \nobject \torg.archive.modules.deciderules.TransclusionDecideRu=\r\nle\nglobal \nroot:controller:processors:LinksScoper:log-rejects-rules:rules:2=\r\n \nobject \torg.archive.modules.deciderules.TooManyHopsDecideRule\nglobal \troo=\r\nt:controller:processors:LinksScoper:scope \treference \nroot:scope\nglobal \tro=\r\not:controller:processors:Scheduler \tobject \norg.archive.crawler.postprocess=\r\nor.FrontierScheduler\nglobal \troot:controller:processors:Scheduler:decide-ru=\r\nles:rules \nlist \torg.archive.modules.deciderules.DecideRule\nglobal \troot:co=\r\nntroller:processors:Scheduler:decide-rules:rules:0 \nobject \torg.archive.mod=\r\nules.deciderules.RejectDecideRule\nglobal \troot:controller:processors:Schedu=\r\nler:decide-rules:rules:1 \nobject \torg.archive.modules.deciderules.Transclus=\r\nionDecideRule\nglobal \nroot:controller:processors:Scheduler:decide-rules:rul=\r\nes:1:max-trans-hops\n\tint \t0\nglobal \troot:controller:processors:Scheduler:de=\r\ncide-rules:rules:2 \nobject \torg.archive.modules.deciderules.TooManyHopsDeci=\r\ndeRule\nglobal \nroot:controller:processors:Scheduler:decide-rules:rules:2:ma=\r\nx-hops \tint \t0\nglobal \troot:controller:statistics-tracker \tobject \norg.arch=\r\nive.crawler.framework.StatisticsTrackerImpl \n\n\n\n--- In archive-crawler@yaho=\r\nogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; haidong.pan wrote:\n&gt; &gt; Hi\n=\r\n&gt; &gt; \n&gt; &gt; I&#39;m trying to fetch html pages only defined in seeds using\n&gt; &gt; her=\r\nitrix2.0. and don&#39;t want other pages linked in html pages be\ndownload.\n&gt; \n&gt;=\r\n If I understand correctly, you only want your seed URIs to be fetched, \n&gt; =\r\nand nothing else -- no in-page resources (like images, scripts, or\nCSS), \n&gt;=\r\n and no linked HTML or other documents. Is that correct?\n&gt; \n&gt; There are a c=\r\nouple ways to do this -- but the settings you&#39;ve changed \n&gt; aren&#39;t directly=\r\n relevant.\n&gt; \n&gt; &gt; This is my settings:\n&gt; &gt; global \n&gt; &gt; root:controller:proc=\r\nessors:LinksScoper:seed-redirects-new-seeds \n&gt; &gt; boolean \tfalse \n&gt; \n&gt; We ca=\r\nll the set of rules for determining which discovered URIs\nshould be \n&gt; craw=\r\nled the &#39;scope&#39; of the crawl. Some scopes are defined in terms of \n&gt; the se=\r\ned URI patterns. For example, if &#39;www.example.com&#39; is a seed, all \n&gt; URIs w=\r\nith a host of &#39;www.example.com&#39; are ruled-in.\n&gt; \n&gt; This &#39;seed-redirects-new=\r\n-seeds&#39; setting makes the target URIs of \n&gt; redirects (eg HTTP 301/302 resp=\r\nonses) from seeds also be considered \n&gt; seeds, for the purpose of scope rul=\r\nes. So if your seed\n&#39;www.example.com&#39; \n&gt; redirects to &#39;www.other-site.com&#39;,=\r\n &#39;www.other-site.com&#39; will be treated \n&gt; just as if you had entered it as a=\r\n seed. Depending on your scope rules, \n&gt; that might cause all &#39;www.other-si=\r\nte.com&#39; URIs to be ruled in-scope.\n&gt; \n&gt; So, this is only tangentially relat=\r\ned to your goal.\n&gt; \n&gt; &gt; global \troot:controller:processors:Scheduler:decide=\r\n-rules:rules \n&gt; &gt; list \torg.archive.modules.deciderules.DecideRule\n&gt; &gt; glob=\r\nal \troot:controller:processors:Scheduler:decide-rules:rules:0 \n&gt; &gt; object \t=\r\norg.archive.modules.deciderules.SeedAcceptDecideRule\n&gt; &gt; global \troot:contr=\r\noller:processors:Scheduler:decide-rules:rules:1 \n&gt; &gt; object \torg.archive.mo=\r\ndules.deciderules.RejectDecideRule \n&gt; \n&gt; Here, you are setting extra decide=\r\n-rules on the &#39;Scheduler&#39; processor. \n&gt; The rules that are set up per-proce=\r\nssor only affect whether that \n&gt; processor runs on certain URIs -- they are=\r\n a mechanism for certain\nsteps \n&gt; to be skipped for some URIs.\n&gt; \n&gt; Rules a=\r\nre applied in order with the last rule to apply a decision \n&gt; &#39;winning&#39;. (L=\r\nater rules may PASS.)\n&gt; \n&gt; Your rules have no initial default decision in t=\r\nhe first position\n(which \n&gt; is a good practice to have). So URIs start out =\r\nwith a neutral PASS \n&gt; decision. Then your rule #0 takes any URI that is a =\r\nseed and sets its \n&gt; decision to ACCEPT. Then your rule #1 sets every URIs =\r\ndecision to \n&gt; REJECT. So the net effect is every URI is given a REJECT dec=\r\nision, and \n&gt; the Scheduler is *never* run.\n&gt; \n&gt; The Scheduler processor ta=\r\nkes discovered URIs and inserts them into the \n&gt; Frontier for later crawlin=\r\ng, so this guarantees no URIs discovered \n&gt; during the crawl will be crawle=\r\nd. However, even crawling your seeds \n&gt; requires some non-seed URIs to be s=\r\ncheduled -- the DNS and Robots.txt \n&gt; URIs that are prerequisites of your U=\r\nRIs.\n&gt; \n&gt; So I would expect your changes here to prevent anything from bein=\r\ng \n&gt; crawled at all.\n&gt; \n&gt; &gt; But it&#39;s not works.\n&gt; &gt; \n&gt; &gt; Can i have any hel=\r\np?\n&gt; \n&gt; Here are 2 different ways you could achieve a crawl of only your\nse=\r\ned URIs:\n&gt; \n&gt; (1) In the chain of processors, remove or disable all Extract=\r\nor. These \n&gt; processors&#39; names all begin &quot;Extractor&quot;. If none are present o=\r\nr\nenabled, \n&gt; no URIs will be found in fetched results -- not in their resp=\r\nonse \n&gt; headers (for redirects), HTML, or anywhere else.\n&gt; \n&gt; (2) Adjust th=\r\ne crawl scope so that even though other URIs are \n&gt; discovered, they do not=\r\n pass scope-testing and are not scheduled.\n&gt; \n&gt; The most simple way to do t=\r\nhis is to take the decide-rule configuration \n&gt; from our default profile an=\r\nd, in the scope, change the \n&gt; TooManyHopsDecideRule &#39;max-hops&#39; setting to =\r\n0 (meaning anything more \n&gt; than 1 normal hop from seeds will be REJECTed) =\r\nand the \n&gt; TransclusionDecideRule &#39;max-trans-hops&#39; setting to 0 (meaning no=\r\n \n&gt; transcluded &#39;inline&#39; hops will be ACCEPTed).\n&gt; \n&gt; Another way would be =\r\nto create a custom scope that starts with a \n&gt; REJECT-all rule then only ad=\r\nds both the SeedAcceptDecideRule (so your \n&gt; seeds aren&#39;t REJECTed) and Pre=\r\nrequisiteAcceptDecideRule (so that the \n&gt; prerequisite DNS and robots URIs =\r\naren&#39;t REJECTed).\n&gt; \n&gt; Hope this helps clear things up,\n&gt; \n&gt; - Gordon @ IA\n=\r\n&gt;\n\n\n\n"}}