{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"53oEhGfu6Y2QmA6SigH29jgHqBFEjV0vT0oZgK3l84wR_9Gbc2zu1WRgLfWFh6LFgH4vjKQOvgeQgF-uUBrM39ZBaUo","spamInfo":{"isSpam":false,"reason":"12"},"subject":"questions before we restart the crawl","postDate":"1327105212","msgId":7556,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMUEwNEJDLjYwMTA0MDJAY3MuY211LmVkdT4=","inReplyToHeader":"PDRGMTlCMzkxLjEwNzA0MDFAY3MuY211LmVkdT4=","referencesHeader":"PDRGMTU5NEQwLjIwOTA4MDhAY3MuY211LmVkdT4gPDRGMTVCQjNBLjUwMzA2QGFyY2hpdmUub3JnPiA8NEYxOUIzOTEuMTA3MDQwMUBjcy5jbXUuZWR1Pg=="},"prevInTopic":7555,"nextInTopic":7560,"prevInTime":7555,"nextInTime":7557,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"We have collected about 550 million pages along with the images and supporting documents on our 5 instance crawl that was started Dec. 23rd. Although we are","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 68448 invoked from network); 21 Jan 2012 00:20:19 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m8.grp.sp2.yahoo.com with QMQP; 21 Jan 2012 00:20:19 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.95)\n  by mta1.grp.sp2.yahoo.com with SMTP; 21 Jan 2012 00:20:19 -0000\r\nX-Received: from [128.2.209.200] (SAVOY.LTI.CS.CMU.EDU [128.2.209.200])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id q0L0KDl8014889\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tFri, 20 Jan 2012 19:20:14 -0500\r\nMessage-ID: &lt;4F1A04BC.6010402@...&gt;\r\nDate: Fri, 20 Jan 2012 19:20:12 -0500\r\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: Gordon Mohr &lt;gojomo@...&gt;, archive-crawler@yahoogroups.com\r\nCc: Noah Levitt &lt;nlevitt@...&gt;\r\nReferences: &lt;4F1594D0.2090808@...&gt; &lt;4F15BB3A.50306@...&gt; &lt;4F19B391.1070401@...&gt;\r\nIn-Reply-To: &lt;4F19B391.1070401@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2011.5.19.222118\r\nX-SMTP-Spam-Clean: 8% (\n BODY_SIZE_4000_4999 0, BODY_SIZE_5000_LESS 0, BODY_SIZE_7000_LESS 0, DATE_TZ_NEG_0500 0, __ANY_URI 0, __CP_MEDIA_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __SANE_MSGID 0, __TO_MALFORMED_2 0, __URI_NO_MAILTO 0, __URI_NO_PATH 0, __URI_NO_WWW 0, __URI_NS , __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.95\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=500983475; y=Ac78JEvl7A8399Q-Po_TRhrSRKA8qLZn3a2XrlsWtoSivG9fHnT9DA\r\nX-Yahoo-Profile: david_pane1\r\n\r\n\nWe have collected about 550 million pages along with the images and \nsupporting documents on our 5 instance crawl that was started Dec. 23rd. \n  Although we are please with the amount of data we captured to date, we \nare very concerned about the state of the Heritrix instances.  If fact, \nwe aren&#39;t very confident that the instances will last until the end of \nFebruary.  We are now running on a total of over 500 less threads than \nthe configured 1200 threads/instance.\n\n0 - not running right now.\n1 - running on 1198 ( 2 less)\n2 - running on  931 (269 less)\n3 - running on  987 (213 less)\n4 - running on 1170 (30 less)\n\nSince we are seriously considering throwing away this past month&#39;s work \nand starting over, we would like to pick your brain on some strategies \nthat will help us avoid getting into this situation again.  We were \nhoping to be done crawling by the end of February so this restart will \nput us behind schedule.\n\n1) Can we continue from here but with &quot;clean&quot; Heritrix instances?\n\nIs there a way that we can continue from the this point forward, but \nstart with Heritrix instances that will not be corrupt due to sever \nerror?  (e.g. using the \nhttps://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery )  If \nso, would you recommend doing this?  You mentioned that this could be \ntime consuming.  Each of our instances has downloaded around 170M URIs, \nthey have over 700M queued URIs, what is your time estimate for \nsomething this large?\n\nWe are willing to sacrifice a few days to get our crawler to a clean \nstate again so we can crawl for another 30 days at the pace we have been \ncrawling.\n\n2) What can be done to avoid corrupting the Heritrix instances?\n\n  - What kind of strategies might we take to keep the crawl error free?\n\n  - Do you think the SEVER errors that we have seen are deterministic or \nrandom (e.g., triggered by occasional flaky network conditions, disks, \nrace conditions, or whatever)?\n\n  - Do you believe that we can reliably backup to the previous \ncheckpoint if we watch the logs and stop as soon as we see the first \nSEVER error?  If we do this, do you speculate that the same SEVER will \noccur again?\n\n  - Is there any reason why a Heritrix instance that is run while binded \nto one ip address can&#39;t be resumed  binded to a different ip address?\n\n3) Should we configure the crawler with more instances and switch \nbetween them?\n\nWe have seen that we can run a single instance to 100M pages + \nsupporting images and documents.  Perhaps this means that we need 10 or \nmore instances instead of 5.  That raises the possibility of running 2 \ninstances per machine.  If we could run 2, or even 4, instances on a \nsingle machine, they would each run half as long.\n\n  - Can you suggest a way to start/stop instances from a script so we \ncan change between instances automatically?\n\n  - Have you seen frequent starting / stopping of instances introduce \ninstability?\n\n4) Crawl slows but restarting seems to improve the speed again.\n\nWe noticed that the all of our instances would initially run at a fast \npace.  We would collect an average of 25M + pages/day for 2-3 days and \nthen the crawl would slow down to 10M pages/day over the next few days. \n(these numbers are totals of all 5 instances combined).  When we \nrestarted the instances, the average pages would improve back to 25M + \npages/day. The total crawled numbers (TiB) also reflected the slow down.\n\n  - Is this something that others have experienced as well?\n\n5) We are capturing tweets from twitter, harvesting the urls and want to \ncrawl those urls within 1 day of receiving the tweet. Can you recommend \na strategy for doing this with the 5 instances we are running?\n\n  - Do we need to run a separate crawler dedicated to this?  If so, can \nyou suggest a way to crawl out from the tweeted urls but when we get \nadditional urls from the tweets, quickly change focus to these urls \ninstead of the ones branching out.  When adding urls as seeds, can you \nset a high priority to crawl those before the discovered urls?  Do you \nrecommend maybe setting up a specific crawl for these urls and then only \ncrawl a few hopes from the seeds - injecting the urls from the tweets as \nseeds?\n\n6) I think the answer is no for this question, but I will ask it anyway. \n  If you have a Heritrix instance that is configured for 1200 threads on \none machine, can you recover from a checkpoint from that 1200 thread \nconfiguration on a different machine with an Heritrix instance that is \nconfigured for less threads (e.g. the default 25 threads)?\n\n\nThanks,\n\nDavid\n\n\n"}}