{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"E2Wqx8te-vkKuPhxAArmaJnML5gIH6aFdTmu9_JrRUfaYeoZa8txk7js3pDMOh3iW12rlZLCOYqqwKjZA37WMguCLBjmhoU","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] slow crawls with 1.8: mail per heritrix list","postDate":"1151532222","msgId":2987,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QTJGQ0JFLjgwNzAwMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ0QTI0RUQ1LjgwMDA1MDFAc3NsbWl0LnVuaWJvLml0Pg==","referencesHeader":"PDQ0QTI0RUQ1LjgwMDA1MDFAc3NsbWl0LnVuaWJvLml0Pg=="},"prevInTopic":2984,"nextInTopic":2995,"prevInTime":2986,"nextInTime":2988,"topicId":2984,"numMessagesInTopic":5,"msgSnippet":"I don t know what could be causing this; there are no known problems fitting that description. Once it reaches the slow state, it would be useful to check: -","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 7289 invoked from network); 28 Jun 2006 22:03:42 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m40.grp.scd.yahoo.com with QMQP; 28 Jun 2006 22:03:42 -0000\r\nReceived: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta7.grp.scd.yahoo.com with SMTP; 28 Jun 2006 22:03:41 -0000\r\nReceived: (qmail 71168 invoked from network); 28 Jun 2006 22:03:39 -0000\r\nReceived: from unknown (HELO ?10.0.10.103?) (unknown)\n  by unknown with SMTP; 28 Jun 2006 22:03:39 -0000\r\nX-pair-Authenticated: 71.141.103.244\r\nMessage-ID: &lt;44A2FCBE.8070002@...&gt;\r\nDate: Wed, 28 Jun 2006 15:03:42 -0700\r\nUser-Agent: Thunderbird 1.5.0.4 (Windows/20060516)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Eros Zanchetta &lt;eros@...&gt;\r\nReferences: &lt;44A24ED5.8000501@...&gt;\r\nIn-Reply-To: &lt;44A24ED5.8000501@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] slow crawls with 1.8: mail per heritrix list\r\nX-Yahoo-Group-Post: member; u=137285340; y=7-eFrppccK5b9geoscwzehQsZNYnYe2yUViuhgHLwxjK\r\nX-Yahoo-Profile: gojomo\r\n\r\nI don&#39;t know what could be causing this; there are no known \nproblems fitting that description.\n\nOnce it reaches the &#39;slow&#39; state, it would be useful to check:\n  - does the UI show any alerts?\n  - does the heritrix_out.log show any exceptions (esp. those that \nmight cause thread death)?\n  - what does the UI &#39;threads report&#39; show as the state of running \nthreads?\n  - what does the &#39;frontier report&#39; show as the distribution of \nqueue states?\n\nThe crawler naturally becomes slow when there is little that can \nbe simultaneously crawled -- but that doesn&#39;t sound like the case \nfor you. (If somehow by mistake this has happened, it should be \nevident in the frontier report.)\n\nAnother outside possibility is a large number of non-responsive \nhosts. Depending on your settings, non-responsive hosts could take \ntens of seconds to minutes to time out. Having hundreds or \nthousands among the queues could monopolize all the threads \nwiating for timeouts. This would likely be evident in the threads \nreport if this were the case.\n\nHope this helps -- feel free to send whatever crawl output you \nfeel comfortable sharing to the list or to me directly for more \nanalysis.\n\n- Gordon @ IA\n\nMarco Baroni wrote:\n&gt; Dear Heritrixers,\n&gt; \n&gt; After running some very successful surt-scoped crawls with heritrix 1.4\n&gt; last summer/fall (about 350GB of html-only data per crawl), we have\n&gt; recently come back to using heritrix for more data.\n&gt; \n&gt; We are using the same computer (RH Fedora Core 3 with 4 GB RAM, Dual Xeon\n&gt; 4.3 GHz CPUs and about 2.5 TB hard disk space, kernel version: Linux\n&gt; 2.6.15-1_SPFsmp #3 SMP, java version: 1.4.2_08, JRE Standard Edition build\n&gt; 1.4.2_08-b03) we used for the previous crawls, but we updated to Heritrix 1.8.\n&gt; \n&gt; We tried several crawls, including one with exactly the same\n&gt; parameters of one of our successful earlier crawls. However, we keep\n&gt; encountering the same problem, namely: after about 30 minutes of crawling, the\n&gt; number of active threads starts decreasing, until it gets stuck to 0 or 1\n&gt; active threads. Consequently, the amount of data retrieved stops growing\n&gt; significantly beyond a few hundreds MB -- we can leave the crawl for days\n&gt; in this zombie state, and virtually nothing happens (we might get 1MB more\n&gt; data per hour, or less). We also noticed that the proportion of \n&gt; downloaded-to-queued is very\n&gt; high (90% and more), whereas in the 1.4 crawls it tended to stabilize\n&gt; around 15% or so, until we killed the crawl &#39;cause we had enough data.\n&gt; There are no alerts.\n&gt; \n&gt; Given that the manually specified parameters and the machine are the same\n&gt; we used before, we suspect that something changed in the default parameters\n&gt; between 1.4 and 1.8 that motivates the problem.\n&gt; \n&gt; These are the parameters we tune manually in a crawl that is a perfect\n&gt; replica of a successful 1.4 crawl (everything else not listed here is\n&gt; inherited from the 1.8 default profile, plus the obvious stuff like the \n&gt; operator name, contact email, etc.):\n&gt; \n&gt; - seeds file of 3,000 seeds (we have also experimented with seed files up\n&gt; to 10k seeds, with the same pattern: problems in 1.8 where we did not have\n&gt; problems in 1.4)\n&gt; \n&gt; - heritrix invoked as follows:\n&gt; \n&gt;   JAVA_OPTS=&quot;-Xmx1024m&quot; heritrix -a user:pw\n&gt; \n&gt; - surt scope, based on the following pattern (and no surt extraction from\n&gt; seed file):\n&gt; \n&gt; http://(it,\n&gt; \n&gt; - 150 threads\n&gt; \n&gt; - Tom Emerson&#39;s &quot;focusing on HTML&quot; filters, i.e.:\n&gt; \n&gt; &lt;newObject name=&quot;JustHTMLRegExp&quot;\n&gt; class=&quot;org.archive.crawler.filter.URIRegExpFilter&quot;&gt;\n&gt; &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt; &lt;boolean name=&quot;if-match-return&quot;&gt;true&lt;/boolean&gt;\n&gt; &lt;string name=&quot;regexp&quot;&gt;\n&gt; .*(?i)&#92;.(a|ai|aif|aifc|aiff|... lots of other extensions\n&gt; ...|xslt|xwd|xyz|z|zip)$\n&gt; &lt;/string&gt;\n&gt; &lt;/newObject&gt;\n&gt; \n&gt; \n&gt; &lt;map name=&quot;write-processors&quot;&gt;\n&gt; &lt;newObject name=&quot;Archiver&quot;\n&gt; class=&quot;org.archive.crawler.writer.ARCWriterProcessor&quot;&gt;\n&gt; &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt; &lt;map name=&quot;filters&quot;&gt;\n&gt; &lt;newObject name=&quot;JustHTMLWrite&quot;\n&gt; class=&quot;org.archive.crawler.filter.ContentTypeRegExpFilter&quot;&gt;\n&gt; &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt; &lt;boolean name=&quot;if-match-return&quot;&gt;true&lt;/boolean&gt;\n&gt; &lt;string name=&quot;regexp&quot;&gt;(?i)text/html.*&lt;/string&gt;\n&gt; &lt;/newObject&gt;\n&gt; &lt;/map&gt;\n&gt; ...\n&gt; &lt;/map&gt;\n&gt; \n&gt; &lt;map name=&quot;fetch-processors&quot;&gt;\n&gt; ...\n&gt; &lt;map name=&quot;midfetch-filters&quot;&gt;\n&gt; &lt;newObject name=&quot;JustHTMLMidFetch&quot;\n&gt; class=&quot;org.archive.crawler.filter.ContentTypeRegExpFilter&quot;&gt;\n&gt; &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt; &lt;boolean name=&quot;if-match-return&quot;&gt;true&lt;/boolean&gt;\n&gt; &lt;string name=&quot;regexp&quot;&gt;(?i)text/html.*&lt;/string&gt;\n&gt; &lt;/newObject&gt;\n&gt; &lt;/map&gt;\n&gt; ...\n&gt; &lt;/map&gt;\n&gt; \n&gt; \n&gt; That&#39;s it.\n&gt; \n&gt; As I said, the same parameters (also, same seed list) used for a\n&gt; default-profile-based 1.4 crawl worked fine (in the sense that the crawl\n&gt; was healthy and long lived...)\n&gt; \n&gt; Any advice on what we are doing wrong?\n&gt; \n&gt; Thanks in advance,\n&gt; \n&gt; Marco\n&gt; \n\n"}}