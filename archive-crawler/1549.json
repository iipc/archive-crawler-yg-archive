{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":214587980,"authorName":"Christian Kohlschuetter","from":"Christian Kohlschuetter &lt;ck-heritrix@...&gt;","replyTo":"LIST","senderId":"j-scVO6b7EkF2n20HXmGc4eioGN2SEAJoGZ-AoD284dwzVy3y7evfUDL54V-FQuSnGl-L9BKwUptnhfePyFKgocFpjmdUPX3MsWPk9UbyU_bP0DBMX_NDQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] RFE: New queue assignment policy","postDate":"1108389365","msgId":1549,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDUwMjE0MTQ1Ni4wNTM3OS5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4=","inReplyToHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBMDZFQzVFQHNrYXJmdXIuYm9rLmxvY2FsPg==","referencesHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBMDZFQzVFQHNrYXJmdXIuYm9rLmxvY2FsPg=="},"prevInTopic":1548,"nextInTopic":1550,"prevInTime":1548,"nextInTime":1550,"topicId":1545,"numMessagesInTopic":13,"msgSnippet":"Hey Kris, indeed, I am currently doing _very_ broad crawls :) Using Heritrix, I would like to fetch about 60-100 million representative pages in the DMOZ","rawEmail":"Return-Path: &lt;ck-heritrix@...&gt;\r\nX-Sender: ck-heritrix@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 26974 invoked from network); 14 Feb 2005 13:57:02 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m16.grp.scd.yahoo.com with QMQP; 14 Feb 2005 13:57:02 -0000\r\nReceived: from unknown (HELO mail.newsclub.de) (130.75.2.42)\n  by mta4.grp.scd.yahoo.com with SMTP; 14 Feb 2005 13:57:01 -0000\r\nReceived: (qmail 11305 invoked by uid 2002); 14 Feb 2005 13:57:00 -0000\r\nReceived: from ck-heritrix@... by nhf3.rrzn.uni-hannover.de by uid 207 with qmail-scanner-1.21 \n (clamscan: 0.67. spamassassin: 2.63.  Clear:RC:0(130.75.87.112):SA:0(-2.6/5.0):. \n Processed in 1.728915 secs); 14 Feb 2005 13:57:00 -0000\r\nX-Spam-Status: No, hits=-2.6 required=5.0\r\nReceived: from pc112.l3s.uni-hannover.de (HELO mail.newsclub.de) (webmail@...@130.75.87.112)\n  by nhf3.rrzn.uni-hannover.de with RC4-MD5 encrypted SMTP; 14 Feb 2005 13:56:58 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nDate: Mon, 14 Feb 2005 14:56:05 +0100\r\nUser-Agent: KMail/1.7.2\r\nReferences: &lt;0678DB1968EAC7409CC3D0AB7A11B84A06EC5E@...&gt;\r\nIn-Reply-To: &lt;0678DB1968EAC7409CC3D0AB7A11B84A06EC5E@...&gt;\r\nOrganization: NewsClub\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n  charset=&quot;iso-8859-15&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nMessage-Id: &lt;200502141456.05379.ck-heritrix@...&gt;\r\nX-eGroups-Remote-IP: 130.75.2.42\r\nFrom: Christian Kohlschuetter &lt;ck-heritrix@...&gt;\r\nSubject: Re: [archive-crawler] RFE: New queue assignment policy\r\nX-Yahoo-Group-Post: member; u=214587980\r\n\r\nHey Kris,\n\nindeed, I am currently doing _very_ broad crawls :)\n\nUsing Herit=\r\nrix, I would like to fetch about 60-100 million representative \npages in th=\r\ne DMOZ sphere (currently, the link depth is set to 7, which is \njust too mu=\r\nch for a host-oriented assignment).\n\nPerhaps you have an idea how to do suc=\r\nh a crawl with Heritrix current \nabilities? I would be very interested in a=\r\n quick solution.\n\nBest regards,\n\nChristian\n\nOn Monday 14 February 2005 14:3=\r\n1, Kristinn Sigurdsson wrote:\n&gt; Hey Christian,\n&gt;\n&gt; I&#39;ve got a quick questio=\r\nn for you: just how many hosts/ips are you\n&gt; crawling??\n&gt;\n&gt; I&#39;ve conducted =\r\ncrawls over about 1000-1200 domains (maybe a total of\n&gt; ten-fifty times tha=\r\nt many hosts once you count offsite images etc.) that\n&gt; covered well over a=\r\n million documents (as many as 5 million in fact). And\n&gt; this was with the =\r\nmuch less efficient HostQueuesFrontier.\n&gt;\n&gt; I&#39;ve also tested the BDBFrontie=\r\nr (with 1GB of heap) running on 11 thousand\n&gt; domains (and quite a few more=\r\n hosts in total) without running into any\n&gt; problems. That crawl collected =\r\nover 2 million documents before I shut it\n&gt; down.\n&gt;\n&gt; So, I&#39;m a little surp=\r\nrised by the this. Are you running a strict broad\n&gt; crawl? The only way I c=\r\nould see the number of hosts become an issue within\n&gt; the first 1 million d=\r\nocuments would be in a very broad oriented crawl...?\n&gt;\n&gt; - Kris\n&gt;\n&gt; -----Or=\r\niginal Message-----\n&gt; From: Christian Kohlschuetter [mailto:ck-heritrix@new=\r\nsclub.de]\n&gt; Sent: 14. febr=FAar 2005 13:15\n&gt; To: archive-crawler@yahoogroup=\r\ns.com\n&gt; Subject: [archive-crawler] RFE: New queue assignment policy\n&gt;\n&gt;\n&gt; H=\r\ni,\n&gt;\n&gt; here&#39;s another feature which I would like to contribute.\n&gt;\n&gt; Current=\r\nly, I am performing broad crawls using BroadScope/BdbFrontier.\n&gt; However,\n&gt;=\r\n due to the number of host- or IP-keyed queues, an OutOfMemoryError occurs\n=\r\n&gt; very quickly after starting the crawl. One reason for this is the RAM-bas=\r\ned\n&gt; bookkeeping of subqueues -- the more queues, the more heap.\n&gt;\n&gt; I have=\r\n evaded this by writing a BucketQueueAssignmentPolicy class, which\n&gt; produc=\r\nes a _fixed_ number of subqueues (&quot;buckets&quot;), not one per host or per\n&gt; IP.=\r\n The queue key is computed by hashing the hostname (or the IP, if\n&gt; availab=\r\nle) modulo N (a fixed number, such as 1000).\n&gt;\n&gt; This way, I was able to in=\r\ncrease the number of fetched pages from ca.\n&gt; 400,000\n&gt; to 1,000,000. For s=\r\nome other reason, I still get OOMEs, but I think that is\n&gt; caused by a diff=\r\nerent problem -- the number of queues did not grow over the\n&gt; specified lim=\r\nit.\n&gt;\n&gt; Furthermore, I have modified AbstractFrontier to be able to choose\n=\r\n&gt; arbitrary\n&gt;\n&gt; queue assignment policies and replaced the current &quot;ip-poli=\r\ntness&quot; option by\n&gt; a\n&gt; selectbox.\n&gt;\n&gt; The patch against CVS HEAD is attache=\r\nd.\n&gt;\n&gt; Greetings,\n\n-- \nChristian Kohlsch=FCtter\nmailto: ck -at- NewsClub.de=\r\n\n\n"}}