{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":275305800,"authorName":"Leo Dagum","from":"&quot;Leo Dagum&quot; &lt;leo_dagum@...&gt;","profile":"leo_dagum","replyTo":"LIST","senderId":"PtZZ5tPPQQobGN3LUzhvtzDGQaHVk-RdwfmIX2fX2-4lCARN3W5Hz0nP-ww1FLzmW5y_muNsDwAnI6bKNupdvMEdnbpjH58H","spamInfo":{"isSpam":false,"reason":"12"},"subject":"semantics of queue-total-budget","postDate":"1195065404","msgId":4685,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{},"prevInTopic":0,"nextInTopic":4686,"prevInTime":4684,"nextInTime":4686,"topicId":4685,"numMessagesInTopic":4,"msgSnippet":"Hi All, I m trying to configure a crawl across a broad number of sites but fetching only a restricted number of pages from each site.  Specifically,  I d like ","rawEmail":"Return-Path: &lt;leo_dagum@...&gt;\r\nX-Sender: leo_dagum@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 76693 invoked from network); 14 Nov 2007 20:22:31 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m45.grp.scd.yahoo.com with QMQP; 14 Nov 2007 20:22:31 -0000\r\nX-Received: from unknown (HELO smtp123.plus.mail.sp1.yahoo.com) (69.147.95.86)\n  by mta16.grp.scd.yahoo.com with SMTP; 14 Nov 2007 20:22:31 -0000\r\nX-Received: (qmail 97589 invoked from network); 14 Nov 2007 18:36:44 -0000\r\nX-Received: from unknown (HELO foxtrot) (leo_dagum@209.213.209.230 with login)\n  by smtp123.plus.mail.sp1.yahoo.com with SMTP; 14 Nov 2007 18:36:43 -0000\r\nX-YMail-OSG: NjNX640VM1n5D6QwTme4N8zAihmQGKyV9D1g4JpBW1CQrdoPnBWX_llVutCxIQENEoFnep2FKA--\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Wed, 14 Nov 2007 10:36:44 -0800\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0029_01C826AA.415A9390&quot;\r\nX-Mailer: Microsoft Office Outlook, Build 11.0.5510\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.3198\r\nThread-Index: Acgm7U5pP2179+aOSRW3l74yLHEb7w==\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Leo Dagum&quot; &lt;leo_dagum@...&gt;\r\nSubject: semantics of queue-total-budget\r\nX-Yahoo-Group-Post: member; u=275305800; y=9MaSY9347iw4DilBdJPXAT9K_S9IrrjTacwLIhfe5i04m8LF\r\nX-Yahoo-Profile: leo_dagum\r\n\r\n\r\n------=_NextPart_000_0029_01C826AA.415A9390\r\nContent-Type: text/plain;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: 7bit\r\n\r\nHi All,\n\n \n\nI&#39;m trying to configure a crawl across a broad number of sites but fetching\nonly a restricted number of pages from each site.  Specifically,  I&#39;d like\nto get only 20pages from each site across about 200k sites that are provided\nthrough a seed list.  I set q-t-b to 20 and UnitCostAssignment but did not\nget the behavior I expected.  \n\n \n\nI&#39;m using HostnameQueueAsssingment policy, so I expected 200k queues to be\ncreated and that they get exhausted as they reached their 20 unit budget.\nHowever what I saw was ~180k queues almost immediately marked as exhausted\nand the crawler busy on just 20k queues.  My understanding is that once a\nqueue is exhausted it does not get scheduled again.  Is that correct?  Or\nwill the exhausted queues get reactivated at some point?  \n\n \n\nThanks,\n\n \n\n- leo\n\n\r\n------=_NextPart_000_0029_01C826AA.415A9390\r\nContent-Type: text/html;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;html xmlns:o=3D&quot;urn:schemas-microsoft-com:office:office&quot; xmlns:w=3D&quot;urn:sc=\r\nhemas-microsoft-com:office:word&quot; xmlns=3D&quot;http://www.w3.org/TR/REC-html40&quot;&gt;=\r\n\n\n&lt;head&gt;\n&lt;meta http-equiv=3DContent-Type content=3D&quot;text/html; charset=3Dus=\r\n-ascii&quot;&gt;\n&lt;meta name=3DGenerator content=3D&quot;Microsoft Word 11 (filtered medi=\r\num)&quot;&gt;\n&lt;style&gt;\n&lt;!--\n /* Style Definitions */\n p.MsoNormal, li.MsoNormal, div=\r\n.MsoNormal\n\t{margin:0in;\n\tmargin-bottom:.0001pt;\n\tfont-size:12.0pt;\n\tfont-f=\r\namily:&quot;Times New Roman&quot;;}\na:link, span.MsoHyperlink\n\t{color:blue;\n\ttext-dec=\r\noration:underline;}\na:visited, span.MsoHyperlinkFollowed\n\t{color:purple;\n\tt=\r\next-decoration:underline;}\nspan.EmailStyle17\n\t{mso-style-type:personal-comp=\r\nose;\n\tfont-family:Arial;\n\tcolor:windowtext;}\n@page Section1\n\t{size:8.5in 11=\r\n.0in;\n\tmargin:1.0in 1.25in 1.0in 1.25in;}\ndiv.Section1\n\t{page:Section1;}\n--=\r\n&gt;\n&lt;/style&gt;\n\n&lt;/head&gt;\n\n&lt;body lang=3DEN-US link=3Dblue vlink=3Dpurple&gt;\n\n&lt;div c=\r\nlass=3DSection1&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 face=3DArial&gt;&lt;span st=\r\nyle=3D&#39;font-size:10.0pt;\nfont-family:Arial&#39;&gt;Hi All,&lt;o:p&gt;&lt;/o:p&gt;&lt;/span&gt;&lt;/font=\r\n&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 face=3DArial&gt;&lt;span style=3D&#39;font=\r\n-size:10.0pt;\nfont-family:Arial&#39;&gt;&lt;o:p&gt;&nbsp;&lt;/o:p&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p cl=\r\nass=3DMsoNormal&gt;&lt;font size=3D2 face=3DArial&gt;&lt;span style=3D&#39;font-size:10.0pt=\r\n;\nfont-family:Arial&#39;&gt;I&#8217;m trying to configure a crawl across a broad n=\r\number\nof sites but fetching only a restricted number of pages from each sit=\r\ne. &nbsp;Specifically,\n&nbsp;I&#8217;d like to get only 20pages from each s=\r\nite across about 200k sites\nthat are provided through a seed list.&nbsp; I =\r\nset q-t-b to 20 and\nUnitCostAssignment but did not get the behavior I expec=\r\nted.&nbsp; &lt;o:p&gt;&lt;/o:p&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=\r\n=3D2 face=3DArial&gt;&lt;span style=3D&#39;font-size:10.0pt;\nfont-family:Arial&#39;&gt;&lt;o:p&gt;=\r\n&nbsp;&lt;/o:p&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 face=3D=\r\nArial&gt;&lt;span style=3D&#39;font-size:10.0pt;\nfont-family:Arial&#39;&gt;I&#8217;m using H=\r\nostnameQueueAsssingment policy, so I\nexpected 200k queues to be created and=\r\n that they get exhausted as they reached\ntheir 20 unit budget.&nbsp; Howeve=\r\nr what I saw was ~180k queues almost\nimmediately marked as exhausted and th=\r\ne crawler busy on just 20k queues.&nbsp;\nMy understanding is that once a qu=\r\neue is exhausted it does not get scheduled\nagain.&nbsp; Is that correct?&nb=\r\nsp; Or will the exhausted queues get\nreactivated at some point?&nbsp; &lt;o:p&gt;=\r\n&lt;/o:p&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 face=3DArial&gt;=\r\n&lt;span style=3D&#39;font-size:10.0pt;\nfont-family:Arial&#39;&gt;&lt;o:p&gt;&nbsp;&lt;/o:p&gt;&lt;/span=\r\n&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 face=3DArial&gt;&lt;span style=\r\n=3D&#39;font-size:10.0pt;\nfont-family:Arial&#39;&gt;Thanks,&lt;o:p&gt;&lt;/o:p&gt;&lt;/span&gt;&lt;/font&gt;&lt;/=\r\np&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 face=3DArial&gt;&lt;span style=3D&#39;font-si=\r\nze:10.0pt;\nfont-family:Arial&#39;&gt;&lt;o:p&gt;&nbsp;&lt;/o:p&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=\r\n=3DMsoNormal&gt;&lt;font size=3D2 face=3DArial&gt;&lt;span style=3D&#39;font-size:10.0pt;\nf=\r\nont-family:Arial&#39;&gt;- leo&lt;o:p&gt;&lt;/o:p&gt;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;/div&gt;\n\n&lt;/body&gt;\n\n&lt;/ht=\r\nml&gt;\n\r\n------=_NextPart_000_0029_01C826AA.415A9390--\r\n\n"}}