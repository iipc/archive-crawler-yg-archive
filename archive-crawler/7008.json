{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"pzB2kgF2bruJbElakvu102jVo9ewbmy745FCnyRBTddAO1L5C0LcpXxbO_vayGv5nrM6vsLiIBuEoJxs5wX9ujka97c8ydE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix Frontier","postDate":"1297209384","msgId":7008,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENTFEODI4LjkwNzA5MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEM5Nzc0OTQwLjIxMUUlbXVyYWxpa3BAYW1hem9uLmNvbT4=","referencesHeader":"PEM5Nzc0OTQwLjIxMUUlbXVyYWxpa3BAYW1hem9uLmNvbT4="},"prevInTopic":7007,"nextInTopic":7012,"prevInTime":7007,"nextInTime":7009,"topicId":7005,"numMessagesInTopic":11,"msgSnippet":"... If you want your prioritization to also take effect within the standard Heritrix frontier queues, there s a (seldom-used) pair of features, queue and URI","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 81775 invoked from network); 8 Feb 2011 23:56:27 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m13.grp.re1.yahoo.com with QMQP; 8 Feb 2011 23:56:27 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.re1.yahoo.com with SMTP; 8 Feb 2011 23:56:27 -0000\r\nX-Received: (qmail 43222 invoked by uid 0); 8 Feb 2011 23:56:25 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 8 Feb 2011 23:56:25 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4D51D828.9070902@...&gt;\r\nDate: Tue, 08 Feb 2011 15:56:24 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;Krishna, Murali&quot; &lt;muralikp@...&gt;\r\nReferences: &lt;C9774940.211E%muralikp@...&gt;\r\nIn-Reply-To: &lt;C9774940.211E%muralikp@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix Frontier\r\nX-Yahoo-Group-Post: member; u=137285340; y=kPRXOwMPBOHgOEBtOlDVneAb5R0qOqypYMczki0WYcUi\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 2/8/11 5:55 AM, Krishna, Murali wrote:\n&gt; Thanks Gordon for the detailed response.\n&gt;      We don&#39;t have all the urls upfront, it is a continuous stream of urls\n&gt; and we don&#39;t want to wait for the previous heritrix jobs to finish.\n&gt; Essentially, we want to schedule them as and when is possible (honoring\n&gt; politeness). Also some of the urls have higher priority for crawling.\n\nIf you want your prioritization to also take effect within the standard \nHeritrix frontier queues, there&#39;s a (seldom-used) pair of features, \nqueue and URI &#39;precedence&#39; values, which affect (1) how inactive queues \nare sorted while waiting for their chance to be active; (2) where URIs \nare inserted into individual queues (if all the other factors which \naffect whether they are pushed-near-the-top or queued-to-the-back are \nequal).\n\n&gt;     The problem with BDB frontier is that it is tied to the box and is a\n&gt; reliability concern if the machine goes down. We are thinking of having the\n&gt; urls in reliable queue service in a different cluster and make the heritrix\n&gt; read from that queue. This makes heritrix instance stateless (the crawled\n&gt; content goes to another cluster) and easy to replace with another box. Of\n&gt; course this calls for a new checkpointing mechanism outside the box.\n\nWe&#39;ve considered possibilities for this as well, and some sort of \nremote/distributed frontier is likely in the future for Heritrix, though \nnothing is definitively scheduled/prioritized for an upcoming release.\n\nSome of the range of possibilities could include:\n\n- Replacing the current BDB-JE binary key-value store in the standard \nfrontier with a remote/distributed alternative. This might be most \nstraightforward, though a few issues that don&#39;t come up with the local \nstore would have to be addressed, including: (1) new latencies/overhead; \n(2) multiple crawl processes sharing the same store, (3) what recovery \nfrom arbitrary crashes might be possible.\n\n- Still using the traditional default frontier locally, but pull URIs in \nbatches from the shared/remote store. (This might not require any \nrewriting of the local frontier; just some other module that reports \nresults up, and pulls the right batches of new URIs down.) The shared \nURI-queues/history info could be very different in its representations, \ncheckpointing, etc.\n\n- Replacing the usual local frontier with an alternative which operates \nin a whole new way with the remote queues/store, just implementing the \nminimal expected frontier behavior. We want the code to support this \npossibility, but there are almost certainly hidden assumptions in other \nparts of the code that the frontier behaves like our standard one, which \nwould need some clean up when tested by this new approach. And, much of \nthe existing timing/politeness/ordering behavior would have to be \nreimplementing in broadly-similar ways... though perhaps much of the \nexisting design could be mirrored albeit with remote queues/sets.\n\n&gt;     I understand it is an overkill to use heritrix, but In future, we might\n&gt; need depth crawl which we can easily implement by scheduling the newly\n&gt; detected urls back into the reliable queue service. We are just trying to\n&gt; leverage the politeness, threading and pluggable processor frameworks of\n&gt; heritrix.\n&gt;\n&gt; Thoughts?\n\nI better understand your motivations and they seem reasonable. I&#39;ll be \nvery interested to hear any architectural directions you take, and can \ndiscuss. If either the code, or simply the changes that make Heritrix \nbetter able to rely on remote frontier functionality, can be contributed \nback, it will likely be of interest to the IA and other Heritrix users \nas well!\n\n- Gordon @ IA\n\n\n&gt; Thanks,\n&gt; Murali\n&gt;\n&gt; On 2/8/11 1:56 PM, &quot;Gordon Mohr&quot;&lt;gojomo@...&gt;  wrote:\n&gt;\n&gt;&gt; On 2/7/11 3:17 AM, Krishna, Murali wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; Hi all,\n&gt;&gt;&gt; We have a list of urls to be crawled, essentially just a fetch and some\n&gt;&gt;&gt; processing. Assume that the list can be huge and run into billions. So,\n&gt;&gt;&gt; we are thinking of writing a new Frontier which will accomplish this.\n&gt;&gt;&gt; Will have multiple heritrix worker boxes, each of the worker�s frontier\n&gt;&gt;&gt; will get one portion of the centralized url repository (distributed\n&gt;&gt;&gt; storage) and schedule them for crawling.\n&gt;&gt;\n&gt;&gt; You probably won&#39;t need a new Frontier for this; the default frontier\n&gt;&gt; (BdbFrontier) should work for tens to even hundreds of millions of\n&gt;&gt; queued URIs per node.\n&gt;&gt;\n&gt;&gt; I have more confidence in H3 for loading millions of seed URIs at\n&gt;&gt; startup (which should be even more efficient in H3 SVN TRUNK and the\n&gt;&gt; next H3 release), though you could also feed them in smaller batches via\n&gt;&gt; the H1 JMX interface, or in batches via the &#39;action&#39; directory mechanism\n&gt;&gt; in H3.\n&gt;&gt;\n&gt;&gt; If you simply have a large static list of URIs to crawl -- and don&#39;t\n&gt;&gt; need link-extraction and any other running analysis/reporting --\n&gt;&gt; Heritrix may be overkill for your purposes.\n&gt;&gt;\n&gt;&gt;&gt; 1. Can we achieve this by extending the WorkQueueFrontier ? I couldn�t\n&gt;&gt;&gt; find much documentation on how WQF handles politeness. I am thinking of\n&gt;&gt;&gt; grouping the urls into workqueue based on politeness requirement, will\n&gt;&gt;&gt; it automatically take care of politeness if I group correctly? Can I\n&gt;&gt;&gt; configure crawl-delay per WorkQueue?\n&gt;&gt;\n&gt;&gt; You can control what is crawled -- whether outlinks from your starting\n&gt;&gt; URIs are followed, for example, to get inline resources or other linked\n&gt;&gt; pages -- by customizing the scoping rules. If grouping URIs by hostname\n&gt;&gt; into queues is insufficient, you can implement a new\n&gt;&gt; QueueAssignmentPolicy. In H3, politeness delays per URI -- affecting the\n&gt;&gt; queue from which the URI came -- are configured outside the frontier, in\n&gt;&gt; the DispositionProcessor. So lots of behavioral customization doesn&#39;t\n&gt;&gt; require reimplementing or specializing the frontier.\n&gt;&gt;\n&gt;&gt; The queues are the units of politeness: by default, only one URI from a\n&gt;&gt; queue will be in-process at a time. When a URI finishes, a configurable\n&gt;&gt; pause (see the minDelay, maxDelay, delayFactor, and\n&gt;&gt; respectCrawlDelayUpToSeconds settings on DispositionProcessor in H3) is\n&gt;&gt; applied to that queue before any other URIs are tried. Note that URI\n&gt;&gt; domain-lookup/connectivity failures cause the same URI to be pushed back\n&gt;&gt; atop the queue, a longer (frontier retryDelaySeconds) pause to be taken,\n&gt;&gt; and multiple (frontier maxRetries) attempts to be made, before the next\n&gt;&gt; URI is tried. This means you usually do not want URIs on different\n&gt;&gt; hosts, where one host might be unreachable, mixed in the same queue --\n&gt;&gt; one failure will delay them all -- unless you also knock the\n&gt;&gt; retries/retryDelay way down.\n&gt;&gt;\n&gt;&gt; You can use the settings &#39;sheet overlay&#39; (aka &#39;overrides&#39; in H1) to set\n&gt;&gt; different politeness values for different URIs by host or other\n&gt;&gt; patterns; the queue then takes on the delay of the URI that was just\n&gt;&gt; offered/completed.\n&gt;&gt;\n&gt;&gt; You should also look at previous list traffic about HashCrawlMapper for\n&gt;&gt; ideas on splitting the URI space, and the BloomUriUniqFilter as an\n&gt;&gt; option for an all in-memory URI-already-seen filter that may be\n&gt;&gt; appropriate for larger crawls.\n&gt;&gt;\n&gt;&gt;&gt; 2. What are inactive queues, retired queues and getURIList here? (sorry,\n&gt;&gt;&gt; couldn�t find doc)\n&gt;&gt;\n&gt;&gt; &#39;inactive&#39; queues are those that are not yet being considered to keep a\n&gt;&gt; thread busy. All those queues that are &#39;active&#39; round-robin to provide\n&gt;&gt; URIs to available threads, until the queue exhausts its &#39;session&#39;\n&gt;&gt; budget; then it goes to the back of all &#39;inactive&#39; queues. If a thread\n&gt;&gt; can&#39;t be kept busy with an available &#39;active&#39; queue, then the top\n&gt;&gt; &#39;inactive&#39; queue is activated. The intent is for the crawler to\n&gt;&gt; intensely focus on some queues for a while -- hoping to finish them, or\n&gt;&gt; at least get a big batch of URIs with as little time-skew as possible ��\n&gt;&gt; but then rotate others into activity eventually. (The &#39;budgeting&#39; and\n&gt;&gt; URI &#39;cost&#39; parameters affect this cycle.)\n&gt;&gt;\n&gt;&gt; &#39;retired&#39; queues have already offered up URIs whose total &#39;cost&#39; exceeds\n&gt;&gt; their &#39;totalBudget&#39;, and so they continue to collect newly-discovered\n&gt;&gt; URIs, but will never b considered for &#39;active&#39; rotation (unless you\n&gt;&gt; raise their &#39;totalBudget&#39;). You probably don&#39;t need this feature, and\n&gt;&gt; won&#39;t notice any &#39;retired&#39; queues unless you set a &#39;totalBudget&#39; and\n&gt;&gt; nonzero URI cost policy.\n&gt;&gt;\n&gt;&gt; I don&#39;t know what you mean by &quot;getURIList&quot;.\n&gt;&gt;\n&gt;&gt;&gt; 3. How does checkpointing work, I want to restart from the last crawled\n&gt;&gt;&gt; state. Is there a callback to do the frequent checkpointing for\n&gt;&gt;&gt; WorkQueueFrontier�s implementations.\n&gt;&gt;\n&gt;&gt; Checkpointing tries to save the whole crawl state at requested points.\n&gt;&gt; You can set it to automatically checkpoint at a certain interval. In H1,\n&gt;&gt; it&#39;s via a system property; see Checkpointer.initialize(). In H3, it&#39;s\n&gt;&gt; CheckpointService&#39;s checkpointIntervalMinutes property. In H1, the crawl\n&gt;&gt; must reach a full pause for a checkpoint to occur; with long downloads\n&gt;&gt; and connection timeouts, this can mean a slowdown in the tens of\n&gt;&gt; minutes. In H3, a checkpoint requires a much narrower lock, so may only\n&gt;&gt; take a few seconds or a minute or two, and long network fetches may\n&gt;&gt; continue during the checkpoint.\n&gt;&gt;\n&gt;&gt; In both cases, you need to retain the &#39;state&#39; directory files\n&gt;&gt; (.JDB/.DEL) corresponding to the checkpoints from which you might want\n&gt;&gt; to resume. (If not needed for a checkpoint, you can freely delete the\n&gt;&gt; .DELs.) Resuming from an earlier checkpoint may foul any other\n&gt;&gt; subsequent-but-unused checkpoints (though future checkpoints should be\n&gt;&gt; fine).\n&gt;&gt;\n&gt;&gt; The checkpointing system has always been a bit rough but I have more\n&gt;&gt; confidence in its flexibility and speed in H3. I would not yet count on\n&gt;&gt; it for perfect resumability in a large crawl without more experience\n&gt;&gt; using it. If not using checkpoints, or checkpoints fail for some reason,\n&gt;&gt; an approximation of a frontier&#39;s state at the time of a crash can be\n&gt;&gt; recreated from the &#39;frontier recovery log&#39; also kept by the crawler.\n&gt;&gt; (Not all running stats/state can be reconstructed, but essentially all\n&gt;&gt; the same pending URIs will be reenqueued.)\n&gt;&gt;\n&gt;&gt; I believe there&#39;s a JMX call in H1 to request a checkpoint, and in H3\n&gt;&gt; it&#39;s just one of the web UI/web service calls easy to trigger by a web hit:\n&gt;&gt;\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Herit\n&gt;&gt; rix3.0APIGuide-CheckpointJob\n&gt;&gt;\n&gt;&gt; Hope this helps!\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}