{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"kF62gGF665che-O5ivT886yxh8AfHNdmlaVxEU9I06FVOu-5AHhnwljmrOjFHn-20av_kUvGngmVzJkcBy3ICgVSPLbkXCg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Slow (?) loading millions of seeds","postDate":"1334266476","msgId":7652,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGODc0QTZDLjMwNjAxMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MEVBNDVENDhAdzJrMy1ic3BleDE+","referencesHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MEVBNDVCQ0VAdzJrMy1ic3BleDE+IDw3RDMxNjA3RS0yNkQ4LTRGRUEtQTE0Ni05MUQ0NjBBREREOEFAYXJjaGl2ZS5vcmc+IDxDNjMxNzQ2QS1BQkJDLTQ1N0UtOENDRC1ERTUxNDAyRjg4NDZAYXJjaGl2ZS5vcmc+IDw3NEM5N0U3REY1QTc3ODREOTk3MjE3RkY3NUQxMjE2NjBFQTQ1RDQ4QHcyazMtYnNwZXgxPg=="},"prevInTopic":7651,"nextInTopic":7655,"prevInTime":7651,"nextInTime":7653,"topicId":7645,"numMessagesInTopic":9,"msgSnippet":"We d need to know more about your crawl design and goals, and maybe collect some info about the process during the slow period, to understand the bottleneck","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 40874 invoked from network); 12 Apr 2012 21:34:37 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m15.grp.sp2.yahoo.com with QMQP; 12 Apr 2012 21:34:37 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta1.grp.sp2.yahoo.com with SMTP; 12 Apr 2012 21:34:37 -0000\r\nX-Received: (qmail 52819 invoked by uid 0); 12 Apr 2012 21:34:35 -0000\r\nX-Received: from 174.253.242.97 (HELO silverbook.local) (174.253.242.97)\n  by relay02.pair.com with SMTP; 12 Apr 2012 21:34:35 -0000\r\nX-pair-Authenticated: 174.253.242.97\r\nMessage-ID: &lt;4F874A6C.3060103@...&gt;\r\nDate: Fri, 13 Apr 2012 07:34:36 +1000\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:11.0) Gecko/20120327 Thunderbird/11.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;Coram, Roger&quot; &lt;Roger.Coram@...&gt;\r\nReferences: &lt;74C97E7DF5A7784D997217FF75D121660EA45BCE@w2k3-bspex1&gt; &lt;7D31607E-26D8-4FEA-A146-91D460ADDD8A@...&gt; &lt;C631746A-ABBC-457E-8CCD-DE51402F8846@...&gt; &lt;74C97E7DF5A7784D997217FF75D121660EA45D48@w2k3-bspex1&gt;\r\nIn-Reply-To: &lt;74C97E7DF5A7784D997217FF75D121660EA45D48@w2k3-bspex1&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Slow (?) loading millions of seeds\r\nX-Yahoo-Group-Post: member; u=137285340; y=YzPSJIgeTQrGUevDbJPpkau406nC9Sbj-tlHwh99Gg1H\r\nX-Yahoo-Profile: gojomo\r\n\r\nWe&#39;d need to know more about your crawl design and goals, and maybe \ncollect some info about the process during the &#39;slow&#39; period, to \nunderstand the bottleneck you&#39;re hitting.\n\nStill, some other ideas not yet mentioned:\n\nWith millions of seeds, you&#39;ll *not* want to use any of the scoping \nrules that auto-update based on each seed -- like SurtPrefixedDecideRule \nwith seedsAsSurtPrefixes==true.\n\n(If you do, a giant in-memory sorted map of acceptable prefixes based on \nyour seeds is being built... perhaps hundreds of MB of heap for your \nseed list.)\n\nFor a &#39;slash&#39; (aka root) page crawl, you probably want to start broad \n(with an accept-all rule) then have the MaxHops rule at &#39;0&#39; -- all \ndiscovered navigational outlinks (hop-type &#39;L&#39;) will be considered too \nfar, while inline outlinks may still be retrieved based on whether you \ntune the TransclusionDecideRule.\n\nAs just a slash-page crawl, you might not need to split it to multiple \nmachines just yet unless you want to increase the parallelism (compress \nthe time-skew) beyond what the threads of one machine can do.\n\nYou might want to feed in batches via the action directory, for added \nunderstanding of what&#39;s been done, or dynamic adds after initial launch. \nBut on thing to watch out with that technique is that it is totally \nasynchronous/oblivious to checkpointing, so you&#39;d want to make sure any \nparticular file you&#39;d dropped there was completed (renamed to &#39;done&#39; \nsubdirectory) before creating a checkpoint.\n\nOften with large seed lists, large numbers of the seeds are \nobsolete/unresponsive hosts, which can also lead to large lulls in \ncrawl-rate. (They&#39;ll appear as queued URIs reasonably fast, but all of \nyour worker threads might be in some sort of slow-connection-timeout \n(20-seconds-plus) with unresponsive servers at once.) This can be \nespecially bad when the seed list is sorted and thousands of \nequivalently-unresponsive hosts are listed alongside each other. When \nyou see or expect this problem, some ways to minimize it are:\n\n� randomize the seed order\n� decrease configurable connection timeouts (fewer thread-seconds lost \nper failure)\n� decrease the retry counts (fewer repeated delays)\n� increase the slow-retry interval (essentially so more other URIs can \nbe tried in the time between tries of probably-hopeless URIs)\n� be a bit more aggressive on thread count, since many will be doing a \nslow (but not CPU/RAM-intensive) timeout for more of the crawl\n\n- Gordon\n\nOn 4/12/12 5:18 PM, Coram, Roger wrote:\n&gt;\n&gt;\n&gt; Hi Kris,\n&gt;\n&gt; Many thanks for the advice. We�ll amend our configuration and see what\n&gt; happens.\n&gt;\n&gt; Thanks,\n&gt;\n&gt; Roger\n&gt;\n&gt; *From:*archive-crawler@yahoogroups.com\n&gt; [mailto:archive-crawler@yahoogroups.com] *On Behalf Of *Kris Carpenter\n&gt; Negulescu\n&gt; *Sent:* 11 April 2012 23:22\n&gt; *To:* archive-crawler@yahoogroups.com\n&gt; *Cc:* Kris Carpenter Negulescu\n&gt; *Subject:* Re: [archive-crawler] Slow (?) loading millions of seeds\n&gt;\n&gt; You might also want to look at leveraging the Action Directory and\n&gt; splitting the seeds into multiple files that get loaded incrementally vs\n&gt; as a single monolithic file....\n&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Crawl+Operators#H3DevNotesforCrawlOperators-MillionsofseedsOK\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Crawl+Opera\n&gt; tors#H3DevNotesforCrawlOperators-MillionsofseedsOK&gt;\n&gt;\n&gt;\n&gt;       ActionDirectory for post-launch URI-loading\n&gt;\n&gt; A bean class ActionDirectory, if present in a crawl configuration (and\n&gt; it is recommended to become part of all standard configurations),\n&gt; watches a configured &#39;action&#39; directory for any files which appear\n&gt; (rechecking a configurable interval, default 30 seconds). For each file,\n&gt; an action is taken in accordance with the file&#39;s suffix, then the file\n&gt; is moved to a &#39;done&#39; directory.\n&gt;\n&gt; A file ending &#39;.seeds&#39; will trigger the addition of more seeds. A file\n&gt; ending &#39;.recover&#39; will be treated as a traditional recovery log -- with\n&gt; all &#39;Fs&#39; lines considered included (to suppress recrawling) then all\n&gt; &#39;F+&#39; lines rescheduled. A file ending &#39;.include&#39;, &#39;.schedule&#39;, or\n&gt; &#39;.force&#39; respectively will be treated as if a recovery-log format (with\n&gt; 3-character prefix tag per line), but all URIs listed (regardless of\n&gt; prefix-tag) will be considered-included, scheduled, or force-scheduled\n&gt; respectively.\n&gt;\n&gt; Any of these files may be gzip-compressed (with a &#39;.gz&#39; extension), and\n&gt; those in recovery-log-format may have a &#39;.s.&#39; inserted prior to the\n&gt; functional suffix (eg &#39;frontier.s.recover.gz&#39;) to indicate that prior to\n&gt; other steps, scoping should be attempted against the included URIs.\n&gt;\n&gt; Drop ping the proper files (possibly filtered) into this directory will\n&gt; likely be the recommended way to recover prior crawl frontier state, or\n&gt; perform other bulk adds to a running crawler.\n&gt;\n&gt; On Apr 11, 2012, at 3:15 PM, Kris Carpenter Negulescu wrote:\n&gt;\n&gt;\n&gt;\n&gt; For very large crawls you will want to consider using the\n&gt; HashCrawlMapper to spread the crawl over multiple crawl instances.\n&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Multiple+Machine+Crawling\n&gt;\n&gt; For the volume of seeds you describe, you will want to distribute across\n&gt; 10 or more crawl instances.\n&gt;\n&gt; Let us know if you need more detail/help in getting started.\n&gt;\n&gt; Kris\n&gt;\n&gt; Kris Carpenter Negulescu\n&gt; Director, Web Group\n&gt; Internet Archive\n&gt; kcarpenter@... &lt;mailto:kcarpenter@...&gt;\n&gt; skypeid: kris.carpenter\n&gt;\n&gt; On Apr 11, 2012, at 12:55 AM, Coram, Roger wrote:\n&gt;\n&gt;\n&gt;\n&gt; We�re trying to begin a slash-page crawl of 10,000,000+ seeds using\n&gt; Heritrix 3.0.0. While the initial few million load relatively quickly\n&gt; after a few hours the rate slows to a (pardon the pun) crawl. The time\n&gt; take thus far is now measurable in days.\n&gt;\n&gt; We�ve tried doubling the memory allocated to the JVM, switching to the\n&gt; late st version of Java and tried launching a crawl with Heritrix 3.1.0\n&gt; for comparison but the results don�t seem noticeably improved. Is this\n&gt; to be expected? How long should 10 or even 20 million seeds optimally\n&gt; take to read? Is there anything obvious we can do to improve things?\n&gt;\n&gt; Many thanks,\n&gt;\n&gt; Roger\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}