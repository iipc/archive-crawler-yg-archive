{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"bZvV4TSPn4X2cfCl5FnNVMzz59DyCKNJHR4IEb5uGfIi_2VrzXdGdH76r6wLrCMpasI1FoJ4t7tB7bAqZ08RjBKu546xjpk","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Re: Web-Scale Frontier","postDate":"1255567469","msgId":6107,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBRDY3MDZELjUwOTAyMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRBRDVGOUMzLjMwODA1MDJAYmF5YXJlYS5uZXQ+","referencesHeader":"PGhiNHFrOSttanZkQGVHcm91cHMuY29tPiA8NEFENUY5QzMuMzA4MDUwMkBiYXlhcmVhLm5ldD4="},"prevInTopic":6105,"nextInTopic":0,"prevInTime":6106,"nextInTime":6108,"topicId":6099,"numMessagesInTopic":6,"msgSnippet":"... I d love to be able to poke at a crawler that reached such a state to find out what was actually wrong, because none of the structures (as designed) of the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 35080 invoked from network); 15 Oct 2009 00:44:34 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m3.grp.sp2.yahoo.com with QMQP; 15 Oct 2009 00:44:34 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta3.grp.sp2.yahoo.com with SMTP; 15 Oct 2009 00:44:34 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 741E1483C0\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 14 Oct 2009 17:48:13 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id gWLIMgHVNoBR for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 14 Oct 2009 17:48:12 -0700 (PDT)\r\nX-Received: from [192.168.1.11] (cpe-70-112-233-153.austin.res.rr.com [70.112.233.153])\n\tby mail.archive.org (Postfix) with ESMTPSA id 83014483BB\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 14 Oct 2009 17:48:12 -0700 (PDT)\r\nMessage-ID: &lt;4AD6706D.5090205@...&gt;\r\nDate: Wed, 14 Oct 2009 17:44:29 -0700\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;hb4qk9+mjvd@...&gt; &lt;4AD5F9C3.3080502@...&gt;\r\nIn-Reply-To: &lt;4AD5F9C3.3080502@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Web-Scale Frontier\r\nX-Yahoo-Group-Post: member; u=137285340; y=8Mc2g4G_t7gmIg7inec4KDyUuhv-rlj9pI2dRmgoLPcv\r\nX-Yahoo-Profile: gojomo\r\n\r\nJohn Lekashman wrote:\n&gt; I used 16G memory, and 16 - 32 G swap.  After a few months,\n&gt; the crawls would all pretty much fall down at about 600 - 700M urls\n&gt; per system.  Swapping pretty heavily then, unfortunately, as the \n&gt; frontier grew.\n&gt; (downloaded.  The discovered frontiers varied widely.)\n&gt; \n&gt; Don&#39;t know the reason they all tended to have problems at that range,\n&gt; Gordon and I have talked about it a few times, as to where the\n&gt; problem lies.\n\nI&#39;d love to be able to poke at a crawler that reached such a state to \nfind out what was actually wrong, because none of the structures (as \ndesigned) of the crawl will grow to use an unbounded amount of RAM over \ntime.\n\nIn particular, the frontier won&#39;t use an unbounded amount of RAM. Its \nqueues are always disk-based. Its &#39;already-included&#39; structure is either \ndisk-based (BdbUriUniqFilter) or uses a constant amount of memory \n(BloomUriUniqFilter).\n\nThe Java max-heap setting ensures that the object space won&#39;t grow \nlarger than the configured max-heap, which should always be chosen to be \nless than physical RAM. (If object space tried to grow larger, there&#39;d \nbe an obvious &#39;OOME&#39; OutOfMemoryError in the logs, as opposed to \nnon-specific slowdowns and failures.)\n\nSwapping would have to be due to other non-object-heap memory \nconsumption on the machine -- perhaps the native memory usage of the \nJVM. There have been a number of JVM issues that cause excessive use of \nnative memory, or sluggish reclamation of such memory -- but the latest \nJVMs have relevant fixes, and we&#39;ve included workarounds in later \nHeritrix versions to avoid relying on unpredictable JVM/GC behavior.\n\nSo with (for example)...\n\n- Heritrix 1.14.3 or Heritrix 3-beta\n- a Java6 update-16 JVM\n- appropriate max-heap set at ~75-85% of physical RAM\n- reasonable toethread count (perhaps, 1 per 5MB of heap remaining after \nthe 60% BDB cache is considered)\n\n...I wouldn&#39;t expect to see any swapping or OOMEs, after runs of any \nduration. (If they were encountered, it&#39;d be a new and interesting -- \nand probably easy to resolve -- bug.)\n\nSlowdown from an ever-larger number of seeks/reads to do enqueues or \nalready-seen checks *would* be somewhat expected... but could also be \nminimized if the bloom-filter option is acceptable at the target scale.\n\n- Gordon @ IA\n\n\n&gt; John\n&gt; \n&gt; farbgeist wrote:\n&gt; \n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; Thanks for the quick answer Gordon!\n&gt;&gt;\n&gt;&gt;&gt; Not true; the crawl should continue without problems as long as you&#39;ve\n&gt;&gt;&gt; still got free disk space on whatever volume holds your crawl&#39;s &#39;state&#39;\n&gt;&gt;&gt; directory. Frontier operations -- testing URIs for prior inclusion, and\n&gt;&gt;&gt; enqueuing new URIs -- will become slower as they require seeks/reads\n&gt;&gt;&gt; over ever-larger disk structures.\n&gt;&gt;&gt;\n&gt;&gt; Do you have some examples of typical URIs/s on different \n&gt;&gt; configurations (memory/# cpus) for crawls that already use the disk?\n&gt;&gt; A first test of a broad crawl (4 hops, 400 toe-threads on a dual core \n&gt;&gt; Athlon 3000+ with 2Gb ram and 100Mb bandwith resulted in ~50 URIs/s \n&gt;&gt; using Heritrix 2.0.2 after ~ 2 hours which did not change \n&gt;&gt; significantly after 10 hours..\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt; For crawls where a single machine is expected to visit &gt; 100 million\n&gt;&gt;&gt; URIs, to avoid the slowdown as the crawl grows, we usually swap the\n&gt;&gt;&gt; disk-based already-included class (BdbUriUniqFilter) for an in-memory\n&gt;&gt;&gt; implementation (BloomUriUniqFilter) that doesn&#39;t slow over time, but\n&gt;&gt;&gt; instead has a small false-positive rate (that then grows if the filter\n&gt;&gt;&gt; becomes oversaturated). (The defaults, which are adjustable if you have\n&gt;&gt;&gt; more RAM, use 512MB to acheive a 1-in-4-million false-positive rate up\n&gt;&gt;&gt; through 125 million discovered URIs.)\n&gt;&gt;&gt;\n&gt;&gt; Where is the upper limit? Do you need 512MB every 125 million URIs or\n&gt;&gt; is the false positive rate increasing drastically?\n&gt;&gt;\n&gt;&gt;&gt; IA has done crawls up to 2 billion URIs with Heritrix using multiple\n&gt;&gt;&gt; machines, and we know of outside teams who have done crawls of over 8\n&gt;&gt;&gt; billion URIs using the same general techniques, which can be scaled\n&gt;&gt;&gt; further with more machines.\n&gt;&gt;&gt;\n&gt;&gt; Can you tell numbers of how many machines (RAM, cores, bandwith) you \n&gt;&gt; used in which timeframe for the 2 billion URIs? In that case you used \n&gt;&gt; the BdbUriUniqFilter, right?\n&gt;&gt;\n&gt;&gt;&gt; Separate but related: we&#39;ve long been interested in having a\n&gt;&gt;&gt; already-included structure matching that described in the Mercator\n&gt;&gt;&gt; papers (or as updated in the recent IRLbot paper), which would offer a\n&gt;&gt;&gt; disk-based structure that wouldn&#39;t slow as much with growth as our\n&gt;&gt;&gt; current implementation.\n&gt;&gt; (The manner in which candidate\n&gt;&gt;&gt; URIs are passed through a duplicate filter, allowing for batching and\n&gt;&gt;&gt; without the assumption of instant enqueuing, was designed to allow \n&gt;&gt; these\n&gt;&gt;&gt; techniques to be dropped-in when needed.)\n&gt;&gt;&gt;\n&gt;&gt; Do you believe HBase on a cluster could possibly act as a substitute \n&gt;&gt; to DRUM?\n&gt;&gt;\n&gt;&gt; My test-crawl resulted in less than 1/5 of bandwith usage (about \n&gt;&gt; 2000KB/s), while 1,600,000 pages where downloaded and ~ 5,000,000 \n&gt;&gt; queued. Why?\n&gt;&gt;\n&gt;&gt; best regards\n&gt;&gt; farbgeist\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}