{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"K1Ro_jhhv0kRKiOGd1tuDOmM1gSn-dttcJHrocOxLGIV4bJkNMk1EVZ1aDiqzJaVCFOjhL1e-I0bX-pVJ3OH4ORA2OMVPBk","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Stucked in 690 million discovered.","postDate":"1344466710","msgId":7743,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwMjJFRjE2LjQwMjA2MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDUwMjFBNEVBLjcwNjA1MDJAYmF5YXJlYS5uZXQ+","referencesHeader":"PGp1dTkwNyt1Zm1pQGVHcm91cHMuY29tPiA8NTAxMkNCNzguNDA4MDMwNUBiYXlhcmVhLm5ldD4gPDUwMjFBMEE2LjUwMDAyMDVAYXJjaGl2ZS5vcmc+IDw1MDIxQTRFQS43MDYwNTAyQGJheWFyZWEubmV0Pg=="},"prevInTopic":7740,"nextInTopic":0,"prevInTime":7742,"nextInTime":7744,"topicId":7731,"numMessagesInTopic":6,"msgSnippet":"I would guess that the case for you then (and for Elverton now) is a saturated Bloom filter that was never sized larger than the default. Our default is to","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 76737 invoked from network); 8 Aug 2012 22:58:34 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m11.grp.sp2.yahoo.com with QMQP; 8 Aug 2012 22:58:34 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta2.grp.sp2.yahoo.com with SMTP; 8 Aug 2012 22:58:33 -0000\r\nX-Received: (qmail 62471 invoked by uid 0); 8 Aug 2012 22:58:32 -0000\r\nX-Received: from 70.36.143.78 (HELO silverbook.local) (70.36.143.78)\n  by relay03.pair.com with SMTP; 8 Aug 2012 22:58:32 -0000\r\nX-pair-Authenticated: 70.36.143.78\r\nMessage-ID: &lt;5022EF16.4020604@...&gt;\r\nDate: Wed, 08 Aug 2012 15:58:30 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:14.0) Gecko/20120713 Thunderbird/14.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;juu907+ufmi@...&gt; &lt;5012CB78.4080305@...&gt; &lt;5021A0A6.5000205@...&gt; &lt;5021A4EA.7060502@...&gt;\r\nIn-Reply-To: &lt;5021A4EA.7060502@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Stucked in 690 million discovered.\r\nX-Yahoo-Group-Post: member; u=137285340; y=DL-GEhMrgknIJmkPIDrAYPgAuJu2EAJKiOoSW3WSeUAU\r\nX-Yahoo-Profile: gojomo\r\n\r\nI would guess that the case for you then (and for Elverton now) is a \nsaturated Bloom filter that was never sized larger than the default.\n\nOur default is to expect n=125,000,000 elements and use k=22 hash \nfunctions. A bit size m for which that number of hash functions is \noptimal given by:\n\n  m = n * k / ln(2)\n  m ≈ 3,967,411,363 bits\n\nThe approximate false positive probability for such a filter, given n&#39; \nactual inserts, is (per Wikipedia):\n\n  p ≈ 0.6185^(m/n&#39;)\n\nAt n&#39; = 700,000,000, p ≈ 92%, so even truly new URIs would be rejected \nas duplicateswith 92% probability. (And at that point in many crawls, \nmost outlink URIs found would already be true duplicates, and rejected \n100% since a Bloom filter does not give false-negatives.)\n\nThe discovered total would only increase very slowly, and regions of the \nweb graph of interest could be totally undiscoverable (if the only \noutlink URI paths from visited URIs to the other regions are all \nfalse-positives).\n\nOther than this mechanism, there&#39;s no designed-in hard-limit (other than \nexhausting available disk space) that would cap crawl size. If there&#39;s \nan unknown bug causing this, it would likely exist in H3 as well as H1, \nas the major data structures and URI-evaluation-flows are very similar.\n\nIf any sort of empirical limit is again observed, and can&#39;t be explained \nby a saturated Bloom filter, it&#39;s an anomaly that should get further \ninvestigation while observable. Some details worth capturing in such a \ncase would be:\n\n• the exact number of URIs fetched/queued/discovered, and whether the \ndiscovered total is *creeping up slowly* or *frozen at a specific \nnumber*. (More generally: the entire contents of all progress-statistics \nlogs would be useful.)\n\n• the full XML configuration\n\n• any alerts or exceptions evident in the heritrix_out.log\n\n- Gordon\n\nOn 8/7/12 4:29 PM, John Lekashman wrote:\n&gt; Hi Gordon,\n&gt;\n&gt; Yeah, there shouldn&#39;t be any such limit.\n&gt;\n&gt; Nonetheless, when I was much younger, and running 8B site\n&gt; crawls with H1, the crawlers always stopped at about 700M.\n&gt;\n&gt; Doubled memory, changed percents, etc.\n&gt; Still stopped there.  So, the empirical evidence is\n&gt; that yes, it does.\n&gt;\n&gt; John\n&gt;\n&gt;\n&gt; On 8/7/12 4:11 PM, Gordon Mohr wrote:\n&gt;&gt; There&#39;s no inherent limit in either H1 or H3 of the number of URIs a\n&gt;&gt; crawl can discover: if properly configured, with enough disk space, it\n&gt;&gt; will keep crawling... just slower and slower, as lookups on ever-larger\n&gt;&gt; disk structures take longer.\n&gt;&gt;\n&gt;&gt; However, in the standard configuration (using BdbUriUniqFilter), this\n&gt;&gt; can be get slow indeed, so broader crawls expected to grow into the tens\n&gt;&gt; or hundreds of millions of URIs often use the BloomUriUniqFilter\n&gt;&gt; instead. Bloom filters only use a fixed amount of main memory... and so\n&gt;&gt; can &#39;fill up&#39;: beyond their designed insert-size, their false-positive\n&gt;&gt; rate (saying a URI was seen when it wasn&#39;t) will climb, eventually to\n&gt;&gt; 100%.\n&gt;&gt;\n&gt;&gt; If you swapped in our BloomFilter option, and didn&#39;t change its default\n&gt;&gt; configuration (which used ~500MB of RAM and is size-optimized for 125\n&gt;&gt; million discovered URIs and 1-in-4-million false-positive rate), that\n&gt;&gt; might explain what you&#39;re seeing. (I believe even at almost 5x designed\n&gt;&gt; size, *some* URIs would still be considered new, but the numbers you&#39;ve\n&gt;&gt; included don&#39;t make it clear if the &#39;discovered&#39; count is frozen or just\n&gt;&gt; growing very slowly.)\n&gt;&gt;\n&gt;&gt; If that&#39;s not the problem, other things to consider:\n&gt;&gt;\n&gt;&gt; • maybe you really have discovered all URIs allowed by your scope and\n&gt;&gt; transitively linked from your seeds -- are you sure the expected &quot;2\n&gt;&gt; million&quot; domains are both (a) allowed by your scope; and (b) reachable\n&gt;&gt; by outlink paths that are at each hop allowed by your scope as well?\n&gt;&gt;\n&gt;&gt; • some sort of subtle error or data corruption which has allowed\n&gt;&gt; crawling to proceed but interfered with the enqueuing of new URIs. I\n&gt;&gt; don&#39;t know of any known errors that would cause this, but it&#39;s worth\n&gt;&gt; checking the alerts/error-logs/heritrix_out.log for any suspicious\n&gt;&gt; Exceptions/Errors.\n&gt;&gt;\n&gt;&gt; If it is the saturated BloomFilter issue, you would want to relaunch\n&gt;&gt; with a filter sized appropriately for your expected crawl size. (It\n&gt;&gt; can&#39;t be resized mid-crawl, and unless you were separately redundanly\n&gt;&gt; logging outlinks you&#39;ll have to revisit/re-extract many pages to\n&gt;&gt; discover the links that were discarded as false duplicates.) In H1, the\n&gt;&gt; filter-sizing parameters must be set via system properties -- see\n&gt;&gt; BloomUriUniqFilter(). In H3, it&#39;s a bit easier: the component can be\n&gt;&gt; swapped out or configured in the same way as others.\n&gt;&gt;\n&gt;&gt; If your crawl is even larger than your RAM allows a properly-sized\n&gt;&gt; filter, you might want to split it into separate non-overlapping crawls,\n&gt;&gt; dividing the URI space between separate machines (or serial runs). See\n&gt;&gt; discussion on the list and project wiki around &#39;HashCrawlMapper&#39; for\n&gt;&gt; some ways to achieve this. (Again, it&#39;s a bit easier in H3.)\n&gt;&gt;\n&gt;&gt; - Gordon\n&gt;&gt;\n&gt;&gt; On 7/27/12 10:10 AM, John Lekashman wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt; Hi,\n&gt;&gt;&gt; You still on Heretrix 1?\n&gt;&gt;&gt;\n&gt;&gt;&gt; I know that H 1 had a problem of an upper limit of around 700M urls per\n&gt;&gt;&gt; crawler.\n&gt;&gt;&gt; Split the crawl with a hashmapper.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Don&#39;t know if H 3 has that problem, I always split those as well, to get\n&gt;&gt;&gt; things done\n&gt;&gt;&gt; in finite time.\n&gt;&gt;&gt;\n&gt;&gt;&gt; John\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 7/27/12 7:35 AM, Elverton wrote:\n&gt;&gt;&gt;&gt; Hello everybody.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Well, I&#39;m having a big trouble this time. Before I explain the\n&gt;&gt;&gt;&gt; problem, here is the system configuration:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - 24 GB RAM\n&gt;&gt;&gt;&gt; - Intel(R) Xeon(R) CPU E5520 @ 2.27GHz\n&gt;&gt;&gt;&gt; - 1.8TB hard disk for Heritrix. (I don&#39;t use warc in this crawl. My\n&gt;&gt;&gt;&gt; only target is to know how many (approx.) URLs a domain has.) The\n&gt;&gt;&gt;&gt; usage of the disk is: used 500GB, free 1.3TB.\n&gt;&gt;&gt;&gt; - 16GB java heap size for heritrix.\n&gt;&gt;&gt;&gt; - Java 1.7.0_05\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Here is the Heritrix configuration that I consider helpful to the\n&gt;&gt;&gt;&gt; problem:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - bdb-cache-percent = 25\n&gt;&gt;&gt;&gt; - frontier = BdbFrontier\n&gt;&gt;&gt;&gt; - max-delay-ms = 10000\n&gt;&gt;&gt;&gt; - min-delay-ms = 2000\n&gt;&gt;&gt;&gt; - respect-crawl-delay-up-to-secs = 300\n&gt;&gt;&gt;&gt; - max-retries = 10\n&gt;&gt;&gt;&gt; - retry-delay-seconds = 30\n&gt;&gt;&gt;&gt; - timeout-seconds = 1200\n&gt;&gt;&gt;&gt; - sotimeout-ms = 20000\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; % ----------------------------------------------------------\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; So, my problem is: the crawl stucked in 690 million discovered.\n&gt;&gt;&gt;&gt; (Queued it&#39;s around 520 million and downloaded is around 170 million).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; The strange thing is the download/uri rate.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Docs/s(avg): 53.2(60.77)\n&gt;&gt;&gt;&gt; KB/s(avg): 2069(3205)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; It continues, in some way, good in theory (about 3 or 4 million uri\n&gt;&gt;&gt;&gt; crawled per day if you have 53.2 uri&#39;s during all day), but the real\n&gt;&gt;&gt;&gt; crawled per day is below 500.000 (discovered).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Looking at some number in the last five days:\n&gt;&gt;&gt;&gt; Queued Downloaded\n&gt;&gt;&gt;&gt; 541054381 133121289\n&gt;&gt;&gt;&gt; 535322185 138522175\n&gt;&gt;&gt;&gt; 530280577 143176680\n&gt;&gt;&gt;&gt; 525907149 147086865\n&gt;&gt;&gt;&gt; 520568517 151604201\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Notice that the queued decreases at the &quot;same&quot; rate that downloaded\n&gt;&gt;&gt;&gt; increases. The problem could be getting URIs to the queue. A possible\n&gt;&gt;&gt;&gt; is the URIs discovered now had be crawled before, and doesn&#39;t go the\n&gt;&gt;&gt;&gt; the queue anymore. But the domain I&#39;m crawling has about 2 million\n&gt;&gt;&gt;&gt; domains and I got only 70.000, so there&#39;re many URI&#39;s to be crawled\n&gt;&gt;&gt;&gt; yet. :)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Other possibility I thought could be a swap problem (too much I/O).\n&gt;&gt;&gt;&gt; For my surprise (using vmstat), the swpd is 0.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Another problem could be know if a URI was crawled already.\n&gt;&gt;&gt;&gt; Before the URI goes to the frontier, heritrix verifies it in a queue,\n&gt;&gt;&gt;&gt; using the hash technique. If the crawling is big enough, the search\n&gt;&gt;&gt;&gt; get slower, even using hash, because there are many URI&#39;s for a key in\n&gt;&gt;&gt;&gt; hash table.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; But, I really don&#39;t know the exactly problem. Anyone had this problem\n&gt;&gt;&gt;&gt; or could point a direction?\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Thanks,\n&gt;&gt;&gt;&gt; Elverton.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n\n"}}