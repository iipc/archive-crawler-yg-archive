{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"7DjU2JekxgXDpaFEvGBdgzWh_hyPoNbaH5c-ODRo8FFZIeDpAQ-hoG9HZszgMxAkdzZAfkzt2jr6jRqcAGsO3yHtvvmL1D4","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling","postDate":"1176421765","msgId":4106,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2MUVDNTg1LjkwNzA2MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGV2bHI2ZStuc2MyQGVHcm91cHMuY29tPg==","referencesHeader":"PGV2bHI2ZStuc2MyQGVHcm91cHMuY29tPg=="},"prevInTopic":4103,"nextInTopic":4228,"prevInTime":4105,"nextInTime":4107,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"It s a trap! The site is trickling out bytes about 1 per second, so the socket timeout isn t triggering. Additionally, the HTTPClient library we use has a bug","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 95306 invoked from network); 12 Apr 2007 23:47:32 -0000\r\nReceived: from unknown (66.218.66.72)\n  by m36.grp.scd.yahoo.com with QMQP; 12 Apr 2007 23:47:32 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta14.grp.scd.yahoo.com with SMTP; 12 Apr 2007 23:47:32 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 8ED4F1416BCA8\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 12 Apr 2007 16:46:52 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 27018-08-43 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 12 Apr 2007 16:46:52 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id E20541416BC7F\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 12 Apr 2007 16:46:51 -0700 (PDT)\r\nMessage-ID: &lt;461EC585.9070604@...&gt;\r\nDate: Thu, 12 Apr 2007 16:49:25 -0700\r\nUser-Agent: Thunderbird 1.5.0.10 (X11/20070306)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;evlr6e+nsc2@...&gt;\r\nIn-Reply-To: &lt;evlr6e+nsc2@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=137285340; y=fnZ3HwjVrfq_cC0AJzRkWEjvImlm1VbguaPKc6DmL2Mk\r\nX-Yahoo-Profile: gojomo\r\n\r\nIt&#39;s a trap!\n\nThe site is trickling out bytes about 1 per second, so the socket \ntimeout isn&#39;t triggering.\n\nAdditionally, the HTTPClient library we use has a bug when dealing with \nboundlessly-sized HTTP header information -- it never gives up and has \nno facility for a user-chosen practical limit.\n\nWe&#39;ve previously used a workaround in our passthrough stream recording \ncode: if there&#39;s not a clear end to the header material within a certain \nsize limit (independent of the operator configured size/time limits), \nthrow an exception in complaint that aborts the fetch.\n\nUnfortunately, that hard-coded size limit is 1MB. At a byte-per-second, \nit will take 11 days for that limit to be hit. :(\n\nI&#39;m looking at other workarounds, such as a way to consistently apply \nthe operator&#39;s length/time/rate limits even to the header-parsing.\n\n- Gordon @ IA\n\njoehung302 wrote:\n&gt; This morning I was trying to pause a crawler and was not able to \n&gt; after a long time (20+ minutes?). I looked at the toethread report \n&gt; from the console and found the following trace.\n&gt; \n&gt; From past experience whenever I cannot pause a thread, usually there \n&gt; was some sort of parsing loop on bad pages. But this one is \n&gt; concerning to me. I would think that the thread really should stop. \n&gt; The un-predicability of not being able to pause the crawl really \n&gt; impact our ability to automate the checkpoint and diversion during a \n&gt; big crawl.\n&gt; \n&gt; Anything info that you need for this problem?\n&gt; \n&gt; ================\n&gt;    ToeThread #6\n&gt; [ToeThread #6: http://even42.cs.ohiou.edu/robots.txt\n&gt;  CrawlURI http://even42.cs.ohiou.edu/robots.txt LLLLLLLLLLLLP \n&gt; http://even42.cs.ohiou.edu/~osterman/dpierce/    0 attempts\n&gt;     in processor: HTTP\n&gt;     ACTIVE for 19h2m28s490ms\n&gt;     step: ABOUT_TO_BEGIN_PROCESSOR for 19h2m28s490ms\n&gt;     java.net.SocketInputStream.socketRead0(Native Method)\n&gt;     java.net.SocketInputStream.read(SocketInputStream.java:129)\n&gt;     java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n&gt;     java.io.BufferedInputStream.read(BufferedInputStream.java:235)\n&gt;     org.archive.io.RecordingInputStream.read\n&gt; (RecordingInputStream.java:100)\n&gt;     org.apache.commons.httpclient.HttpParser.readRawLine\n&gt; (HttpParser.java:78)\n&gt;     org.apache.commons.httpclient.HttpParser.readLine\n&gt; (HttpParser.java:106)\n&gt;     org.apache.commons.httpclient.HttpConnection.readLine\n&gt; (HttpConnection.java:1144)\n&gt;     org.apache.commons.httpclient.HttpMethodBase.readStatusLine\n&gt; (HttpMethodBase.java:1852)\n&gt;     org.apache.commons.httpclient.HttpMethodBase.readResponse\n&gt; (HttpMethodBase.java:1612)\n&gt;     org.apache.commons.httpclient.HttpMethodBase.execute\n&gt; (HttpMethodBase.java:1002)\n&gt;     org.archive.httpclient.HttpRecorderGetMethod.execute\n&gt; (HttpRecorderGetMethod.java:116)\n&gt;     org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry\n&gt; (HttpMethodDirector.java:397)\n&gt;     org.apache.commons.httpclient.HttpMethodDirector.executeMethod\n&gt; (HttpMethodDirector.java:170)\n&gt;     org.apache.commons.httpclient.HttpClient.executeMethod\n&gt; (HttpClient.java:396)\n&gt;     org.apache.commons.httpclient.HttpClient.executeMethod\n&gt; (HttpClient.java:346)\n&gt;     org.archive.crawler.fetcher.FetchHTTP.innerProcess\n&gt; (FetchHTTP.java:489)\n&gt;     org.archive.crawler.framework.Processor.process\n&gt; (Processor.java:109)\n&gt;     org.archive.crawler.framework.ToeThread.processCrawlUri\n&gt; (ToeThread.java:302)\n&gt;     org.archive.crawler.framework.ToeThread.run(ToeThread.java:151)\n&gt; ]\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}