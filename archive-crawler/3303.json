{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"H2pFhQNMeq020x5OSe9TfbYB22b-1N5R3pX6GO5wAkhfsOgmaEyS3jMMrg7nl6dzNJwglXjHkfZm3qv4MG-lfzI5mzRCupU","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] howto neverending job?","postDate":"1158362747","msgId":3303,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1MEIzNjdCLjgwNTA2MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDk3ODJlMzM1MDYwOTE1MTUyN3YzZDFmOWNjM2xhNmI3Zjc3ZTk3MmVmNzdkQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PDk3ODJlMzM1MDYwOTE1MTUyN3YzZDFmOWNjM2xhNmI3Zjc3ZTk3MmVmNzdkQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":3302,"nextInTopic":3307,"prevInTime":3302,"nextInTime":3304,"topicId":3301,"numMessagesInTopic":4,"msgSnippet":"... You could try the pause-at-finish option. Then, after adding new URIs, you d have to also unpause the crawl. As it stands now, an crawl with no pending","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 42959 invoked from network); 15 Sep 2006 23:22:42 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m29.grp.scd.yahoo.com with QMQP; 15 Sep 2006 23:22:42 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta1.grp.scd.yahoo.com with SMTP; 15 Sep 2006 23:22:42 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 1C0681415FE34\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 15 Sep 2006 16:22:08 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 06096-02-94 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tFri, 15 Sep 2006 16:22:07 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-71-198-60-165.hsd1.ca.comcast.net [71.198.60.165])\n\tby mail.archive.org (Postfix) with ESMTP id D6B001415FDE6\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 15 Sep 2006 16:22:07 -0700 (PDT)\r\nMessage-ID: &lt;450B367B.8050602@...&gt;\r\nDate: Fri, 15 Sep 2006 16:25:47 -0700\r\nUser-Agent: Thunderbird 1.5.0.5 (X11/20060728)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;9782e3350609151527v3d1f9cc3la6b7f77e972ef77d@...&gt;\r\nIn-Reply-To: &lt;9782e3350609151527v3d1f9cc3la6b7f77e972ef77d@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] howto neverending job?\r\nX-Yahoo-Group-Post: member; u=137285340; y=qNeRQErfVFZ_GdayGwaYuatlpkSRosPvn03hWZIDG_tA\r\nX-Yahoo-Profile: gojomo\r\n\r\nLaurian Gridinoc wrote:\n&gt; Hello,\n&gt; \n&gt; I would like to have a crawl job that won&#39;t end when there is nothing\n&gt; more to crawl, but just sit there and wait for any JMX injected URIs.\n&gt; Somethink like the AdaptiveRevitsFrontier, but with a BdbFrontier (so,\n&gt; I can use JMX). Any clue how to do it?\n\nYou could try the &#39;pause-at-finish&#39; option. Then, after adding new URIs, \nyou&#39;d have to also unpause the crawl.\n\nAs it stands now, an crawl with no pending URIs must either terminate or \npause. (If it pauses, and you unpause it without adding URIs, it will \nterminate.)\n\n- Gordon @ IA\n\n\n"}}