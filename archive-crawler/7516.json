{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264138474,"authorName":"Kenji Nagahashi","from":"Kenji Nagahashi &lt;knagahashi@...&gt;","profile":"kenznag","replyTo":"LIST","senderId":"WQJ7j2U_5M-AFqez-VsX6oVbAlYTM_yZi0SfWirtAQfvzPA0ucXiCk9WLlO-sRSukGUI-Zh3Ix6ypqGkBXl0Mmvuk6DI52vqjTlbPmg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Question about H3 crawl management","postDate":"1326392552","msgId":7516,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMEYyNEU4LjQwMzAwMDBAZ21haWwuY29tPg==","inReplyToHeader":"PDRGMENEQkM1LjQwNzA3MDBAY3MuY211LmVkdT4=","referencesHeader":"PDRGMEM3NjRFLjQwMDA3MDBAY3MuY211LmVkdT4gPDRGMEM4QjQ4LjQwMjAwMDlAYmF5YXJlYS5uZXQ+IDw0RjBDQzlEMy4yMDUwMTA0QGNzLmNtdS5lZHU+IDw0RjBDRDgyOS45MDMwODAxQGJheWFyZWEubmV0PiA8NEYwQ0RCQzUuNDA3MDcwMEBjcy5jbXUuZWR1Pg=="},"prevInTopic":7509,"nextInTopic":7521,"prevInTime":7515,"nextInTime":7517,"topicId":7505,"numMessagesInTopic":7,"msgSnippet":"Hi David, That s because H3 uses self-signed certificate by default. Take a look at this page for a solution: ","rawEmail":"Return-Path: &lt;knagahashi@...&gt;\r\nX-Sender: knagahashi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 99643 invoked from network); 12 Jan 2012 18:22:37 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m7.grp.sp2.yahoo.com with QMQP; 12 Jan 2012 18:22:37 -0000\r\nX-Received: from unknown (HELO mail-tul01m020-f179.google.com) (209.85.214.179)\n  by mta5.grp.sp2.yahoo.com with SMTP; 12 Jan 2012 18:22:37 -0000\r\nX-Received: by obbuo13 with SMTP id uo13so1069736obb.38\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 12 Jan 2012 10:22:36 -0800 (PST)\r\nX-Received: by 10.50.76.162 with SMTP id l2mr1577432igw.1.1326392556368;\n        Thu, 12 Jan 2012 10:22:36 -0800 (PST)\r\nReturn-Path: &lt;knagahashi@...&gt;\r\nX-Received: from kenji-mbp.local (router300.sf.archive.org. [208.70.27.190])\n        by mx.google.com with ESMTPS id va6sm9935354igc.6.2012.01.12.10.22.34\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Thu, 12 Jan 2012 10:22:35 -0800 (PST)\r\nMessage-ID: &lt;4F0F24E8.4030000@...&gt;\r\nDate: Thu, 12 Jan 2012 10:22:32 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;4F0C764E.4000700@...&gt; &lt;4F0C8B48.4020009@...&gt; &lt;4F0CC9D3.2050104@...&gt; &lt;4F0CD829.9030801@...&gt; &lt;4F0CDBC5.4070700@...&gt;\r\nIn-Reply-To: &lt;4F0CDBC5.4070700@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nSubject: Re: [archive-crawler] Question about H3 crawl management\r\nX-Yahoo-Group-Post: member; u=264138474; y=4jv5bYo1Uiua9bzeiHc83kEXsUUecWID4QqyDXEYK8YJaw\r\nX-Yahoo-Profile: kenznag\r\n\r\nHi David,\n\nThat&#39;s because H3 uses self-signed certificate by default. Take a look \nat this page for a solution: \nhttp://stackoverflow.com/questions/6795030/how-to-ignore-certificate-verify-failed-error-in-perl\n\nCheers,\n--Kenji\n\n(1/10/12 4:45 PM), David Pane wrote:\n&gt; John,\n&gt;\n&gt; I am only interested in the successful mime-type: text/html pages.  The\n&gt; mimetype-report.txt report generates this information.  The\n&gt; crawl-report.txt has the total crawled bytes.\n&gt;\n&gt; I am attempting to generate these reports on a regular basis.  The only\n&gt; way that I know how is by requesting these urls for each crawler.\n&gt;\n&gt; https://crawler_ipaddress:8443/engine/job/bigcrawl/report/CrawlSummaryReport\n&gt; https://crawler_ipaddress:8443/engine/job/bigcrawl/report/MimetypesReport\n&gt;\n&gt; Using Perl LWP I get these errors:\n&gt;\n&gt; Can&#39;t connect to crawler_ipaddress:8443 (certificate verify failed)\n&gt;\n&gt; LWP::Protocol::https::Socket: SSL connect attempt failed with unknown\n&gt; errorerror:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate\n&gt;\n&gt; --David\n&gt;\n&gt; On 1/10/2012 7:30 PM, John Lekashman wrote:\n&gt;&gt; Hi David,\n&gt;&gt; Why would ssl have anything to do with it?\n&gt;&gt; Trying to read the UI from a script isn&#39;t the best way,\n&gt;&gt; but good luck if you must.\n&gt;&gt;\n&gt;&gt; A script:\n&gt;&gt; Now, this is pretty rough, and if you are generating various\n&gt;&gt; versions of crawl.log, you have to deal with it.\n&gt;&gt;\n&gt;&gt; # # of pages. Run wc on crawl log and add &#39;em up.\n&gt;&gt; #!/bin/bash\n&gt;&gt;\n&gt;&gt; rm /tmp/$sometmpfilename\n&gt;&gt; for i in a1 a2 a3 a4 a5 # or `cat my_list_of_crawlers`\n&gt;&gt; do\n&gt;&gt; ssh $i wc /home/crawl/logs/crawl.log | awk &#39;{print $1}&#39;&gt;&gt;\n&gt;&gt; /tmp/$sometmpfilename\n&gt;&gt; echo &quot;+&quot;&gt;&gt;  /tmp/$sometmpfilename\n&gt;&gt; done\n&gt;&gt;\n&gt;&gt; echo &quot;+p&quot;&gt;&gt;  /tmp/$sometmpfilename\n&gt;&gt;\n&gt;&gt; cat /tmp/$sometmpfilename | dc\n&gt;&gt;\n&gt;&gt; And what you do with the output of cat, well, you figure that out.\n&gt;&gt;\n&gt;&gt; # Total size of crawled data. Pretty much the same thing, although you\n&gt;&gt; look at the third field in crawl.log\n&gt;&gt; Instead of wc use something like this: the grep -v removes the &#39;-&#39; from\n&gt;&gt; 0 length records, which would confuse\n&gt;&gt; the output.\n&gt;&gt;\n&gt;&gt; ssh $i cat crawl.log | awk &#39;{print $3}&#39; | grep -v &#39;-&#39;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; On 1/10/12 3:29 PM, David Pane wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thank you for your responses John.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Can you be more specific about your thoughts on writing a script to\n&gt;&gt;&gt; generate these? I have tried to generate the mime report and crawl\n&gt;&gt;&gt; summary reports using perl LWP, but haven&#39;t figured out how to deal with\n&gt;&gt;&gt; the SSL certificates.\n&gt;&gt;&gt;\n&gt;&gt;&gt; --David\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; 5) Although I can capture the below statistics manually, can you\n&gt;&gt;&gt; suggest\n&gt;&gt;&gt;&gt;&gt; a way that I can automatically generate/collect the following\n&gt;&gt;&gt; statistics\n&gt;&gt;&gt;&gt;&gt; from the crawl. I would like to generate this data at least once every\n&gt;&gt;&gt;&gt;&gt; 24 hours and possibly as often as every hour.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Well, you could write a script to do it.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; a) Total size of crawled data.\n&gt;&gt;&gt;&gt;&gt; b) total number of pages crawled (mime-type: text/html).\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n"}}