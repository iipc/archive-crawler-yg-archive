{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":215515511,"authorName":"Mike Schwartz","from":"Mike Schwartz &lt;mschwartz@...&gt;","profile":"mfschwartz","replyTo":"LIST","senderId":"fvnuRE3DnU87muH8K5eg9MHJ9eozX8OSNL1upAiP9Lv9S8rnRSNzokRr1O1_Ps8Uytv0bsXG4qOflQJE-bbr8svoIhkCutDa9oc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] keeping threads active during crawls","postDate":"1109707943","msgId":1633,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMi4wLjE0LjIuMjAwNTAzMDExMzAzMDEuMDJiMDVmMzhAdmhvc3Q2LmF0b21pY3NlcnZlcnMuY29tPg==","inReplyToHeader":"PDQyMjRDNjMzLjUwNzA3MDBAYXJjaGl2ZS5vcmc+","referencesHeader":"PDYuMi4wLjE0LjIuMjAwNTAyMjgxNDA1MzAuMDJiM2YxZThAdmhvc3Q2LmF0b21pY3NlcnZlcnMuY29tPiA8NDIyNEM2MzMuNTA3MDcwMEBhcmNoaXZlLm9yZz4="},"prevInTopic":1632,"nextInTopic":1634,"prevInTime":1632,"nextInTime":1634,"topicId":1622,"numMessagesInTopic":8,"msgSnippet":"I have enough machine resources to start up another crawler instance (once it gets to this low thread parallelism state the CPU consumption drops way down) the","rawEmail":"Return-Path: &lt;mschwartz@...&gt;\r\nX-Sender: mschwartz@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 93803 invoked from network); 1 Mar 2005 20:13:28 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m17.grp.scd.yahoo.com with QMQP; 1 Mar 2005 20:13:28 -0000\r\nReceived: from unknown (HELO vhost6.atomicservers.com) (216.58.160.194)\n  by mta4.grp.scd.yahoo.com with SMTP; 1 Mar 2005 20:13:28 -0000\r\nReceived: from dev4lt.aptas.com (nat1.aptas.com [64.78.237.254] (may be forged))\n\t(authenticated (0 bits))\n\tby vhost6.atomicservers.com (8.11.6/8.11.6) with ESMTP id j21KCS129928\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 1 Mar 2005 13:12:28 -0700\r\nMessage-Id: &lt;6.2.0.14.2.20050301130301.02b05f38@...&gt;\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.2.0.14\r\nDate: Tue, 01 Mar 2005 13:12:23 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;4224C633.5070700@...&gt;\r\nReferences: &lt;6.2.0.14.2.20050228140530.02b3f1e8@...&gt;\n &lt;4224C633.5070700@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;=====================_1382739921==.ALT&quot;\r\nX-eGroups-Remote-IP: 216.58.160.194\r\nFrom: Mike Schwartz &lt;mschwartz@...&gt;\r\nSubject: Re: [archive-crawler] keeping threads active during crawls\r\nX-Yahoo-Group-Post: member; u=215515511\r\nX-Yahoo-Profile: mfschwartz\r\n\r\n\r\n--=====================_1382739921==.ALT\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\n\r\nI have enough machine resources to start up another crawler instance (once \nit gets to this low thread parallelism state the CPU consumption drops way \ndown)\n\nthe issue for me is really that once we get to this state, it just takes a \nlong time to finish crawling.  And having to do that for each DomainScope&#39;d \ncrawl in sequence means it will take a very long time to get thru all the \ncrawling.\n\nI have a new strategy I&#39;m going to try, which is basically:\n    * run crawl until it gets to low parallelism state, then kill that \ncrawl, extracting the list of domains that were still not completely crawled\n    * do the above for each of my DomainScope&#39;d crawls, and at the end put \ntogether the list of all domains not completely crawled, and start a new \ncrawl with them.  At that point there should hopefully be enough of them to \nagain have good thread parallelism (sort of a refactoring of crawl seeds)\n\nbasically, that final composite crawl should consist of all the domains for \nwhich there are many individual URLs to retrieve and/or the site is \nresponding very slowly, with the result that we only wait for the long tail \nto complete once at the end of all DomainScope crawls, rather than once for \neach DomainScope crawl.\n\nthis will mean more manual admin effort, but I think once I gain experience \nwith this approach I can automate the detection and refactoring of the \ncrawls (especially once the checkpointing mechanism you mentioned is \nimplemented).\n\nthanks\n  - Mike\n\n\nAt 12:44 PM 3/1/2005, you wrote:\n&gt;Mike Schwartz wrote:\n&gt;\n&gt; &gt; hi,\n&gt; &gt;\n&gt; &gt; I need to run a series of DomainScope crawls.  I notice that each such\n&gt; &gt; crawl gets good thread parallelism for quite a while but then gets to\n&gt; &gt; a point where most of what&#39;s left to do is crawling many pages within\n&gt; &gt; a small number of sites (e.g., all the product pages at each of a few\n&gt; &gt; sites).  At that point only a few points are active, and make very\n&gt; &gt; slow progress because I only visit each site once every few seconds.\n&gt;\n&gt; &gt;\n&gt; &gt; This problem wouldn&#39;t arise if I were doing a broad-scope crawl, since\n&gt; &gt; at any point in time there are more sites left to crawl than there are\n&gt; &gt; available threads.\n&gt; &gt;\n&gt; &gt; Does anyone have a suggestion how I could keep most/all of the threads\n&gt; &gt; active during a sequence of DomainScope crawls?  I could try adding a\n&gt; &gt; new set of sites when I get down to the state of many pages left\n&gt; &gt; within just a few sites, but it seems to me that&#39;s a problem because\n&gt; &gt; in essence I&#39;m running one much larger crawl instead of a set of\n&gt; &gt; limited scope crawls - and if that crawl gets into a bad state and I\n&gt; &gt; have to kill it, I don&#39;t end up knowing exactly which parts of which\n&gt; &gt; sites have completed being crawled.\n&gt;\n&gt;We&#39;re looking into adding accounting that will allow the running of\n&gt;multiple crawls inside of a single running instance -- More to follow on\n&gt;this after it gets flushed out -- but until then, running an instance\n&gt;per crawl seems to be your only option.\n&gt;\n&gt;Do you have sufficent resources to start up a new crawl instance to go\n&gt;against a new domain on the machine that has the trailing-off crawl\n&gt;running on it?  Or are your crawls up against Heritrix bounds?\n&gt;\n&gt;When we have a checkpointing system in place, you&#39;ll be able to\n&gt;checkpoint the dying crawl, stop it, and then restart it inside of a\n&gt;smaller heap letting it crawl to completion making room to run the new\n&gt;crawl.\n&gt;\n&gt;St.Ack\n&gt;\n&gt; &gt;\n&gt; &gt; (I&#39;ve tried restarting crawls from recover.gz, but haven&#39;t been\n&gt; &gt; sucessful with that - I get out-of-memory errors, even when I set the\n&gt; &gt; JVM to have 1 GB of RAM or more.)\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; &gt;\n&gt; &gt; thanks\n&gt; &gt;  - Mike Schwartz\n&gt; &gt;    Aptas, Inc.\n&gt; &gt; *Yahoo! Groups Sponsor*\n&gt; &gt; ADVERTISEMENT\n&gt; &gt; click here\n&gt; &gt; \n&gt; &lt;&lt;http://us.ard.yahoo.com/SIG=129jkuth1/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109711161/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&gt;http://us.ard.yahoo.com/SIG=129jkuth1/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109711161/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&gt; \n&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ------------------------------------------------------------------------\n&gt; &gt; *Yahoo! Groups Links*\n&gt; &gt;\n&gt; &gt;     * To visit your group on the web, go to:\n&gt; &gt; \n&gt; &lt;http://groups.yahoo.com/group/archive-crawler/&gt;http://groups.yahoo.com/group/archive-crawler/\n&gt; &gt;\n&gt; &gt;     * To unsubscribe from this group, send an email to:\n&gt; &gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt; &gt; \n&gt; &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt; &gt;\n&gt; &gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt; &gt;       Service \n&gt; &lt;&lt;http://docs.yahoo.com/info/terms/&gt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Sponsor\n&gt;ADVERTISEMENT\n&gt;&lt;http://us.ard.yahoo.com/SIG=129e0c6q6/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109793222/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&gt;\n&gt;click here\n&gt;\n&gt;\n&gt;\n&gt;----------\n&gt;Yahoo! Groups Links\n&gt;    * To visit your group on the web, go to:\n&gt;    * \n&gt; &lt;http://groups.yahoo.com/group/archive-crawler/&gt;http://groups.yahoo.com/group/archive-crawler/ \n&gt;\n&gt;    *\n&gt;    * To unsubscribe from this group, send an email to:\n&gt;    * \n&gt; &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;archive-crawler-unsubscribe@yahoogroups.com \n&gt;\n&gt;    *\n&gt;    * Your use of Yahoo! Groups is subject to the \n&gt; &lt;http://docs.yahoo.com/info/terms/&gt;Yahoo! Terms of Service.\n\r\n--=====================_1382739921==.ALT\r\nContent-Type: text/html; charset=&quot;us-ascii&quot;\r\n\r\n&lt;html&gt;\n&lt;body&gt;\nI have enough machine resources to start up another crawler instance\n(once it gets to this low thread parallelism state the CPU consumption\ndrops way down)&lt;br&gt;&lt;br&gt;\nthe issue for me is really that once we get to this state, it just takes\na long time to finish crawling.&nbsp; And having to do that for each\nDomainScope&#39;d crawl in sequence means it will take a very long time to\nget thru all the crawling.&lt;br&gt;&lt;br&gt;\nI have a new strategy I&#39;m going to try, which is basically:\n&lt;ul&gt;\n&lt;li&gt;run crawl until it gets to low parallelism state, then kill that\ncrawl, extracting the list of domains that were still not completely\ncrawled\n&lt;li&gt;do the above for each of my DomainScope&#39;d crawls, and at the end put\ntogether the list of all domains not completely crawled, and start a new\ncrawl with them.&nbsp; At that point there should hopefully be enough of\nthem to again have good thread parallelism (sort of a refactoring of\ncrawl seeds)\n&lt;/ul&gt;&lt;br&gt;\nbasically, that final composite crawl should consist of all the domains\nfor which there are many individual URLs to retrieve and/or the site is\nresponding very slowly, with the result that we only wait for the long\ntail to complete once at the end of all DomainScope crawls, rather than\nonce for each DomainScope crawl.&lt;br&gt;&lt;br&gt;\nthis will mean more manual admin effort, but I think once I gain\nexperience with this approach I can automate the detection and\nrefactoring of the crawls (especially once the checkpointing mechanism\nyou mentioned is implemented).&lt;br&gt;&lt;br&gt;\nthanks&lt;br&gt;\n&nbsp;- Mike&lt;br&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;font size=3&gt;At 12:44 PM 3/1/2005, you wrote:&lt;br&gt;\n&lt;/font&gt;&lt;blockquote type=cite class=cite cite=&quot;&quot;&gt;&lt;tt&gt;Mike Schwartz\nwrote:&lt;br&gt;&lt;br&gt;\n&gt; hi,&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; I need to run a series of DomainScope crawls.&nbsp; I notice that\neach such &lt;br&gt;\n&gt; crawl gets good thread parallelism for quite a while but then gets\nto &lt;br&gt;\n&gt; a point where most of what&#39;s left to do is crawling many pages\nwithin &lt;br&gt;\n&gt; a small number of sites (e.g., all the product pages at each of a\nfew &lt;br&gt;\n&gt; sites).&nbsp; At that point only a few points are active, and make\nvery &lt;br&gt;\n&gt; slow progress because I only visit each site once every few\nseconds.&lt;br&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; This problem wouldn&#39;t arise if I were doing a broad-scope crawl,\nsince &lt;br&gt;\n&gt; at any point in time there are more sites left to crawl than there\nare &lt;br&gt;\n&gt; available threads.&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; Does anyone have a suggestion how I could keep most/all of the\nthreads &lt;br&gt;\n&gt; active during a sequence of DomainScope crawls?&nbsp; I could try\nadding a &lt;br&gt;\n&gt; new set of sites when I get down to the state of many pages left\n&lt;br&gt;\n&gt; within just a few sites, but it seems to me that&#39;s a problem because\n&lt;br&gt;\n&gt; in essence I&#39;m running one much larger crawl instead of a set of\n&lt;br&gt;\n&gt; limited scope crawls - and if that crawl gets into a bad state and I\n&lt;br&gt;\n&gt; have to kill it, I don&#39;t end up knowing exactly which parts of which\n&lt;br&gt;\n&gt; sites have completed being crawled.&lt;br&gt;&lt;br&gt;\nWe&#39;re looking into adding accounting that will allow the running of &lt;br&gt;\nmultiple crawls inside of a single running instance -- More to follow on\n&lt;br&gt;\nthis after it gets flushed out -- but until then, running an instance\n&lt;br&gt;\nper crawl seems to be your only option.&lt;br&gt;&lt;br&gt;\nDo you have sufficent resources to start up a new crawl instance to go\n&lt;br&gt;\nagainst a new domain on the machine that has the trailing-off crawl &lt;br&gt;\nrunning on it?&nbsp; Or are your crawls up against Heritrix bounds?\n&lt;br&gt;&lt;br&gt;\nWhen we have a checkpointing system in place, you&#39;ll be able to &lt;br&gt;\ncheckpoint the dying crawl, stop it, and then restart it inside of a\n&lt;br&gt;\nsmaller heap letting it crawl to completion making room to run the new\n&lt;br&gt;\ncrawl.&lt;br&gt;&lt;br&gt;\nSt.Ack&lt;br&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; (I&#39;ve tried restarting crawls from recover.gz, but haven&#39;t been\n&lt;br&gt;\n&gt; sucessful with that - I get out-of-memory errors, even when I set\nthe &lt;br&gt;\n&gt; JVM to have 1 GB of RAM or more.)&lt;br&gt;&lt;br&gt;\n&lt;br&gt;&lt;br&gt;\n&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; thanks&lt;br&gt;\n&gt;&nbsp; - Mike Schwartz&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp; Aptas, Inc.&lt;br&gt;\n&gt; *Yahoo! Groups Sponsor*&lt;br&gt;\n&gt; ADVERTISEMENT&lt;br&gt;\n&gt; click here &lt;br&gt;\n&gt;\n&lt;&lt;a href=&quot;http://us.ard.yahoo.com/SIG=129jkuth1/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109711161/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&quot;&gt;\nhttp://us.ard.yahoo.com/SIG=129jkuth1/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109711161/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&lt;/a&gt;\n&gt; &lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;\n------------------------------------------------------------------------&lt;br&gt;\n&gt; *Yahoo! Groups Links*&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp; * To visit your group on the web, go\nto:&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&lt;a href=&quot;http://groups.yahoo.com/group/archive-crawler/&quot;&gt;\nhttp://groups.yahoo.com/group/archive-crawler/&lt;/a&gt;&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp; * To unsubscribe from this group, send an\nemail to:&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\narchive-crawler-unsubscribe@yahoogroups.com&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&lt;&lt;a href=&quot;mailto:archive-crawler-unsubscribe@yahoogroups.com%3Fsubject=Unsubscribe&quot; eudora=&quot;autourl&quot;&gt;\nmailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&lt;/a&gt;\n&gt;&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp; * Your use of Yahoo! Groups is subject to\nthe Yahoo! Terms of&lt;br&gt;\n&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Service\n&lt;&lt;a href=&quot;http://docs.yahoo.com/info/terms/&quot;&gt;\nhttp://docs.yahoo.com/info/terms/&lt;/a&gt;&gt;.&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;&lt;br&gt;\n&lt;/tt&gt;&lt;font size=3&gt;&lt;br&gt;\n&lt;/font&gt;&lt;font size=2 color=&quot;#003399&quot;&gt;&lt;b&gt;Yahoo! Groups\nSponsor&lt;/b&gt;&lt;/font&gt;&lt;font size=3&gt; &lt;br&gt;\n&lt;/font&gt;&lt;font face=&quot;arial&quot; size=1&gt;ADVERTISEMENT&lt;/font&gt;&lt;font size=3&gt;&lt;br&gt;\n&lt;a href=&quot;http://us.ard.yahoo.com/SIG=129e0c6q6/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109793222/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&quot;&gt;\n&lt;img src=&quot;http://us.a1.yimg.com/us.yimg.com/a/ne/netflix/22305_0205_016_b_300250_a.gif&quot; width=300 height=250 alt=&quot;click here&quot;&gt;\n&lt;/a&gt;&lt;br&gt;&lt;br&gt;\n&lt;hr&gt;\n&lt;/font&gt;&lt;tt&gt;Yahoo! Groups Links&lt;/b&gt;\n&lt;ul&gt;\n&lt;li&gt;To visit your group on the web, go to:\n&lt;li&gt;&lt;a href=&quot;http://groups.yahoo.com/group/archive-crawler/&quot;&gt;\nhttp://groups.yahoo.com/group/archive-crawler/&lt;/a&gt;\n&lt;li&gt;&nbsp; \n&lt;li&gt;To unsubscribe from this group, send an email to:\n&lt;li&gt;\n&lt;a href=&quot;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&quot;&gt;\narchive-crawler-unsubscribe@yahoogroups.com&lt;/a&gt;\n&lt;li&gt;&nbsp; \n&lt;li&gt;Your use of Yahoo! Groups is subject to the\n&lt;a href=&quot;http://docs.yahoo.com/info/terms/&quot;&gt;Yahoo! Terms of Service&lt;/a&gt;. \n&lt;/ul&gt;&lt;/blockquote&gt;&lt;/body&gt;\n&lt;/html&gt;\n\r\n--=====================_1382739921==.ALT--\r\n\n"}}