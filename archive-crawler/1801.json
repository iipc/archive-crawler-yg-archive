{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":27093708,"authorName":"svc@kb.dk","from":"svc@...","profile":"svc400","replyTo":"LIST","senderId":"k0e93nd_3he0RfZMgFbOGBN69_07lJiwEmBmMgzxz9E3Wm1mDpAoVY_wYh8Mg7XQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Not possible to override &quot;max-document-download&quot;","postDate":"1115381343","msgId":1801,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PE9GNTM2MDZEODkuRjQ1QzVEMTQtT05DMTI1NkZGOS4wMDQyNjc3OS1DMTI1NkZGOS4wMDQyQkYzNEBrYi5kaz4=","inReplyToHeader":"PDQyNzk1MzZDLjcwODA4MDFAYXJjaGl2ZS5vcmc+"},"prevInTopic":1796,"nextInTopic":1806,"prevInTime":1800,"nextInTime":1802,"topicId":1784,"numMessagesInTopic":8,"msgSnippet":"Thanks for your help, everyone! Even though it is deprecated, the solution using the DomainSensitiveFrontier fits the bill perfectly. because we need to make ","rawEmail":"Return-Path: &lt;svc@...&gt;\r\nX-Sender: svc@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 32063 invoked from network); 6 May 2005 12:09:58 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m30.grp.scd.yahoo.com with QMQP; 6 May 2005 12:09:58 -0000\r\nReceived: from unknown (HELO hans.kb.dk) (130.226.229.20)\n  by mta1.grp.scd.yahoo.com with SMTP; 6 May 2005 12:09:57 -0000\r\nReceived: from hans.kb.dk (localhost [127.0.0.1])\n\tby localhost (Postfix) with SMTP id 7081D6003D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri,  6 May 2005 14:09:25 +0200 (CEST)\r\nReceived: from notes02.kb.dk (notes02.kb.dk [130.226.222.11])\n\tby hans.kb.dk (Postfix) with ESMTP id 5507D60184\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri,  6 May 2005 14:09:21 +0200 (CEST)\r\nReceived: from notes00.kb.dk ([130.226.220.23])\n          by notes02.kb.dk (Lotus Domino Release 6.5.2)\n          with ESMTP id 2005050614090392-104996 ;\n          Fri, 6 May 2005 14:09:03 +0200 \r\nIn-Reply-To: &lt;4279536C.7080801@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nX-Mailer: Lotus Notes Release 6.5.2 June 01, 2004\r\nMessage-ID: &lt;OF53606D89.F45C5D14-ONC1256FF9.00426779-C1256FF9.0042BF34@...&gt;\r\nDate: Fri, 6 May 2005 14:09:03 +0200\r\nMIME-Version: 1.0\r\nX-MIMETrack: Serialize by Router on Notes00/Kglbib(Release 6.5.2|June 01, 2004) at 06-05-2005\n 14:09:03,\n\tItemize by SMTP Server on Notes02/Kglbib(Release 6.5.2|June 01, 2004) at 06-05-2005\n 14:09:03,\n\tSerialize by Router on Notes02/Kglbib(Release 6.5.2|June 01, 2004) at 06-05-2005\n 14:09:04,\n\tSerialize complete at 06-05-2005 14:09:04\r\nContent-transfer-encoding: quoted-printable\r\nContent-type: text/plain; charset=ISO-8859-1\r\nX-PMX-Version: 4.7.1.128075, Antispam-Engine: 2.0.3.0, Antispam-Data: 2005.5.6.8\r\nX-PerlMx-Spam: Gauge=IIIIIII, Probability=7%, Report=&#39;NO_REAL_NAME 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __HAS_X_MAILER 0, __MIME_VERSION 0, __SANE_MSGID 0, __query.bondedsender.org_TIMEOUT &#39;\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: svc@...\r\nSubject: Re: [archive-crawler] Not possible to override &quot;max-document-download&quot;\r\nX-Yahoo-Group-Post: member; u=27093708\r\nX-Yahoo-Profile: svc400\r\n\r\nThanks for your help, everyone!\n\nEven though it is deprecated, the solution=\r\n using the\nDomainSensitiveFrontier fits the bill perfectly. because we need=\r\n to make\nthe restriction\non the domain-level, and not just on the host-leve=\r\nl.\n\nregards\n------------------------------------------------------------\nS=\r\n=F8ren Vejrup Carlsen, DDA, Det Kongelige Bibliotek\ntlf: (+45) 33 47 48 41\n=\r\nemail: svc@...\nemail: svc@...\n-------------------------=\r\n------------------------------------\nNon omnia possumus omnes\n--- Macrobius=\r\n, Saturnalia, VI, 1, 35 -------\n\n\n                                         =\r\n                                  \n             &quot;Gordon Mohr               =\r\n                                   \n             (Internet                 =\r\n                                    \n             Archive)&quot;                =\r\n                                  To \n             &lt;gojomo@archive.o       =\r\n  archive-crawler@yahoogroups.com     \n             rg&gt;                    =\r\n                                    cc \n             Sent by:              =\r\n                                        \n             archive-crawler@y    =\r\n                                 Subject \n             ahoogroups.com      =\r\n      Re: [archive-crawler] Not possible  \n                                =\r\n       to override &quot;max-document-download&quot; \n                               =\r\n                                            \n             05-05-2005 00:57 =\r\n                                             \n                             =\r\n                                              \n                            =\r\n                                               \n             Please respond=\r\n to                                             \n             archive-crawl=\r\ner@y                                             \n              ahoogroups.=\r\ncom                                               \n                        =\r\n                                                   \n                       =\r\n                                                    \n\n\n\n\nExcellent suggesti=\r\non. Until these details get integrated into the User\nManual,\nyou can learn =\r\nabout these &quot;budgetting&quot; settings in the wiki:\n\n    http://crawler.archive.=\r\norg/cgi-bin/wiki.pl?BudgetingFrontier\n\nIn particular, the &#39;queue-total-budg=\r\net&#39; can be used almost like a\nper-host max-documents, but note the followin=\r\ng:\n\n  - Budget amounts are deducted on each *try* from a queue -- that is,\n=\r\n    item dequeued, even if it gets requeued -- rather than each success.\n  =\r\n- Each dequeue only counts as &#39;1&#39; if you&#39;re using the\nUnitCostAssignmentPol=\r\nicy.\n    Other policies may consider some dequeues more &#39;costly&#39;, such as\n =\r\n   those that look like calendars or deep dynamic content.\n  - Even normal =\r\nerror-free operation involves some URLs being pulled\n    from their queue m=\r\nore than once, for example when an URL is considered\n    but then put off f=\r\nor the fetching of a prerquisite DNS or /robots.txt\n    resource.\n\nSo, even=\r\n a UnitCostAssignmentPolicy and queue-total-budget of (for example)\n1000 wo=\r\nn&#39;t lead to exactly 1000 resources collected -- the actual number\nwill be s=\r\nlightly lower, or much lower if there were many problems requiring\nretries.=\r\n\n\n- Gordon @ IA\n\nbja@... wrote:\n&gt; *A someway similar behavi=\r\nor could be obtained by using the cost-policies\nto\n&gt; ensure download of a m=\r\naximum of e.g. 2000 objects from each host at a\ntime -\n&gt; this would not sto=\r\np the crawler after 2000 documents per host but this\nensures\n&gt; that each ho=\r\nst is visited before going nuts (in crawler traps and other\n&gt; unpleasent th=\r\nings like that)*\n&gt;\n&gt; *best\n&gt; Bjarne Andersen*\n&gt;\n&gt; *----- Original Message -=\r\n----*\n&gt;\n&gt; *From*: Kristinn Sigurdsson &lt;kris@...&gt;\n&gt;\n&gt; *Date*: Wednes=\r\nday, May 4, 2005 5:22 pm\n&gt;\n&gt; *Subject*: RE: [archive-crawler] Not possible =\r\nto override\n&quot;max-document-download&quot;\n&gt;\n&gt;  &gt; What Stack fails to mention is t=\r\nhat the DomainSensitiveFrontier is\n&gt;  &gt; based on\n&gt;  &gt; the now deprecated Ho=\r\nstQueuesFrontier.\n&gt;  &gt;\n&gt;  &gt; A possible workaround for you Soren, would be t=\r\no create a\n&gt;  &gt; processor and a\n&gt;  &gt; filter that attaches to the scope.\n&gt;  =\r\n&gt;\n&gt;  &gt; The processor would be somewhere after the fetch part of the\n&gt;  &gt; ch=\r\nain. It would\n&gt;  &gt; simply tally the number of URIs that pass through for ea=\r\nch host.\n&gt;  &gt; It would\n&gt;  &gt; also provide static access to look-ups on any o=\r\nf these values.\n&gt;  &gt; Actually, the\n&gt;  &gt; StatisticsTracker does keep track o=\r\nf this at the moment, but you\n&gt;  &gt; cant access\n&gt;  &gt; the information in a us=\r\neful fashion.\n&gt;  &gt;\n&gt;  &gt; The filter (which is going to be a part of the scop=\r\ne) would then\n&gt;  &gt; check with\n&gt;  &gt; this processor for each URI to see if it=\r\ns host has exceeded its\n&gt;  &gt; maximumvalue. The filters maximum value would =\r\nbe configurable and,\n&gt;  &gt; moreimportantly, overridable in the usual manner.=\r\n\n&gt;  &gt;\n&gt;  &gt; Set the Preselector to recheck scope and once a host has exceede=\r\nd\n&gt;  &gt; its max,\n&gt;  &gt; all other URIs belonging to it will be tossed.\n&gt;  &gt;\n&gt; =\r\n &gt; Any other (more elegant) solutions would require tinkering with the\n&gt;  &gt;=\r\n BdbFrontier.\n&gt;  &gt;\n&gt;  &gt; - Kris\n&gt;  &gt;\n&gt;  &gt; -----Original Message-----\n&gt;  &gt; Fr=\r\nom: archive-crawler@yahoogroups.com\n&gt;  &gt; [archive-crawler@yahoogroups.com] =\r\nOn Behalf Of stack\n&gt;  &gt; Sent: 4. ma=ED 2005 15:06\n&gt;  &gt; To: archive-crawler@=\r\nyahoogroups.com\n&gt;  &gt; Subject: Re: [archive-crawler] Not possible to overrid=\r\ne\n&gt;  &gt; &quot;max-document-download&quot;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; svc@... wrote:\n&gt;  &gt;\n&gt;  &gt; &gt; =\r\nIt would be nice for us, if we could override the &quot;max-document-\n&gt;  &gt; downl=\r\noad&quot;&gt; setting, i.e have it defined pr. domain\n&gt;  &gt; &gt; We are planning to cra=\r\nwl a bunch of domains in a single job, but\n&gt;  &gt; need to\n&gt;  &gt; &gt; set a limit =\r\nto the number of objects crawled for each of the\n&gt;  &gt; domains.&gt;\n&gt;  &gt; &gt; This=\r\n seems not to be possible in the current version of heritrix.\n&gt;  &gt; &gt; Maybe =\r\nthis could be implemented instead as a processor.\n&gt;  &gt; &gt; Suggestions are ve=\r\nry welcome.\n&gt;  &gt;\n&gt;  &gt; Take a look at the DomainSensitiveFrontier (See\n&gt;  &gt; =\r\n&#39;6.1.2.3. DomainSensitveFrontier&#39; in the User Manual\n&gt;  &gt; http://crawler.ar=\r\nchive.org/articles/user_manual.html#modules).\n&gt;  &gt; This\n&gt;  &gt; frontier allow=\r\ns specification of maximum documents per domain\n&gt;  &gt; adding a\n&gt;  &gt; filter t=\r\no a domain-specific override to prevent further downloads\n&gt;  &gt; once\n&gt;  &gt; th=\r\ne maximum has been reached. The DSF is based on Heritrix&#39;s\n&gt;  &gt; previous\n&gt; =\r\n &gt; default frontier, the HostQueuesFrontier so your crawls will not\n&gt;  &gt; be=\r\n as\n&gt;  &gt; long lasting -- but perhaps it will be sufficent to your purposes\n=\r\n&gt;  &gt; (I&#39;ve\n&gt;  &gt; added a feature request that we add this ability to the\n&gt;  =\r\n&gt; BdbFrontier:\n&gt;  &gt; http://sourceforge.net/tracker/index.php?func=3Ddetail\n=\r\n&gt;  &gt; &lt;\nhttp://sourceforge.net/tracker/index.php?func=3Ddetail&aid=3D1195298=\r\n&group_id=3D7\n&gt;  &gt; 3833&atid=3D539102).&gt; &aid=3D1195298&group_id=3D73833&at=\r\nid=3D539102).\n&gt;  &gt; St.Ack\n&gt;  &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; regards\n&gt;  &gt; &gt; ---------------=\r\n---------------------------------------------\n&gt;  &gt; &gt; S=F8ren Vejrup Carlsen=\r\n, DDA, Det Kongelige Bibliotek\n&gt;  &gt; &gt; tlf: (+45) 33 47 48 41\n&gt;  &gt; &gt; email: =\r\nsvc@...\n&gt;  &gt; &gt; email: svc@...\n&gt;  &gt; &gt; ------------------=\r\n-------------------------------------------\n&gt;  &gt; &gt; Non omnia possumus omnes=\r\n\n&gt;  &gt; &gt; --- Macrobius, Saturnalia, VI, 1, 35 -------\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; -=\r\n----------------------------------------------------------------\n&gt;  &gt; -----=\r\n--\n&gt;  &gt; &gt; *Yahoo! Groups Links*\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; * To visit your group on the =\r\nweb, go to:\n&gt;  &gt; &gt; http://groups.yahoo.com/group/archive-crawler/\n&gt;  &gt; &gt;\n&gt; =\r\n &gt; &gt; * To unsubscribe from this group, send an email to:\n&gt;  &gt; &gt; archive-cra=\r\nwler-unsubscribe@yahoogroups.com\n&gt;  &gt; &gt;\n&gt;  &gt; &lt;&#39;)&quot; &gt;archive-crawler-unsubscr=\r\nibe@yahoogroups.com?subject=3DUnsubscribe&gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; * Your use of Yaho=\r\no! Groups is subject to the Yahoo! Terms of\n&gt;  &gt; &gt; Service &lt;.&quot; target=3D&quot;l&quot;=\r\n&gt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; ____=\r\n_\n&gt;  &gt;\n&gt;  &gt; Yahoo! Groups Links\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; * To visit your group on the=\r\n web, go to:\n&gt;  &gt; http://groups.yahoo.com/group/archive-crawler/\n&gt;  &gt;\n&gt;  &gt;\n=\r\n&gt;  &gt; * To unsubscribe from this group, send an email to:\n&gt;  &gt; archive-crawl=\r\ner-unsubscribe@yahoogroups.com\n&gt;  &gt; &lt;&#39;)&quot; &gt;archive-crawler-\n&gt;  &gt; unsubscribe=\r\n@yahoogroups.com?subject=3DUnsubscribe&gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; * Your use of Yahoo!=\r\n Groups is subject to the Yahoo! Terms of Service\n&gt;  &gt; &lt;&quot; target=3D&quot;l&quot;&gt;http=\r\n://docs.yahoo.com/info/terms/&gt; .\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;\n-------------------------=\r\n-------------------------------------------------------\n\n&gt; *Yahoo! Groups L=\r\ninks*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://group=\r\ns.yahoo.com/group/archive-crawler/\n&gt;\n&gt;     * To unsubscribe from this group=\r\n, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;  =\r\n     &lt;\nmailto:archive-crawler-unsubscribe@yahoogroups.com?subject=3DUnsubsc=\r\nribe&gt;\n&gt;\n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of=\r\n Service\n&gt;       &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n\n\nYahoo! Groups=\r\n Links\n\n\n\n\n\n\n\n\n\n"}}