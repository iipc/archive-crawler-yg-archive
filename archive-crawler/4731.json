{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":330034103,"authorName":"allen.vachon","from":"&quot;allen.vachon&quot; &lt;loneeaglejag@...&gt;","profile":"allen.vachon","replyTo":"LIST","senderId":"bF7bnpBkSCcqccbJ3CcAtF7sl8Yrful3QgejfxHxgtqnsZ8bnmQmmBJRtDJpenN_ibNChgAWLRKDA6WXLwmNKlCEIEP9CXa4tohiyjQuZjc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: New to Heritrix and need a boost! :)","postDate":"1195586888","msgId":4731,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZodmNnOCsyMHBuQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ3NDMxNTZFLjYwMjAzMDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":4728,"nextInTopic":0,"prevInTime":4730,"nextInTime":4732,"topicId":4726,"numMessagesInTopic":3,"msgSnippet":"Hi Gordon, Thanks for your help. Here more informations for my problem if you have time to look at it. I am trying to start on the right track from the","rawEmail":"Return-Path: &lt;loneeaglejag@...&gt;\r\nX-Sender: loneeaglejag@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 9522 invoked from network); 20 Nov 2007 19:28:10 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m47.grp.scd.yahoo.com with QMQP; 20 Nov 2007 19:28:10 -0000\r\nX-Received: from unknown (HELO n46b.bullet.mail.sp1.yahoo.com) (66.163.168.160)\n  by mta15.grp.scd.yahoo.com with SMTP; 20 Nov 2007 19:28:09 -0000\r\nX-Received: from [216.252.122.219] by n46.bullet.mail.sp1.yahoo.com with NNFMP; 20 Nov 2007 19:28:09 -0000\r\nX-Received: from [66.218.69.5] by t4.bullet.sp1.yahoo.com with NNFMP; 20 Nov 2007 19:28:09 -0000\r\nX-Received: from [66.218.66.87] by t5.bullet.scd.yahoo.com with NNFMP; 20 Nov 2007 19:28:09 -0000\r\nDate: Tue, 20 Nov 2007 19:28:08 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;fhvcg8+20pn@...&gt;\r\nIn-Reply-To: &lt;4743156E.6020303@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;allen.vachon&quot; &lt;loneeaglejag@...&gt;\r\nSubject: Re: New to Heritrix and need a boost! :)\r\nX-Yahoo-Group-Post: member; u=330034103; y=6KeAfaJwcFse9NLixWTRNvkGsUBPP0VcwOoOmuRqJkBg3zaF6P5B\r\nX-Yahoo-Profile: allen.vachon\r\n\r\nHi Gordon,\n\nThanks for your help.\n\nHere more informations for my problem if=\r\n you have time to look at it.\nI am trying to start on the right track from =\r\nthe beginning.\nI want to understand what I will need before starting progra=\r\nmming.\n\n* My goal is to extract specific informations on products (image to=\r\no) \nfrom many sites,\n  not simply save an html of all available pages.\n* Ea=\r\nch sites will have to be handled differently. I guess a profile \nper site?\n=\r\n* I would like to control how to navigate from a place to the next \none so =\r\nI can gather informations\n  for the writer. I do not need to crawl everythi=\r\nng from a site. So, \nfrom a list of all available links\n  of a page, I coul=\r\nd decide if it will be crawled or ignored.\n* The writer will write to a fil=\r\ne (eventually PostgreQL) and always \nwrite the same informations:\n  {SiteNa=\r\nme}, {Category}, {Sub-Category}, {Sub-Category}, \n{Manufacturer}, {ItemName=\r\n}, {Description}, {Price}, {...}\n  All other level are simply to gather inf=\r\normations and proceed to \nlower level until a product is found.\n* Have a DO=\r\nM to analyse what inside the page to find \ninformations/links.\n\nTigerDirect=\r\n\n-----------\n\nOne information important for us is the manufactuer and this =\r\nsite can \nhave it easily.\nSo, we can craw for each Manufacturer and find it=\r\ns products.\n\nLevel 0: &lt;http://www.tigerdirect.com/applications/searchtools/=\r\nall.asp&gt;\nLevel 1:    For each manufacturer: {Ignore other links}\nLevel 2:  =\r\n     For each product:\nLevel 3:          Gather information on product\n    =\r\n              WRITER: Save gathered informations.\n\nStaples - English\n------=\r\n-----------\n\nThe Manufacturer is not available easily, but the Category is.=\r\n\n\nLevel 0: &lt;http://www.staples.ca/ENG/Catalog/stap_home.asp&gt; Get main \ncate=\r\ngories list\nLevel 1:    &lt;Office Supplies&gt; get sub-category\nLevel 2:       &lt;=\r\nBadges & Holders&gt; get sub-category\nLevel 3:          &lt;Badge Holders&gt; Get al=\r\nl available products.\nLevel 4:             Proceed each available product a=\r\nnd pages\nLevel 5:                Gather information on product\n            =\r\n            WRITER: Save gathered informations.\nLevel 2:       &lt;...&gt;\n...\nLe=\r\nvel 1:    &lt;Furniture&gt;\nLevel 2:       ...\n...\n\nThanks\n\n--- In archive-crawle=\r\nr@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; \nwrote:\n&gt;\n&gt; allen.vachon wrote:=\r\n\n&gt; &gt; Hi,\n&gt; &gt; \n&gt; &gt; New to Heritrix and WebCrawling with knowledge of Java.\n&gt;=\r\n &gt; \n&gt; &gt; I am analyzing how things can be done for our needs.\n&gt; &gt; We want to=\r\n crawl specific sites and extract data (product, price, \n&gt; &gt; description, e=\r\ntc).\n&gt; &gt; \n&gt; &gt; 1) Some site are multilingual, so should I use 2 seeds (2 \nla=\r\nnguages)\n&gt; &gt;    for that site or should I write a &quot;Frontier&quot; or else?\n&gt; \n&gt; =\r\nIt&#39;s rare that most crawl operators would need to write or \ncustomize the \n=\r\n&gt; code of a Frontier; it&#39;s a core component that&#39;s already very \nflexible.\n=\r\n&gt; \n&gt; How many seeds to use depends on how many are necessary to discover \na=\r\nll \n&gt; the content you want collected. If the site makes different-\nlanguage=\r\n \n&gt; content available via different URLs -- such as:\n&gt; \n&gt;    http://en.exam=\r\nple.com/[content]\n&gt;    http://fr.example.com/[content]\n&gt; \n&gt; ...or...\n&gt; \n&gt;  =\r\n  http://example.com/en/[content]\n&gt;    http://example.com/fr/content\n&gt; \n&gt; .=\r\n..then it&#39;s easy to crawl in many ways. Maybe one seed to a \nlanguage \n&gt; se=\r\nlector page is enough to find all variants, or two seeds to good \n&gt; startin=\r\ng points for each language, or even two crawls if you prefer \n&gt; having comp=\r\nletely distinct logs/output.\n&gt; \n&gt; Deeper problems arise when a site serves =\r\nalternate language content \nfrom \n&gt; the exact same URLs, based on a cookie =\r\nthat can be toggled by the \nuser. \n&gt; Then, a single crawl may show a mix of=\r\n language content. In such \ncases, \n&gt; you typically need to crawl twice, in=\r\n one crawl ensuring that the \ncookie \n&gt; stays at one value, in the other th=\r\ne other cookie value.\n&gt; \n&gt; &gt; 2) I need different way to process a URI based=\r\n on the depth level\n&gt; &gt;    by analyzing the DOM and its links:\n&gt; &gt; \n&gt; &gt;    =\r\nLevel 0 (root)    : Get some specific links and keep some\n&gt; &gt;              =\r\n          informations for that level.\n&gt; &gt;    Level 1 (Category): {Same as =\r\nLevel 0}\n&gt; &gt;    Level 2 (Product) : Get information on the product plus all=\r\n\n&gt; &gt;                        previous&#39; one (previous levels).\n&gt; &gt;           =\r\n             Proceed for next pages (click on next page)\n&gt; &gt;               =\r\n         for that level until end.\n&gt; &gt; \n&gt; &gt;    How can I do that? Do I need=\r\n a &quot;Frontier&quot;?\n&gt; &gt;    It not simply as fetch everything for a site.\n&gt; \n&gt; Yo=\r\nu could probably write a &#39;Processor&#39; module that you would insert \n&gt; somewh=\r\nere near the existing Extractor modules in the chains of \n&gt; processing step=\r\ns. It would examine the URL pattern, page content, \nand \n&gt; possibly the URL=\r\n&#39;s &#39;via&#39; (predecessor URL where it was discovered) \nand \n&gt; &#39;hops-path&#39; (sym=\r\nbolic string of discovery hop-types, eg &#39;LLL&#39; for 3 \n&gt; navigational links i=\r\nn a row) to determine what to do, and then do \nyour \n&gt; custom analysis. See=\r\n this portion of the Developer Manual for \n&gt; information on writing a Proce=\r\nssor:\n&gt; \n&gt; http://crawler.archive.org/articles/developer_manual/processor.h=\r\ntml\n&gt; \n&gt; Alternatively, you could collect all the info by a crawl, but then=\r\n \ndo \n&gt; the custom analysis as postprocessing on the captured content.\n&gt; \n&gt;=\r\n - Gordon @ IA\n&gt;\n\n\n\n"}}