{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":475160622,"authorName":"Krishna, Murali","from":"&quot;Krishna, Murali&quot; &lt;muralikp@...&gt;","profile":"muralikpbhat","replyTo":"LIST","senderId":"VAI30_Yq_XKDG7Mt9m-CHbOGl5r1gO6aNCmcPatpwZgsW7vdQ3NjDHyX7Ue6d3KBbl2cbZT4kgsbMjudxxO0JiwOrKnu-BzjkZzlxQ_u","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Heritrix Frontier","postDate":"1297077453","msgId":7005,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PEM5NzVEMkE1LjIwMjElbXVyYWxpa3BAYW1hem9uLmNvbT4="},"prevInTopic":0,"nextInTopic":7006,"prevInTime":7004,"nextInTime":7006,"topicId":7005,"numMessagesInTopic":11,"msgSnippet":"Hi all, We have a list of urls to be crawled, essentially just a fetch and some processing. Assume that the list can be huge and run into billions. So, we are","rawEmail":"Return-Path: &lt;muralikp@...&gt;\r\nReceived: (qmail 96191 invoked from network); 7 Feb 2011 18:36:07 -0000\r\nReceived: from unknown (98.137.34.46)\n  by m4.grp.sp2.yahoo.com with QMQP; 7 Feb 2011 18:36:07 -0000\r\nReceived: from unknown (HELO n45b.bullet.mail.sp1.yahoo.com) (66.163.168.159)\n  by mta3.grp.sp2.yahoo.com with SMTP; 7 Feb 2011 18:36:07 -0000\r\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoogroups.com; s=lima; t=1297103767; bh=mC50N1IyVlwYGnvyEN/Tp2YnkC/j9m/FzvO8Z64GLSY=; h=Received:Received:X-Sender:X-Apparently-To:X-Received:X-Received:X-Received:X-IronPort-AV:X-Received:X-Received:X-Received:X-Received:To:Date:Thread-Topic:Thread-Index:Message-ID:Accept-Language:Content-Language:X-MS-Has-Attach:X-MS-TNEF-Correlator:user-agent:acceptlanguage:Content-Type:MIME-Version:X-Originating-IP:X-eGroups-Msg-Info:From:Subject:X-Yahoo-Group-Post:X-Yahoo-Profile:X-YGroups-SubInfo:Sender:X-Yahoo-Newman-Property:X-eGroups-Approved-By:X-eGroups-Auth; b=UG/NNkqKc3/5HcUn7QMttFd0L4QLi3oqAdtRxKNdsvAuzShsEvHqSqOYvm7JtdqK/j29wQKLnjpUpRsXelVVkedIx/YVR/B5tUkRDfkoO7gKHmCCohykhD80jneKWw4n\r\nReceived: from [69.147.65.173] by n45.bullet.mail.sp1.yahoo.com with NNFMP; 07 Feb 2011 18:36:07 -0000\r\nReceived: from [98.137.35.12] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 07 Feb 2011 18:36:07 -0000\r\nX-Sender: muralikp@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 24833 invoked from network); 7 Feb 2011 11:17:52 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.sp2.yahoo.com with QMQP; 7 Feb 2011 11:17:52 -0000\r\nX-Received: from unknown (HELO smtp-fw-4101.amazon.com) (72.21.198.25)\n  by mta3.grp.sp2.yahoo.com with SMTP; 7 Feb 2011 11:17:52 -0000\r\nX-IronPort-AV: E=Sophos;i=&quot;4.60,437,1291593600&quot;; \n   d=&quot;scan&#39;208,217&quot;;a=&quot;346752716&quot;\r\nX-Received: from smtp-in-9003.sea19.amazon.com ([10.186.104.20])\n  by smtp-border-fw-out-4101.iad4.amazon.com with ESMTP/TLS/DHE-RSA-AES256-SHA; 07 Feb 2011 11:17:51 +0000\r\nX-Received: from ex-hub-12010.ant.amazon.com (ex-hub-12010.ant.amazon.com [10.32.49.103])\n\tby smtp-in-9003.sea19.amazon.com (8.13.8/8.13.8) with ESMTP id p17BHFUZ011794\n\t(version=TLSv1/SSLv3 cipher=AES128-SHA bits=128 verify=FAIL)\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 7 Feb 2011 11:17:50 GMT\r\nX-Received: from ex-hub-hyd1-4.ant.amazon.com (10.43.31.202) by\n ex-hub-12010.ant.amazon.com (10.32.49.103) with Microsoft SMTP Server (TLS)\n id 8.2.254.0; Mon, 7 Feb 2011 11:17:37 +0000\r\nX-Received: from ex-mail-hyd1-1.ant.amazon.com ([fe80::556:4cdd:76c1:d940]) by\n ex-hub-hyd1-4.ant.amazon.com ([::1]) with mapi; Mon, 7 Feb 2011 16:47:35\n +0530\r\nTo: &quot;archive-crawler@yahoogroups.com&quot; &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Mon, 7 Feb 2011 16:47:33 +0530\r\nThread-Topic: Heritrix Frontier\r\nThread-Index: AcvGuJ2RShnxHXqVbk20ymb91LzWfw==\r\nMessage-ID: &lt;C975D2A5.2021%muralikp@...&gt;\r\nAccept-Language: en-US\r\nContent-Language: en-US\r\nX-MS-Has-Attach: \r\nX-MS-TNEF-Correlator: \r\nuser-agent: Microsoft-Entourage/13.6.0.100712\r\nacceptlanguage: en-US\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;_000_C975D2A52021muralikpamazoncom_&quot;\r\nMIME-Version: 1.0\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Krishna, Murali&quot; &lt;muralikp@...&gt;\r\nSubject: Heritrix Frontier\r\nX-Yahoo-Group-Post: member; u=475160622; y=WXicuzG9j0Ye-2gfho77fZbXVHMdTIknmcO_XLNRVnPX7dzZBCCb\r\nX-Yahoo-Profile: muralikpbhat\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 07 Feb 2011 18:36:04 -0000\r\n\r\n\r\n--_000_C975D2A52021muralikpamazoncom_\r\nContent-Type: text/plain; charset=&quot;Windows-1252&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi all,\n    We have a list of urls to be crawled, essentially just a fetch =\r\nand some processing. Assume that the list can be huge and run into billions=\r\n. So, we are thinking of writing a new Frontier which will accomplish this.=\r\n Will have multiple heritrix worker boxes, each of the worker=92s frontier =\r\nwill get one portion of the centralized url repository (distributed storage=\r\n) and schedule them for crawling.\n\n   1. Can we achieve this by extending t=\r\nhe WorkQueueFrontier ? I couldn=92t find much documentation on how WQF hand=\r\nles politeness. I am thinking of grouping the urls into workqueue based on =\r\npoliteness requirement, will it automatically take care of politeness if I =\r\ngroup correctly? Can I configure crawl-delay per WorkQueue?\n\n   2. What are=\r\n inactive queues, retired queues and getURIList here? (sorry, couldn=92t fi=\r\nnd  doc)\n\n   3. How does checkpointing work, I want to restart from the las=\r\nt crawled state. Is there a callback to do the frequent checkpointing for W=\r\norkQueueFrontier=92s implementations.\n\nThanks,\nMurali\n\r\n--_000_C975D2A52021muralikpamazoncom_\r\nContent-Type: text/html; charset=&quot;Windows-1252&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;html&gt;&lt;head&gt;\n&lt;meta http-equiv=3D&quot;Content-Type&quot; content=3D&quot;text/html; charse=\r\nt=3DWindows-1252&quot;&gt;&lt;title&gt;Heritrix Frontier&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;font fac=\r\ne=3D&quot;Calibri, Verdana, Helvetica, Arial&quot;&gt;&lt;span style=3D&quot;font-size:11pt&quot;&gt;Hi =\r\nall, &lt;br&gt;\n&nbsp;&nbsp;&nbsp;&nbsp;We have a list of urls to be crawled, ess=\r\nentially just a fetch and some processing. Assume that the list can be huge=\r\n and run into billions. So, we are thinking of writing a new Frontier which=\r\n will accomplish this. Will have multiple heritrix worker boxes, each of th=\r\ne worker=92s frontier will get one portion of the centralized url repositor=\r\ny (distributed storage) and schedule them for crawling.&lt;br&gt;\n&lt;br&gt;\n&nbsp;&nbs=\r\np;&nbsp;1. Can we achieve this by extending the WorkQueueFrontier ? I could=\r\nn=92t find much documentation on how WQF handles politeness. I am thinking =\r\nof grouping the urls into workqueue based on politeness requirement, will i=\r\nt automatically take care of politeness if I group correctly? Can I configu=\r\nre crawl-delay per WorkQueue?&lt;br&gt;\n&lt;br&gt;\n&nbsp;&nbsp;&nbsp;2. What are inacti=\r\nve queues, retired queues and getURIList here? (sorry, couldn=92t find &nbs=\r\np;doc)&lt;br&gt;\n&lt;br&gt;\n&nbsp;&nbsp;&nbsp;3. How does checkpointing work, I want to=\r\n restart from the last crawled state. Is there a callback to do the frequen=\r\nt checkpointing for WorkQueueFrontier=92s implementations.&lt;br&gt;\n&lt;br&gt;\nThanks,=\r\n&lt;br&gt;\nMurali&lt;/span&gt;&lt;/font&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\r\n--_000_C975D2A52021muralikpamazoncom_--\r\n\n"}}