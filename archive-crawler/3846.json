{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"8HwGVj6mmc5-hmx2gPhxNcffx0ogDxjbR1l0W-Mupr6FdVkddLHbt7Zht20I0LmbRbsOLmGYi33ChZxprjL8YUDI3DqoTFQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Distributed Crawling","postDate":"1172522899","msgId":3846,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1RTM0NzkzLjkwODA1MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGVya211citwa2M5QGVHcm91cHMuY29tPg==","referencesHeader":"PGVya211citwa2M5QGVHcm91cHMuY29tPg=="},"prevInTopic":3834,"nextInTopic":4006,"prevInTime":3845,"nextInTime":3847,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"... In a crude fashion, yes. It is more manual and less dynamic than we would like, but at IA we ve run crawls over up to 6 machines ( 600 million URLs","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 41337 invoked from network); 26 Feb 2007 20:48:01 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m24.grp.scd.yahoo.com with QMQP; 26 Feb 2007 20:48:01 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta9.grp.scd.yahoo.com with SMTP; 26 Feb 2007 20:48:01 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 5B2E3141A5865\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 26 Feb 2007 12:47:21 -0800 (PST)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 02585-02-99 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 26 Feb 2007 12:47:19 -0800 (PST)\r\nReceived: from [192.168.1.203] (c-67-180-203-212.hsd1.ca.comcast.net [67.180.203.212])\n\tby mail.archive.org (Postfix) with ESMTP id 03D191418E115\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 26 Feb 2007 12:47:18 -0800 (PST)\r\nMessage-ID: &lt;45E34793.9080504@...&gt;\r\nDate: Mon, 26 Feb 2007 12:48:19 -0800\r\nUser-Agent: Thunderbird 1.5.0.9 (X11/20070104)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;erkmur+pkc9@...&gt;\r\nIn-Reply-To: &lt;erkmur+pkc9@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=137285340; y=wbmmE4tCS5JzoVbWvzBDExN1VnKt0LiCnQa2dXFY11bu\r\nX-Yahoo-Profile: gojomo\r\n\r\nnt_bdr wrote:\n&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n\nIn a crude fashion, yes. It is more manual and less dynamic than we \nwould like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600 \nmillion URLs visited), and know of work elsewhere over up to 8 machines \n(&gt;1 billion URLs fetched).\n\nFor background see some previous threads including:\n\n   http://tech.groups.yahoo.com/group/archive-crawler/message/2909\n   http://tech.groups.yahoo.com/group/archive-crawler/message/3060\n\nRoughly how we do it:\n\n  - Use BloomFilterUriUniqFilter with its defaults -- which devotes \nabout 500MB to this structure and keeps the false-positive (mistakenly \nbelieved to have been previously-scheduled) rate under 1-in-4-million up \nthrough 125 million URIs discovered.\n\n  - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+ heap\n\n  - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in queues \nnamed by the reversed-host (com,example,) rather than usual host \n(example.com)\n\n  - Insert HashCrawlMapper processors at 2 places in the processor chain:\n\n    * Once, immediately before the PreconditionEnforcer. This one has \n&#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any scheduled \nURIs that should be handled by other crawlers -- chiefly seeds.)\n    * Again, immediately before the FrontierScheduler. This one has \n&#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any discovered \noutlinks before they are scheduled.)\n\n    Both HashCrawlMappers should have the same &#39;local-name&#39; (a number 0 \nto n-1, where n is the nubmer of crawlers in use) per machine, and all \nmachines should have the same &#39;crawler-count&#39; (number of crawlers, n).\n\n    HashCrawlMapper looks at the queue key of a URI -- here, the SURT \nauthority part, because of the above choice -- and decides if a URI is \nhandled by the current crawler or one of its siblings. If mapped to a \nsibling, the URI is dumped to a log rather than crawled locally. \nDepending on the character of your crawl, you may want to feed these \nlogs to the other crawlers occasionally or it may be OK to ignore them.\n\n    The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key before \nmapping -- used to ensure that all subdomains of example.com are treated \nthe same as example.com for mapping purposes. The first match of this \npattern, if present, is what is used for mapping purposes. A small \nexample would be:\n\n    ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n\n    For 3-letter domains (com, org, net), this uses everything through \nthe 2nd-level domain for mapping purposes. For everything else, it uses \nthe first 9 characters. You could imagine more complicated patterns that \ntake into account other TLDs. (For example, some 2-letter TLDs, like \n&#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level \ndomains.)\n\n   - All crawlers are launched with the same configuration, including \nthe same seeds, but otherwise do not (themselves) communicate. Seeds \nthat don&#39;t belong on any one crawler are dropped out by the early \nHashCrawlMapper. Discovered outlinks logs that need to be cross-fed are \ndone so by an external process/scripts.\n\n- Gordon @ IA\n\n"}}