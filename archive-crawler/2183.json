{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":224113134,"authorName":"Dennis Hotson","from":"Dennis Hotson &lt;dwh@...&gt;","replyTo":"LIST","senderId":"_gvOzba_3BqjGmYeBpLoYw_UpxiT_Cy3wI0p4fIZIMBIY6A31Z49vGkI9--WEQbtpaSE-q8T9cRPyxFWunjyOrOTkmaGYCttJrs","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Incremental Crawling","postDate":"1126570084","msgId":2183,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDExMjY1NzAwODQuNjQ2OS4zMi5jYW1lbEBkd2hwYy5tZHMucm1pdC5lZHUuYXU+","inReplyToHeader":"PDQzMjVDRENCLjQwODA3MDlAYXJjaGl2ZS5vcmc+","referencesHeader":"PDExMjY0OTA4NzEuNjQ2OS4xNC5jYW1lbEBkd2hwYy5tZHMucm1pdC5lZHUuYXU+CSA8NDMyNUNEQ0IuNDA4MDcwOUBhcmNoaXZlLm9yZz4="},"prevInTopic":2181,"nextInTopic":2184,"prevInTime":2182,"nextInTime":2184,"topicId":2177,"numMessagesInTopic":8,"msgSnippet":"... The filter I wrote manually does a HEAD request and then checks the date against the database I m storing pages in.. if the page needs updating then the","rawEmail":"Return-Path: &lt;dwh@...&gt;\r\nX-Sender: dwh@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 85797 invoked from network); 13 Sep 2005 00:08:17 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m22.grp.scd.yahoo.com with QMQP; 13 Sep 2005 00:08:17 -0000\r\nReceived: from unknown (HELO its-mu-mail2.its.rmit.edu.au) (131.170.2.22)\n  by mta3.grp.scd.yahoo.com with SMTP; 13 Sep 2005 00:08:17 -0000\r\nReceived: from pan.mds.rmit.edu.au (pan.mds.rmit.edu.au [131.170.70.15])\n\tby its-mu-mail2.its.rmit.edu.au (8.13.1/8.12.11/mail2) with ESMTP id j8D084P7021930\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 13 Sep 2005 10:08:04 +1000 (EST)\r\nReceived: from dwhpc.mds.rmit.edu.au (dwhpc.mds.rmit.edu.au [131.170.70.89])\n\tby pan.mds.rmit.edu.au (Postfix) with ESMTP id AF0B35016E;\n\tTue, 13 Sep 2005 10:07:59 +1000 (EST)\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;4325CDCB.4080709@...&gt;\r\nReferences: &lt;1126490871.6469.14.camel@...&gt;\n\t &lt;4325CDCB.4080709@...&gt;\r\nContent-Type: text/plain\r\nDate: Tue, 13 Sep 2005 10:08:04 +1000\r\nMessage-Id: &lt;1126570084.6469.32.camel@...&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Evolution 2.0.2 (2.0.2-3) \r\nContent-Transfer-Encoding: 7bit\r\nX-Scanned-By: MIMEDefang 2.44\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Dennis Hotson &lt;dwh@...&gt;\r\nSubject: Re: [archive-crawler] Incremental Crawling\r\nX-Yahoo-Group-Post: member; u=224113134\r\n\r\nOn Mon, 2005-09-12 at 11:49 -0700, stack wrote:\n\n&gt; &gt; I&#39;ve written a proof of concept filter class that does this (well\n&gt; &gt; actually, it&#39;s not quite working yet).\n&gt; \n&gt; How does your filter work?\n\nThe filter I wrote manually does a HEAD request and then checks the date\nagainst the database I&#39;m storing pages in.. if the page needs updating\nthen the filter passes.. otherwise the filter will fail and the url gets\nskipped.\nWhen I get time, I&#39;ll finish it off and test to see if it actually\nworks.. :P\n\nI just had a look at the AdaptiveRevisitingFrontier... from my\nunderstanding it will behave like a regular frontier except that it\nchecks whether content needs updating first. After getting the content\nit will then wait a specified time before crawling it again. Is this\ncorrect?\nHow does it check if the page needs updating? I&#39;m using Heritrix to\ninsert pages into a TeraText database (http://www.teratext.com.au) and\nI&#39;ve written a custom Writer/Processor module to insert the pages into\nthe database (as opposed to the default ARCWriter module).\n\nWill this frontier still work with my setup?\n\nThanks very much for you help. :)\n\n\n"}}