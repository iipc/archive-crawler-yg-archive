{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"oRic86g7D-A8eJk24Zu9jQBDCZ_n1uV-IXYTyKSrAvOBmcLh4g_zKCPd_pDTxm-6-UBcFxVRFZfPr_M9UzP4brmUB_O9lG0","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix crawling strategy","postDate":"1320780955","msgId":7395,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFQjk4NDlCLjYwMjA2MDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRFQjk4MjgzLjkwNzA2MDhAY3MuY211LmVkdT4=","referencesHeader":"PGo4bXRkcisybnI1QGVHcm91cHMuY29tPiA8NEVCODEyRjQuNDA3MDYwOEBhcmNoaXZlLm9yZz4gPDRFQjgyQ0E3LjYwMDA3MDRAY3MuY211LmVkdT4gPDRFQjgzNTA5LjUwMzAyMDlAYXJjaGl2ZS5vcmc+IDw0RUI4NDM4Ri4zMDQwNjA2QGNzLmNtdS5lZHU+IDw0RUI4OTk5My44MDQwMjA4QGFyY2hpdmUub3JnPiA8NEVCOTgyODMuOTA3MDYwOEBjcy5jbXUuZWR1Pg=="},"prevInTopic":7394,"nextInTopic":7401,"prevInTime":7394,"nextInTime":7396,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"Sounds like a bug; it should only be in STOPPING for about as long as it takes for all URIs in process to finish, so if zero threads show as active, then no","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 5868 invoked from network); 8 Nov 2011 19:35:58 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m12.grp.sp2.yahoo.com with QMQP; 8 Nov 2011 19:35:58 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta1.grp.sp2.yahoo.com with SMTP; 8 Nov 2011 19:35:57 -0000\r\nX-Received: (qmail 93185 invoked by uid 0); 8 Nov 2011 19:35:55 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay01.pair.com with SMTP; 8 Nov 2011 19:35:55 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4EB9849B.6020601@...&gt;\r\nDate: Tue, 08 Nov 2011 11:35:55 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:7.0.1) Gecko/20110929 Thunderbird/7.0.1\r\nMIME-Version: 1.0\r\nTo: David Pane &lt;dpane@...&gt;\r\nCc: archive-crawler@yahoogroups.com, Noah Levitt &lt;nlevitt@...&gt;\r\nReferences: &lt;j8mtdr+2nr5@...&gt; &lt;4EB812F4.4070608@...&gt; &lt;4EB82CA7.6000704@...&gt; &lt;4EB83509.5030209@...&gt; &lt;4EB8438F.3040606@...&gt; &lt;4EB89993.8040208@...&gt; &lt;4EB98283.9070608@...&gt;\r\nIn-Reply-To: &lt;4EB98283.9070608@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix crawling strategy\r\nX-Yahoo-Group-Post: member; u=137285340; y=zmY2Sq4zD4YLu6BgrRBZgOeuyOmaTExZq9R23X_W9X-x\r\nX-Yahoo-Profile: gojomo\r\n\r\nSounds like a bug; it should only be in STOPPING for about as long as it \ntakes for all URIs in process to finish, so if zero threads show as \nactive, then no more than a few seconds (to write out final reports, \netc.) should pass before a clean stop.\n\nI suggest taking a thread dump (via the &#39;jstack&#39; facility or watching \nstderr/heritrix_out.log after a sending a SIGQUIT signal), and filing a \nbug with that thread dump attached. If it&#39;s easy to reproduce it should \nbe easy to fix as well.\n\n- Gordon\n\nOn 11/8/11 11:26 AM, David Pane wrote:\n&gt; Thanks for all of this information. It really helps.\n&gt;\n&gt; Another thing that I have seen when running the small test crawls\n&gt; (setting maxTimeSeconds) is that when the time expires, the crawler\n&gt; starts to stop and the job gets stuck in the &quot;Active: STOPPING&quot; state.\n&gt; Rate is 0 URIs/sec and Load is 0 active of 0 threads.\n&gt;\n&gt; Selecting terminate and/or teardown does not help. I&#39;ve had to force\n&gt; kill the crawler in the past, but then the reports are not created. Do\n&gt; you have any insight on why this is happening?\n&gt;\n&gt; I currently have a job in the stopping state for the past 8 hrs.\n&gt;\n&gt; --David\n&gt;\n&gt; On 11/7/2011 9:53 PM, Gordon Mohr wrote:\n&gt;&gt; On 11/7/11 12:46 PM, David Pane wrote:\n&gt;&gt;&gt; Thank you Gordon.\n&gt;&gt;&gt;\n&gt;&gt;&gt; We do not plan to do repeated crawls, but capture a 1 billion page\n&gt;&gt;&gt; dataset to be used for information retrieval research purposes. We would\n&gt;&gt;&gt; like to capture as many high quality pages as possible.\n&gt;&gt;\n&gt;&gt; If the end goal is a &quot;best billion&quot; dataset that is then reused for a\n&gt;&gt; long time, you still might want to do that in indirect/incremental steps\n&gt;&gt; or recrawls: for example, crawl 2 billion, then pick the best billion.\n&gt;&gt; Or crawl 500 million with a naive approach, do an offline PR\n&gt;&gt; calculation, then do the &#39;production&#39; 1B crawl biased by the results of\n&gt;&gt; the 500 million. Etc...\n&gt;&gt;\n&gt;&gt;&gt; We would also like to have a blacklist of unwanted sites. This blacklist\n&gt;&gt;&gt; would possibly contain millions hosts. Do you have any experience in the\n&gt;&gt;&gt; resources needed for this? Would this slow down the speed of the crawl?\n&gt;&gt;\n&gt;&gt; I&#39;ve not done anything with a blacklist that big. You certainly wouldn&#39;t\n&gt;&gt; want to use a list of regexes!\n&gt;&gt;\n&gt;&gt; The SURT-based DecideRules use a sorted list of prefixes, all in memory,\n&gt;&gt; and have roughly O(log n) lookup. Perhaps that would work for your\n&gt;&gt; purposes, if you have a big RAM machine; you should do some tests and\n&gt;&gt; back-of-the-envelope calculations to check for sure. If the blacklist\n&gt;&gt; entries are always hosts, some other hash-based structure might work\n&gt;&gt; with even less RAM overhead and O(1) lookup.\n&gt;&gt;\n&gt;&gt;&gt; In our small crawls that we have been running, we found that although we\n&gt;&gt;&gt; have setup for a breadth first crawl, days into the crawl and 10&#39;s of\n&gt;&gt;&gt; million of pages crawled, the crawler has still only crawled a small\n&gt;&gt;&gt; percentage of seeds (under 10% of a 2 million host seed list - 1200\n&gt;&gt;&gt; threads). It appears that most of the seeds are in a separate queue so\n&gt;&gt;&gt; would cycling through the queues (balance-replenish-amount to a lower\n&gt;&gt;&gt; amount maybe 100) help cover all of the queues?\n&gt;&gt;\n&gt;&gt; Yes, lowering the balance-replenish-amount will cycle through the queues\n&gt;&gt; more quickly, since each will stay in active rotation a shorter amount\n&gt;&gt; of time until other queues are given a chance.\n&gt;&gt;\n&gt;&gt; Another factor which has been previously noted as slowing the progress\n&gt;&gt; through a diversity of hosts, in crawls with giant seed lists, is if\n&gt;&gt; many of the hosts turn out to be unresponsive.\n&gt;&gt;\n&gt;&gt; In some cases, being unresponsive means a long 20-second socket-timeout,\n&gt;&gt; which occupies the worker thread that whole time. Then, the queue goes\n&gt;&gt; into a &#39;long snooze&#39; (default 15 minutes) before retrying. Only after a\n&gt;&gt; bunch of retries (default 30) will the URI completely fail out as having\n&gt;&gt; failed for too many times over too long. Imagining a worst-case\n&gt;&gt; scenario: if your first 54000 seeds were all unresponsive like that,\n&gt;&gt; your 1200 threads could be doing nothing but these timeouts for over 7\n&gt;&gt; hours.\n&gt;&gt;\n&gt;&gt; Some possible ways to minimize this effect:\n&gt;&gt;\n&gt;&gt; � shorten the soTimeoutMs (socket timeout) in FetchHTTP below the\n&gt;&gt; default 20 seconds (less time holding a thread)\n&gt;&gt;\n&gt;&gt; � lengthen the retryDelaySeconds above the default 900 seconds (more\n&gt;&gt; time between held sessions\n&gt;&gt;\n&gt;&gt; � lessen the maxRetries below the default 30 (give up entirely on a URI\n&gt;&gt; sooner)\n&gt;&gt;\n&gt;&gt; � prescreen the URIs outside Heritrix to eliminate those not\n&gt;&gt; resolving/running-any-webserver\n&gt;&gt;\n&gt;&gt; Though, changing those default parameters does make the process less\n&gt;&gt; robust against the sort of temporary network/server outages that are\n&gt;&gt; common and which you normally wouldn&#39;t want to completely exclude a site\n&gt;&gt; from your crawl.\n&gt;&gt;\n&gt;&gt; Looking at the source now, it also seems like a option in Heritrix 1.0\n&gt;&gt; that could push &#39;long snoozes&#39; to the back of the list of waiting queues\n&gt;&gt; (preventing a logjam of unresponsive hosts among the active queues) does\n&gt;&gt; not appear to have been translated properly into H3, so is now inactive.\n&gt;&gt; (The WorkQueueFrontier property snoozeLongMs, a threshold above which\n&gt;&gt; snoozes are supposed to result in deactivation to the end of the list of\n&gt;&gt; waiting queues -- and thus potentially a much longer snooze -- to give\n&gt;&gt; other queues a chance.) Fixing this could improve this behavior, in\n&gt;&gt; crawls reaching many unresponsive hosts, as well.\n&gt;&gt;\n&gt;&gt; - Gordon\n&gt;&gt;\n&gt;&gt;&gt; --David\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 11/7/2011 2:44 PM, Gordon Mohr wrote:\n&gt;&gt;&gt;&gt; Heritrix is not very accommodating for an OPIC-ordered crawl in the way\n&gt;&gt;&gt;&gt; that it is not easy to reorder URIs inside a single queue, once they\n&gt;&gt;&gt;&gt; are\n&gt;&gt;&gt;&gt; queued. (You&#39;d have to peel them all off, sort, and re-queue... which\n&gt;&gt;&gt;&gt; might offset some or all of whatever ordering benefit you were\n&gt;&gt;&gt;&gt; hoping to\n&gt;&gt;&gt;&gt; achieve.)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; You can, however, re-prioritize entire queues via the queue-precedence\n&gt;&gt;&gt;&gt; mechanism. So you could bias the crawler to spend more early effort on\n&gt;&gt;&gt;&gt; sites that are large, or have a larger number of total inlinks, or\n&gt;&gt;&gt;&gt; other\n&gt;&gt;&gt;&gt; measures.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; If you are performing repeated crawls, you could also use PR\n&gt;&gt;&gt;&gt; calculations from a completed earlier crawl to bias the ordering of a\n&gt;&gt;&gt;&gt; later crawl (because you would have PR values usable at the moment of\n&gt;&gt;&gt;&gt; queueing).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; From a quick glance at the paper you reference, it doesn&#39;t seem like\n&gt;&gt;&gt;&gt; the magnitude of benefit for incremental-PR-estimation is very\n&gt;&gt;&gt;&gt; large. As\n&gt;&gt;&gt;&gt; they attribute to their footnote [11], and other broad-crawl studies\n&gt;&gt;&gt;&gt; have also suggested, &quot;even a random strategy can perform well on the\n&gt;&gt;&gt;&gt; Web, in the sense that a random walk on the Web is biased towards pages\n&gt;&gt;&gt;&gt; with high Pagerank&quot;. For this reason, I wouldn&#39;t be too concerned about\n&gt;&gt;&gt;&gt; crawling in PR-order unless you know you often have to cut your crawl\n&gt;&gt;&gt;&gt; short and thus strongly suspect more important pages are not being\n&gt;&gt;&gt;&gt; crawled... and even in that case, in a repeated crawl series you could\n&gt;&gt;&gt;&gt; adjust future crawls to offset this problem.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Also important to note: a strict &#39;breadth-first&#39; crawl isn&#39;t a\n&gt;&gt;&gt;&gt; realistic\n&gt;&gt;&gt;&gt; description of any large politeness-limited crawl. Almost\n&gt;&gt;&gt;&gt; immediately in\n&gt;&gt;&gt;&gt; any crawl, politeness-to-servers means you may only be a few hops deep\n&gt;&gt;&gt;&gt; into big sites, while being arbitrarily deep on other paths that are\n&gt;&gt;&gt;&gt; spread over many non-politeness-bottlenecked servers. This certainly\n&gt;&gt;&gt;&gt; helps in discovering (and crawling to completeness) new and\n&gt;&gt;&gt;&gt; small-URI-count independent servers... though it leaves the risk that\n&gt;&gt;&gt;&gt; potentially &#39;important&#39; (by PR) pages, &#39;deep&#39; within large servers, may\n&gt;&gt;&gt;&gt; not be reached within some capped time/storage budget.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - Gordon\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; On 11/7/11 11:08 AM, David Pane wrote:\n&gt;&gt;&gt;&gt;&gt; Hi Noah,\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; OPIC stands for Online Page Importance Computation.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; This scoring is in Nutch as a plugin\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Here is a paper that shows that OPIC scoring is better than a breadth\n&gt;&gt;&gt;&gt;&gt; first crawl.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; &quot;Crawling a country: better strategies than breadth-first for web page\n&gt;&gt;&gt;&gt;&gt; ordering&quot;\n&gt;&gt;&gt;&gt;&gt; http://dl.acm.org/citation.cfm?id=1062768\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; --David\n&gt;&gt;&gt;&gt;&gt; On 11/7/2011 12:18 PM, Noah Levitt wrote:\n&gt;&gt;&gt;&gt;&gt;&gt; Hello David,\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; What is OPIC?\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Noah\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; On 2011-10-31 12:37 , david_pane1 wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Is it possible to apply a crawling strategy based OPIC algorithm\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; to a\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Heritrix 3.x crawl?\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; We initially though that we could use the URI precedence, but it\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; appears that there isn&#39;t any provision for a URI&#39;s precedence to\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; change after initial assignment and as stated here\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/6022 by\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Gordon, depending on how they are queued, they could be crawled\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; out of\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; order.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Can anyone share some insight on this?\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; --David\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;\n\n"}}