{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234926651,"authorName":"Frank McCown","from":"Frank McCown &lt;fmccown@...&gt;","profile":"mccownf","replyTo":"LIST","senderId":"1tDqZEJK-7iKUnrL4xuACJrCfovnjbxbSQ8R_VwkDjYl2n6dPrSdZw-5Mf4hXf_4S-fjJ_nT12Moa3_EzXepXRT1f6Ml-Vll","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] General statistics","postDate":"1161087042","msgId":3451,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1MzRDODQyLjcwNjAxQGNzLm9kdS5lZHU+","inReplyToHeader":"PDQ1MzQ5RjdFLjgwNDAyMDZAY2VzY2EuZXM+","referencesHeader":"PDY4QzIyMTg1REI5MENBNDFBNUFDQkQ4RTgzNEM1RUNEMDM0Njc0RDRAZ29vZnkud3Bha2Iua2Iubmw+IDw0NTM0OUY3RS44MDQwMjA2QGNlc2NhLmVzPg=="},"prevInTopic":3450,"nextInTopic":3454,"prevInTime":3450,"nextInTime":3452,"topicId":3447,"numMessagesInTopic":8,"msgSnippet":"Natalia, An easy way to do this is to append the crawl data from other jobs into a single file and then access it in Heritrix by examining the Crawl Report.","rawEmail":"Return-Path: &lt;fmccown@...&gt;\r\nX-Sender: fmccown@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 85669 invoked from network); 17 Oct 2006 12:13:00 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m30.grp.scd.yahoo.com with QMQP; 17 Oct 2006 12:13:00 -0000\r\nReceived: from unknown (HELO cartero.cs.odu.edu) (128.82.4.9)\n  by mta4.grp.scd.yahoo.com with SMTP; 17 Oct 2006 12:13:00 -0000\r\nReceived: from [128.82.7.106] (bang.seven.research.odu.edu [128.82.7.106])\n\tby cartero.cs.odu.edu (8.13.6/8.13.6) with ESMTP id k9HC8vk1022557\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 17 Oct 2006 08:09:06 -0400 (EDT)\r\nMessage-ID: &lt;4534C842.70601@...&gt;\r\nDate: Tue, 17 Oct 2006 08:10:42 -0400\r\nUser-Agent: Mozilla Thunderbird 1.0 (Windows/20041206)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;68C22185DB90CA41A5ACBD8E834C5ECD034674D4@...&gt; &lt;45349F7E.8040206@...&gt;\r\nIn-Reply-To: &lt;45349F7E.8040206@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Frank McCown &lt;fmccown@...&gt;\r\nSubject: Re: [archive-crawler] General statistics\r\nX-Yahoo-Group-Post: member; u=234926651; y=sAJ1GNJVZDiRAYcG5JzGXWU5savZdDzR0xlllIqowCAaeQ\r\nX-Yahoo-Profile: mccownf\r\n\r\nNatalia,\n\nAn easy way to do this is to append the crawl data from other jobs into \na single file and then access it in Heritrix by examining the Crawl \nReport.  The crawl report is generated by examining several of the .txt \nfiles that are created for the completed job.\n\nFor example, create a directory called\n\n$HERITRIX_HOME/jobs/combined_job\n\nConcatenate the following files from each of your jobs:\n   responsecode-report.txt\n   mimetype-report.txt\n   hosts-report.txt\nand place them in $HERITRIX_HOME/jobs/combined_job.  Make sure you \nremove additional headers ([rescode] [#urls]) from the appended files.\n\nManually create a $HERITRIX_HOME/jobs/combined_job/crawl-report.txt to \nreflect the totals for all jobs (you could write a script to do this \nmuch easier).\n\nNow restart Heritrix (so it finds combined_job) and view the crawl report.\n\nFrank\n\n\nNatalia Torres wrote:\n&gt; \n&gt; \n&gt; Thanks Barts,\n&gt; \n&gt; I know about the information about each jop separatly, but I want to\n&gt; work with all this files at the same time.\n&gt; I want to make full reports and statics including the information of all\n&gt; crawled jobs as one.\n&gt; \n&gt; I need data like:\n&gt; \n&gt; Total Jobs Crawled: 25\n&gt; Total Seeds Crawled: 150\n&gt; Total Seeds Crawled from Job1: 150\n&gt; Total Seeds Crawled from Job2: 150\n&gt; ....\n&gt; Total Hosts Crawled: 18\n&gt; Total Hosts Crawled from Job1: 18\n&gt; Total Hosts Crawled from Job12: 18\n&gt; ...\n&gt; Total Documents Crawled: 889\n&gt; Total Documents Crawled from Job1: 8\n&gt; Total Documents Crawled from Job2: 58\n&gt; ....\n&gt; \n&gt; Total Raw Data Size in Bytes: 34829016 (33 MB)\n&gt; Total Raw Data Size in Bytes from Job1: 3 (xx MB)\n&gt; Total Raw Data Size in Bytes from Job2: 16 (xx MB)\n&gt; .....\n&gt; \n&gt; and show some graphs\n&gt; \n&gt; Natalia\n&gt; \n\n"}}