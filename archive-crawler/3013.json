{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Ed58EdW2Bpv_uxQWo_qCkBpVrEX9zFwf_4OgSt9_tRMTtA2MkQ-gYGv9xs2M3USuXP3O3N_gclpJPQriWvT31Ip1LwsLWII","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] robots.txt from every directory?","postDate":"1152224345","msgId":3013,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QUQ4QzU5LjYwODA4MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIwMDYwNzA2MjIxMTQ4LkdBMTE4NjNAZHV2ZWwuaXIuaWl0LmVkdT4=","referencesHeader":"PDIwMDYwNzA2MjIxMTQ4LkdBMTE4NjNAZHV2ZWwuaXIuaWl0LmVkdT4="},"prevInTopic":3012,"nextInTopic":3015,"prevInTime":3012,"nextInTime":3014,"topicId":3012,"numMessagesInTopic":3,"msgSnippet":"... The robots.txt standard only provides for root/host-level robots.txt files, so that s the only URI automatically checked by Heritrix. I suspect the pages","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 74962 invoked from network); 6 Jul 2006 22:17:43 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m37.grp.scd.yahoo.com with QMQP; 6 Jul 2006 22:17:43 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta1.grp.scd.yahoo.com with SMTP; 6 Jul 2006 22:17:43 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 57FB414156B12\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  6 Jul 2006 15:17:45 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 23542-01-23 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 6 Jul 2006 15:17:44 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id BF00114156A62\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  6 Jul 2006 15:17:44 -0700 (PDT)\r\nMessage-ID: &lt;44AD8C59.6080807@...&gt;\r\nDate: Thu, 06 Jul 2006 15:19:05 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;20060706221148.GA11863@...&gt;\r\nIn-Reply-To: &lt;20060706221148.GA11863@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] robots.txt from every directory?\r\nX-Yahoo-Group-Post: member; u=137285340; y=9H0E6wuQF0dk-GLslwu51Z3ZP4f6DXnO_Cld8Qwc-zb1\r\nX-Yahoo-Profile: gojomo\r\n\r\nEric wrote:\n&gt; In examining my crawl logs, I find that heritrix is trying to download\n&gt; robots.txt from every directory I access on the server, i.e.\n&gt; \n&gt; http://www.somewhere.com/a/robots.txt\n&gt; http://www.somewhere.com/b/robots.txt\n&gt; \n&gt; even though it&#39;s able to fetch the root\n&gt; http://www.somewhere.com/robots.txt just fine (it&#39;s in the arcs)\n&gt; \n&gt; This is a problem because some of my crawl has CGI&#39;s which use the\n&gt; path as their argument list, so I have many different &quot;directories&quot;\n&gt; that are really just CGI parameters.  How can I turn this off?\n&gt; \n&gt; I tried changing\n&gt; \n&gt; PreconditionEnforcer.java:176\n&gt; String prereq = curi.getUURI().resolve(&quot;/robots.txt&quot;).toString();\n&gt; \n&gt; But it doesn&#39;t seem to have done the trick.  \n\nThe robots.txt standard only provides for root/host-level robots.txt \nfiles, so that&#39;s the only URI automatically checked by Heritrix.\n\nI suspect the pages you are crawling must include some reference to \nthese other URIs. (Or, a redirect is occuring from some other URI to these.)\n\nFor the crawl.log lines showing these other fetches, what are the \n&#39;hops-path&#39; (string of capital letters like &#39;P&#39; or &#39;LLE&#39;) and &#39;via&#39; \n(precedent URI) shown?\n\nFor the Heritrix auto-fetch, the hops-path would end with &#39;P&#39;.\n\n- Gordon @ IA\n\n"}}