{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Vw1vBaFPNe0uhFvGH8nCAikNVvgj7uh2bWJkI4bMLXFnnDNljnhnFwJngPSoc7Z-kDvkeseifuPQbzaA1qOMvleBXsgRgKk","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Bandwidth Utilization [1 Attachment]","postDate":"1287370482","msgId":6773,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDQkJCNkYyLjYwNjAzMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRDQkFFRTNELjIwODAyMDRAYmliYWxleC5vcmc+","referencesHeader":"PDRDQkFFRTNELjIwODAyMDRAYmliYWxleC5vcmc+"},"prevInTopic":6771,"nextInTopic":6774,"prevInTime":6772,"nextInTime":6774,"topicId":6771,"numMessagesInTopic":5,"msgSnippet":"Hello, Youssef. The key things to look at in such a situation are: - is the machine swapping at all? If so, adjust other processes and your Java heap size so","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 56561 invoked from network); 18 Oct 2010 02:54:44 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.sp2.yahoo.com with QMQP; 18 Oct 2010 02:54:44 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta1.grp.sp2.yahoo.com with SMTP; 18 Oct 2010 02:54:44 -0000\r\nX-Received: (qmail 13322 invoked from network); 18 Oct 2010 02:54:42 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay02.pair.com with SMTP; 18 Oct 2010 02:54:42 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4CBBB6F2.6060305@...&gt;\r\nDate: Sun, 17 Oct 2010 19:54:42 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.9) Gecko/20100915 Thunderbird/3.1.4\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;4CBAEE3D.2080204@...&gt;\r\nIn-Reply-To: &lt;4CBAEE3D.2080204@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Bandwidth Utilization [1 Attachment]\r\nX-Yahoo-Group-Post: member; u=137285340; y=3rIZLBLyTMs4Ao7IvMip-NposJc_PRvbsRuek_S80Xcy\r\nX-Yahoo-Profile: gojomo\r\n\r\nHello, Youssef.\n\nThe key things to look at in such a situation are:\n\n- is the machine swapping at all? If so, adjust other processes and your \nJava heap size so that does not. Java memory access across its object \nheap don&#39;t have much predictable locality, so any swapping is fatal to \nperformance.\n\n- are all worker ToeThreads busy? If not, the politeness \n(one-request-to-a-host or delays-between-requests) is a major limiting \nfactor, and patience may be the best policy.\n\n- are the machine&#39;s CPU(s) maxed? if so you may be trying to do too much \non one machine. (What is your hardware and how many sites are you trying \nto crawl?)\n\n- is the machine mostly in IO-waits, or do disks (via iostat) show \nmaximum utilization? If so, IO is the bottleneck. Some big helps here \nare to spread the crawl&#39;s IO over more independent disks. Most \nimportant: store W/ARCs to a different volume than any other activity \n(like the &#39;state&#39; or &#39;scratch&#39; directories), and ideally to more than \none independent store-path.\n\n- does the &#39;threads&#39; report show all threads busy in the same steps, or \nspending a lot of time on unresponsive hosts? This report often gives a \nclue as to what&#39;s unique about a given crawl. (Occasionally, a crawler \nwill come across a large number of semi-responsive sites that all \nrequire the maximum 20-minute connection timeout for each fetch-try. \nThat can tie-up all threads doing nothing but waiting; if this is the \ncase it would be clear from the threads report and timeouts in the \nlocal-errors.log.)\n\nMy guess is that swapping, politeness or IO bottlenecks are most likely \nthe cause of your slowness.\n\nSome other comments on our order.xml -- not necessarily related to the \nprogress-rate issue:\n\n- you&#39;re using default paths for &#39;scratch&#39; and &#39;state&#39; disk IO, so it&#39;s \nall happening in the default job directory, so that one disk may be the \nbottleneck. If the machine has multiple independent disks, you can \nspecify absolute paths to other disks there. (I see you are already \nspecifying 4 paths for ARC-writing; that&#39;s good, but you should probably \nleave off any volumes that are the target of other IO.)\n\n- 300 ToeThreads might be high unless the machine has 4 or more cores. \n(We don&#39;t have any strong rules of thumbs here; we&#39;ve used 150-250 \nthreads at times on dual-core machines with 4GB-8GB RAM.)\n\n- if your machine has a large amount of RAM, you could increase the \n&quot;recorder-in-buffer-bytes&quot; value to save a lot of writing to/from \n&#39;scratch&#39; when dealing with large resources. (In H3, the default has \nbeen raised to 512KB from 64KB, but you could go even larger. Just \nremember one such buffer is preallocated for each ToeThread.)\n\n- You&#39;re using the legacy DomainScope (which is discouraged in H1.14 and \ngone from H3). We recommend building on the example DecidingScope.\n\n- The User-Agent URL you provide, &lt;http://www.archive.bibalex.org&gt;, does \nnot include information about the purpose of your crawl or how to \ncontact the operator when problems arise. (Most webmasters don&#39;t log and \ncan&#39;t see the &#39;From&#39; value provided, so the URL has to explain what \nwebmasters can do to contact you.)\n\n- Once you&#39;ve heavily customized the crawler behavior, it&#39;s more \nimportant that your User-Agent describe your institution than \n&#39;heritrix&#39;. (You might be a more nice or more nasty crawler than other \naverage &#39;heritrix&#39; users, and so may not benefit from being categorized \nas the same from a website and robots.txt perspective.)\n\n- Setting all your politeness-delays to &#39;0&#39; may have made sense as an \nexperiment trying to get past your slowness, but is likely to cause \nproblems for some sites, and get your crawler blocked, when the crawler \nis working against those sites at full speed.\n\n- A cost-policy other than the ZeroCost one can help rotate the \ncrawler&#39;s attention over a large number of sites, in large crawls.\n\nHope this helps,\n\n// Gordon @ IA\n\nOn 10/17/10 5:38 AM, Youssef Eldakar wrote:\n&gt; We are experiencing difficulty maintaining good bandwidth utilization\n&gt; with Heritrix similar to what Dereck Pappas described in an earlier message:\n&gt;\n&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/6353\n&gt;\n&gt; We are running Heritrix on a 155-Mbps link to the Internet, and we have\n&gt; tried both H1 and H3 with no significant change in bandwidth\n&gt; utilization. As described in the message referenced above, a crawl\n&gt; starts out making sort of OK utilization (3 Mbps per crawling machine)\n&gt; but then gradually drops down to only a few kilobits/second. We tried\n&gt; tweaking the &quot;politeness&quot; settings in Heritrix, but that still did not\n&gt; help keep utilization from dropping to the kilobit/second scale. We have\n&gt; been experimenting with seed lists with as many as 10,000 and even\n&gt; 100,000 entries per list. I am attaching a copy of the H1 order.xml we\n&gt; typically use.\n&gt;\n&gt; Any suggestions on how to get Heritrix to maintain better bandwidth\n&gt; utilization are appreciated.\n&gt;\n\n"}}