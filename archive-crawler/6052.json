{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"oNFEoXKxn4QDLuPPtgY23Qz_knbsVjVu4SeZy8KjvUgDVc-GCObPix4GjHWtMxUSO-kfG1UKoIxKHrfqd14ohTt2Rdj1Il4","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Basic question: How to limit the crawling scope within a host?","postDate":"1253984136","msgId":6052,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBQkU0Nzg4LjMwNDAzMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGg5bGJvZyticWpiQGVHcm91cHMuY29tPg==","referencesHeader":"PGg5bGJvZyticWpiQGVHcm91cHMuY29tPg=="},"prevInTopic":6051,"nextInTopic":6053,"prevInTime":6051,"nextInTime":6053,"topicId":6051,"numMessagesInTopic":6,"msgSnippet":"... The included deciding-default profile should work for the purpose of getting all pages on a single web host. ... Supplying that as a seed, with the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 19406 invoked from network); 26 Sep 2009 16:55:43 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m5.grp.sp2.yahoo.com with QMQP; 26 Sep 2009 16:55:43 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta1.grp.sp2.yahoo.com with SMTP; 26 Sep 2009 16:55:43 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 4AE7B352EF\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Sat, 26 Sep 2009 09:56:35 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id Hv13qplP8x2O for &lt;archive-crawler@yahoogroups.com&gt;;\n\tSat, 26 Sep 2009 09:56:33 -0700 (PDT)\r\nX-Received: from [10.0.13.17] (adsl-70-137-161-154.dsl.snfc21.sbcglobal.net [70.137.161.154])\n\tby mail.archive.org (Postfix) with ESMTPSA id CAC7734E19\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Sat, 26 Sep 2009 09:56:33 -0700 (PDT)\r\nMessage-ID: &lt;4ABE4788.3040304@...&gt;\r\nDate: Sat, 26 Sep 2009 09:55:36 -0700\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;h9lbog+bqjb@...&gt;\r\nIn-Reply-To: &lt;h9lbog+bqjb@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Basic question: How to limit the crawling scope\n within a host?\r\nX-Yahoo-Group-Post: member; u=137285340; y=3IYAWoSkIy3gcs3IfHD1nU8dXSfMvyE6iznXlRNQn07c\r\nX-Yahoo-Profile: gojomo\r\n\r\nshichuanwuhan@... wrote:\n&gt; Hi all,\n&gt; \n&gt; I am using version 1.14.3. My final goal is to get all the URLs of professors&#39; mainpages on one host i.e www.cs.cmu.edu. So firstly, I plan to fetch all pages that are within the host. However, I fail to do that.\n&gt; \n&gt; I tried both the traditional &#39;hostscope&#39; and recommended &#39;decidingscope&#39; but I still cannot achieve my goal. As English is not my native language, maybe I misunderstand something in &#39;user manual&#39;. Would someone kindly answer several questions?\n\nThe included &#39;deciding-default&#39; profile should work for the purpose of \ngetting all pages on a single web host.\n\n&gt; 1.My seed is simply: &#39;http://www.cs.cmu.edu/&#39;. Is it right?\n\nSupplying that as a seed, with the deciding-default settings, should \ncause the crawler to:\n\n(1) start by visiting &quot;http://www.cs.cmu.edu/&quot;, examining the outlinks \nof that page for &quot;in-scope&quot; URIs\n\n(2) evaluate any URIs that begin &quot;http://www.cs.cmu.edu/&quot; as being \n&quot;in-scope&quot; (along with some other rules), and thus eligible for \nrecursive fetching\n\nThe default configuration, with only the single seed, will only wander \noff &quot;www.cs.cmu.edu&quot; to fetch URIs that appears necessary to render \nanother page (like inline references to scripts, images, frames, etc., \nor URLs found in Javascript that may auto-load).\n\nI see that the &#39;faculty&#39; link from &#39;www.cs.cmu.edu&#39; goes to another \nhost, &#39;people.cs.cmu.edu. Your crawl will not in general visit that \nother host without additional scope customization to say those URLs are \nof interest. Also, it appears that faculty web pages are on a variety of \nhosts (including &#39;www-2&#39; and other departmental servers).\n\n&gt; 2.Is it possible that I simply use &#39;hostscope&#39; to achieve my goal? \n\nIt might be possible, but it is not recommended -- HostScope is \ndeprecated, less efficient and flexible than the DecidingScope + \nSurtPrefixedDecideRule mechanism.\n\n&gt; 3.If not, how to configure &#39;decidingscope&#39;?\n\nYou probably want to tell your crawl that begins at www.cs.cmu.edu that \nit may accept URIs on other hosts, like &#39;people.cs.cmu.edu&#39; and others, \nas &#39;in-scope&#39;. This involves giving the SurtPrefixedDecideRule more \nacceptable &#39;SURT&#39; (URI-like) prefixes.\n\nThis can be done either by specifying a file with such prefixes as the \nSurtPrefixedDecideRule&#39;s &#39;surt-source-file&#39;, or by adding lines to your \nseeds list that begin &#39;+&#39;.\n\nYou can read more about SURTs as a means of scoping at:\n\nhttp://crawler.archive.org/articles/user_manual/config.html#surtprefixscope\n\nAnother strategy might be to start your crawling at the faculty directory:\n\nhttp://people.cs.cmu.edu/faculty/index.html\n\nSupplying that URI as a seed will cause the &quot;implied scope&quot; to be all \nURLs beginning &quot;http://people.cs.cmu.edu/faculty/&quot; -- which should get \nall the other pages of the directory, as well. *If* you are confident \nall homepages always contain the &#39;~&#39; character, you could also add a new \nrule to the list of rules, such as a MatchesRegExpDecideRule, that \nalways ACCEPTs any URI with a &#39;~&#39; character. That would get the \ndirectory, and all the linked pages with &#39;~&#39; anywhere in their URI (and \nquite probably other pages, at other hosts and universities, when their \nURIs with &#39;~&#39; are discovered).\n\nHope this helps,\n\n- Gordon @ IA\n\n\n\n\n\n\n"}}