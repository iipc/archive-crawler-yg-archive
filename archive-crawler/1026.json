{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":183483403,"authorName":"Sebastian de Castelberg","from":"Sebastian de Castelberg &lt;sdecaste@...&gt;","replyTo":"LIST","senderId":"9oQyPzeu5PgPr--FySWgc4ktVM2boE7pyMx0SyR815zEZPXvJCkeZ7Yx0KPgs38xquuHDxInW9mU0hdeN0UQhSiV7NKzCik1ZhLe2p7SO74","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Large experimental crawl","postDate":"1095931922","msgId":1026,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxNTI5ODEyLjIwNjA0MDNAaHNyLmNoPg==","inReplyToHeader":"PDE2NzE4LjUzOTMwLjg2MzAxOC44NjkzOTJAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","referencesHeader":"PDE2NzE4LjUzOTMwLjg2MzAxOC44NjkzOTJAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4="},"prevInTopic":1008,"nextInTopic":1027,"prevInTime":1025,"nextInTime":1027,"topicId":1005,"numMessagesInTopic":6,"msgSnippet":"Hi, for a research-project we have to implement two different random-walk algorithms for uniform page sampling. This needs us to gather about 2-4Mio. URL s. So","rawEmail":"Return-Path: &lt;sdecaste@...&gt;\r\nX-Sender: sdecaste@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 3641 invoked from network); 23 Sep 2004 09:36:31 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m8.grp.scd.yahoo.com with QMQP; 23 Sep 2004 09:36:31 -0000\r\nReceived: from unknown (HELO hsrmx1.hsr.ch) (152.96.36.50)\n  by mta3.grp.scd.yahoo.com with SMTP; 23 Sep 2004 09:36:31 -0000\r\nReceived: from localhost (hsrmx1 [127.0.0.1])\n\tby hsrmx1.hsr.ch (Postfix) with ESMTP id 33BDD1138EC\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Sep 2004 11:36:26 +0200 (CEST)\r\nReceived: from hsrmx1.hsr.ch ([127.0.0.1])\n by localhost (hsrmx1 [127.0.0.1]) (amavisd-new, port 10024) with ESMTP\n id 20931-09 for &lt;archive-crawler@yahoogroups.com&gt;;\n Thu, 23 Sep 2004 11:36:23 +0200 (CEST)\r\nReceived: from sid00062.hsr.ch (unknown [152.96.21.100])\n\tby hsrmx1.hsr.ch (Postfix) with ESMTP id 793FB1138E8\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Sep 2004 11:36:23 +0200 (CEST)\r\nReceived: from sid00060.hsr.ch ([152.96.22.60]) by sid00062.hsr.ch with Microsoft SMTPSVC(6.0.3790.0);\n\t Thu, 23 Sep 2004 11:35:10 +0200\r\nReceived: from sid00060-2.hsr.ch ([152.96.22.60]) by sid00060.hsr.ch with Microsoft SMTPSVC(6.0.3790.0);\n\t Thu, 23 Sep 2004 11:35:10 +0200\r\nReceived: from [192.168.1.34] ([80.238.204.34]) by sid00060-2.hsr.ch with Microsoft SMTPSVC(6.0.3790.0);\n\t Thu, 23 Sep 2004 11:35:10 +0200\r\nMessage-ID: &lt;41529812.2060403@...&gt;\r\nDate: Thu, 23 Sep 2004 11:32:02 +0200\r\nUser-Agent: Mozilla Thunderbird 0.7.3 (X11/20040912)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;16718.53930.863018.869392@...&gt;\r\nIn-Reply-To: &lt;16718.53930.863018.869392@...&gt;\r\nX-Enigmail-Version: 0.85.0.0\r\nX-Enigmail-Supports: pgp-inline, pgp-mime\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-OriginalArrivalTime: 23 Sep 2004 09:35:10.0119 (UTC) FILETIME=[9E75C370:01C4A150]\r\nX-eGroups-Remote-IP: 152.96.36.50\r\nFrom: Sebastian de Castelberg &lt;sdecaste@...&gt;\r\nReply-To: archive-crawler@yahoogroups.com\r\nSubject: Large experimental crawl\r\nX-Yahoo-Group-Post: member; u=183483403\r\n\r\nHi,\n\nfor a research-project we have to implement two different random-walk \nalgorithms for uniform page sampling. This needs us to gather about \n2-4Mio. URL&#39;s.\nSo we have to chose an adapted Broad Crawl, which chooses the URL&#39;s, \nwhich are fed back into the frontier, randomly. So the fetch queue\nwouldn&#39;t grow exponential. We also do not need to write the whole \ncontent to disk.\nHeritrix seemed to work quite well as crawler for antother project \n(thanks for the good development work at this place). But we got often \nproblems, based on memory limitations.\n\nOn the known limitations page, there&#39;s written that it is possible to \ncrawl about 6Mio URL&#39;s and about 10000 hosts with default settings.\nMy question is: What&#39;s the best setup to reach this number of URL&#39;s\n(HW/Java heap size/Heritrix config)?\nDo we need special hardware, or can it be done with a common pc (p4 \n2.6GHz 512-1024 MB ram)?\n\nWe planned to use Debian GNU/Linux as os. Maybe there&#39;s someone who has \nalready experiences with large-scaled crawls and can give me some hints.\n\nthanks\nsebastian de castelberg\n\n"}}