{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"FjAdWqXv-ZcrVkIV9pV_lz3jFuOjixLD2M49_pEVcklKw-7L52xa6RTup0nVEH6KSycG6F4hNTKRMZUdGf5H6CLxUbG-y-o","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re-discover links","postDate":"1300500872","msgId":7066,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREODQxMTg4LjkwNDA1MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDU4Nzk1Mi45NTAyNC5xbUB3ZWIxNTkwNC5tYWlsLmNuYi55YWhvby5jb20+","referencesHeader":"PDU4Nzk1Mi45NTAyNC5xbUB3ZWIxNTkwNC5tYWlsLmNuYi55YWhvby5jb20+"},"prevInTopic":7058,"nextInTopic":7071,"prevInTime":7065,"nextInTime":7067,"topicId":7044,"numMessagesInTopic":11,"msgSnippet":"Both Heritrix 1.14.4 and Heritrix 3.0.0 include the WARCWriterProcessor as an alternative way to write crawled content. in both versions, if you use the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 23380 invoked from network); 19 Mar 2011 02:14:33 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m16.grp.re1.yahoo.com with QMQP; 19 Mar 2011 02:14:33 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta1.grp.re1.yahoo.com with SMTP; 19 Mar 2011 02:14:33 -0000\r\nX-Received: (qmail 47974 invoked by uid 0); 19 Mar 2011 02:14:32 -0000\r\nX-Received: from 24.27.47.249 (HELO silverbook.local) (24.27.47.249)\n  by relay01.pair.com with SMTP; 19 Mar 2011 02:14:32 -0000\r\nX-pair-Authenticated: 24.27.47.249\r\nMessage-ID: &lt;4D841188.9040506@...&gt;\r\nDate: Fri, 18 Mar 2011 19:14:32 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.15) Gecko/20110303 Thunderbird/3.1.9\r\nMIME-Version: 1.0\r\nTo: HONGYING YI &lt;y195322@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;587952.95024.qm@...&gt;\r\nIn-Reply-To: &lt;587952.95024.qm@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re-discover links\r\nX-Yahoo-Group-Post: member; u=137285340; y=ArzZtV3Fqs2LILHkL5RVicUYOIFkm3UMWddW-F-GeTWz\r\nX-Yahoo-Profile: gojomo\r\n\r\nBoth Heritrix 1.14.4 and Heritrix 3.0.0 include the WARCWriterProcessor \nas an alternative way to write crawled content.\n\nin both versions, if you use the WARCWriterProcessor,  then &#39;metadata&#39; \nrecords that include outlinks will be written alongside the other content.\n\nSo you don&#39;t need to upgrade just to get the outlinks written to a file \nduring crawling.\n\nIf having the outlinks listed in the WARC file is not enough, then you \ncan write your own code that processes the oulinks in some other way. I \nthink adding your own modules is a little easier in Heritrix 3, but is \njust as possible in Heritrix 1.14.4.\n\n- Gordon @ IA\n\nOn 3/9/11 1:51 PM, HONGYING YI wrote:\n&gt; Hi Gordon,\n&gt; Thanks for your informative reply!\n&gt; I am using Heritrix 1.14.4 now. For WARCWriterProcessor, if I still use\n&gt; Heritrix 1.14.4, it seems that I will need to make some changes in the\n&gt; source codes. If I upgrade it to Heritrix 3, then I can get all the\n&gt; outlinks in the data without making changes to the source codes. Is that\n&gt; correct?\n&gt; Thanks!\n&gt;\n&gt;\n&gt; --- *11年3月3日，周四, Gordon Mohr /&lt;gojomo@...&gt;/* 写道：\n&gt;\n&gt;\n&gt;     发件人: Gordon Mohr &lt;gojomo@...&gt;\n&gt;     主题: Re: [archive-crawler] Re-discover links\n&gt;     收件人: &quot;HONGYING YI&quot; &lt;y195322@...&gt;\n&gt;     抄送: archive-crawler@yahoogroups.com\n&gt;     日期: 2011年3月3日,周四,上午9:35\n&gt;\n&gt;     On 3/2/11 3:47 PM, HONGYING YI wrote:\n&gt;      &gt; Hi Gordon,\n&gt;      &gt;\n&gt;      &gt; Thanks for your reply!\n&gt;      &gt;\n&gt;      &gt; I&#39;m still a little confused. Are all the outlinks of a URI logged\n&gt;     in the\n&gt;      &gt; crawl.log whether or not those outlinks have already been discovered?\n&gt;\n&gt;     Outlinks (specifically outlinks) are not logged to crawl.log. The\n&gt;     crawl.log is a log of *completed* URIs. So to appear in crawl.log, a\n&gt;     URI has to:\n&gt;\n&gt;     (1) be discovered (as a seed or outlink from elsewhere); then...\n&gt;     (2) pass scoping rules, so that it is enqueued to be fetched; then...\n&gt;     (3) either be successfully fetched, or suffer definitive failure\n&gt;     (certain errors from the server or no-success after many retries).\n&gt;\n&gt;     This crawl.log logging happens arbitrarily long after the URI was\n&gt;     first discovered. (The URI might be waiting in a queue for its\n&gt;     chance to be crawled for hours, days, weeks, months.)\n&gt;\n&gt;     In a typical snapshot crawl, most normal URIs are only fetched once,\n&gt;     so they only appear in the crawl.log once. (The &#39;via&#39; URI that is\n&gt;     shown later on the crawl.log line is the one other URI from which it\n&gt;     was specifically discovered first, even though it may have later\n&gt;     been found many times.)\n&gt;\n&gt;     (URIs which must be retried to refresh our knowledge, like DNS and\n&gt;     /robots.txt, will be fetched and appear in crawl.log multiple times.\n&gt;     Also, there are certain non-standard ways you can force a URI to be\n&gt;     refetched. But you wouldn&#39;t use those mechanisms to achieve what I\n&gt;     believe is your goal, discovering all the links to a page.)\n&gt;\n&gt;      &gt; I&#39;m not familiar with WARCWriterProcessor, could you explain more\n&gt;     on that?\n&gt;\n&gt;     In Heritrix 1.x releases, the default way to write web content was\n&gt;     the ARCWriterProcessor, which wrote HTTP responses to long\n&gt;     transcript files called ARCs (with extension .arc.gz). Since only\n&gt;     the raw content was written, to get the outlinks again at a later\n&gt;     time, you&#39;d need to parse the HTML again.\n&gt;\n&gt;     In Heritrix 3, a new &#39;WARC&#39; format is preferred, via the\n&gt;     WARCWriterProcessor. While similar to the ARC format it has more\n&gt;     places for extra metadata. By default, it writes not just the HTTP\n&gt;     response (like in ARC) but also the exact HTTP request and an extra\n&gt;     &#39;metadata&#39; record with a list of the outlinks that were discovered.\n&gt;     Outlinks are listed here even if they did not pass scope-testing (so\n&gt;     would not be enqueued/fetched and thus would never appear in the\n&gt;     crawl.log).\n&gt;\n&gt;     You can use WARCWriterProcessor in H1, also, by replacing the\n&gt;     reference to ARCWriterProcessor with WARCWriterProcessor.\n&gt;\n&gt;     Even if you don&#39;t want to use WARCWriterProcessor, you could look at\n&gt;     its source code to see how it looks at the list of discovered\n&gt;     outlinks, and log them to someplace of your choosing, by writing\n&gt;     your own custom Processor code to plug into Heritrix.\n&gt;\n&gt;     Hope this helps,\n&gt;\n&gt;     - Gordon @ IA\n&gt;\n&gt;      &gt; Thanks!\n&gt;      &gt;\n&gt;      &gt; --- *11年3月1日，周二, Gordon Mohr /&lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;/*\n&gt;     写道：\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt; 发件人: Gordon Mohr &lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;\n&gt;      &gt; 主题: Re: [archive-crawler] Re-discover links\n&gt;      &gt; 收件人: &quot;HONGYING YI&quot; &lt;y195322@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;&gt;\n&gt;      &gt; 抄送: archive-crawler@yahoogroups.com\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt; 日期: 2011年3月1日,周二,上午7:40\n&gt;      &gt;\n&gt;      &gt; All the outlinks discovered from a URI are inside the CrawlURI during\n&gt;      &gt; its processing, until it is finished (and logged in the crawl.log).\n&gt;      &gt;\n&gt;      &gt; If you use the WARCWriterProcessor, they are part of the data\n&gt;      &gt; written in\n&gt;      &gt; the &#39;metadata&#39; record associated with the URI.\n&gt;      &gt;\n&gt;      &gt; Alternatively, you could insert your own custom processor to log this\n&gt;      &gt; data somewhere else (using the WARCWriterProcessor&#39;s writeMetadata\n&gt;      &gt; method as a model).\n&gt;      &gt;\n&gt;      &gt; In this way, you can get the link data you want, without\n&gt;     revisiting the\n&gt;      &gt; same URLs many times (indeed infinitely via link-loops!).\n&gt;      &gt;\n&gt;      &gt; - Gordon @ IA\n&gt;      &gt;\n&gt;      &gt; On 2/28/11 3:16 PM, HONGYING YI wrote:\n&gt;      &gt; &gt; Hi Gordon,\n&gt;      &gt; &gt; We want to know all the out-going links and in-coming links for\n&gt;     each\n&gt;      &gt; &gt; page, and in the example, for aol.com, aolhealth.com is an\n&gt;     out-going\n&gt;      &gt; &gt; link for aol.com, and drugchecker.aolhealth.com is an in-coming\n&gt;      &gt; link for\n&gt;      &gt; &gt; aol.com. If the link from drugchecker.aolhealth.com to aol.com\n&gt;      &gt; does not\n&gt;      &gt; &gt; show up, we will not get all the in-coming links for aol.com. Is\n&gt;      &gt; there\n&gt;      &gt; &gt; any way to retrieve that link? Can any configuration setting in the\n&gt;      &gt; &gt; crawl get round that?\n&gt;      &gt; &gt; Thanks!\n&gt;      &gt; &gt; Yang\n&gt;      &gt; &gt;\n&gt;      &gt; &gt;\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; --- *11年3月1日，周二, Gordon Mohr /&lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;/*\n&gt;      &gt; 写道：\n&gt;      &gt; &gt;\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; 发件人: Gordon Mohr &lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;\n&gt;      &gt; &gt; 主题: Re: [archive-crawler] Re-discover links\n&gt;      &gt; &gt; 收件人: archive-crawler@yahoogroups.com\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt;\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt; &gt; 抄送: &quot;y195322&quot; &lt;y195322@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;&gt;\n&gt;      &gt; &gt; 日期: 2011年3月1日,周二,上午6:33\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; On 2/28/11 10:20 AM, y195322 wrote:\n&gt;      &gt; &gt; &gt; Hi!We are doing web crawling using Heritrix, and found one issue\n&gt;      &gt; &gt; in that once a link has been &#39;discovered,&#39; it won&#39;t be\n&gt;      &gt; &gt; &#39;rediscovered.&#39; This means that if we have something like:\n&gt;      &gt; &gt; &gt; aol.com-&gt;aolhealth.com\n&gt;      &gt; &gt; &gt; aolhealth.com-&gt;drugchecker.aolhealth.com\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; and if drugchecker.aolhealth.com links to aol.com, that link will\n&gt;      &gt; &gt; not show up in the file.\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; Is there any way to fix this problem? Thanks!\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; Usually a single snapshot crawl only wants to retrieve each URL\n&gt;      &gt; &gt; once, so\n&gt;      &gt; &gt; this is by-design, rather than a problem.\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; Why do you want to fetch aol.com twice in a single crawl? (Or am I\n&gt;      &gt; &gt; misunderstanding?)\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; - Gordon @ IA\n&gt;      &gt; &gt;\n&gt;      &gt; &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt;\n&gt;\n\n"}}