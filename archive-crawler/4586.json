{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":317026262,"authorName":"Martin Kammerlander","from":"Martin Kammerlander &lt;Martin.Kammerlander@...&gt;","profile":"mkammerlander","replyTo":"LIST","senderId":"jvYjLeFSXD5WgcZKmF_mMOvoCamBvS3TfILIZB6RbsFEu3bwQWzXV6f9wZF4zbHmeKDMBm2fuIz5IGB2OYSguDlWAOAR6DrrypC5N1b-7SaCuWOKWWmLWx4mgNVO14jCi8oP","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: fetch only pdf data","postDate":"1191517629","msgId":4586,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PDExOTE1MTc2MjkuNDcwNTFkYmQ3ZDUwOEB3ZWItbWFpbDEudWliay5hYy5hdD4=","inReplyToHeader":"PDQ2OUU2MjU5LjIwMzAwMDZAYXJjaGl2ZS5vcmc+","referencesHeader":"PGY3bGwwOSs3cmJvQGVHcm91cHMuY29tPiA8NDY5RTYyNTkuMjAzMDAwNkBhcmNoaXZlLm9yZz4="},"prevInTopic":4444,"nextInTopic":0,"prevInTime":4585,"nextInTime":4587,"topicId":4442,"numMessagesInTopic":4,"msgSnippet":"Hi Gordon, Hi all! Sorry for replying late. Gordon you are right, that is exactly what I wanted! The proposal from joeyfreund indeed causes that all seed urls","rawEmail":"Return-Path: &lt;Martin.Kammerlander@...&gt;\r\nReceived: (qmail 42087 invoked from network); 4 Oct 2007 17:11:51 -0000\r\nReceived: from unknown (69.147.108.202)\n  by m49.grp.scd.yahoo.com with QMQP; 4 Oct 2007 17:11:51 -0000\r\nReceived: from unknown (HELO n18a.bullet.sp1.yahoo.com) (69.147.64.127)\n  by mta3.grp.re1.yahoo.com with SMTP; 4 Oct 2007 17:11:51 -0000\r\nReceived: from [216.252.122.218] by n18.bullet.sp1.yahoo.com with NNFMP; 04 Oct 2007 17:09:26 -0000\r\nReceived: from [66.218.69.2] by t3.bullet.sp1.yahoo.com with NNFMP; 04 Oct 2007 17:09:26 -0000\r\nReceived: from [66.218.66.86] by t2.bullet.scd.yahoo.com with NNFMP; 04 Oct 2007 17:09:26 -0000\r\nX-Sender: Martin.Kammerlander@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 54526 invoked from network); 4 Oct 2007 17:07:13 -0000\r\nReceived: from unknown (69.147.108.201)\n  by m57.grp.scd.yahoo.com with QMQP; 4 Oct 2007 17:07:13 -0000\r\nReceived: from unknown (HELO smtp.uibk.ac.at) (138.232.1.142)\n  by mta2.grp.re1.yahoo.com with SMTP; 4 Oct 2007 17:07:13 -0000\r\nReceived: from localhost (lwm1.uibk.ac.at [138.232.1.160] Martin.Kammerlander@...)\n        by smtp.uibk.ac.at (8.13.8/8.13.8/F1) with ESMTP id l94H79xX009532\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 4 Oct 2007 19:07:09 +0200\r\nReceived: from 85.127.20.97 ([85.127.20.97]) \n\tby web-mail1.uibk.ac.at (IMP) with HTTP \n\tfor &lt;csad3934@...&gt;; Thu,  4 Oct 2007 19:07:09 +0200\r\nMessage-ID: &lt;1191517629.47051dbd7d508@...&gt;\r\nDate: Thu,  4 Oct 2007 19:07:09 +0200\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f7ll09+7rbo@...&gt; &lt;469E6259.2030006@...&gt;\r\nIn-Reply-To: &lt;469E6259.2030006@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nUser-Agent: Internet Messaging Program (IMP) 3.2.6\r\nX-Forwarded-For: \r\nX-Spam-Score: () -7.4 ALL_TRUSTED,RCV_SMTP_UIBK,RCV_WEBMAIL\r\nX-Scanned-By: MIMEDefang 2.61 at uibk.ac.at on 138.232.1.140\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Martin Kammerlander &lt;Martin.Kammerlander@...&gt;\r\nSubject: Re: [archive-crawler] Re: fetch only pdf data\r\nX-Yahoo-Group-Post: member; u=317026262; y=J6ayOj0-miFhMc2Ezya4hhRKweLx_prePQlLcO8Ds0Xdzogx5QSeTQ\r\nX-Yahoo-Profile: mkammerlander\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 04 Oct 2007 17:09:25 -0000\r\n\r\nHi Gordon, Hi all!\n\nSorry for replying late. Gordon you are right, that is exactly what I wanted!\nThe proposal from joeyfreund indeed causes that all seed urls are rejected which\nare not direkt links to pdf files. As you mentioned Gordon, I wanna do a crawl\nbased on seed entries. And of course it should crawl also non pdf sites but\nonly &#39;save&#39; me then direct links to pdf files.\n\n&gt; This would mean:\n&gt;   - changing the &#39;scope&#39; with decide-rules that reject URLs that won&#39;t\n&gt; contain links (like image file extensions)\n&gt;   - adding rules like Joey suggests to the writing processor\n&gt; (ArcWriterProcessor in usual configurations) that cause it to reject\n&gt; (not process) all non-PDF document\n\nCould you tell me the above in a little bit more detailed way? I suppose that\nthe first point you mentioned is to set in the &quot;Select Crawl Scope&quot; module and\nfurthermore rules in its submodules, isn&#39;t it like that? What exactly should I\ntake there as a rule?\n\nthanks\nmartin\n\n\nZitat von Gordon Mohr &lt;gojomo@...&gt;:\n\n&gt; This will indeed restrict the crawler to only fetching &quot;.pdf&quot; files --\n&gt; but that also means it won&#39;t find any others that aren&#39;t directly added\n&gt; as seeds.\n&gt;\n&gt; I suspect Martin will want to crawl and link-extract any file type that\n&gt; might contain links, but only *save* discovered PDFs.\n&gt;\n&gt; This would mean:\n&gt;   - changing the &#39;scope&#39; with decide-rules that reject URLs that won&#39;t\n&gt; contain links (like image file extensions)\n&gt;   - adding rules like Joey suggests to the writing processor\n&gt; (ArcWriterProcessor in usual configurations) that cause it to reject\n&gt; (not process) all non-PDF documents\n&gt;\n&gt; Hope this helps,\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt; joeyfreund wrote:\n&gt; &gt; Add two filters in the decide rules -\n&gt; &gt; 1. org.archive.crawler.deciderules.RejectDecideRule\n&gt; &gt; 2. org.archive.crawler.deciderules.MatchesFilePatternDecideRule\n&gt; &gt;\n&gt; &gt; Go to &quot;settings&quot; and set the second rule to accept .pdf files.\n&gt; &gt;\n&gt; &gt; Joey\n&gt; &gt;\n&gt; &gt; --- In archive-crawler@yahoogroups.com, &quot;mkammerlander&quot;\n&gt; &gt; &lt;Martin.Kammerlander@...&gt; wrote:\n&gt; &gt;&gt; Hi\n&gt; &gt;&gt;\n&gt; &gt;&gt; First of all: I&#39;m new on heritrix.\n&gt; &gt;&gt;\n&gt; &gt;&gt; I have a few seed urls and I want to do a crawling based on this\n&gt; &gt;&gt; seeds. Now I&#39;m not sure how to handle it that only web sites that\n&gt; &gt; are\n&gt; &gt;&gt; pdf&#39;s will be fetched...all other &#39;mime&#39; types should be ignored.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Any ideas which configuration I have to take for reaching this?\n&gt; &gt;&gt;\n&gt; &gt;&gt; thx\n&gt; &gt;&gt; martin\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;\n\n\n\n\n"}}