{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":278782393,"authorName":"Paul Jack","from":"Paul Jack &lt;pjack@...&gt;","profile":"poetbeware","replyTo":"LIST","senderId":"J5g-CkDiRHxlRDCkfSxUkJ7t6-_AVtUI73NqU6szsHCjnFLSp9dq7R3AnPR4WDz3i8rccuKuq-m9JK1PJMEb-4iZskE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Sheets","postDate":"1198013476","msgId":4859,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDc1MTU2RTQyLTIzMzItNEUxMC05MEVBLThFQ0M1RjA2RkE5M0BhcmNoaXZlLm9yZz4=","inReplyToHeader":"PDI1ODk4Mi4yNzM3Ni5xbUB3ZWI1NDYwMi5tYWlsLnJlMi55YWhvby5jb20+","referencesHeader":"PDI1ODk4Mi4yNzM3Ni5xbUB3ZWI1NDYwMi5tYWlsLnJlMi55YWhvby5jb20+"},"prevInTopic":4858,"nextInTopic":4860,"prevInTime":4858,"nextInTime":4860,"topicId":4795,"numMessagesInTopic":24,"msgSnippet":"... Most people want to preserve the log files as well as the ARCs generated by a crawl. The crawl.log is useful for generating reports and link analysis and","rawEmail":"Return-Path: &lt;pjack@...&gt;\r\nX-Sender: pjack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 15473 invoked from network); 18 Dec 2007 21:31:23 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m54.grp.scd.yahoo.com with QMQP; 18 Dec 2007 21:31:23 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta16.grp.scd.yahoo.com with SMTP; 18 Dec 2007 21:31:23 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id ADCC947960\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Dec 2007 13:40:29 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id F8T16h1jHSPx for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 18 Dec 2007 13:40:29 -0800 (PST)\r\nX-Received: from [192.168.1.8] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 555855C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Dec 2007 13:40:29 -0800 (PST)\r\nMime-Version: 1.0 (Apple Message framework v752.3)\r\nIn-Reply-To: &lt;258982.27376.qm@...&gt;\r\nReferences: &lt;258982.27376.qm@...&gt;\r\nContent-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed\r\nMessage-Id: &lt;75156E42-2332-4E10-90EA-8ECC5F06FA93@...&gt;\r\nContent-Transfer-Encoding: 7bit\r\nDate: Tue, 18 Dec 2007 13:31:16 -0800\r\nTo: archive-crawler@yahoogroups.com\r\nX-Mailer: Apple Mail (2.752.3)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Paul Jack &lt;pjack@...&gt;\r\nSubject: Re: [archive-crawler] Sheets\r\nX-Yahoo-Group-Post: member; u=278782393; y=trveRohH4T5Rc1aNTEtKoT88e6D4R7MEAP7rvQEpGB4j4wz0kQ\r\nX-Yahoo-Profile: poetbeware\r\n\r\n&gt; The problem is that Heritrix is always switching log files when it  \n&gt; starts a new job. It would be nice to direct all logging to common  \n&gt; files. So that I could just tail one crawl.log file, and watch each  \n&gt; job as it&#39;s processed.\n\nMost people want to preserve the log files as well as the ARCs  \ngenerated by a crawl. The crawl.log is useful for generating reports  \nand link analysis and so on.\n\nBut if you really have no interest in preserving the logs, you can  \nspecify an absolute directory for the logs directory. I haven&#39;t  \ntested this but I believe it will blow away the old log files as each  \nnew crawl starts.\n\n&gt; Also, I would like to have the feature to truncate log files. Is  \n&gt; this possible? I can do this with the Apache web server, tell it to  \n&gt; limit the log file size to 200K.\n\nYou can use system properties to configure the FileHandler we use to  \nwrite the log files. See\nhttp://java.sun.com/j2se/1.5.0/docs/api/java/util/logging/ \nFileHandler.html\n\nHowever I just opened HER-1382 to allow for modifying the FileHandler  \nproperties via the settings system, and HER-1381 for the ability to  \nmanually rotate logs via JMX.\n\n-Paul\n\n"}}