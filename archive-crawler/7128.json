{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"qnX-NwpFIgmxS9S-3OWa9iAoPOxNYnA5lxhicxnMyo2m05t8BQtwE_rafdnYBFHEmtuz5jH22vJWvXxL_tRfLX5KO1Mm4AI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] is &quot;Requires 5 Gigabytes for 1 billion URLs&quot;  true?","postDate":"1303692783","msgId":7128,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREQjRDNUVGLjYwOTA2MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDI5NDQ1Ni45MTY3OC5xbUB3ZWIxMjE2MTYubWFpbC5uZTEueWFob28uY29tPg==","referencesHeader":"PDI5NDQ1Ni45MTY3OC5xbUB3ZWIxMjE2MTYubWFpbC5uZTEueWFob28uY29tPg=="},"prevInTopic":7127,"nextInTopic":0,"prevInTime":7127,"nextInTime":7129,"topicId":7127,"numMessagesInTopic":2,"msgSnippet":"See this FAQ on the project wiki for general reasons why a crawl might not be going as fast you d expect: ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 88088 invoked from network); 25 Apr 2011 00:53:05 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m11.grp.re1.yahoo.com with QMQP; 25 Apr 2011 00:53:05 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta2.grp.re1.yahoo.com with SMTP; 25 Apr 2011 00:53:05 -0000\r\nX-Received: (qmail 79683 invoked by uid 0); 25 Apr 2011 00:53:03 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 25 Apr 2011 00:53:03 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4DB4C5EF.6090606@...&gt;\r\nDate: Sun, 24 Apr 2011 17:53:03 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.15) Gecko/20110303 Thunderbird/3.1.9\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: David Stafen &lt;stafend@...&gt;\r\nReferences: &lt;294456.91678.qm@...&gt;\r\nIn-Reply-To: &lt;294456.91678.qm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] is &quot;Requires 5 Gigabytes for 1 billion URLs&quot;\n  true?\r\nX-Yahoo-Group-Post: member; u=137285340; y=3hWfLeHerMVZMMPvII294zDfslQb9jPCsF3uOhOSgPfc\r\nX-Yahoo-Profile: gojomo\r\n\r\nSee this FAQ on the project wiki for general reasons why a crawl might \nnot be going as fast you&#39;d expect:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/crawl+rate+considerations\n\nYou didn&#39;t mention where you found the &quot;5 Gigabytes for 1 billion URLs&quot; \ntext, but I found it with Google in what appears to be someone&#39;s \nclassroom presentation, from 2006 or earlier, perhaps at some Asian \nuniversity.\n\nThe default URL-seen structure in Heritrix is not memory only; it uses \nBerkeleyDB-JavaEdition to overflow to disk. But, my very-rough \nrule-of-thumb on crawls with 2-3GB heaps has been that this structure&#39;s \ndisk overhead can become a noticeable bottleneck after 15-40 million \nURLs crawled. At that point, you either want (1) more heap for BDBJE&#39;s \ncache; (2) to split the crawl over several machines; or (3) swap out the \ndefault BdbUriUniqFilter for an alternative. (One alternative, the \nBloomUriUniqFilter, stays all in-memory but accepts a small error rate \nthrough an expected range of sizes.)\n\nStill, until you&#39;ve ruled out other things mentioned in the FAQ (like \nonly having a few remaining target hosts, which can&#39;t be politely \ncrawled any faster), adjusting the RAM/URL-seen-structure would be \npremature.\n\n- Gordon @ IA\n\n\n\nOn 4/23/11 7:16 AM, David Stafen wrote:\n&gt;\n&gt; Hi to all\n&gt; I&#39;m using heritrix 1.14.4 on a system with 16 Gigabyte Ram and i\n&gt; assigned 10 Gigabyte heap with 100 threads to heritrix, it is\n&gt; interesting; up to now i downloaded 30,000,000 web pages and only\n&gt; 60,000,000 URLs are queued . The HDD LED is always on and something like\n&gt; swapping is happening, at first i thought it has low memory (heap) but i\n&gt; found a document that reject this theory.\n&gt; &quot;Represent URL by 8-byte checksum.Maintain in-memory hash table of URLs.\n&gt; Requires 5 Gigabytes for 1 billion URLs.&quot;\n&gt; I have a good bandwidth (16 MB) but only at the beginning of the\n&gt; crawling process i could consume all of the bandwidth and now only 6MB\n&gt; is consumed and my download rate decreased. i have only two decide rules\n&gt; (only URLs in a specific domain with &quot;TEXT/HTML&quot; content are accepted).\n&gt; I wanna to crawl 200,000,000 web pages, but in this situation what\n&gt; should i do? do i need a system with stronger resource?\n&gt;\n&gt;\n&gt; \n\n"}}