{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234926651,"authorName":"Frank McCown","from":"Frank McCown &lt;fmccown@...&gt;","profile":"mccownf","replyTo":"LIST","senderId":"QrprtX2N3G4stt_Z11TPemh83Trvx0Ke_2Tw2OQ4O536UkRK3pPGXBe8NXcyy_ouX0Xl1CPBWSfqt7emKlHbzOLUa1-s67vL","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Release 1.10.0 pending","postDate":"1157487294","msgId":3237,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0RkREQUJFLjIwNTA2MDlAY3Mub2R1LmVkdT4=","inReplyToHeader":"PDQ0RkRENDBGLjkwMTAwMDZAYXJjaGl2ZS5vcmc+","referencesHeader":"PGVkZ3JuNit1NHRnQGVHcm91cHMuY29tPiA8NDRGREI3NkEuMTA2MDUwOUBhcmNoaXZlLm9yZz4gPDQ0RkRCRUMwLjMwMDAyMDZAY3Mub2R1LmVkdT4gPDQ0RkRENDBGLjkwMTAwMDZAYXJjaGl2ZS5vcmc+"},"prevInTopic":3236,"nextInTopic":3238,"prevInTime":3236,"nextInTime":3238,"topicId":3198,"numMessagesInTopic":23,"msgSnippet":"I could give it a stab in the next few days.  Is there a hard deadline for finishing up the documentation? Frank","rawEmail":"Return-Path: &lt;fmccown@...&gt;\r\nX-Sender: fmccown@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 85230 invoked from network); 5 Sep 2006 20:18:56 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m30.grp.scd.yahoo.com with QMQP; 5 Sep 2006 20:18:56 -0000\r\nReceived: from unknown (HELO cartero.cs.odu.edu) (128.82.4.9)\n  by mta1.grp.scd.yahoo.com with SMTP; 5 Sep 2006 20:18:56 -0000\r\nReceived: from [128.82.7.106] (bang.seven.research.odu.edu [128.82.7.106])\n\tby cartero.cs.odu.edu (8.13.6/8.13.6) with ESMTP id k85KExUw023136\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 5 Sep 2006 16:14:59 -0400 (EDT)\r\nMessage-ID: &lt;44FDDABE.2050609@...&gt;\r\nDate: Tue, 05 Sep 2006 16:14:54 -0400\r\nUser-Agent: Mozilla Thunderbird 1.0 (Windows/20041206)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;edgrn6+u4tg@...&gt; &lt;44FDB76A.1060509@...&gt; &lt;44FDBEC0.3000206@...&gt; &lt;44FDD40F.9010006@...&gt;\r\nIn-Reply-To: &lt;44FDD40F.9010006@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Frank McCown &lt;fmccown@...&gt;\r\nSubject: Re: [archive-crawler] Release 1.10.0 pending\r\nX-Yahoo-Group-Post: member; u=234926651; y=6j6UMqYR3-WQA7uJc7NxNaV6CUY_dXA3-GUdKS87nwi7kw\r\nX-Yahoo-Profile: mccownf\r\n\r\nI could give it a stab in the next few days.  Is there a hard deadline \nfor finishing up the documentation?\n\nFrank\n\n\nMichael Stack wrote:\n&gt; \n&gt; \n&gt; Sounds great Frank. Any chance you&#39;d like to take a first cut at it\n&gt; even if it was only an outline for the rest of us to fill in.\n&gt; Good stuff,\n&gt; St.Ack\n&gt; \n&gt; Frank McCown wrote:\n&gt;  &gt;\n&gt;  &gt; Stack,\n&gt;  &gt;\n&gt;  &gt; It may be a little late to request this, but I thought it would be\n&gt;  &gt; really useful to list several use cases in the user manual for the most\n&gt;  &gt; typical types of crawls that people like to do. For example:\n&gt;  &gt;\n&gt;  &gt; Case 1: Suppose you want to crawl only pages from a particular host, and\n&gt;  &gt; you want to avoid crawling too many pages of the dynamically generated\n&gt;  &gt; calendar. Here&#39;s how you would use the DecidingScope to setup this\n&gt;  &gt; crawl...\n&gt;  &gt;\n&gt;  &gt; Case 2: Suppose you wanted to only grab 5 pages from each of the 10 the\n&gt;  &gt; seeds...\n&gt;  &gt;\n&gt;  &gt; Case 3: Suppose you only want to crawl URLs that match\n&gt;  &gt; http://foo.org/bar/* &lt;http://foo.org/bar/*&gt; &lt;http://foo.org/bar/* \n&gt; &lt;http://foo.org/bar/*&gt;&gt; and you only want to crawl\n&gt;  &gt; URLs that end with .html...\n&gt;  &gt;\n&gt;  &gt; Each of the use cases could point out how the fetchers, extractors,\n&gt;  &gt; decide rules, canonicalizer, etc. would effect the crawl. This would\n&gt;  &gt; have been really helpful to me when I was a newbie. :)\n&gt;  &gt;\n&gt;  &gt; Frank\n&gt;  &gt;\n\n"}}