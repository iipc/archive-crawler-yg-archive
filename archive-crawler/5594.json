{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":325624130,"authorName":"Noah Levitt","from":"Noah Levitt &lt;nlevitt@...&gt;","profile":"nlevitt0","replyTo":"LIST","senderId":"gWJpf_HsgReizADORtzRz_3eKQQ8PUnHVSMwe27oItVPCFKUm0yQZwKIo5Kh87cMbVjSbSOK7p2zE6vopXJBT3dCdHyuaAgY","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] 74 millions docs - Out Of Memory","postDate":"1228345971","msgId":5594,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5MzcxMjczLjgwNTAzMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGdoNzJjcis4M3U3QGVHcm91cHMuY29tPg==","referencesHeader":"PGdoNzJjcis4M3U3QGVHcm91cHMuY29tPg=="},"prevInTopic":5593,"nextInTopic":5595,"prevInTime":5593,"nextInTime":5595,"topicId":5593,"numMessagesInTopic":3,"msgSnippet":"Hello Adam, thanks for the report. I can t claim to know anything about the more serious second issue. But the first issue appears to be ","rawEmail":"Return-Path: &lt;nlevitt@...&gt;\r\nX-Sender: nlevitt@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 89731 invoked from network); 3 Dec 2008 23:12:48 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m35.grp.scd.yahoo.com with QMQP; 3 Dec 2008 23:12:48 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta15.grp.scd.yahoo.com with SMTP; 3 Dec 2008 23:12:48 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id B47DF14E86D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  3 Dec 2008 15:12:48 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id IbOSKGJw+0g7 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed,  3 Dec 2008 15:12:47 -0800 (PST)\r\nX-Received: from noah-levitts-computer.local (c-76-103-251-45.hsd1.ca.comcast.net [76.103.251.45])\n\tby mail.archive.org (Postfix) with ESMTPSA id 179D331846\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  3 Dec 2008 15:12:47 -0800 (PST)\r\nMessage-ID: &lt;49371273.8050301@...&gt;\r\nDate: Wed, 03 Dec 2008 15:12:51 -0800\r\nUser-Agent: Thunderbird 2.0.0.18 (Macintosh/20081105)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;gh72cr+83u7@...&gt;\r\nIn-Reply-To: &lt;gh72cr+83u7@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Noah Levitt &lt;nlevitt@...&gt;\r\nSubject: Re: [archive-crawler] 74 millions docs - Out Of Memory\r\nX-Yahoo-Group-Post: member; u=325624130; y=qQ4hXxR1YYREjqs-Y6dZITAdJSuG_AesOWw1btM_JUfmljU\r\nX-Yahoo-Profile: nlevitt0\r\n\r\nHello Adam, thanks for the report.\n\nI can&#39;t claim to know anything about the more serious second issue. But \nthe first issue appears to be \nhttp://webteam.archive.org/jira/browse/HER-1482, which has been fixed in \nsvn on the 2.2.x line. If this is the case, the files that triggered \nthis exception would have to be larger than 2gb. Can you confirm? I \nbelieve the only effect of this bug is to prevent link extraction from \nthe affected urls. I don&#39;t think it is related to your second issue.\n\nNoah\n\n\ngoblin_cz wrote:\n&gt; Hi,\n&gt;\n&gt; the Czech crawl again :) . I started with default profile and set some\n&gt; specific rules (100MB limit etc.) and run the crawl again. You can\n&gt; find the order.xml here:\n&gt;\n&gt; http://raptor.webarchiv.cz/heritrix/order.xml\n&gt;\n&gt; Tech spec:\n&gt; Heritrix 1.14.2\n&gt; 8 core Xeon\n&gt; 8GB RAM\n&gt; 64bit sun java\n&gt; 3GB java heap\n&gt; Debian 4.1\n&gt;\n&gt; Everything goes without any serious trouble. The only exception that\n&gt; was thrown was:\n&gt;\n&gt; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n&gt; Stacktrace: java.lang.IllegalArgumentException: Size exceeds\n&gt; Integer.MAX_VALUE\n&gt; \tat sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:707)\n&gt; \tat\n&gt; org.archive.io.GenericReplayCharSequence.getReadOnlyMemoryMappedBuffer(GenericReplayCharSequence.java:277)\n&gt; \tat\n&gt; org.archive.io.GenericReplayCharSequence.decodeToFile(GenericReplayCharSequence.java:219)\n&gt; \tat\n&gt; org.archive.io.GenericReplayCharSequence.(GenericReplayCharSequence.java:164)\n&gt; \tat\n&gt; org.archive.io.RecordingOutputStream.getReplayCharSequence(RecordingOutputStream.java:559)\n&gt; \tat\n&gt; org.archive.io.RecordingOutputStream.getReplayCharSequence(RecordingOutputStream.java:515)\n&gt; \tat\n&gt; org.archive.io.RecordingInputStream.getReplayCharSequence(RecordingInputStream.java:314)\n&gt; \tat\n&gt; org.archive.util.HttpRecorder.getReplayCharSequence(HttpRecorder.java:295)\n&gt; \tat\n&gt; org.archive.crawler.extractor.ExtractorHTML.extract(ExtractorHTML.java:540)\n&gt; \tat\n&gt; org.archive.crawler.extractor.Extractor.innerProcess(Extractor.java:67)\n&gt; \tat org.archive.crawler.framework.Processor.process(Processor.java:112)\n&gt; \tat\n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:302)\n&gt; \tat org.archive.crawler.framework.ToeThread.run(ToeThread.java:151)\n&gt;\n&gt; This happened on few pages ï¿½ for example\n&gt; http://dermoporadkyne.cz/poradenstvi.pdf and I guess it is caused by\n&gt; never ending scripts.\n&gt;\n&gt; After six days of crawling (I had 74 millions of documents and 4 TB of\n&gt; data) happened something much more serious. Heritrix threw about 150\n&gt; exceptions and paused itself.\n&gt; All exceptions look like this:\n&gt; Serious error occured trying to process &#39;CrawlURI\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543\n&gt; LLL\n&gt; http://www.sperky.vltava.cz/DANFIL/vyrobce=1001130574/?category-id=16706\n&gt; in Scheduler&#39;\n&gt; [ToeThread #132:\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543\n&gt;  CrawlURI\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543\n&gt; LLL\n&gt; http://www.sperky.vltava.cz/DANFIL/vyrobce=1001130574/?category-id=16706\n&gt;    0 attempts\n&gt;     in processor: Scheduler\n&gt;     ACTIVE for 1h44m26s863ms\n&gt;     step: ABOUT_TO_BEGIN_PROCESSOR for 1h44m5s663ms\n&gt;     java.lang.Thread.getStackTrace(Thread.java:1436)\n&gt;     org.archive.crawler.framework.ToeThread.reportTo(ToeThread.java:514)\n&gt;     org.archive.crawler.framework.ToeThread.reportTo(ToeThread.java:592)\n&gt;     org.archive.util.DevUtils.extraInfo(DevUtils.java:65)\n&gt;    \n&gt; org.archive.crawler.framework.ToeThread.seriousError(ToeThread.java:230)\n&gt;    \n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:325)\n&gt;     org.archive.crawler.framework.ToeThread.run(ToeThread.java:151)\n&gt; ]\n&gt;            timestamp  discovered      queued   downloaded      \n&gt; doc/s(avg)  KB/s(avg)   dl-failures   busy-thread   mem-use-KB \n&gt; heap-size-KB   congestion   max-depth   avg-depth\n&gt; 2008-11-30T19:49:37Z   226231290   147999609     74002484        \n&gt; 0(121.9)    0(6828)        293362           197      2339025      \n&gt; 2831488     7,213.49      320897         104\n&gt;  (in thread &#39;ToeThread #132:\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543&#39;;\n&gt; in processor &#39;Scheduler&#39;)\n&gt;\n&gt; java.lang.OutOfMemoryError: Java heap space\n&gt; Stacktrace: java.lang.OutOfMemoryError: Java heap space\n&gt; \tat java.lang.Class.getDeclaredFields0(Native Method)\n&gt; \tat java.lang.Class.privateGetDeclaredFields(Class.java:2291)\n&gt; \tat java.lang.Class.getDeclaredField(Class.java:1880)\n&gt; \tat java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1610)\n&gt; \tat java.io.ObjectStreamClass.access$700(ObjectStreamClass.java:52)\n&gt; \tat java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:425)\n&gt; \tat java.security.AccessController.doPrivileged(Native Method)\n&gt; \tat java.io.ObjectStreamClass.(ObjectStreamClass.java:413)\n&gt; \tat java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:310)\n&gt; \tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1106)\n&gt; \tat\n&gt; java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1509)\n&gt; \tat\n&gt; java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1474)\n&gt; \tat\n&gt; java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1392)\n&gt; \tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1150)\n&gt; \tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326)\n&gt; \tat\n&gt; com.sleepycat.bind.serial.SerialBinding.objectToEntry(SerialBinding.java:171)\n&gt; \tat com.sleepycat.collections.DataView.useValue(DataView.java:548)\n&gt; \tat com.sleepycat.collections.DataCursor.initForPut(DataCursor.java:824)\n&gt; \tat com.sleepycat.collections.DataCursor.put(DataCursor.java:758)\n&gt; \tat\n&gt; com.sleepycat.collections.StoredContainer.putKeyValue(StoredContainer.java:319)\n&gt; \tat com.sleepycat.collections.StoredMap.put(StoredMap.java:257)\n&gt; \tat org.archive.util.CachedBdbMap.expungeStaleEntry(CachedBdbMap.java:562)\n&gt; \tat\n&gt; org.archive.util.CachedBdbMap.expungeStaleEntries(CachedBdbMap.java:533)\n&gt; \tat org.archive.util.CachedBdbMap.get(CachedBdbMap.java:358)\n&gt; \tat\n&gt; org.archive.crawler.datamodel.ServerCache.getHostFor(ServerCache.java:146)\n&gt; \tat\n&gt; org.archive.crawler.datamodel.ServerCache.getHostFor(ServerCache.java:175)\n&gt; \tat\n&gt; org.archive.crawler.framework.WriterPoolProcessor.getHostAddress(WriterPoolProcessor.java:344)\n&gt; \tat\n&gt; org.archive.crawler.writer.ARCWriterProcessor.innerProcess(ARCWriterProcessor.java:132)\n&gt; \tat org.archive.crawler.framework.Processor.process(Processor.java:112)\n&gt; \tat\n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:302)\n&gt; \tat org.archive.crawler.framework.ToeThread.run(ToeThread.java:151)\n&gt;\n&gt; When I resume the crawl, only new exceptions are thrown and the crawl\n&gt; is getting back to the paused state.\n&gt; I&#39;ve searched and browsed this mailing list and there are a lot of\n&gt; topics about OOM, but I am not 100% sure, which one can be exactly the\n&gt; same situation and actual.\n&gt;\n&gt; Can I modify the crawl to be able resume it? \n&gt; If not, how to crawl it again without storing already downloaded data?\n&gt; (checkpoint and recovery?)\n&gt; What can I do to avoid this situation?\n&gt;\n&gt; Thank you,\n&gt;\n&gt; Best regards\n&gt;\n&gt; Adam\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;   \n\n"}}