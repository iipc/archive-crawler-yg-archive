{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (archive.org)","from":"&quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"UzoIIDkolz6YsJ8QEAyKPzCmdlUqhsxJJ5XOSOVQsVb8TXW4hUzaa1HrwPDPsszXV-lAGl1uDRjNPSLU-4YnTKkdwDBGODm2p7UDfh-HpK5izueS51b3","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: OutOfMemoryError on small crawl","postDate":"1141156283","msgId":2719,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0MDRBOUJCLjYwNjA0MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGR1MXI3cis0MWJkQGVHcm91cHMuY29tPg==","referencesHeader":"PGR1MXI3cis0MWJkQGVHcm91cHMuY29tPg=="},"prevInTopic":2718,"nextInTopic":2720,"prevInTime":2718,"nextInTime":2720,"topicId":2709,"numMessagesInTopic":7,"msgSnippet":"Adam, FYI, I never received your second message on this thread (the one you quote yourself, Yes, our heap size is definitely capped... ) -- not even caught by","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60338 invoked from network); 28 Feb 2006 19:51:52 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m35.grp.scd.yahoo.com with QMQP; 28 Feb 2006 19:51:52 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta2.grp.scd.yahoo.com with SMTP; 28 Feb 2006 19:51:52 -0000\r\nReceived: (qmail 6488 invoked by uid 100); 28 Feb 2006 19:44:06 -0000\r\nReceived: from adsl-71-130-102-77.dsl.pltn13.pacbell.net (HELO ?192.168.1.10?) (gojomo@...@71.130.102.77)\n  by mail-dev.archive.org with SMTP; 28 Feb 2006 19:44:06 -0000\r\nMessage-ID: &lt;4404A9BB.6060405@...&gt;\r\nDate: Tue, 28 Feb 2006 11:51:23 -0800\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc4 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;du1r7r+41bd@...&gt;\r\nIn-Reply-To: &lt;du1r7r+41bd@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-88.3 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 2:12:4:0\r\nFrom: &quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: OutOfMemoryError on small crawl\r\nX-Yahoo-Group-Post: member; u=137285340; y=8fWRopKdE0qkXzctrw2uUr8KvRrmbRdYHIqqfloNoVl-\r\nX-Yahoo-Profile: gojomo\r\n\r\nAdam,\n\nFYI, I never received your second message on this thread (the one you \nquote yourself, &quot;Yes, our heap size is definitely capped...&quot;) -- not \neven caught by junk filters. It is in the list archives, though.\n\nThe memory footprint in long runs should not generally be a function \nof how many URIs have been crawled or are pending. Such records are \nall kept in BerkeleyDB-JE structures, and those are capped at using \nthe assigned amount of JE cache memory (default 60% of heap). So no \nmatter how many URIs are crawled, that shouldn&#39;t trigger an OOME.\n\nToeThreads each have a constant amount of memory overhead, and then \nmore as each URI is processed, but their consumption should not grow \nover time.\n\nMemory usage will grow with respect to the number of queues ever \nencountered, so broad crawls (or domain crawls where there are many, \nmany subdomains) tend to use more memory. I believe some of the \nstatistics structures will grow with the number of categories tracked \n(hosts encountered / MIME types encountered / etc.) so some crawls or \npathological servers could cause problems there. But, I don&#39;t think \nany of those would apply to your crawl, from how you&#39;ve described it.\n\nWhat version of Heritrix are you using, and what JVM? Those aren&#39;t \n64-bit Xeons by any chance, are they? (If so, see the release note at \n&lt;http://crawler.archive.org/articles/releasenotes.html#bdb_64bit&gt;).\n\nIt&#39;s hard to say what&#39;s typical for website size -- &quot;most&quot; sites (by \nhostname) are tiny, but plenty of sites (especially media and \ngovernment sites) have many hundreds of thousands of legitimate URLs. \nAlso, given both dynamic content and traps/chaff, the &#39;size&#39; of a site \nis often simply when you run out of patience/interest in crawling any \ndeeper. I don&#39;t know what kinds of sites you&#39;re crawling, but your \nnumbers don&#39;t seem atypical.\n\n- Gordon @ IA\n\n\nAdam Fisk wrote:\n&gt; This is running well now, just for anyone else searching through the \n&gt; forum.  I first increased the mx to 256MB and ran out of memory again. \n&gt;  I then bumped it up to 600MB, and the crawl has been running for \n&gt; about 3 days with about 800,000 html pages downloaded.\n&gt; \n&gt; Does my volume sound pretty typical to other folks out there?  I&#39;m \n&gt; only crawling 20 sites, but they include sites like nytimes.com.  \n&gt; That&#39;s about 40,000 html pages per site, and the crawl&#39;s still not \n&gt; done.  Is that a pretty typical size for larger sites out there?\n&gt; \n&gt; Thanks very much.\n&gt; \n&gt; -Adam\n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, &quot;Adam Fisk&quot; &lt;adamfisk@...&gt; \n&gt; wrote:\n&gt; \n&gt;&gt;Thanks Gordon-\n&gt;&gt;\n&gt;&gt;Yes -- our heap size is definitely capped at the Java default.  I\n&gt;&gt;can&#39;t go much higher than 256 on this machine, but I&#39;ll see what I \n&gt; \n&gt; get\n&gt; \n&gt;&gt;with that.  I just didn&#39;t want to mask another problem by going \n&gt; \n&gt; ahead\n&gt; \n&gt;&gt;and cranking the memory, especially for such a small crawl.  Those \n&gt; \n&gt; 20\n&gt; \n&gt;&gt;were bringing in upwards of 40,000 raw text/html pages, though, so I\n&gt;&gt;guess memory issues are to be expected.\n&gt;&gt;\n&gt;&gt;Where does most of the memory go?  I assume the ToeThreads \n&gt; \n&gt; accumulate\n&gt; \n&gt;&gt;a lot of data over time?  We&#39;re going to be doing much larger crawls\n&gt;&gt;soon (hundreds of sites), and we&#39;d prefer not to dedicate a full\n&gt;&gt;machine to this task, but it looks like we might have to.\n&gt;&gt;\n&gt;&gt;Thanks again.\n&gt;&gt;\n&gt;&gt;-Adam\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@&gt; wrote:\n&gt;&gt;\n&gt;&gt;&gt;It looks like your heap is capped at a maximum size of 64MB -- \n&gt;&gt;&gt;that&#39;s the Java default in the absence of any -Xmx setting in Java \n&gt;&gt;&gt;1.4 and previous -- so your 2GB of RAM isn&#39;t doing the crawler any \n&gt;&gt;&gt;good.\n&gt;&gt;&gt;\n&gt;&gt;&gt;Use of -Xmx is definitely indicated; if the machine is dedicated \n&gt;&gt;&gt;to crawling, and you want the crawler to be able to use all the \n&gt;&gt;&gt;RAM, -Xmx1500m would be justified.\n&gt;&gt;&gt;\n&gt;&gt;&gt;(It&#39;s probably possible to crawl 20 hosts in 64MB, if there isn&#39;t \n&gt;&gt;&gt;an explosion of subdomains, and you use only a small number of \n&gt;&gt;&gt;threads -- but I doubt that&#39;s a real constraint you want to try to \n&gt;&gt;&gt;live within.)\n&gt;&gt;&gt;\n&gt;&gt;&gt;The &#39;max-depth&#39; and &#39;average-depth&#39; readings on that status line \n&gt;&gt;&gt;(and in the crawler console) refer to the size of queues: \n&gt;&gt;&gt;&#39;max-depth&#39; is the longest queue in the frontier, &#39;average-depth&#39; \n&gt;&gt;&gt;is the average of all queue sizes.\n&gt;&gt;&gt;\n&gt;&gt;&gt;- Gordon @ IA\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}