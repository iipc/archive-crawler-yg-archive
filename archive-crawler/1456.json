{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"IT-Ri4w1zwi4tHt5tprsj9gDh1XrlBpqrqRD5NlNKFDESS7GWkpnBDpO-hbAv079A0pM8F-BMmiVovb8mBCDzWJglW_KcTrod_nGuIm0cg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] continuous crawling proposal","postDate":"1107272021","msgId":1456,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBMDZFQzQ3QHNrYXJmdXIuYm9rLmxvY2FsPg==","inReplyToHeader":"PFBpbmUuTE5YLjQuNTYuMDUwMjAxMTAwNjMzMC4xNTY2NUBwaWtlc3BlYWsubWV0YWNhcnRhLmNvbT4="},"prevInTopic":1455,"nextInTopic":1467,"prevInTime":1455,"nextInTime":1457,"topicId":1452,"numMessagesInTopic":24,"msgSnippet":"Hey John, Partly the reason for a seperate frontier is one of parallel development. When I started working on it there was no BDBFrontier and there Some other","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 70449 invoked from network); 1 Feb 2005 15:33:48 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m10.grp.scd.yahoo.com with QMQP; 1 Feb 2005 15:33:48 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta5.grp.scd.yahoo.com with SMTP; 1 Feb 2005 15:33:48 -0000\r\nReceived: (qmail 25244 invoked by uid 100); 1 Feb 2005 15:17:58 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (kris@...@130.208.152.83)\n  by mail-dev.archive.org with SMTP; 1 Feb 2005 15:17:58 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Tue, 1 Feb 2005 15:33:41 -0000\r\nMessage-ID: &lt;0678DB1968EAC7409CC3D0AB7A11B84A06EC47@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0000_01C50873.695470D0&quot;\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nImportance: Normal\r\nIn-Reply-To: &lt;Pine.LNX.4.56.0502011006330.15665@...&gt;\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1441\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.7 required=6.5 tests=AWL,HTML_20_30,\n\tHTML_FONTCOLOR_BLUE,HTML_MESSAGE autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] continuous crawling proposal\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n------=_NextPart_000_0000_01C50873.695470D0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHey John,\n \nPartly the reason for a seperate frontier is one of parallel de=\r\nvelopment.\nWhen I started working on it there was no BDBFrontier and there\n=\r\n \nSome other differences (some of these may be out of date since I&#39;ve not\ne=\r\nntirely kept up with development of the BdbFrontier):\n \nThe BdbFrontier use=\r\ns an &#39;already seen&#39; map to reject duplicates. Clearly\nthis is unacceptable.=\r\n  The ARFrontier uses the actual queues to weed out\nduplicates.\nThe ARFront=\r\nier implements a significantly different HostQueue structure,\ntailored for =\r\nrepeated crawling of the same URIs. These maintain a priority\nqueue of thei=\r\nr URIs, when it is safe to issue them and so forth.\nThe ARFrontier does NOT=\r\n discard the AList contents (only parts of it) when a\nCURI is completed, bu=\r\nt rather stores it for next time. Currently the\nBdbFrontier completely disc=\r\nards the CURI and all its data on successful\ncompletion of processing.\n \nBo=\r\nth frontiers use BDB to serialize data.\n \nWith the addition of an AbstractF=\r\nrontier it may be possible to have the\nARFrontier utilize that and thus lim=\r\nit duplication of code.  The plain truth\nhowever is that iterative and snap=\r\nshot crawling make very different demands\non resources etc. The BdbFrontier=\r\n is implemented with the clear intention of\noptimizing it for large scale, =\r\nnon repeating crawls. The ARFrontier is\nhowever designed for repeatedly cra=\r\nwling the same set of URIs and adds a\nconsiderable amount of managment for =\r\nthat task. An example of this, the\nARFrontier will always ensure (within po=\r\nliteness constricts) that the URI\nmost overdue for a visit will be the next=\r\n one issued.  The BdbFrontier\noffers little guarantee as to what URI will b=\r\ne issued next, except the\nqueues are FIFO and allows for queues to be held =\r\ninactive when enough active\nqueues are being used.\n \n \nI&#39;ll admit that you =\r\ncould construct a single Frontier to serve both\npurposes, but it would be a=\r\n matter of compromise. It is (in my opinion)\nbetter to have specialized Fro=\r\nntiers for (what are fundamentally) different\ncrawling strategies.\n \nThe AR=\r\nFrontier is furthermore designed as a generic repeating frontier (I&#39;m\nplani=\r\nng on renaming it Repeating or RevisitingFrontier since it does not\ncontain=\r\n any adaptive ability itself) and allows the processors to greatly\naffect t=\r\nhe ordering by which URIs are issued. This is a function that I do\nnot fors=\r\nee in the BdbFrontier (at least not with the level of granularity\nthe ARFro=\r\nntier provides). \n \n \n- Kris\n \n \n\n-----Original Message-----\nFrom: jrf@meta=\r\ncarta.com [mailto:jrf@...] On Behalf Of John R.\nFrank\nSent: 1. fe=\r\nbr=FAar 2005 15:11\nTo: archive-crawler@yahoogroups.com\nSubject: RE: [archiv=\r\ne-crawler] continuous crawling proposal\n\n\nKris,\n\nI was aware of your AR mod=\r\nule and should have asked a couple questions\nabout it in that earlier post.=\r\n  The algorithm I suggested could be written\nas a subclass to your WaitEval=\r\nuator.\n\nThe big issue with AR is:  why is a new Frontier required for this?=\r\n  The\nBDB-based Frontier solves (or will soon solve :) several scalability\n=\r\nissues that our crucial for production use of Heritrix (at least IMHO).\n\nAs=\r\n far as I can see, the only requirement of the Frontier is logic to\nprevent=\r\n dequeuing before the time determined by the WaitEvaluator.  Is\nthis true? =\r\n Or did I miss something?\n\nI&#39;ll keep you posted on content hashing.\n\nJohn\n\n=\r\n\n\nOn Tue, 1 Feb 2005, Kristinn Sigurdsson wrote:\n\n&gt; Yes, the AR module (cur=\r\nrently available as a branch of the Heritrix\nproject,\n&gt;\nhttp://crawltools.a=\r\nrchive.org:8080/cruisecontrol/buildresults/BRANCH-heritri\n&gt; x-Kris-ARbranch=\r\n2) provides the functionality required for incrimental\n&gt; crawling.\n&gt;\n&gt; To d=\r\no this it implements a new Frontier that uses priority queues for the\n&gt; URI=\r\ns (this also does away with the &#39;already included fingerprints of\nURIs).\n&gt;\n=\r\n&gt; A processor (dubbed a WaitEvaluator) is inserted to calculate the &#39;time o=\r\nf\n&gt; next processing&#39; before a URI is returned to the Frontier. Currently th=\r\ne\n&gt; WaitEvaluator uses a much simpler algorithm than John discusses, but a\n=\r\nmore\n&gt; sophisticated one could easily replace it.\n&gt;\n&gt; Currently it relies o=\r\nn a strict hash for content changes (although a\n&gt; processor for prestrippin=\r\ng problematic sections using regular expressions\nis\n&gt; provided). If you hav=\r\ne ideas for better content hashes, I&#39;d love to hear\n&gt; them.\n&gt;\n&gt; Currently t=\r\nhis add on to Heritrix is being tested, if all goes well it\nwill\n&gt; become a=\r\n part of of the next Heritrix release.\n&gt;\n&gt; - Kris\n&gt;\n&gt; -----Original Message=\r\n-----\n&gt; From: Bjarne Andersen [mailto:bja@...]\n&gt; Sent: 1. f=\r\nebr=FAar 2005 08:53\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Subject: Re: [a=\r\nrchive-crawler] continuous crawling proposal\n&gt;\n&gt;\n&gt; You might want to take a=\r\n look at the Automated Revisiting Module being\n&gt; developed at the moment by=\r\n kris@...\n&gt;\n&gt; It does implement a new Frontier including a new Quei=\r\nng-mechanism along\n&gt; with a simple algoritm to decide when to bring URI&#39;s t=\r\no the top of the\n&gt; queue (based on historical results - next-ready-time goe=\r\ns either up or\n&gt; down everytime a page is fetched and compared to the last =\r\nfetch)\n&gt;\n&gt; It is available from the continous build box at crawler.archive.=\r\norg (a\n&gt; CVS-brach called *_AR....)\n&gt;\n&gt; best\n&gt; Bjarne Andersen\n&gt;\n&gt; John R. =\r\nFrank wrote:\n&gt; &gt; Stack,\n&gt; &gt;\n&gt; &gt; What do you think of these three steps as a=\r\n possible way to implement\n&gt; &gt; continuous crawling in Heritrix?  Details fo=\r\nr each of these three are\n&gt; &gt; discussed below.\n&gt; &gt;\n&gt; &gt; 1) extend the work q=\r\nueue&#39;s logic to only dequeue &quot;ready&quot; URLs\n&gt; &gt;\n&gt; &gt; 2) decide when to recheck=\r\n a given page based on a simple model\n&gt; &gt;\n&gt; &gt; 3) detect substantive page ch=\r\nanges and store the info in AlreadySeen\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; 1) To do this, we n=\r\need a queueing mechanism that blocks the dequeuing of\na\n&gt; &gt; CrawlURI until =\r\nwe are past its assigned next &quot;okay to crawl&quot; moment.\n&gt; &gt; Where should we s=\r\ntore this timestamp?  As you said, the BDB keys are\n&gt; &gt; overloaded with too=\r\n much meaning already, so putting a seconds since the\n&gt; &gt; epoch in there is=\r\n not so good:\n&gt; &gt;\n&gt;\nhttp://crawler.archive.org/xref/org/archive/crawler/fro=\r\nntier/BdbMultipleWork\n&gt; Queues.html#246\n&gt; &gt;\n&gt; &gt; Perhaps this timestamp belo=\r\nngs in CandidateURI where the\n&gt; &gt; schedulingDirective pieces are?  For exam=\r\nple, if the schedulingDirective\n&gt; &gt; is negative, then it could be interpret=\r\ned as a timestamp.  So for a\n&gt; &gt; document next Wednesday, it would get:\n&gt; &gt;=\r\n\n&gt; &gt; jrf@localhost~$ date +%s -d &quot;Feb 9 01:20:29 EST 2005&quot;\n&gt; &gt; 1107930029\n&gt;=\r\n &gt;\n&gt; &gt; with a minus sign in front:  -1107930029\n&gt; &gt;\n&gt; &gt; Adding `date +%s -d=\r\n now` now to that will give a positive number when it\n&gt; &gt; is okay to crawl.=\r\n  This would require modifications to the four-bit\n&gt; &gt; interpretation of pr=\r\niority in the BDB key computation.  An escape value\n&gt; &gt; of 1111 might tell =\r\nthe queue that it needs to look in the object to find\n&gt; &gt; the value of the =\r\ntimestamp.\n&gt; &gt;\n&gt; &gt; The problem with this is that suddenly the queue is not =\r\na queue, because\n&gt; &gt; the keys are the sorting mechanism on the queue in the=\r\n BDB\nimplementation,\n&gt; &gt; right?  Do you see a way fix to this besides cycli=\r\nng through all the\ncuri\n&gt; &gt; with 1111 looking at the full value of the time=\r\nstamp?\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; 2) To compute the time at which to refetch, we can p=\r\nredict the\nlikelihood\n&gt; &gt; that the page has been modified as a function of =\r\ntime since last\nmodified.\n&gt; &gt; See below for discussion of detecting last mo=\r\ndified.  An easy function\nto\n&gt; &gt; use for this modeling comes from the Micha=\r\nelis-Menton model of enzyme\n&gt; &gt; kinetics:\n&gt; &gt;\n&gt; &gt;       t is time elapsed a=\r\nfter a modification of the content\n&gt; &gt;\n&gt; &gt;       P(t) is the probability th=\r\nat the page has changed by time t\n&gt; &gt;\n&gt; &gt;                          a\n&gt; &gt;   =\r\n                    k t\n&gt; &gt;              P(t) =3D  ---------------\n&gt; &gt;     =\r\n                       a\n&gt; &gt;                    1  + k t\n&gt; &gt;\n&gt; &gt; At small t=\r\n, P is small.  At large t, P tends to 1, i.e. certainty of\n&gt; &gt; change.  It =\r\nis easiest to choose a=3D2, which is the smallest integer\nvalue\n&gt; &gt; that gi=\r\nves step-like behavior.  We can then set k by fitting this\nfunction\n&gt; &gt; to =\r\nany given page&#39;s history (details below).\n&gt; &gt;\n&gt; &gt; Given k, we can use P(t) =\r\nto predict when to refetch a document.  We pick\na\n&gt; &gt; threshold probability=\r\n above which we want to refetch.  If we set it low,\n&gt; &gt; then we want to rec=\r\nheck more often.  If we set it high, that means we&#39;re\n&gt; &gt; willing to tolera=\r\nte more stale content in order to not recheck as often.\n&gt; &gt;\n&gt; &gt; When P(t) e=\r\nxceeds the chosen threshold, then we want to recheck it.  By\n&gt; &gt; inverting =\r\nthe probability function, this threshold gives us a time to\n&gt; &gt; wait before=\r\n rechecking the page:\n&gt; &gt;\n&gt; &gt;                  k                (-1/a)\n&gt; &gt; =\r\n       t =3D ( ----------   -   k )\n&gt; &gt;              threshold\n&gt; &gt;\n&gt; &gt;    F=\r\nor clarity that is:\n&gt; &gt;    t =3D (((k / threshold) - k ) ^ (-1/a))\n&gt; &gt;\n&gt; &gt; =\r\nt is the time interval between the most recent known modification event\n&gt; &gt;=\r\n and that time in the future when the expected probability of change is\n&gt; &gt;=\r\n just above threshold.  For robustness, we should define a time within\n&gt; &gt; =\r\nwhich all pages must be rechecked.  In python, the function might look\n&gt; &gt; =\r\nlike:\n&gt; &gt;\n&gt; &gt; def delay(k, threshold):\n&gt; &gt;     Days =3D 60 * 60 * 24\n&gt; &gt;   =\r\n  maxRecheckDelay =3D 20 * Days\n&gt; &gt;     thresholdDelay =3D (((k / threshold=\r\n) - k ) ^ (-1/alpha))\n&gt; &gt;     return minimum(maxRecheckDelay, thresholdDela=\r\ny)\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; 3. Since last-modified information is not universally provid=\r\ned by\ndocument\n&gt; &gt; repositories, we need a mechanism to detect substantive =\r\ncontent changes\n&gt; &gt; and record them in the BDB.  I&#39;m looking into tools for=\r\n this kind of\n&gt; &gt; content hashing that could be run in the extractor chain =\r\nand stored in\nthe\n&gt; &gt; object that gets into BDB.\n&gt; &gt;\n&gt; &gt; Suppose we have su=\r\nch a content hash, then the interval of time between\ntwo\n&gt; &gt; known modifica=\r\ntion events is an upper bound because there could have\nbeen\n&gt; &gt; a modificat=\r\nion event that we did not observe between these observed\n&gt; &gt; events.  If th=\r\nese upper bounds are not far off the real value, then we\ncan\n&gt; &gt; accurately=\r\n approximate the probability of modification at half the\n&gt; &gt; observed inter=\r\nval as being 50%.  That is, we use the previously observed\n&gt; &gt; modification=\r\n intervals to estimate what the next actual interval will\nbe.\n&gt; &gt; If the av=\r\nerage of the last, say, five observed intervals is T, then we\n&gt; &gt; estimate =\r\nthat at (T/2) in the future, the probability of modification\nwill\n&gt; &gt; be 50=\r\n%.  This let&#39;s us estimate a value for k based on previously\nobserved\n&gt; &gt; m=\r\nodification intervals:\n&gt; &gt;\n&gt; &gt;                -a\n&gt; &gt;        k =3D (T/2)    =\r\nwhich for a=3D2 is (two over T) squared.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Some logic is required=\r\n to make sure that only the last few, say five,\n&gt; &gt; meaningful modification=\r\n intervals are averaged to make T.  This moving\n&gt; &gt; window average allows H=\r\neritrix to more rapidly adapt to pages that have\n&gt; &gt; varying update frequen=\r\ncies, which is most pages.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Thoughts?  Reactions?\n&gt; &gt;\n&gt; &gt; John\n\n=\r\n\n  _____  \n\nYahoo! Groups Links\n\n\n*\tTo visit your group on the web, go to:\n=\r\nhttp://groups.yahoo.com/group/archive-crawler/\n  \n\n*\tTo unsubscribe from th=\r\nis group, send an email to:\narchive-crawler-unsubscribe@yahoogroups.com\n&lt;ma=\r\nilto:archive-crawler-unsubscribe@yahoogroups.com?subject=3DUnsubscribe&gt; \n  =\r\n\n\n*\tYour use of Yahoo! Groups is subject to the Yahoo! Terms of Service\n&lt;ht=\r\ntp://docs.yahoo.com/info/terms/&gt; . \n\n\n\r\n------=_NextPart_000_0000_01C50873.695470D0\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n&lt;TITLE&gt;Message&lt;/TITLE&gt;\n&lt;META http-equiv=3DContent-Type content=3D&quot;text/html=\r\n; charset=3Diso-8859-1&quot;&gt;\n&lt;META content=3D&quot;MSHTML 6.00.2800.1476&quot; name=3DGEN=\r\nERATOR&gt;&lt;/HEAD&gt;\n&lt;BODY&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DAr=\r\nial color=3D#0000ff size=3D2&gt;Hey \nJohn,&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN clas=\r\ns=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;=\r\n&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DAria=\r\nl color=3D#0000ff size=3D2&gt;Partly \nthe reason for a seperate frontier is on=\r\ne of parallel development.&nbsp; When I \nstarted working on it there was no=\r\n BDBFrontier and there&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-010=\r\n22005&gt;&lt;FONT face=3DArial color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DI=\r\nV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff=\r\n size=3D2&gt;Some \nother differences (some of these may be out of date since I=\r\n&#39;ve not entirely kept \nup with development of the BdbFrontier):&lt;/FONT&gt;&lt;/SPA=\r\nN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#=\r\n0000ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01=\r\n022005&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;The \nBdbFrontier uses an=\r\n &#39;already seen&#39; map to reject duplicates. Clearly this is \nunacceptable.&nb=\r\nsp; The ARFrontier uses the actual queues to weed out \nduplicates.&lt;/FONT&gt;&lt;/=\r\nSPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DArial color=\r\n=3D#0000ff size=3D2&gt;The \nARFrontier implements a significantly different Ho=\r\nstQueue structure, tailored \nfor repeated crawling of the same URIs. These =\r\nmaintain a priority queue of their \nURIs, when it is safe to issue them and=\r\n so forth.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT =\r\nface=3DArial color=3D#0000ff size=3D2&gt;The \nARFrontier does NOT discard the =\r\nAList contents (only parts of it) when a CURI is \ncompleted, but rather sto=\r\nres it for next time. Currently the BdbFrontier \ncompletely discards the CU=\r\nRI and all its data on successful completion of \nprocessing.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;=\r\n/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#000=\r\n0ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022=\r\n005&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;Both \nfrontiers use BDB to =\r\nserialize data.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;=\r\nFONT face=3DArial color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV=\r\n&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff size=\r\n=3D2&gt;With \nthe addition of an AbstractFrontier it may be possible to have t=\r\nhe ARFrontier \nutilize that and thus limit duplication of code.&nbsp; The p=\r\nlain truth however \nis that iterative and snapshot crawling make very diffe=\r\nrent demands on resources \netc.&nbsp;The BdbFrontier is implemented with th=\r\ne clear intention of optimizing \nit for large scale, non repeating crawls. =\r\nThe ARFrontier is however designed for \nrepeatedly crawling the same set of=\r\n URIs and adds a considerable amount of \nmanagment for that task. An exampl=\r\ne of this, the ARFrontier will always ensure \n(within politeness constricts=\r\n) that the URI most overdue for a visit will be the \nnext one issued.&nbsp;=\r\n The BdbFrontier offers little guarantee as to what URI \nwill be issued nex=\r\nt, except the queues are FIFO and allows for queues to be held \ninactive wh=\r\nen enough active queues are being used.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN clas=\r\ns=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;=\r\n&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DAria=\r\nl color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D0=\r\n92301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;I&#39;ll \nadmit t=\r\nhat you could construct a single Frontier to serve both purposes, but it \nw=\r\nould be a matter of compromise. It is (in my opinion) better to have \nspeci=\r\nalized Frontiers for (what are fundamentally) different crawling \nstrategie=\r\ns.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DA=\r\nrial color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=\r\n=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;The \nARFr=\r\nontier is furthermore designed as a generic repeating frontier (I&#39;m planing=\r\n \non renaming it Repeating or RevisitingFrontier since it does not contain =\r\nany \nadaptive ability itself) and allows the processors to greatly affect t=\r\nhe \nordering by which URIs are issued. This is a function that I do not for=\r\nsee in \nthe BdbFrontier (at least not with the level of granularity the ARF=\r\nrontier \nprovides). &lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022=\r\n005&gt;&lt;FONT face=3DArial color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;=\r\n\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff \n=\r\nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;=\r\nFONT face=3DArial color=3D#0000ff size=3D2&gt;- \nKris&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV=\r\n&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT face=3DArial color=3D#0000ff \nsize=\r\n=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D092301415-01022005&gt;&lt;FONT=\r\n face=3DArial color=3D#0000ff \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;BLOCKQU=\r\nOTE \nstyle=3D&quot;PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #0000ff 2px=\r\n solid; MARGIN-RIGHT: 0px&quot;&gt;\n  &lt;DIV&gt;&lt;/DIV&gt;\n  &lt;DIV class=3DOutlookMessageHead=\r\ner lang=3Den-us dir=3Dltr align=3Dleft&gt;&lt;FONT \n  face=3DTahoma size=3D2&gt;----=\r\n-Original Message-----&lt;BR&gt;&lt;B&gt;From:&lt;/B&gt; \n  jrf@... [mailto:jrf@met=\r\nacarta.com] &lt;B&gt;On Behalf Of &lt;/B&gt;John R. \n  Frank&lt;BR&gt;&lt;B&gt;Sent:&lt;/B&gt; 1. febr=FA=\r\nar 2005 15:11&lt;BR&gt;&lt;B&gt;To:&lt;/B&gt; \n  archive-crawler@yahoogroups.com&lt;BR&gt;&lt;B&gt;Subjec=\r\nt:&lt;/B&gt; RE: [archive-crawler] \n  continuous crawling proposal&lt;BR&gt;&lt;BR&gt;&lt;/FONT&gt;=\r\n&lt;/DIV&gt;&lt;TT&gt;Kris,&lt;BR&gt;&lt;BR&gt;I was aware \n  of your AR module and should have ask=\r\ned a couple questions&lt;BR&gt;about it in that \n  earlier post.&nbsp; The algori=\r\nthm I suggested could be written&lt;BR&gt;as a \n  subclass to your WaitEvaluator.=\r\n&lt;BR&gt;&lt;BR&gt;The big issue with AR is:&nbsp; why is \n  a new Frontier required f=\r\nor this?&nbsp; The&lt;BR&gt;BDB-based Frontier solves (or \n  will soon solve :) s=\r\neveral scalability&lt;BR&gt;issues that our crucial for \n  production use of Heri=\r\ntrix (at least IMHO).&lt;BR&gt;&lt;BR&gt;As far as I can see, the \n  only requirement o=\r\nf the Frontier is logic to&lt;BR&gt;prevent dequeuing before the \n  time determin=\r\ned by the WaitEvaluator.&nbsp; Is&lt;BR&gt;this true?&nbsp; Or did I \n  miss some=\r\nthing?&lt;BR&gt;&lt;BR&gt;I&#39;ll keep you posted on content \n  hashing.&lt;BR&gt;&lt;BR&gt;John&lt;BR&gt;&lt;B=\r\nR&gt;&lt;BR&gt;&lt;BR&gt;On Tue, 1 Feb 2005, Kristinn Sigurdsson \n  wrote:&lt;BR&gt;&lt;BR&gt;&gt; Yes=\r\n, the AR module (currently available as a branch of the \n  Heritrix project=\r\n,&lt;BR&gt;&gt; &lt;A \n  href=3D&quot;http://crawltools.archive.org:8080/cruisecontrol/bu=\r\nildresults/BRANCH-heritri&quot;&gt;http://crawltools.archive.org:8080/cruisecontrol=\r\n/buildresults/BRANCH-heritri&lt;/A&gt;&lt;BR&gt;&gt; \n  x-Kris-ARbranch2) provides the =\r\nfunctionality required for incrimental&lt;BR&gt;&gt; \n  crawling.&lt;BR&gt;&gt;&lt;BR&gt;&gt;=\r\n To do this it implements a new Frontier that uses \n  priority queues for t=\r\nhe&lt;BR&gt;&gt; URIs (this also does away with the &#39;already \n  included fingerpr=\r\nints of URIs).&lt;BR&gt;&gt;&lt;BR&gt;&gt; A processor (dubbed a \n  WaitEvaluator) is i=\r\nnserted to calculate the &#39;time of&lt;BR&gt;&gt; next processing&#39; \n  before a URI =\r\nis returned to the Frontier. Currently the&lt;BR&gt;&gt; WaitEvaluator \n  uses a =\r\nmuch simpler algorithm than John discusses, but a more&lt;BR&gt;&gt; \n  sophistic=\r\nated one could easily replace it.&lt;BR&gt;&gt;&lt;BR&gt;&gt; Currently it relies \n  on=\r\n a strict hash for content changes (although a&lt;BR&gt;&gt; processor for \n  pre=\r\nstripping problematic sections using regular expressions is&lt;BR&gt;&gt; \n  prov=\r\nided). If you have ideas for better content hashes, I&#39;d love to \n  hear&lt;BR&gt;=\r\n&gt; them.&lt;BR&gt;&gt;&lt;BR&gt;&gt; Currently this add on to Heritrix is being \n  te=\r\nsted, if all goes well it will&lt;BR&gt;&gt; become a part of of the next Heritri=\r\nx \n  release.&lt;BR&gt;&gt;&lt;BR&gt;&gt; - Kris&lt;BR&gt;&gt;&lt;BR&gt;&gt; -----Original \n  Messa=\r\nge-----&lt;BR&gt;&gt; From: Bjarne Andersen \n  [mailto:bja@...]&lt;B=\r\nR&gt;&gt; Sent: 1. febr=FAar 2005 08:53&lt;BR&gt;&gt; \n  To: archive-crawler@yahoogr=\r\noups.com&lt;BR&gt;&gt; Subject: Re: [archive-crawler] \n  continuous crawling prop=\r\nosal&lt;BR&gt;&gt;&lt;BR&gt;&gt;&lt;BR&gt;&gt; You might want to take a \n  look at the Automa=\r\nted Revisiting Module being&lt;BR&gt;&gt; developed at the moment \n  by kris@arch=\r\nive.org&lt;BR&gt;&gt;&lt;BR&gt;&gt; It does implement a new Frontier including \n  a new=\r\n Queing-mechanism along&lt;BR&gt;&gt; with a simple algoritm to decide when to \n =\r\n bring URI&#39;s to the top of the&lt;BR&gt;&gt; queue (based on historical results -=\r\n \n  next-ready-time goes either up or&lt;BR&gt;&gt; down everytime a page is fetc=\r\nhed and \n  compared to the last fetch)&lt;BR&gt;&gt;&lt;BR&gt;&gt; It is available from=\r\n the continous \n  build box at crawler.archive.org (a&lt;BR&gt;&gt; CVS-brach cal=\r\nled \n  *_AR....)&lt;BR&gt;&gt;&lt;BR&gt;&gt; best&lt;BR&gt;&gt; Bjarne Andersen&lt;BR&gt;&gt;&lt;BR&gt;&g=\r\nt; John R. \n  Frank wrote:&lt;BR&gt;&gt; &gt; Stack,&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; Wh=\r\nat do you think \n  of these three steps as a possible way to implement&lt;BR&gt;&=\r\ngt; &gt; continuous \n  crawling in Heritrix?&nbsp; Details for each of thes=\r\ne three are&lt;BR&gt;&gt; &gt; \n  discussed below.&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; 1) e=\r\nxtend the work queue&#39;s logic to \n  only dequeue &quot;ready&quot; URLs&lt;BR&gt;&gt; &gt;&lt;B=\r\nR&gt;&gt; &gt; 2) decide when to recheck \n  a given page based on a simple mod=\r\nel&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; 3) detect \n  substantive page changes and stor=\r\ne the info in AlreadySeen&lt;BR&gt;&gt; \n  &gt;&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt;=\r\n &gt; 1) To do this, we need a queueing \n  mechanism that blocks the dequeu=\r\ning of a&lt;BR&gt;&gt; &gt; CrawlURI until we are \n  past its assigned next &quot;okay=\r\n to crawl&quot; moment.&lt;BR&gt;&gt; &gt; Where should we \n  store this timestamp?&nb=\r\nsp; As you said, the BDB keys are&lt;BR&gt;&gt; &gt; \n  overloaded with too much =\r\nmeaning already, so putting a seconds since \n  the&lt;BR&gt;&gt; &gt; epoch in th=\r\nere is not so good:&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &lt;A \n  href=3D&quot;http://crawler.archi=\r\nve.org/xref/org/archive/crawler/frontier/BdbMultipleWork&quot;&gt;http://crawler.ar=\r\nchive.org/xref/org/archive/crawler/frontier/BdbMultipleWork&lt;/A&gt;&lt;BR&gt;&gt; \n  =\r\nQueues.html#246&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; Perhaps this timestamp belongs in=\r\n \n  CandidateURI where the&lt;BR&gt;&gt; &gt; schedulingDirective pieces are?&nbs=\r\np; For \n  example, if the schedulingDirective&lt;BR&gt;&gt; &gt; is negative, the=\r\nn it could be \n  interpreted as a timestamp.&nbsp; So for a&lt;BR&gt;&gt; &gt; do=\r\ncument next \n  Wednesday, it would get:&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; jrf@local=\r\nhost~$ date +%s -d \n  &quot;Feb 9 01:20:29 EST 2005&quot;&lt;BR&gt;&gt; &gt; 1107930029&lt;BR&gt;=\r\n&gt; &gt;&lt;BR&gt;&gt; &gt; \n  with a minus sign in front:&nbsp; -1107930029&lt;BR&gt;=\r\n&gt; &gt;&lt;BR&gt;&gt; &gt; Adding \n  `date +%s -d now` now to that will give a =\r\npositive number when it&lt;BR&gt;&gt; &gt; \n  is okay to crawl.&nbsp; This would =\r\nrequire modifications to the \n  four-bit&lt;BR&gt;&gt; &gt; interpretation of pri=\r\nority in the BDB key \n  computation.&nbsp; An escape value&lt;BR&gt;&gt; &gt; of =\r\n1111 might tell the queue \n  that it needs to look in the object to find&lt;BR=\r\n&gt;&gt; &gt; the value of the \n  timestamp.&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; The pro=\r\nblem with this is that suddenly \n  the queue is not a queue, because&lt;BR&gt;&gt=\r\n; &gt; the keys are the sorting \n  mechanism on the queue in the BDB implem=\r\nentation,&lt;BR&gt;&gt; &gt; right?&nbsp; Do \n  you see a way fix to this besides=\r\n cycling through all the curi&lt;BR&gt;&gt; &gt; \n  with 1111 looking at the full=\r\n value of the timestamp?&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; \n  &gt;&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; =\r\n&gt; 2) To compute the time at which to refetch, we \n  can predict the like=\r\nlihood&lt;BR&gt;&gt; &gt; that the page has been modified as a \n  function of tim=\r\ne since last modified.&lt;BR&gt;&gt; &gt; See below for discussion of \n  detectin=\r\ng last modified.&nbsp; An easy function to&lt;BR&gt;&gt; &gt; use for this \n  mod=\r\neling comes from the Michaelis-Menton model of enzyme&lt;BR&gt;&gt; &gt; \n  kinet=\r\nics:&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; t is \n  =\r\ntime elapsed after a modification of the content&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; \n  &g=\r\nt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P(t) is the probability that the pag=\r\ne \n  has changed by time t&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; \n  &gt;&nbsp;&nbsp;&nbsp;&n=\r\nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  a&lt;BR&gt;&gt; \n  &gt=\r\n;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n=\r\nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  k t&lt;BR&gt;&gt; \n=\r\n  &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb=\r\nsp;&nbsp; \n  P(t) =3D&nbsp; ---------------&lt;BR&gt;&gt; \n  &gt;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  =\r\na&lt;BR&gt;&gt; \n  &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  1&nbsp; + k t&lt;B=\r\nR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; At small t, P is small.&nbsp; At large \n  t, P ten=\r\nds to 1, i.e. certainty of&lt;BR&gt;&gt; &gt; change.&nbsp; It is easiest to \n  c=\r\nhoose a=3D2, which is the smallest integer value&lt;BR&gt;&gt; &gt; that gives \n =\r\n step-like behavior.&nbsp; We can then set k by fitting this function&lt;BR&gt;&g=\r\nt; \n  &gt; to any given page&#39;s history (details below).&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt=\r\n; &gt; \n  Given k, we can use P(t) to predict when to refetch a document.&n=\r\nbsp; We pick \n  a&lt;BR&gt;&gt; &gt; threshold probability above which we want to=\r\n refetch.&nbsp; If \n  we set it low,&lt;BR&gt;&gt; &gt; then we want to recheck m=\r\nore often.&nbsp; If we \n  set it high, that means we&#39;re&lt;BR&gt;&gt; &gt; willin=\r\ng to tolerate more stale \n  content in order to not recheck as often.&lt;BR&gt;&g=\r\nt; &gt;&lt;BR&gt;&gt; &gt; When P(t) \n  exceeds the chosen threshold, then we wan=\r\nt to recheck it.&nbsp; By&lt;BR&gt;&gt; \n  &gt; inverting the probability functio=\r\nn, this threshold gives us a time \n  to&lt;BR&gt;&gt; &gt; wait before rechecking=\r\n the page:&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; \n  &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n  k&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp; \n  (-1/a)&lt;BR&gt;&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp=\r\n; t =3D ( \n  ----------&nbsp;&nbsp; -&nbsp;&nbsp; k )&lt;BR&gt;&gt; \n  &gt;&nbsp;=\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n =\r\n threshold&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt;&nbsp;&nbsp;&nbsp; For clarity that \n  =\r\nis:&lt;BR&gt;&gt; &gt;&nbsp;&nbsp;&nbsp; t =3D (((k / threshold) - k ) ^ \n  (-1/a=\r\n))&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; t is the time interval between the most \n  rec=\r\nent known modification event&lt;BR&gt;&gt; &gt; and that time in the future when =\r\n\n  the expected probability of change is&lt;BR&gt;&gt; &gt; just above threshold.=\r\n&nbsp; \n  For robustness, we should define a time within&lt;BR&gt;&gt; &gt; which=\r\n all pages \n  must be rechecked.&nbsp; In python, the function might look&lt;B=\r\nR&gt;&gt; &gt; \n  like:&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; def delay(k, threshold):&lt;BR&gt;=\r\n&gt; \n  &gt;&nbsp;&nbsp;&nbsp;&nbsp; Days =3D 60 * 60 * 24&lt;BR&gt;&gt; \n  &gt;&=\r\nnbsp;&nbsp;&nbsp;&nbsp; maxRecheckDelay =3D 20 * Days&lt;BR&gt;&gt; \n  &gt;&nbsp;=\r\n&nbsp;&nbsp;&nbsp; thresholdDelay =3D (((k / threshold) - k ) ^ \n  (-1/alph=\r\na))&lt;BR&gt;&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp; return \n  minimum(maxRecheckDelay,=\r\n thresholdDelay)&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; \n  &gt; 3. Since last-mo=\r\ndified information is not universally provided by \n  document&lt;BR&gt;&gt; &gt; =\r\nrepositories, we need a mechanism to detect substantive \n  content changes&lt;=\r\nBR&gt;&gt; &gt; and record them in the BDB.&nbsp; I&#39;m looking \n  into tools fo=\r\nr this kind of&lt;BR&gt;&gt; &gt; content hashing that could be run in \n  the ext=\r\nractor chain and stored in the&lt;BR&gt;&gt; &gt; object that gets into \n  BDB.&lt;B=\r\nR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; Suppose we have such a content hash, then the \n  i=\r\nnterval of time between two&lt;BR&gt;&gt; &gt; known modification events is an \n =\r\n upper bound because there could have been&lt;BR&gt;&gt; &gt; a modification even=\r\nt \n  that we did not observe between these observed&lt;BR&gt;&gt; &gt; events.&nb=\r\nsp; If \n  these upper bounds are not far off the real value, then we can&lt;BR=\r\n&gt;&gt; &gt; \n  accurately approximate the probability of modification at hal=\r\nf the&lt;BR&gt;&gt; \n  &gt; observed interval as being 50%.&nbsp; That is, we use=\r\n the previously \n  observed&lt;BR&gt;&gt; &gt; modification intervals to estimate=\r\n what the next actual \n  interval will be.&lt;BR&gt;&gt; &gt; If the average of t=\r\nhe last, say, five observed \n  intervals is T, then we&lt;BR&gt;&gt; &gt; estimat=\r\ne that at (T/2) in the future, the \n  probability of modification will&lt;BR&gt;&=\r\ngt; &gt; be 50%.&nbsp; This let&#39;s us \n  estimate a value for k based on pre=\r\nviously observed&lt;BR&gt;&gt; &gt; modification \n  intervals:&lt;BR&gt;&gt; &gt;&lt;BR&gt;&g=\r\nt; \n  &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp; \n  -a&lt;BR&gt;&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp; k =3D \n  (T/2)&nbsp;&nbsp;&nbsp; which for a=3D2 is (two over T=\r\n) squared.&lt;BR&gt;&gt; \n  &gt;&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; Some logic is required=\r\n to make sure that only \n  the last few, say five,&lt;BR&gt;&gt; &gt; meaningful =\r\nmodification intervals are \n  averaged to make T.&nbsp; This moving&lt;BR&gt;&gt;=\r\n &gt; window average allows \n  Heritrix to more rapidly adapt to pages that=\r\n have&lt;BR&gt;&gt; &gt; varying update \n  frequencies, which is most pages.&lt;BR&gt;&=\r\ngt; &gt;&lt;BR&gt;&gt; &gt;&lt;BR&gt;&gt; &gt; \n  Thoughts?&nbsp; Reactions?&lt;BR&gt;&gt; &g=\r\nt;&lt;BR&gt;&gt; &gt; John&lt;BR&gt;&lt;/TT&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_0000_01C50873.695470D0--\r\n\n"}}