{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":259730011,"authorName":"Sharad Agarwal","from":"Sharad Agarwal &lt;sharadbdc@...&gt;","replyTo":"LIST","senderId":"6v89CT4yfAlcCkk_MO-v1aRFyVhPE6lJwwGh5nw90TYUXkKLfNebOomk5MiiDsIWBbmyoDkySwVzrfy2UbR77eysHi6cv3miJQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Parallelizing crawler","postDate":"1153227044","msgId":3077,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkNEOTI0LjIwNzAzMDRAYW9sLmNvbT4=","inReplyToHeader":"PDQ0QkJCNkU1LjIwNzAyMDhAYXJjaGl2ZS5vcmc+","referencesHeader":"PGU5OGlxZCtpMXMzQGVHcm91cHMuY29tPiA8NDRCN0RGMzUuMzA0MDgwMUBhcmNoaXZlLm9yZz4gPDQ0QkIzRTU3LjEwNTA2QGFvbC5jb20+IDw0NEJCQjZFNS4yMDcwMjA4QGFyY2hpdmUub3JnPg=="},"prevInTopic":3071,"nextInTopic":3079,"prevInTime":3076,"nextInTime":3078,"topicId":3043,"numMessagesInTopic":16,"msgSnippet":"... yes. the same one. ... this can be done via a simple utility. just call the same hashing fn instance passing the uri, it will tell the node owning it. we","rawEmail":"Return-Path: &lt;sharadbdc@...&gt;\r\nX-Sender: sharadbdc@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 49899 invoked from network); 18 Jul 2006 12:52:13 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m24.grp.scd.yahoo.com with QMQP; 18 Jul 2006 12:52:13 -0000\r\nReceived: from unknown (HELO omr-r01.mail.aol.com) (152.163.225.129)\n  by mta2.grp.scd.yahoo.com with SMTP; 18 Jul 2006 12:52:13 -0000\r\nReceived: from aol.com (10.146.144.105)\n  by omr-r01.mail.aol.com with ESMTP; 18 Jul 2006 08:50:48 -0400\r\nMessage-ID: &lt;44BCD924.2070304@...&gt;\r\nDate: Tue, 18 Jul 2006 18:20:44 +0530\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc4 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e98iqd+i1s3@...&gt; &lt;44B7DF35.3040801@...&gt; &lt;44BB3E57.10506@...&gt; &lt;44BBB6E5.2070208@...&gt;\r\nIn-Reply-To: &lt;44BBB6E5.2070208@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Sharad Agarwal &lt;sharadbdc@...&gt;\r\nSubject: Re: [archive-crawler] Re: Parallelizing crawler\r\nX-Yahoo-Group-Post: member; u=259730011\r\n\r\nstack@... wrote:\n\n&gt;Sharad Agarwal wrote:\n&gt;  \n&gt;\n&gt;&gt;We had the requirement of scaling heritrix. We have done it by employing\n&gt;&gt;Consistent hashing scheme. The hashing scheme splits the url space based\n&gt;&gt;on domain (so that politness is ensured) on which each heritrix instance\n&gt;&gt;work on. The whole set up is controller less; each node has the hashing\n&gt;&gt;function. Whenever a out of node&#39;s scope url is found, it is sent to the\n&gt;&gt;node owning it. These urls are collected and sent in batches to the\n&gt;&gt;owning nodes via JMX by a separate thread.\n&gt;&gt;The architecture looks to be scalable in the sense that there is no\n&gt;&gt;controller bottleneck. All heritrix node are identical. The whole scheme\n&gt;&gt;is working pretty fine for us. We have gone quite a few millions with\n&gt;&gt;3-4 heritrix instance nodes.\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;Sounds sweet.\n&gt;\n&gt;+ Did you use the ubicrawler ConsistentHashFunction implementation that \n&gt;is in Heritrix in the dsi.unimi.it jar or did you use something else?\n&gt;  \n&gt;\nyes. the same one.\n\n&gt;+ If, lets say, a hostmaster complained you were hitting her servers too \n&gt;hard, how did you figure which of the crawlers the domain had landed \n&gt;on?  (Was it a problem for you that its effectively opaque which domain \n&gt;landed on which server?)\n&gt;  \n&gt;\n\nthis can be done via a simple utility. just call the same hashing fn instance passing the uri, it will tell the node owning it.\nwe didn&#39;t face any problem due to this opaqueness. \nalso same domain lands on the same server across server restarts (courtesy consistent hashing); making it easy for maintenance.\n\n&gt;+ Do you have anything to say about how well the load was balanced \n&gt;across the crawlers in practise.  I understand that in theory the load \n&gt;should be evenly distributed but I wonder how the practise was.  Could \n&gt;you adjust how much load a machine was carrying post startup?\n&gt;  \n&gt;\n\nThe load is fairly balanced with even 2-3 nodes. As the number of nodes increase, my assumption is it will even balance more.\nWe do have the mechanism of adding nodes at the runtime which inturn updates the hashing fn of all running nodes; and load is balanced.\nFine tuning the load with same number of nodes is not done yet. This can be done by introduction of mapping of multiple buckets to a single node. \nCurrently one node has just 1 bucket. For tuning, one might shift buckets from heavily loaded node to another node. \n\n&gt;+ Is the code proprietary and if not, would you mind sharing it?\n&gt;  \n&gt;\n\nWe do have plans for making the code public. Some work has to be done to integrate it with the latest heritrix release. Ours uses 1.6. Also it has to branch off from our current project code base. We have done many enhancements like focus crawling, host priortization etc. So I beleive it has to be a phased thing to get into the current branch.\n\nSharad\n\n&gt;Good stuff,\n&gt;St.Ack\n&gt;\n&gt;\n&gt;  \n&gt;\n&gt;&gt;- Sharad\n&gt;&gt;\n&gt;&gt;stack@... &lt;mailto:stack%40archive.org&gt; wrote:\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;molzbh wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Cool. Anyways my thoughts on splitting the architecture were to split\n&gt;&gt;&gt;&gt;the Processors across various machines. Have a Statistics server\n&gt;&gt;&gt;&gt;maintain statistics, and a single frontier.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;Here&#39;s a few items you&#39;d have to contend with if you split Heritrix:\n&gt;&gt;&gt;\n&gt;&gt;&gt;+ You&#39;ll have to spend alot of resources serializing and deserializing\n&gt;&gt;&gt;passing rich URLs and configurations.\n&gt;&gt;&gt;+ Certain processors, as written, expect to find on local disk the\n&gt;&gt;&gt;downloaded resources (You could share them using NFS. NFS won&#39;t take\n&gt;&gt;&gt;too kindly to the crawler&#39;s heavy I/O).\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;My idea was to have\n&gt;&gt;&gt;&gt;frontier emit a batch of URLS to the processore boxes, hence instead\n&gt;&gt;&gt;&gt;of next() you have nextBatch(). Anyways, I guess this involves a lot\n&gt;&gt;&gt;&gt;of work, hence I am going in for a Peer2Peer setup, with fully loaded\n&gt;&gt;&gt;&gt;agents doing the URL splits and emmitting batches to the others. I am\n&gt;&gt;&gt;&gt;wondering however on the efficiency of JMX with RMI connectors to do\n&gt;&gt;&gt;&gt;this Job. Do you think going the plain RMI way would be faster?\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;We haven&#39;t measured. JMX is heavyweight but works.\n&gt;&gt;&gt;\n&gt;&gt;&gt;One suggestion has been to use JGroups. Friends of Heritrix were\n&gt;&gt;&gt;experimenting making a &#39;cluster&#39; subclass of Frontier. On\n&gt;&gt;&gt;initialization, it joined a JGroups group and listened in for URL\n&gt;&gt;&gt;broadcasts. Those that fit its portion of the URL space, it picked off\n&gt;&gt;&gt;the bus and added to the local Frontier. Those URLs meant for another\n&gt;&gt;&gt;crawler, were broadcast (They&#39;ve said they&#39;ll write the list if findings\n&gt;&gt;&gt;prove promising).\n&gt;&gt;&gt;\n&gt;&gt;&gt;But depends on your requirements. Joe Hungs&#39; group didn&#39;t bother\n&gt;&gt;&gt;swapping URLs across the cluster. Their experience was that the\n&gt;&gt;&gt;crawlers had sufficient work without exchanging URLs and figured that\n&gt;&gt;&gt;each crawler would discover its URL-segment important pages anyways\n&gt;&gt;&gt;without having to have injection from peers (I don&#39;t know if they\n&gt;&gt;&gt;&#39;proved&#39; this assertion).\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Sorry\n&gt;&gt;&gt;&gt;for pounding you with questions but I am still in the design phase of\n&gt;&gt;&gt;&gt;the system looking for a billion plus crawl, and since it won&#39;t be a\n&gt;&gt;&gt;&gt;one time thing I am also looking at scalability and agent join/leave\n&gt;&gt;&gt;&gt;mechanisms.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;Can we collaborate? This is a problem we need to solve ourselves.\n&gt;&gt;&gt;\n&gt;&gt;&gt;St.Ack\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;--- In archive-crawler@yahoogroups.com \n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;&gt;&lt;mailto:archive-crawler%40yahoogroups.com&gt;, Michael Stack &lt;stack@...&gt;\n&gt;&gt;&gt;&gt;wrote:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;Anmol Bhasin wrote:\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;Thanks! I am wondering however if we leave the central frontier\n&gt;&gt;&gt;&gt;&gt;&gt;machine concept out for a bit, would there be anybenfit to split URL\n&gt;&gt;&gt;&gt;&gt;&gt;space in place of Split Architechture.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;            \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;I am trying to do quick big crawl, hence wondering which approaches\n&gt;&gt;&gt;&gt;&gt;&gt;are the best ways to get moving.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;            \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;If you&#39;re in a hurry, split URL space (and throw hardware at it). Some\n&gt;&gt;&gt;&gt;&gt;fairly large crawls have been achieved using this technique both by us\n&gt;&gt;&gt;&gt;&gt;-- 200million plus -- and by others (See the testimonial cited in the\n&gt;&gt;&gt;&gt;&gt;previous where Joe Hung and his compa�eros did a 1Billion+ pages).\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;What were you thinking regards splitting the architecture?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;Yours,\n&gt;&gt;&gt;&gt;&gt;St.Ack\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt; \n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;\n&gt;\n&gt;  \n&gt;\n\n\n"}}