{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"mblgwFJ1w7sEhXJUaljRfhdlWnh3E03HqViry6Bh8DWstUqQuIRcnIdPW4skbd6r_nk1E1TxvHWAovY2JyhYwA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Ignore Post Re: Correct way to start crawl with large (4M+) seed file?","postDate":"1137442046","msgId":2555,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzQ0JGQ0ZFLjUwMTAyMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGIxM2YzZjA2MDYwMTE1MTY0NWkzZGRlOGM3NXUzMTQ1MjRiYjZiOWRmMmEyQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGIxM2YzZjA2MDYwMTE1MTY0NWkzZGRlOGM3NXUzMTQ1MjRiYjZiOWRmMmEyQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2554,"nextInTopic":0,"prevInTime":2554,"nextInTime":2556,"topicId":2554,"numMessagesInTopic":2,"msgSnippet":"... You re not doing anything wrong.  Trying to load up 4M in one go will pretty much for sure not work, not till we address [ 944987 ] Support case where","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60066 invoked from network); 16 Jan 2006 20:09:12 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m29.grp.scd.yahoo.com with QMQP; 16 Jan 2006 20:09:12 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta3.grp.scd.yahoo.com with SMTP; 16 Jan 2006 20:09:12 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k0GJ0W106229\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 16 Jan 2006 11:00:34 -0800\r\nMessage-ID: &lt;43CBFCFE.5010201@...&gt;\r\nDate: Mon, 16 Jan 2006 12:07:26 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8) Gecko/20051218 SeaMonkey/1.0b\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;b13f3f060601151645i3dde8c75u314524bb6b9df2a2@...&gt;\r\nIn-Reply-To: &lt;b13f3f060601151645i3dde8c75u314524bb6b9df2a2@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:12:4:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Ignore Post Re: Correct way to start crawl\n with large (4M+) seed file?\r\nX-Yahoo-Group-Post: member; u=168599281; y=QepDiITw1oaJ_LfpQ9fCdwmytlZ5JEh6SqMscGcbYNyNn3BSU9Hu6MpF\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nYousef Ourabi wrote:\n&gt; Sorry to answer my own post -- I just found the posting from 8/17/05 \n&gt; about dealing with 7M seeds and trickling them in with the jmx client. \n&gt; That more or less answers the questions I had.\n\nYou&#39;re not doing anything wrong.  Trying to load up 4M in one go will \npretty much for sure not work, not till we address &#39;[ 944987 ] Support \ncase where millions of seeds&#39;.\n\nI wonder how far Matt Ittigson got on his attempted fetch of 7M seeds?\n\nThere should be no memory retention between running of jobs other than a \n1k or so (an entry describing completed job in list of completed jobs).  \nDo you see the memory retention across jobs running &#39;ordinary&#39; jobs?\n\nThanks,\nSt.Ack\n\n&gt;\n&gt; Thanks.\n&gt;\n&gt; -Yousef\n&gt;\n&gt; On 1/15/06, *Yousef Ourabi* &lt;yourabi@... \n&gt; &lt;mailto:yourabi@...&gt;&gt; wrote:\n&gt;\n&gt;     Hello,\n&gt;     I&#39;ve got a seeds.txt file with all unique dmoz links (around 4M).\n&gt;     I created a job, copied over my seeds file and started the job. It\n&gt;     starts by staying in the &#39;preparing&#39; phase for about an hour while\n&gt;     the state directory grows to 4.8gig and the amount of memory used\n&gt;     grows to 1.5 gig (through java_opts i&#39;ve set the -Xmx to 1512M\n&gt;     -Xss to 2M). It then crawls about 131 pages in about 30 min before\n&gt;     throwing countless OOME. Also after I terminate the job it seems\n&gt;     that memory isn&#39;t freed up. The machine is a 2.6 P4 with 2G ram. \n&gt;     I&#39;m using the BdbFrontier, tried both DomainScope and BroadScope.\n&gt;\n&gt;     I&#39;m more than willing to believe I&#39;m doing something wrong\n&gt;     (PEBKAC) -- So what is the proper way to start off with such a\n&gt;     large seed file? Also is there any way to force Heritrix to do\n&gt;     some GC to free up space on the heap in between jobs? Obviously I\n&gt;     could start/stop heritrix, just wondering if there is a sexier way\n&gt;     (through JMX perhaps)?\n&gt;\n&gt;     1532709 KB used\n&gt;     1536384 KB current heap\n&gt;     1536384 KB max heap\n&gt;\n&gt;     31M     ./logs\n&gt;     4.0K    ./checkpoints\n&gt;     4.8G    ./state\n&gt;     44K     ./scratch\n&gt;     208K    ./arcs\n&gt;     5.0G   \n&gt;\n&gt;     -Yousef\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; SPONSORED LINKS\n&gt; Computer security \n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+security&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=BHmcxBg5sKfN9-gcWnJWDg&gt; \n&gt; \tComputer training \n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+training&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=v0JjJWA4s7mLnWQWdFxuTQ&gt; \n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}