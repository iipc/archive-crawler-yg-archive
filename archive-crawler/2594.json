{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":194053897,"authorName":"liewchooichin","from":"&quot;liewchooichin&quot; &lt;liewchooichin@...&gt;","profile":"liewchooichin","replyTo":"LIST","senderId":"WVR4B6Ul_9sg71Xb0a9ERrYcHmDHbfM-0gWT9ATY3R5aE2pd0OPJE5Nx0d2pOJYvWAf7bEDtkjm4dyLOYAg0vd4Mzx2E-7YALc98VEEgZsw5tFI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Crawl a specific page & the hyperlinks in it","postDate":"1138321929","msgId":2594,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRyYnBtOStuNWswQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":2593,"nextInTime":2595,"topicId":2594,"numMessagesInTopic":1,"msgSnippet":"I have just started using Heritrix. I read the user manual that the engine only crawl a specific path, for example http://en.wikipedia.org/wiki/. But, say I","rawEmail":"Return-Path: &lt;liewchooichin@...&gt;\r\nX-Sender: liewchooichin@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 67426 invoked from network); 27 Jan 2006 00:33:36 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m29.grp.scd.yahoo.com with QMQP; 27 Jan 2006 00:33:36 -0000\r\nReceived: from unknown (HELO n7a.bullet.dcn.yahoo.com) (216.155.203.227)\n  by mta2.grp.scd.yahoo.com with SMTP; 27 Jan 2006 00:33:36 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [216.155.201.64] by n7.bullet.dcn.yahoo.com with NNFMP; 27 Jan 2006 00:32:10 -0000\r\nReceived: from [66.218.69.2] by t1.bullet.dcn.yahoo.com with NNFMP; 27 Jan 2006 00:32:10 -0000\r\nReceived: from [66.218.66.83] by t2.bullet.scd.yahoo.com with NNFMP; 27 Jan 2006 00:32:10 -0000\r\nDate: Fri, 27 Jan 2006 00:32:09 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;drbpm9+n5k0@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;liewchooichin&quot; &lt;liewchooichin@...&gt;\r\nSubject: Crawl a specific page & the hyperlinks in it\r\nX-Yahoo-Group-Post: member; u=194053897; y=2h-CImA4n83eSIr2KCNIrOR9JyUCjENCkmLX9laGQG9mjMtrgTeneA\r\nX-Yahoo-Profile: liewchooichin\r\n\r\nI have just started using Heritrix. I read the user manual that the \nengine=\r\n only crawl a specific path, for example \nhttp://en.wikipedia.org/wiki/.\n\nB=\r\nut, say I want to crawl only the page on William Shakespeare \nhttp://en.wik=\r\nipedia.org/wiki/Shakespeare, the engine will still look \nup to the wiki pat=\r\nh and all the documents under it. Is this correct?\n\nQuestion 1:\nHow do I co=\r\nnfigure a job to crawl a specific page like \nhttp://en.wikipedia.org/wiki/S=\r\nhakespeare?\n\nNow, there are many hyperlinks to other documents about Willia=\r\nm \nShakespeare. Most of the pages are from links in /wiki/ path and a \nfew =\r\nfrom external links.\n\nQuestion 2:\na) How do I configure a job to crawl hype=\r\nrlinks within a page? \nb) Can I specify to crawl only certain links, e.g. o=\r\nnly pages \nunder /wiki/ path?\nc) How do I control the external links that i=\r\nt can crawl in terms of:\n   - how many external links?\n   - the size and th=\r\ne number of pages?\n\n\n\n\n\n\n\n"}}