{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"G2iBrBrAxz23IEZ-8ATtWta0aymA9OaxHNV6vuGnwrKLYMqTD1AYmLtuvNQWg-Ku5wi-FiyrTtyXy-kaXwYHSzQilos2OKQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix 3.2.0 deduplication","postDate":"1423338507","msgId":8671,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU0RDY2QzBCLjUwMDA5MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDE0MjMyODI5MTUuMjA4MDguWWFob29NYWlsQW5kcm9pZE1vYmlsZUB3ZWIxOTA2MDYubWFpbC5zZzMueWFob28uY29tPg==","referencesHeader":"PDE0MjMyODI5MTUuMjA4MDguWWFob29NYWlsQW5kcm9pZE1vYmlsZUB3ZWIxOTA2MDYubWFpbC5zZzMueWFob28uY29tPg=="},"prevInTopic":8670,"nextInTopic":8672,"prevInTime":8670,"nextInTime":8672,"topicId":8664,"numMessagesInTopic":10,"msgSnippet":"The main point of the feature is to save storage space, by not writing the same resource content-body again. The revisit record in the latest WARC, after","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 871 invoked by uid 102); 7 Feb 2015 19:48:29 -0000\r\nX-Received: from unknown (HELO mtaq4.grp.bf1.yahoo.com) (10.193.84.143)\n  by m6.grp.bf1.yahoo.com with SMTP; 7 Feb 2015 19:48:29 -0000\r\nX-Received: (qmail 26985 invoked from network); 7 Feb 2015 19:48:29 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (98.139.245.163)\n  by mtaq4.grp.bf1.yahoo.com with SMTP; 7 Feb 2015 19:48:29 -0000\r\nX-Received: (qmail 39981 invoked by uid 0); 7 Feb 2015 19:48:28 -0000\r\nX-Received: from 70.36.143.121 (HELO probook.local) (70.36.143.121)\n  by relay01.pair.com with SMTP; 7 Feb 2015 19:48:28 -0000\r\nX-pair-Authenticated: 70.36.143.121\r\nMessage-ID: &lt;54D66C0B.5000902@...&gt;\r\nDate: Sat, 07 Feb 2015 11:48:27 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;1423282915.20808.YahooMailAndroidMobile@...&gt;\r\nIn-Reply-To: &lt;1423282915.20808.YahooMailAndroidMobile@...&gt;\r\nContent-Type: text/plain; charset=utf-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nSubject: Re: [archive-crawler] Heritrix 3.2.0 deduplication\r\nX-Yahoo-Group-Post: member; u=137285340; y=8ComHiKsAVxCX8W6E0gVDfrxT2I32Ztd4TTOZOZ9m84o\r\nX-Yahoo-Profile: gojomo\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\n\r\nThe main point of the feature is to save storage space, by not writing \nthe same resource content-body again. The revisit record in the latest \nWARC, after duplicate-detection, just includes enough info to find the \nprior content, if you&#39;ve kept it around and have the right indexes.\n\nSo you absolutely need to keep the original WARCs to have any chance of \nviewing the original resource content-body.\n\nI don&#39;t know the current state of OpenWayback support for these kind of \nrevisit records – it would likely at the least need an extra \n(digest-&gt;original) index built. Anyone?\n\n- Gordon\n\nOn 2/6/15 8:21 PM, Sandip Dev devsandip2511@... [archive-crawler] \nwrote:\n&gt;\n&gt;\n&gt; Thanks Gordon will try that. I have a question though when a revisit\n&gt; record is written is the resource physically written to the new archive\n&gt; or is it just a pointer record to a previous archive ? In case it is\n&gt; just a pointer when we playback the new archive via openwayback will it\n&gt; show a &quot;resource not in archive &quot; for the revisit resources if we dont\n&gt; keep the previous archive ?\n&gt;\n&gt; _Sandip_\n&gt;\n&gt; Sent from Yahoo Mail on Android\n&gt; &lt;https://overview.mail.yahoo.com/mobile/?.src=Android&gt;\n&gt;\n&gt; From:&quot;Gordon Mohr gojomo@... [archive-crawler]&quot;\n&gt; &lt;archive-crawler@yahoogroups.com&gt;\n&gt; Date:Sat, Feb 7, 2015 at 4:23 am\n&gt; Subject:Re: [archive-crawler] Heritrix 3.2.0 deduplication\n&gt;\n&gt; It can be helpful in such cases, when learning/testing a feature for the\n&gt; 1st time, to make the crawls toy-sized. That is, crawl just a single URI\n&gt; that you believe would be eligible for duplication-savings. Only when\n&gt; you&#39;re sure the configuration works for the single URI would you then\n&gt; expand to a more realistic crawl.\n&gt;\n&gt; For example, you could try a crawl of just the static resource:\n&gt;\n&gt; http://localhost:8081/artJQWeb/resources/core/main.css\n&gt;\n&gt; In your first crawl, working with no prior state/history, you&#39;d expect\n&gt; this URI to have a normal log-line. In a second crawl that&#39;s set up\n&gt; properly to consult the first crawl&#39;s digest-history, it should appear\n&gt; as a detected duplicate (and then only be written in the WARC as a\n&gt; &#39;revisit&#39; record).\n&gt;\n&gt; Until you have this working for one URI, review your configuration, and\n&gt; make sure the 2nd crawl is properly consulting the history-info from the\n&gt; first.\n&gt;\n&gt; - Gordon\n&gt;\n&gt; On 2/6/15 4:50 AM, Sandip Dev devsandip2511@... [archive-crawler]\n&gt; wrote:\n&gt;  &gt; [Attachment(s) &lt;#TopText&gt; from Sandip Dev included below]\n&gt;  &gt;\n&gt;  &gt; Roger,\n&gt;  &gt; I deleted the state directory and history directory within my job folder\n&gt;  &gt; and then ran the crawl twice .\n&gt;  &gt; I am attaching the crawl log and crawl report for run 1 and 2 in case\n&gt;  &gt; you want to have a look. The crawl logs seems to be very much the same\n&gt;  &gt; for both runs. Do you think it looks ok to you as the warc size remains\n&gt;  &gt; same for both crawl?\n&gt;  &gt;\n&gt;  &gt; I have some other jobs as well which crawled this website so do I need\n&gt;  &gt; to delete the state directory for those jobs as well ?\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; Thanks Sandip\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; On Friday, 6 February 2015 4:59 PM, &quot;Coram, Roger&quot; &lt;Roger.Coram@...&gt;\n&gt;  &gt; wrote:\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; Hi Sandip,\n&gt;  &gt; If, having remove the legacy beans, you’re seeing “warcRevisit:digest”\n&gt;  &gt; in the log this sounds like it’s already deduplicating from an earlier\n&gt;  &gt; crawl—did you delete Heritrix’s state directory before testing? If not,\n&gt;  &gt; Heritrix will try and deduplicate based on the earlier, stored metadata.\n&gt;  &gt; Roger\n&gt;  &gt; *From:*Sandip Dev [mailto:devsandip2511@...]\n&gt;  &gt; *Sent:* 06 February 2015 11:24\n&gt;  &gt; *To:* Coram, Roger\n&gt;  &gt; *Subject:* Re: [archive-crawler] Heritrix 3.2.0 deduplication\n&gt;  &gt; Hi Roger,\n&gt;  &gt; Thanks a lot for your reply.\n&gt;  &gt; I tried testing this on a small web app I created.\n&gt;  &gt; As suggested I removed the Legacy config&#39;s and only kept the\n&gt;  &gt; URL-Agnostic Duplication Reduction .\n&gt;  &gt; I can also see the warcRevisit:digest records in the crawl log and it\n&gt;  &gt; generated a 312 kb warc file which played back fine.\n&gt;  &gt; I ran the crawl once more without changing anything on the site I was\n&gt;  &gt; expecting the size of the warc to reduce as I had not changed anything\n&gt;  &gt; on the site and as all the contents were already present in the first\n&gt;  &gt; warc file. but the size of the archieve still remained 312 kb.\n&gt;  &gt; Is my assumption not correct ?\n&gt;  &gt; Thanks Sandip\n&gt;  &gt; On Friday, 6 February 2015 4:43 PM, &quot;Sandip Dev devsandip2511@...\n&gt;  &gt; [archive-crawler]&quot; &lt;archive-crawler@yahoogroups.com&gt; wrote:\n&gt;  &gt; Hi Roger,\n&gt;  &gt; Thanks a lot for your reply.\n&gt;  &gt; I tried testing this on a small web app I created.\n&gt;  &gt; As suggested I removed the Legacy config&#39;s and only kept the\n&gt;  &gt; URL-Agnostic Duplication Reduction .\n&gt;  &gt; I can also see the warcRevisit:digest records in the crawl log and it\n&gt;  &gt; generated a 312 kb warc file which played back fine.\n&gt;  &gt; I ran the crawl once more without changing anything on the site I was\n&gt;  &gt; expecting the size of the warc to reduce as I had not changed anything\n&gt;  &gt; on the site and as all the contents were already present in the first\n&gt;  &gt; warc file. but the size of the archieve still remained 312 kb.\n&gt;  &gt; Is my assumption not correct ?\n&gt;  &gt; Thanks Sandip\n&gt;  &gt; On Friday, 6 February 2015 3:55 PM, &quot;&#39;Coram, Roger&#39; Roger.Coram@...\n&gt;  &gt; [archive-crawler]&quot; &lt;archive-crawler@yahoogroups.com&gt; wrote:\n&gt;  &gt; Hi Sandip,\n&gt;  &gt; As far as I’m aware you should definitely be using only one of the\n&gt;  &gt; “URL-Agnostic Duplication Reduction” or the “Legacy…” version. Although\n&gt;  &gt; they largely do the same thing they behave differently (effectively the\n&gt;  &gt; newer version keys on the checksum, the older keys on the URL+checksum).\n&gt;  &gt; At first glance your config. looks correct so it might be the inclusion\n&gt;  &gt; of both the above causing the issue. During a crawl, if you watch your\n&gt;  &gt; crawl.log file, you should see entries with annotations like\n&gt;  &gt; “warcRevisit:digest” in the last field—that indicates deduplication is\n&gt;  &gt; working.\n&gt;  &gt; Roger\n&gt;  &gt; *From:*archive-crawler@yahoogroups.com\n&gt;  &gt; [mailto:archive-crawler@yahoogroups.com]\n&gt;  &gt; *Sent:* 06 February 2015 04:07\n&gt;  &gt; *To:* archive-crawler@yahoogroups.com\n&gt;  &gt; *Subject:* [archive-crawler] Heritrix 3.2.0 deduplication [1 Attachment]\n&gt;  &gt; *[Attachment(s)\n&gt;  &gt; &lt;https://in-mg61.mail.yahoo.com/neo/launch?.rand=b829oo386s801#TopText&gt;\n&gt;  &gt; from Sandip Dev included below]*\n&gt;  &gt; Hi,\n&gt;  &gt; I had already posted my questions earlier regarding the issues that I\n&gt;  &gt; have been facing the deduplication.\n&gt;  &gt; Can you please let me know if this is the correct forum where I should\n&gt;  &gt; post my questions ?\n&gt;  &gt; I am trying to achieve incremental crawling using Heritrix 3.2.0.\n&gt;  &gt; However it seems it is always browsing all the URL&#39;s even if there is no\n&gt;  &gt; change.\n&gt;  &gt; I am attaching crawler-beans.xml for the configuration that I am doing.\n&gt;  &gt; Can anyone suggest what I might be doing wrong ?\n&gt;  &gt; Is the spring bean I have put the configurations for both\n&gt;  &gt;\n&gt;  &gt; * URL-Agnostic Duplication Reduction\n&gt;  &gt; * Legacy Duplication Reduction Configuration\n&gt;  &gt;\n&gt;  &gt; Can I use both the features at the same time in the config ? I see the\n&gt;  &gt; archive size remains the same across multiple crawls even if there is no\n&gt;  &gt; change . How do I check if the the deduplication is working ?\n&gt;  &gt; Any help will be greatly appreciated as I am desperately tryinjg to get\n&gt;  &gt; this work :)\n&gt;  &gt; Thanks\n&gt;  &gt; Sandip Dev\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt; ******************************************************************************************************************\n&gt;  &gt; Experience the British Library online at www.bl.uk &lt;http://www.bl.uk/&gt;\n&gt;  &gt; The British Library’s latest Annual Report and Accounts :\n&gt;  &gt; www.bl.uk/aboutus/annrep/index.html\n&gt;  &gt; &lt;http://www.bl.uk/aboutus/annrep/index.html&gt;\n&gt;  &gt; Help the British Library conserve the world&#39;s knowledge. Adopt a Book.\n&gt;  &gt; www.bl.uk/adoptabook &lt;http://www.bl.uk/adoptabook&gt;\n&gt;  &gt; The Library&#39;s St Pancras site is WiFi - enabled\n&gt;  &gt;\n&gt; *****************************************************************************************************************\n&gt;  &gt; The information contained in this e-mail is confidential and may be\n&gt;  &gt; legally privileged. It is intended for the addressee(s) only. If you are\n&gt;  &gt; not the intended recipient, please delete this e-mail and notify the\n&gt;  &gt; postmaster@... &lt;mailto:postmaster@...&gt;: The contents of this e-mail\n&gt;  &gt; must not be disclosed or copied without the sender&#39;s consent.\n&gt;  &gt; The statements and opinions expressed in this message are those of the\n&gt;  &gt; author and do not necessarily reflect those of the British Library. The\n&gt;  &gt; British Library does not take any responsibility for the views of the\n&gt;  &gt; author.\n&gt;  &gt;\n&gt; *****************************************************************************************************************\n&gt;  &gt;\n&gt;  &gt; Think before you print\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}