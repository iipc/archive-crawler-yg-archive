{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"-AHwnggU1ytT_9akq_28JmQNwnHJxag1wCDdK4DztBXCX9KL_RQjRwidJVr_hVHf1SvuUdMO8ZM3ESt1bPcBDg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] keeping threads active during crawls","postDate":"1109710592","msgId":1634,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyMjRENzAwLjUwNTA1MDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDYuMi4wLjE0LjIuMjAwNTAzMDExMzAzMDEuMDJiMDVmMzhAdmhvc3Q2LmF0b21pY3NlcnZlcnMuY29tPg==","referencesHeader":"PDYuMi4wLjE0LjIuMjAwNTAyMjgxNDA1MzAuMDJiM2YxZThAdmhvc3Q2LmF0b21pY3NlcnZlcnMuY29tPiA8NDIyNEM2MzMuNTA3MDcwMEBhcmNoaXZlLm9yZz4gPDYuMi4wLjE0LjIuMjAwNTAzMDExMzAzMDEuMDJiMDVmMzhAdmhvc3Q2LmF0b21pY3NlcnZlcnMuY29tPg=="},"prevInTopic":1633,"nextInTopic":1683,"prevInTime":1633,"nextInTime":1635,"topicId":1622,"numMessagesInTopic":8,"msgSnippet":"... Sounds good.  Let us know if you can think of something we should add to the crawler to help you implement this strategy (You might have suggestions for","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 32517 invoked from network); 1 Mar 2005 21:05:19 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m21.grp.scd.yahoo.com with QMQP; 1 Mar 2005 21:05:19 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta5.grp.scd.yahoo.com with SMTP; 1 Mar 2005 21:05:19 -0000\r\nReceived: (qmail 31586 invoked by uid 100); 1 Mar 2005 20:48:22 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 1 Mar 2005 20:48:22 -0000\r\nMessage-ID: &lt;4224D700.5050509@...&gt;\r\nDate: Tue, 01 Mar 2005 12:56:32 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.3) Gecko/20041007 Debian/1.7.3-5\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;6.2.0.14.2.20050228140530.02b3f1e8@...&gt; &lt;4224C633.5070700@...&gt; &lt;6.2.0.14.2.20050301130301.02b05f38@...&gt;\r\nIn-Reply-To: &lt;6.2.0.14.2.20050301130301.02b05f38@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.7 required=6.0 tests=AWL,CLICK_BELOW autolearn=no \n\tversion=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] keeping threads active during crawls\r\nX-Yahoo-Group-Post: member; u=168599281\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nMike Schwartz wrote:\n\n&gt; ...\n&gt; I have a new strategy I&#39;m going to try, which is basically:\n&gt;\n&gt;     * run crawl until it gets to low parallelism state, then kill that\n&gt;       crawl, extracting the list of domains that were still not\n&gt;       completely crawled\n&gt;     * do the above for each of my DomainScope&#39;d crawls, and at the end\n&gt;       put together the list of all domains not completely crawled, and\n&gt;       start a new crawl with them.  At that point there should\n&gt;       hopefully be enough of them to again have good thread\n&gt;       parallelism (sort of a refactoring of crawl seeds)\n&gt;\n&gt;\n&gt; basically, that final composite crawl should consist of all the \n&gt; domains for which there are many individual URLs to retrieve and/or \n&gt; the site is responding very slowly, with the result that we only wait \n&gt; for the long tail to complete once at the end of all DomainScope \n&gt; crawls, rather than once for each DomainScope crawl.\n&gt;\n&gt; this will mean more manual admin effort, but I think once I gain \n&gt; experience with this approach I can automate the detection and \n&gt; refactoring of the crawls (especially once the checkpointing mechanism \n&gt; you mentioned is implemented).\n\nSounds good.  Let us know if you can think of something we should add to \nthe crawler to help you implement this strategy (You might have \nsuggestions for what to add to the jmxclient: \nhttp://crawler.archive.org/cmdline-jmxclient/).\nYours,\nSt.Ack\n\n&gt;\n&gt; thanks\n&gt;  - Mike\n&gt;\n&gt;\n&gt; At 12:44 PM 3/1/2005, you wrote:\n&gt;\n&gt;&gt; Mike Schwartz wrote:\n&gt;&gt;\n&gt;&gt; &gt; hi,\n&gt;&gt; &gt;\n&gt;&gt; &gt; I need to run a series of DomainScope crawls.  I notice that each such\n&gt;&gt; &gt; crawl gets good thread parallelism for quite a while but then gets to\n&gt;&gt; &gt; a point where most of what&#39;s left to do is crawling many pages within\n&gt;&gt; &gt; a small number of sites (e.g., all the product pages at each of a few\n&gt;&gt; &gt; sites).  At that point only a few points are active, and make very\n&gt;&gt; &gt; slow progress because I only visit each site once every few seconds.\n&gt;&gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; This problem wouldn&#39;t arise if I were doing a broad-scope crawl, since\n&gt;&gt; &gt; at any point in time there are more sites left to crawl than there are\n&gt;&gt; &gt; available threads.\n&gt;&gt; &gt;\n&gt;&gt; &gt; Does anyone have a suggestion how I could keep most/all of the threads\n&gt;&gt; &gt; active during a sequence of DomainScope crawls?  I could try adding a\n&gt;&gt; &gt; new set of sites when I get down to the state of many pages left\n&gt;&gt; &gt; within just a few sites, but it seems to me that&#39;s a problem because\n&gt;&gt; &gt; in essence I&#39;m running one much larger crawl instead of a set of\n&gt;&gt; &gt; limited scope crawls - and if that crawl gets into a bad state and I\n&gt;&gt; &gt; have to kill it, I don&#39;t end up knowing exactly which parts of which\n&gt;&gt; &gt; sites have completed being crawled.\n&gt;&gt;\n&gt;&gt; We&#39;re looking into adding accounting that will allow the running of\n&gt;&gt; multiple crawls inside of a single running instance -- More to follow on\n&gt;&gt; this after it gets flushed out -- but until then, running an instance\n&gt;&gt; per crawl seems to be your only option.\n&gt;&gt;\n&gt;&gt; Do you have sufficent resources to start up a new crawl instance to go\n&gt;&gt; against a new domain on the machine that has the trailing-off crawl\n&gt;&gt; running on it?  Or are your crawls up against Heritrix bounds?\n&gt;&gt;\n&gt;&gt; When we have a checkpointing system in place, you&#39;ll be able to\n&gt;&gt; checkpoint the dying crawl, stop it, and then restart it inside of a\n&gt;&gt; smaller heap letting it crawl to completion making room to run the new\n&gt;&gt; crawl.\n&gt;&gt;\n&gt;&gt; St.Ack\n&gt;&gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; (I&#39;ve tried restarting crawls from recover.gz, but haven&#39;t been\n&gt;&gt; &gt; sucessful with that - I get out-of-memory errors, even when I set the\n&gt;&gt; &gt; JVM to have 1 GB of RAM or more.)\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; thanks\n&gt;&gt; &gt;  - Mike Schwartz\n&gt;&gt; &gt;    Aptas, Inc.\n&gt;&gt; &gt; *Yahoo! Groups Sponsor*\n&gt;&gt; &gt; ADVERTISEMENT\n&gt;&gt; &gt; click here\n&gt;&gt; &gt; &lt; \n&gt;&gt; http://us.ard.yahoo.com/SIG=129jkuth1/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109711161/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075 \n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; \n&gt;&gt; ------------------------------------------------------------------------\n&gt;&gt; &gt; *Yahoo! Groups Links*\n&gt;&gt; &gt;\n&gt;&gt; &gt;     * To visit your group on the web, go to:\n&gt;&gt; &gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt; &gt;       \n&gt;&gt; &gt;     * To unsubscribe from this group, send an email to:\n&gt;&gt; &gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt; &gt;       &lt; \n&gt;&gt; mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe \n&gt;&gt; &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com%3Fsubject=Unsubscribe&gt; \n&gt;&gt; &gt;\n&gt;&gt; &gt;       \n&gt;&gt; &gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt; &gt;       Service &lt; http://docs.yahoo.com/info/terms/&gt;.\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Yahoo! Groups Sponsor*\n&gt;&gt; ADVERTISEMENT\n&gt;&gt; click here \n&gt;&gt; &lt;http://us.ard.yahoo.com/SIG=129e0c6q6/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109793222/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------------------------------------------\n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;     * To visit your group on the web, go to:\n&gt;&gt;     * http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt;     *  \n&gt;&gt;     * To unsubscribe from this group, send an email to:\n&gt;&gt;     * archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;&gt;\n&gt;&gt;     *  \n&gt;&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;&gt;\n&gt;\n&gt; *Yahoo! Groups Sponsor*\n&gt; ADVERTISEMENT\n&gt; click here \n&gt; &lt;http://us.ard.yahoo.com/SIG=129b69vc3/M=298184.6018725.7038619.3001176/D=groups/S=1705004924:HM/EXP=1109794412/A=2593423/R=0/SIG=11el9gslf/*http://www.netflix.com/Default?mqso=60190075&gt; \n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}