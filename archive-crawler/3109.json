{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":205318215,"authorName":"Greg Kempe","from":"&quot;Greg Kempe&quot; &lt;thelonghotsummer@...&gt;","profile":"gregkza","replyTo":"LIST","senderId":"XfnWS-TBz5xjRANjFBL2FKK_OUPwwMrC5YR6F9Ovpnx0Mnry8EAONqLEY6iL3B9SF-IUnOB1l0sZwbsrOrJWvjuAyTTbynqIYp4_3evHWdc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Carrying the already-seen list between crawls","postDate":"1153850624","msgId":3109,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGI4MTI1OWEwMDYwNzI1MTEwM3Q1ZTFmOTBjZWthYmY5YTQ0YjUzNTRlMjQxQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDQ0Nzc5QTFBLjkwMDA5MDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PGI4MTI1OWEwMDYwNDE4MTgwN2w3NThiNmJiM3NiZWJhMWNkNWQ2MjhlMDFiQG1haWwuZ21haWwuY29tPgkgPDQ0NDVEOENBLjMwMjA1MDdAYXJjaGl2ZS5vcmc+CSA8YjgxMjU5YTAwNjA1MTkxMDA4dTIyOWNkN2Zka2ZmMjgyZmZmZmY3ZmVjMTVAbWFpbC5nbWFpbC5jb20+CSA8NDQ2RTA3QjYuODA1MDgwM0BhcmNoaXZlLm9yZz4JIDxiODEyNTlhMDA2MDUyNDE1MzZ0ZjBkYjA5MXNlZWMxMDVlODA0ZDM3MzI3QG1haWwuZ21haWwuY29tPgkgPDQ0Nzc5QTFBLjkwMDA5MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":2881,"nextInTopic":0,"prevInTime":3108,"nextInTime":3110,"topicId":2791,"numMessagesInTopic":7,"msgSnippet":"... For the most part, checkpoints perform well. I recently got this error, though. I m not certain what the root cause is, it may be connected with running","rawEmail":"Return-Path: &lt;thelonghotsummer@...&gt;\r\nX-Sender: thelonghotsummer@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 81700 invoked from network); 25 Jul 2006 18:05:46 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m32.grp.scd.yahoo.com with QMQP; 25 Jul 2006 18:05:46 -0000\r\nReceived: from unknown (HELO py-out-1112.google.com) (64.233.166.178)\n  by mta1.grp.scd.yahoo.com with SMTP; 25 Jul 2006 18:05:46 -0000\r\nReceived: by py-out-1112.google.com with SMTP id b36so2486023pyb\n        for &lt;archive-crawler@yahoogroups.com&gt;; Tue, 25 Jul 2006 11:03:44 -0700 (PDT)\r\nReceived: by 10.35.76.5 with SMTP id d5mr9828197pyl;\n        Tue, 25 Jul 2006 11:03:44 -0700 (PDT)\r\nReceived: by 10.35.74.11 with HTTP; Tue, 25 Jul 2006 11:03:44 -0700 (PDT)\r\nMessage-ID: &lt;b81259a00607251103t5e1f90cekabf9a44b5354e241@...&gt;\r\nDate: Tue, 25 Jul 2006 11:03:44 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;44779A1A.9000907@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;b81259a00604181807l758b6bb3sbeba1cd5d628e01b@...&gt;\n\t &lt;4445D8CA.3020507@...&gt;\n\t &lt;b81259a00605191008u229cd7fdkff282fffff7fec15@...&gt;\n\t &lt;446E07B6.8050803@...&gt;\n\t &lt;b81259a00605241536tf0db091seec105e804d37327@...&gt;\n\t &lt;44779A1A.9000907@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;Greg Kempe&quot; &lt;thelonghotsummer@...&gt;\r\nSubject: Re: [archive-crawler] Carrying the already-seen list between crawls\r\nX-Yahoo-Group-Post: member; u=205318215; y=xcqgzx-VxSEibG67svm5mbA8boBJMi10P8oGUvjZwRXksg\r\nX-Yahoo-Profile: gregkza\r\n\r\nOn 26/05/06, Michael Stack &lt;stack@...&gt; wrote:\n&gt; Greg Kempe wrote:\n&gt; &gt;\n&gt; &gt; That&#39;s a fair point. My primary usecase is to recover from unclean\n&gt; &gt; shutdowns when a checkpoint either failed,\n&gt; Tell me more about fails.  I&#39;m interested.  They seem to work pretty\n&gt; reliably for us.\n\nFor the most part, checkpoints perform well. I recently got this\nerror, though. I&#39;m not certain what the root cause is, it may be\nconnected with running out of disk space.\n\n=== log snippet\n\n07/24/2006 17:15:41 +0000 FINE\norg.archive.crawler.framework.CrawlController checkpoint Rotating log\nfiles.\n07/24/2006 17:15:41 +0000 FINE\norg.archive.crawler.framework.CrawlController checkpoint BigMaps.\ncom.sleepycat.util.RuntimeExceptionWrapper:\norg.archive.crawler.fetcher.FetchHTTP$2\n        at com.sleepycat.bind.serial.SerialBinding.objectToEntry(SerialBinding.java:162)\n        at com.sleepycat.collections.DataView.useValue(DataView.java:502)\n        at com.sleepycat.collections.DataCursor.initForPut(DataCursor.java:625)\n        at com.sleepycat.collections.DataCursor.put(DataCursor.java:559)\n        at com.sleepycat.collections.StoredContainer.put(StoredContainer.java:311)\n        at com.sleepycat.collections.StoredMap.put(StoredMap.java:258)\n        at org.archive.util.CachedBdbMap.sync(CachedBdbMap.java:502)\n        at org.archive.crawler.framework.CrawlController.checkpointBigMaps(CrawlController.java:1966)\n        at org.archive.crawler.framework.CrawlController.checkpoint(CrawlController.java:1181)\n        at org.archive.crawler.framework.Checkpointer$CheckpointingThread.run(Checkpointer.java:194)\nCaused by: java.io.NotSerializableException:\norg.archive.crawler.fetcher.FetchHTTP$2\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1075)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:291)\n        at java.util.Hashtable.writeObject(Hashtable.java:813)\n        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:890)\n        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1333)\n        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1284)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1073)\n        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1369)\n        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1341)\n        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1284)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1073)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:291)\n        at org.archive.crawler.datamodel.CandidateURI.writeObject(CandidateURI.java:583)\n        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:585)\n        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:890)\n        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1333)\n        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1284)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1073)\n        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1369)\n        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1341)\n        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1284)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1073)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:291)\n        at com.sleepycat.bind.serial.SerialBinding.objectToEntry(SerialBinding.java:160)\n        ... 9 more\n07/24/2006 17:15:41 +0000 INFO\norg.archive.crawler.framework.Checkpointer$CheckpointingThread run\nFinished\n\n==== end log snippet\n\n&gt; &gt; &gt; One idea we&#39;ve kicked around is having the recover.gz log start over at\n&gt; &gt; &gt; each checkpoint.  After a checkpoint recovery, optionally you could\n&gt; &gt; &gt; replay the recovery.gz to bring the crawler even closer to the crash\n&gt; &gt; point.\n&gt; &gt;\n&gt; &gt; That would be useful. If the jmx importUris method used the same logic\n&gt; &gt; to read a recover log as the frontier recovery code does, you could\n&gt; &gt; use it to add both frontier uris and already-seen uris.\n\n&gt; It does already (If I understand you properly).  You can pass recovery\n&gt; log files to importUris (or crawl logs or just a URI per line).  You\n&gt; just have to be sure to specify the correct &#39;style&#39;.  See the javadoc:\n&gt; http://crawler.archive.org/apidocs/index.html.\n\nThat&#39;s true for importing frontier uris, but it doesn&#39;t look like it\nis possible to import other information from recover.gz, such as Fs\nentries. A replayRecovery method which uses all the infomormation in a\nrecover.gz file to add frontier uris as well as already-seen uris\nwould allow the use you mentioned above.\n\nThanks\n\nGreg\n\n"}}