{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":259730011,"authorName":"Sharad Agarwal","from":"Sharad Agarwal &lt;sharadbdc@...&gt;","replyTo":"LIST","senderId":"mV7mWW7RjPDnCJX4-Feo-HkJEmgh9s3Ny1Pdr1viZ1j6HLqsr38lvRqugkwmDyDapFwRIq-8oupAZLj7b3jhxUGcHxwSLwQGzA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Parallelizing crawler","postDate":"1153121879","msgId":3060,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkIzRTU3LjEwNTA2QGFvbC5jb20+","inReplyToHeader":"PDQ0QjdERjM1LjMwNDA4MDFAYXJjaGl2ZS5vcmc+","referencesHeader":"PGU5OGlxZCtpMXMzQGVHcm91cHMuY29tPiA8NDRCN0RGMzUuMzA0MDgwMUBhcmNoaXZlLm9yZz4="},"prevInTopic":3052,"nextInTopic":3065,"prevInTime":3059,"nextInTime":3061,"topicId":3043,"numMessagesInTopic":16,"msgSnippet":"We had the requirement of scaling heritrix. We have done it by employing Consistent hashing scheme. The hashing scheme splits the url space based on domain (so","rawEmail":"Return-Path: &lt;sharadbdc@...&gt;\r\nX-Sender: sharadbdc@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 30310 invoked from network); 17 Jul 2006 07:38:06 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m32.grp.scd.yahoo.com with QMQP; 17 Jul 2006 07:38:06 -0000\r\nReceived: from unknown (HELO omr-r01.mail.aol.com) (152.163.225.129)\n  by mta9.grp.scd.yahoo.com with SMTP; 17 Jul 2006 07:38:06 -0000\r\nReceived: from aol.com (10.146.144.105)\n  by omr-r01.mail.aol.com with ESMTP; 17 Jul 2006 03:38:03 -0400\r\nMessage-ID: &lt;44BB3E57.10506@...&gt;\r\nDate: Mon, 17 Jul 2006 13:07:59 +0530\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc4 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e98iqd+i1s3@...&gt; &lt;44B7DF35.3040801@...&gt;\r\nIn-Reply-To: &lt;44B7DF35.3040801@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Sharad Agarwal &lt;sharadbdc@...&gt;\r\nSubject: Re: [archive-crawler] Re: Parallelizing crawler\r\nX-Yahoo-Group-Post: member; u=259730011\r\n\r\nWe had the requirement of scaling heritrix. We have done it by employing \nConsistent hashing scheme. The hashing scheme splits the url space based \non domain (so that politness is ensured) on which each heritrix instance \nwork on. The whole set up is controller less; each node has the hashing \nfunction. Whenever a out of node&#39;s scope url is found, it is sent to the \nnode owning it. These urls are collected and sent in batches to the \nowning nodes via JMX by a separate thread.\nThe architecture looks to be scalable in the sense that there is no \ncontroller bottleneck. All heritrix node are identical. The whole scheme \nis working pretty fine for us. We have gone quite a few millions with \n3-4 heritrix instance nodes.\n\n- Sharad\n\n\nstack@... wrote:\n\n&gt;molzbh wrote:\n&gt;  \n&gt;\n&gt;&gt;Cool. Anyways my thoughts on splitting the architecture were to split\n&gt;&gt;the Processors across various machines. Have a Statistics server\n&gt;&gt;maintain statistics, and a single frontier.\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Here&#39;s a few items you&#39;d have to contend with if you split Heritrix:\n&gt;\n&gt;+ You&#39;ll have to spend alot of resources serializing and deserializing \n&gt;passing rich URLs and configurations.\n&gt;+ Certain processors, as written, expect to find on local disk the \n&gt;downloaded resources (You could share them using NFS.  NFS won&#39;t take \n&gt;too kindly to the crawler&#39;s heavy I/O).\n&gt;\n&gt;\n&gt;  \n&gt;\n&gt;&gt;My idea was to have\n&gt;&gt;frontier emit a batch of URLS to the processore boxes, hence instead\n&gt;&gt;of next() you have nextBatch(). Anyways, I guess this involves a lot\n&gt;&gt;of work, hence I am going in for a Peer2Peer setup, with fully loaded\n&gt;&gt;agents doing the URL splits and emmitting batches to the others. I am\n&gt;&gt;wondering however on the efficiency of JMX with RMI connectors to do\n&gt;&gt;this Job. Do you think going the plain RMI way would be faster?\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;We haven&#39;t measured.  JMX is heavyweight but works.\n&gt;\n&gt;One suggestion has been to use JGroups.  Friends of Heritrix were \n&gt;experimenting making a &#39;cluster&#39; subclass of Frontier.  On \n&gt;initialization, it joined a JGroups group and listened in for URL \n&gt;broadcasts.  Those that fit its portion of the URL space, it picked off \n&gt;the bus and added to the local Frontier.  Those URLs meant for another \n&gt;crawler, were broadcast (They&#39;ve said they&#39;ll write the list if findings \n&gt;prove promising).\n&gt;\n&gt;But depends on your requirements.  Joe Hungs&#39; group didn&#39;t bother \n&gt;swapping URLs across the cluster.  Their experience was that the \n&gt;crawlers had sufficient work without exchanging URLs and figured that \n&gt;each crawler would discover its URL-segment important pages anyways \n&gt;without having to have injection from peers (I don&#39;t know if they \n&gt;&#39;proved&#39; this assertion).\n&gt;\n&gt;\n&gt;  \n&gt;\n&gt;&gt;Sorry\n&gt;&gt;for pounding you with questions but I am still in the design phase of\n&gt;&gt;the system looking for a billion plus crawl, and since it won&#39;t be a\n&gt;&gt;one time thing I am also looking at scalability and agent join/leave\n&gt;&gt;mechanisms.\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Can we collaborate?  This is a problem we need to solve ourselves.\n&gt;\n&gt;St.Ack\n&gt;\n&gt;  \n&gt;\n&gt;&gt;--- In archive-crawler@yahoogroups.com \n&gt;&gt;&lt;mailto:archive-crawler%40yahoogroups.com&gt;, Michael Stack &lt;stack@...&gt; \n&gt;&gt;wrote:\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;Anmol Bhasin wrote:\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Thanks! I am wondering however if we leave the central frontier\n&gt;&gt;&gt;&gt;machine concept out for a bit, would there be anybenfit to split URL\n&gt;&gt;&gt;&gt;space in place of Split Architechture.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;I am trying to do quick big crawl, hence wondering which approaches\n&gt;&gt;&gt;&gt;are the best ways to get moving.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;If you&#39;re in a hurry, split URL space (and throw hardware at it). Some\n&gt;&gt;&gt;fairly large crawls have been achieved using this technique both by us\n&gt;&gt;&gt;-- 200million plus -- and by others (See the testimonial cited in the\n&gt;&gt;&gt;previous where Joe Hung and his compaï¿½eros did a 1Billion+ pages).\n&gt;&gt;&gt;\n&gt;&gt;&gt;What were you thinking regards splitting the architecture?\n&gt;&gt;&gt;\n&gt;&gt;&gt;Yours,\n&gt;&gt;&gt;St.Ack\n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt; \n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;\n&gt;\n&gt;\n&gt;  \n&gt;\n\n\n"}}