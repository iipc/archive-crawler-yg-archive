{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"y5ChCD-yEaXzSyXAXaWZEq_-Pj4UCBSqrTbF0kdKZXuoxnP-cv3UaDve1MgiYUMWJYYC_JnhWEZGPDn35iioYw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] A mixed bag of issues, thoughts and  suggestions (long)]","postDate":"1117056631","msgId":1886,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyOTRFRTc3LjUwMjAyMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDYuMi4wLjE0LjIuMjAwNTA1MjQxNzA0NTQuMDJiZDI1MjhAdmhvc3Q2LmF0b21pY3NlcnZlcnMuY29tPg==","referencesHeader":"PDQyOTNBMUFELjcwNzAwMDhAYXB0YXMuY29tPiA8Ni4yLjAuMTQuMi4yMDA1MDUyNDE3MDQ1NC4wMmJkMjUyOEB2aG9zdDYuYXRvbWljc2VydmVycy5jb20+"},"prevInTopic":1876,"nextInTopic":0,"prevInTime":1885,"nextInTime":1887,"topicId":1876,"numMessagesInTopic":2,"msgSnippet":"... Don t worry Mike.  The recovery journal facility will not be going away any time soon. St.Ack P.S. Any luck testing out Oskars new ","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 31340 invoked from network); 25 May 2005 21:40:07 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m22.grp.scd.yahoo.com with QMQP; 25 May 2005 21:40:07 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta2.grp.scd.yahoo.com with SMTP; 25 May 2005 21:40:06 -0000\r\nReceived: (qmail 21294 invoked by uid 100); 25 May 2005 21:39:40 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 25 May 2005 21:39:40 -0000\r\nMessage-ID: &lt;4294EE77.5020201@...&gt;\r\nDate: Wed, 25 May 2005 14:30:31 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.3) Gecko/20041007 Debian/1.7.3-5\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;4293A1AD.7070008@...&gt; &lt;6.2.0.14.2.20050524170454.02bd2528@...&gt;\r\nIn-Reply-To: &lt;6.2.0.14.2.20050524170454.02bd2528@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-59.8 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] A mixed bag of issues, thoughts and  suggestions\n (long)]\r\nX-Yahoo-Group-Post: member; u=168599281; y=5HxAXSrIL3ah3FgZT4juFNDLZLGEG4UhX_QRZG0B4up27ZtmZEeJ_zfp\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nMike Schwartz wrote:\n\n&gt; hi,\n&gt;\n&gt; I have a request about the note below suggesting getting rid of the \n&gt; recovery journal:\n&gt;\n&gt; Can the IA folks please keep the recovery journal code in the released \n&gt; code base (perhaps as a feature disabled by default)?  The reason is \n&gt; that our crawls need to be able to track each URL crawled back to the \n&gt; seed via which it was emitted, and the RecoveryLogMapper class I \n&gt; contributed a few months ago \n&gt; (org/archive/crawler/util/RecoveryLogMapper.java) reads through the \n&gt; recovery journal to support this function.  I&#39;m guessing there are \n&gt; other folks out there who would also find it useful to be able to do \n&gt; this, too.\n&gt;\n&gt; Thanks\n&gt;  - Mike Schwartz\n\nDon&#39;t worry Mike.  The recovery journal facility will not be going away \nany time soon.\nSt.Ack\n\nP.S. Any luck testing out Oskars&#39; new \n&#39;YetAnotherDomainSensitiveFrontier&#39;?  I&#39;d like to replace \n&#39;DomainSensitiveFrontier&#39; with this new Bdb-based version.\n\n&gt;\n&gt;&gt; -------- Original Message --------\n&gt;&gt; Subject:         [archive-crawler] A mixed bag of issues, thoughts and\n&gt;&gt; suggestions (long)\n&gt;&gt; Date:   Mon, 23 May 2005 10:53:05 -0000\n&gt;&gt; From:   Kristinn Sigurdsson &lt;kris@...&gt;\n&gt;&gt; Reply-To:        archive-crawler@yahoogroups.com\n&gt;&gt; To:      &lt;archive-crawler@yahoogroups.com&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; The following is a mixed bag of issues, thoughts and suggestions that\n&gt;&gt; occurred to me during the last few weeks as I worked on the AR module\n&gt;&gt; and was managing a broad crawl. Some are more thought out then others\n&gt;&gt; and their importance varies wildly.\n&gt;&gt;\n&gt;&gt; Some of this is posted here to get some discussion, other bits are\n&gt;&gt; mostly pointed at the boys at the Archive. Basically, this is a mixed \n&gt;&gt; bag.\n&gt;&gt;\n&gt;&gt; *Remaining time*\n&gt;&gt;\n&gt;&gt; This feature is of negligible use and is more likely to confuse than\n&gt;&gt; inform users. The time estimate is only going to give a reasonable\n&gt;&gt; figure if there is only one host left (or several hosts with a similar\n&gt;&gt; number of documents) and we&#39;re already done with the &#39;discovery&#39; phase.\n&gt;&gt; This may be of use towards the end of some focused crawls, but generally\n&gt;&gt; this is of little value. Since this value is almost always wrong I\n&gt;&gt; believe that it is far more likely to confuse users. I also find it\n&gt;&gt; annoying to have another time counter as I occasionally get it mixed up\n&gt;&gt; when I&#39;m looking up the duration of the crawl, but that&#39;s a lesser issue.\n&gt;&gt;\n&gt;&gt; Unless the estimate can be improved significantly (and this requires\n&gt;&gt; taking into account the number of queues and their relative sizes and\n&gt;&gt; the politeness restrictions for anything approaching a reasonable\n&gt;&gt; estimate) I vote to remove this feature.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *The &#39;Crawl resuming&#39; at the top of the progress-statistics.log*\n&gt;&gt;\n&gt;&gt; A change to how the crawls are started has led to a CRAWL RESUMING being\n&gt;&gt; printed at the top of the progress-statistics.log. Perhaps the events\n&gt;&gt; need to be augmented to include a CRAWL STARTING that the\n&gt;&gt; progress-statistics.log would ignore. In any case, this needs to be \n&gt;&gt; fixed.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Include actual memory used in the progress-statistics.log instead of or\n&gt;&gt; in addition to current heap size.*\n&gt;&gt;\n&gt;&gt; Actual memory used is of much greater interest, especially when looking\n&gt;&gt; through the log to see memory usage trends.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Default Bdb cache percent to something modest, like 35-40% (OOM errors)*\n&gt;&gt;\n&gt;&gt; The AR module encountered some issues with the default setting. I&#39;m not\n&gt;&gt; going to vote for any one value, but I suggest that crawls should always\n&gt;&gt; retain a good chunk of memory after the Bdb cache grows to its maximum\n&gt;&gt; allowed size.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Get rid of the recovery journal in favor of decent crash resistance via\n&gt;&gt; Bdb *\n&gt;&gt;\n&gt;&gt; This is a bigger issue (and I know it&#39;s something you guys want to do).\n&gt;&gt; The recovery journal has never (for me) worked as a crash recovery tool.\n&gt;&gt; The AR frontier is able to reconstruct itself based on its databases\n&gt;&gt; without any trouble. If done right, resuming a crawl could be done in a\n&gt;&gt; matter of seconds, instead of hours (or days). I&#39;d vote to make this the\n&gt;&gt; #1 priority for 1.6!\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Allow frontiers to provide &#39;servlets&#39; that enables more detailed\n&gt;&gt; monitoring and control.*\n&gt;&gt;\n&gt;&gt; I don&#39;t really care how it&#39;s done, but we need to allow modules,\n&gt;&gt; especially (or even exclusively) Frontiers, need to have customized\n&gt;&gt; control pages!\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Display data, 4GB should be 4.442GB etc.*\n&gt;&gt;\n&gt;&gt; Need to improve how data amounts are condensed, currently the jump from\n&gt;&gt; 4095MB to 4GB lacks granularity. I&#39;d be happy to fix this if we could\n&gt;&gt; agree on exactly how things are supposed to be. My suggestion: show four\n&gt;&gt; characters. So we&#39;d go from 4095 MB to 4GB and then 4.001GB etc. when we\n&gt;&gt; hit 40GB we go to 40.01 etc. same for the move from B to KB and KB to MB\n&gt;&gt;\n&gt;&gt; Additionally, we could have a mouse-over pop up the exact number of \n&gt;&gt; bytes.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Default log lines to show be 40 instead of 50*\n&gt;&gt;\n&gt;&gt; A bit of a nit-pick. I&#39;ve got a screen running at 1024 lines of vertical\n&gt;&gt; resolution and 40 lines fill up my screen. Since not many users are\n&gt;&gt; likely to have higher resolution (in fact I&#39;d expect the average to be\n&gt;&gt; lower) I suggest we amend the default value to 40 (or possibly 35).\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Link from help to /help/regexpr.jsp*\n&gt;&gt;\n&gt;&gt; I find this page invaluable when adding new regular expressions,\n&gt;&gt; especially to crawls in progress. Lets you double check those reg.expr.\n&gt;&gt; using the same interpreter. The page has been available for awhile, but\n&gt;&gt; either the link was dropped or it never existed. Either way, a link\n&gt;&gt; should be added to the help page.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Cost assignment on -5000/-5001*\n&gt;&gt;\n&gt;&gt; More of a question, do URIs that return with -500X &#39;cost&#39; anything? I&#39;d\n&gt;&gt; suggest that they shouldn&#39;t. After all, they aren&#39;t actually crawled.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *HostnameQueueAssignmentPolicy attaches the port number to the hostname*\n&gt;&gt;\n&gt;&gt; Is this really sensible considering that these are used as the basic\n&gt;&gt; politeness units. Different ports, same server. I&#39;d think we&#39;d want to\n&gt;&gt; handle it all the same way.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Daily reports? Detailed look at the crawl, printed to a file daily? *\n&gt;&gt;\n&gt;&gt; I was thinking that it might be nice to add a &#39;Daily report&#39; feature to\n&gt;&gt; the StatisticsTracker. It would generate a (new) report file at a\n&gt;&gt; specified time each day. Information regarding the number of URIs,\n&gt;&gt; amount of data, bandwidth usage etc. would be printed, giving a nice\n&gt;&gt; summary on the progress so far.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *URI reg.expr filter (OR based) that has a list of strings instead of\n&gt;&gt; just one.*\n&gt;&gt;\n&gt;&gt; I find myself using a series of URI reg.expr. filters to tackle numerous\n&gt;&gt; issues. I was thinking that it might be useful to add a similar filter\n&gt;&gt; that contained a list of strings. This would make it a lot easier to add\n&gt;&gt; additional filters while a crawl is in progress. I can handle this if\n&gt;&gt; you agree it&#39;s a good idea.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *In help (and perhaps a link from the logs) provide a quick link to the\n&gt;&gt; result codes (local, not on web).*\n&gt;&gt;\n&gt;&gt; I find myself looking these up a lot. Having a handy, local reference\n&gt;&gt; would be useful. Should include both the HTTP codes and the ones\n&gt;&gt; specified by Heritrix.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Changes to Meta-data, description do not have any effect.*\n&gt;&gt;\n&gt;&gt; This has been around for awhile I think. The only time this is written\n&gt;&gt; is when a profile/job is created. Changes seem to have no effect.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *Add items crawled to Bdb reports for each queue.*\n&gt;&gt;\n&gt;&gt; The BdbFrontier report should list the number or URIs crawled for each\n&gt;&gt; queue. Currently you can sort of calculate this based on the expenditure\n&gt;&gt; and average cost (which is of little use of you are using\n&gt;&gt; ZeroCostAssignmentPolicy).\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; *BdbFrontier: Add a log that details when queues are created and move\n&gt;&gt; from active to inactive and finally to exhausted.*\n&gt;&gt;\n&gt;&gt; This would make it easier to monitor crawls. Basically when a queue\n&gt;&gt; moves between any of the above states a line should be written in the\n&gt;&gt; log. This way you could quickly grep through the log and see how that\n&gt;&gt; queue is being processed.\n&gt;&gt;\n&gt;&gt; The log line might look something like this:\n&gt;&gt;\n&gt;&gt; [time-date] [queue-name] [old state] [new state] [queue-size] [URIs\n&gt;&gt; crawled] [total expended] [last cost] [average cost]\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; - Kris\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------------------------------------------\n&gt;&gt; *Yahoo! Groups Links*\n&gt;&gt;\n&gt;&gt;     * To visit your group on the web, go to:\n&gt;&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt;\n&gt;&gt;     * To unsubscribe from this group, send an email to:\n&gt;&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt; &lt; \n&gt;&gt; mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe \n&gt;&gt; &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com%3Fsubject=Unsubscribe&gt; \n&gt;&gt; &gt;\n&gt;&gt;\n&gt;&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt;       Service &lt; http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}