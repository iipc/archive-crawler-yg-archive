{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163922992,"authorName":"John Erik Halse","from":"John Erik Halse &lt;johnh@...&gt;","profile":"johnerikhalse","replyTo":"LIST","senderId":"-fi_jgTrjmfR5x5L5FwB0XcPJHXuIZB-oFUJTDH1irf0eaBRlGlLUm873oSWbJDRwUNhzhEMykBeGWMvBDMUenzrnlc1hCqwCww","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Checkpointing","postDate":"1071101003","msgId":199,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEwNzExMDEwMDMuMTA5MDQuOTEuY2FtZWxAYjExNi1keW4tMzcuYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDNGQkJGNjUzLjkwNjAxMDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PDNGQkJGNjUzLjkwNjAxMDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":178,"nextInTopic":200,"prevInTime":198,"nextInTime":200,"topicId":178,"numMessagesInTopic":7,"msgSnippet":"Since Gordon wrote this mail, he has added a recover log which could be used to replay the crawl. The actual replaying functionality isn t added though. The","rawEmail":"Return-Path: &lt;johnh@...&gt;\r\nX-Sender: johnh@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 99027 invoked from network); 11 Dec 2003 00:06:38 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m20.grp.scd.yahoo.com with QMQP; 11 Dec 2003 00:06:38 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 11 Dec 2003 00:06:38 -0000\r\nReceived: (qmail 15119 invoked by uid 100); 11 Dec 2003 00:05:56 -0000\r\nReceived: from b116-dyn-37.archive.org (johnh@...@209.237.240.37)\n  by mail-dev.archive.org with RC4-MD5 encrypted SMTP; 11 Dec 2003 00:05:56 -0000\r\nSubject: Re: [archive-crawler] Checkpointing\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nIn-Reply-To: &lt;3FBBF653.9060107@...&gt;\r\nReferences: &lt;3FBBF653.9060107@...&gt;\r\nContent-Type: text/plain\r\nMessage-Id: &lt;1071101003.10904.91.camel@...&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Ximian Evolution 1.4.5 \r\nDate: Wed, 10 Dec 2003 16:03:23 -0800\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-6.2 required=6.0\n\ttests=AWL,BAYES_20,EMAIL_ATTRIBUTION,IN_REP_TO,QUOTED_EMAIL_TEXT,\n\t      REFERENCES,REPLY_WITH_QUOTES,TONER,USER_AGENT_XIMIAN\n\tautolearn=ham version=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: John Erik Halse &lt;johnh@...&gt;\r\nX-Yahoo-Group-Post: member; u=163922992\r\nX-Yahoo-Profile: johnerikhalse\r\n\r\nSince Gordon wrote this mail, he has added a recover log which could be\nused to &quot;replay&quot; the crawl. The actual replaying functionality isn&#39;t\nadded though.\n\nThe replaying approach is simple and by adding a little more information\nto the recover log (timestamps) it should be possible to reconstruct the\nstatistics as well. For focused crawls this should work very well. But\nwith very broad crawls, the recover log grows to a size that is not easy\nto handle. And if the crawler sometime in the future will support\ninfinite/incremental crawls, then the recover log will not work.\n\nWhen thinking of different approaches for doing checkpoints I came up\nwith a some questions that should be answered before we try to design\nit.\n\n* How often are we supposed to do a checkpoint (aka how costly is a\ncheckpoint allowed to be).\nIf the checkpoints are very expensive, we could do a combination of\ncheckpoint and recovery log. The recovery log should then be reset at\nevery checkpoint.\n* Is it ok to pause the crawler for a checkpoint? It might take some\ntime to wait for all the threads to finish. Is this acceptable?\n* Is checkpointing just for recovering from crashes?\nIf not:\n  - Should it be possible to manipulate queues in a suspended state? For\nexample adding or removing URIs in the pending queue.\n  - Should it be possible to change implementation of modules between\nsuspend and resume? For example fixing bugs.\n  - Should it be possible to alter the configuration in suspended state\n* Is it ok to insert a checkpoint mark in the working files or should\neverything be copied to a safe location to make sure that a crash would\nnot corrupt files?\n\nIf we add the possibility to run a multiple machine crawl; Should the\ncheckpoint span all the crawler instances or should the checkpoint be\nlocal to a single instance?\n\n\nComments/answers to these questions? Other questions that should be\nasked?\n\nJohn\n\nOn Wed, 2003-11-19 at 15:01, Gordon Mohr wrote:\n&gt; The top frustration during our recent evaluation crawl was\n&gt; that we don&#39;t yet have a working system for resuming a crawl\n&gt; in progress from disk-based state, aka &quot;checkpointing&quot;.\n&gt; \n&gt; There are many ways we could remedy this shortcoming,\n&gt; some incremental, some comprehensive.\n&gt; \n&gt; Two extremes of checkpoint functionality would be &quot;just enough\n&gt; to ensure coverage&quot; and &quot;total crawler state&quot;.\n&gt; \n&gt; In &quot;just enough to ensure coverage&quot;, a resumption might not\n&gt; have internal state and running totals that closely match\n&gt; those that the crawler would have had, if not interrupted.\n&gt; However, it would be trusted to still visit every URI that\n&gt; the original crawler would have. (In some ways, this could be\n&gt; considered a &quot;checkpoint Frontier only&quot; or &quot;checkpoint\n&gt; simplified view of Frontier&quot;.)\n&gt; \n&gt; In &quot;total crawler state&quot;, a resumption would reliably reload\n&gt; the state of any component which accumulates information --\n&gt; such as the statistics tracker or a hypothetical postprocessor\n&gt; which tallies relative proportions of document features -- such\n&gt; that a resumption makes the crawler work exactly like it had\n&gt; never stopped.\n&gt; \n&gt; Several different approaches fall along a continuum of\n&gt; increasing sophistication at crawl/checkpoint time:\n&gt; \n&gt; A &quot;forensic&quot; approach would require almost no crawl-time\n&gt; support. A resume would simply look at the output generated\n&gt; by the crawler, essentially &quot;replaying&quot; the crawl at an\n&gt; accelerated rate (perhaps even repeating the extraction\n&gt; steps against ARCed resources), eventually winding up at\n&gt; a state closely mimicking that of the previously-suspended\n&gt; crawler. This requires little to no support at crawl/checkpoint\n&gt; time, but a sophisticated resumption routing.\n&gt; \n&gt; A &quot;transaction log&quot; approach would generate extra logs\n&gt; at crawl-time to assist in rapid resumption -- for example,\n&gt; all inserts to the froniter would be logged, to save the\n&gt; resumption from having to re-scan source material. This\n&gt; remains very straightforward, if we&#39;re only considering\n&gt; the matter of including/excluding URIs for visitation, and\n&gt; could have other debugging benefits.\n&gt; \n&gt; A &quot;snapshot&quot; approach would, at certain intervals or at\n&gt; operator request, dump some or all of the crawler&#39;s state\n&gt; to files. A resumption could occur quickly, and mimic the\n&gt; dumped state as accurately as we care to enable.\n&gt; \n&gt; ==\n&gt; There are merits and tradeoffs to each approach. My understanding\n&gt; is that Mercator implements a &quot;total crawler state&quot;/&quot;snapshot&quot;\n&gt; approach, giving every module a chance to persist itself to\n&gt; disk at checkpoint time.\n&gt; \n&gt; I think our initial implementation should be a &quot;just enough for\n&gt; coverage&quot;/&quot;transaction log&quot; approach, as this requires minimal\n&gt; invasiveness and effectively tackles the largest issue: unbroken\n&gt; coverage across crawler hiccups. We can re-synthesize any\n&gt; running statistics post-crawl.\n&gt; \n&gt; Later, we should enable the Mercator-style &quot;total state&quot;/&quot;snapshot&quot;\n&gt; approach.\n&gt; \n&gt; Agree or disagree? Comments? Other ideas?\n&gt; \n&gt; - Gordon\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; To unsubscribe from this group, send an email to:\n&gt; archive-crawler-unsubscribe@yahoogroups.com\n&gt; \n&gt;  \n&gt; \n&gt; Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n&gt; \n&gt; \n\n\n"}}