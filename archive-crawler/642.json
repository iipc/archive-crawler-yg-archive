{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"A-eb6RDZpuFfIHd18ZxEehNlijsM4XTHqAm9hlXhs5m6xPaTJuzeUkaD7cSpuJcV-6SOQurwcNhROsuSsjj-PRMADSNvG3rk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] tld crawling","postDate":"1089855765","msgId":642,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwRjVFMTE1LjYwOTA3MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQwRjRGNzFGLjUwMzAxQGhhbGFjc3kuY29tPg==","referencesHeader":"PDQwRjRGNzFGLjUwMzAxQGhhbGFjc3kuY29tPg=="},"prevInTopic":631,"nextInTopic":645,"prevInTime":641,"nextInTime":643,"topicId":631,"numMessagesInTopic":5,"msgSnippet":"Hi Halácsy Péter, ... Since you are using an exclude filter you want to negate your regexp. Maybe somthing like: https?://(?![^/]+ .hu(?:: d+)?/).* ","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 1838 invoked from network); 15 Jul 2004 01:45:00 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m7.grp.scd.yahoo.com with QMQP; 15 Jul 2004 01:45:00 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 15 Jul 2004 01:45:00 -0000\r\nReceived: (qmail 24894 invoked by uid 100); 15 Jul 2004 01:35:01 -0000\r\nReceived: from b116-dyn-56.archive.org (HELO archive.org) (igor@...@209.237.240.56)\n  by mail-dev.archive.org with SMTP; 15 Jul 2004 01:35:01 -0000\r\nMessage-ID: &lt;40F5E115.6090706@...&gt;\r\nDate: Wed, 14 Jul 2004 18:42:45 -0700\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;40F4F71F.50301@...&gt;\r\nIn-Reply-To: &lt;40F4F71F.50301@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.1 required=6.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] tld crawling\r\nX-Yahoo-Group-Post: member; u=137477665\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Hal�csy P�ter,\n\n&gt; Heritrix is a very new stuff for me but seems to be one of the best open \n&gt; source crawlers. I&#39;d like to crawl a top level domain for only texts \n&gt; (html, plain, pdf, ps) but I couldn&#39;t composed the right order.xml.\n&gt; \n&gt; I think I need set broad scope and add some exclude filter at least a \n&gt; regexp filter that ensures the right tld:\n&gt; \n&gt; http://.*&#92;.hu(:&#92;d+)?/.*\n&gt; \n&gt; (I tried that but .com pages were fetched)\n&gt; \n\nSince you are using an exclude filter you want to &quot;negate&quot; your regexp.\nMaybe somthing like:\nhttps?://(?![^/]+&#92;.hu(?::&#92;d+)?/).*\n\nBasically, this means exclude URLs that do not have .hu in their domain.\nIt is a bit confusing so maybe it would be cleaner to use Filter Scope.\nWithin filter scope you can specify any number of filters that define the scope.\nIn your case you can use a single URIRegExpFilter:\nhttps?://[^/]+&#92;.hu(?::&#92;d+)?/).*\n\nI have not used filter scope yet, so if you decide to use it please let us know if it works as expected.\nAlso, it is kind of awkward to set up the filter scope. The following are three necessary steps to \nconfigure it:\n1. change scope at &quot;Modules&quot; configuration page.\n2. add new URIRegExpFilter filter at &quot;Filters&quot; configuration page.\n3. set regular expression to added filter from 2. at &quot;Settings&quot; configuration page.\n\nKeep in mind that transitive and additinalFocusFilter are enabled by default. These two filters \nexpend the scope to crawl embedded objects.\n\nIf you use exclude filter as mentioned above than these two filter are irrelevant.\n\n\n &gt; How can I set that only text files should be fetched?\n\nAt this point Heritrix is supporting this feature only via &quot;Accept&quot; header. However, there has been \nsome discussion on some limitations of this approach. Please see &quot;Problem about link hops&quot; thread at \nhttp://groups.yahoo.com/group/archive-crawler/messages/621?threaded=1 for more info.\n\n&gt; There is an other open source java crawler, \n&gt; called capek could replace part of an url before checking whether it is valid. (see http://www.egothor.org/twiki/bin/view/Know/RulesFile)\n&gt; \n&gt; Can I implement this in heritrix? For example I&#39;d like to skip session ids.\n&gt; \n&gt; This is a simple rule file to gather pages:\n&gt; replace         &amp;                   &\n&gt; replace         &&#92;w+=[0-9a-fA-F]{8,}&   &\n&gt; replace         &&#92;w+=[0-9a-fA-F]{8,}&#92;z  &lt;EMPTY&gt;\n&gt; replace         &#92;?&#92;w+=[0-9a-fA-F]{8,}(&|&#92;z)     &#92;?\n&gt; replace         ;&#92;w+=[0-9a-fA-F]{8,}&#92;?  &#92;?\n&gt; replace         &&                      &lt;EMPTY&gt;\n&gt; replace         &#92;?&                     &#92;?\n&gt; replace         &#92;?$                     &lt;EMPTY&gt;\n\nNo, not at this point. We have been talking of doing something like this in the near future.\n\nTake care,\ni.\n\n\n"}}