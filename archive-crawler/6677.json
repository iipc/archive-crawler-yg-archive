{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"kristsi25","from":"&quot;kristsi25&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"jfIWer-imcRb1gCy-GOJyO8LtVHZ2hYukQW4pw2mkZbu0l9tVceXopscXdXKlG0rC3QvrlYV81OmfdEUBFNmCOtoUdlS","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Problem with robots.txt IGNORE policy","postDate":"1281652713","msgId":6677,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGk0MXQ1OStpdnZxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRDNjQ0NUQ5LjYwNzAxQGFyY2hpdmUub3JnPg=="},"prevInTopic":6675,"nextInTopic":7471,"prevInTime":6676,"nextInTime":6678,"topicId":6671,"numMessagesInTopic":9,"msgSnippet":"... Agreed. Currently, setting IGNORE causes Heritrix to pretend it never saw the robots.txt file, it is never parsed. It seems simple enough to modify this","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 59483 invoked from network); 12 Aug 2010 22:38:48 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m10.grp.re1.yahoo.com with QMQP; 12 Aug 2010 22:38:48 -0000\r\nX-Received: from unknown (HELO n44d.bullet.mail.sp1.yahoo.com) (66.163.169.158)\n  by mta3.grp.re1.yahoo.com with SMTP; 12 Aug 2010 22:38:48 -0000\r\nX-Received: from [69.147.65.148] by n44.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2010 22:38:35 -0000\r\nX-Received: from [98.137.34.36] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2010 22:38:35 -0000\r\nDate: Thu, 12 Aug 2010 22:38:33 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;i41t59+ivvq@...&gt;\r\nIn-Reply-To: &lt;4C6445D9.60701@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;kristsi25&quot; &lt;kris@...&gt;\r\nSubject: Re: Problem with robots.txt IGNORE policy\r\nX-Yahoo-Group-Post: member; u=163406187; y=2CzUqHvNxioUM6cckCncmyivW8mJxZnqXOhfeeWIFOn48eSM\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\n\n\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:=\r\n\n&gt;\n&gt; On 8/12/10 9:09 AM, kristsi25 wrote:\n&gt; &gt; We wish to ignore robots.txt =\r\nas far as they exclude us from content BUT we would like to respect the cra=\r\nwl-delay (at least up to some number of seconds). However, if you currently=\r\n select IGNORE as your honoring policy the crawl-delay is never even read, =\r\nmuch less enforced.\n&gt; &gt;\n&gt; &gt; Is there a way to configure the robots honoring=\r\n to accomplish this or is this a deficiency in Heritrix?\n&gt; &gt;\n&gt; &gt; If this is=\r\n not possible, does it make sense to amend the IGNORE policy or should a ne=\r\nw IGNORE_EXCEPT_CRAWLDELAY policy be added?\n&gt; \n&gt; Because there is a separat=\r\ne &#39;dial&#39; for controlling whether (and to what \n&gt; extend) &#39;Crawl-Delay&#39; is r=\r\nespected, my preference would be for the \n&gt; normal IGNORE policy to be fixe=\r\nd to mean &#39;ignore disallows&#39; -- while \n&gt; still using other info (like Crawl=\r\n-Delay, Sitemap, whatever) that might \n&gt; be in a &#39;robots.txt&#39;.\n\nAgreed. Cur=\r\nrently, setting IGNORE causes Heritrix to pretend it never saw the robots.t=\r\nxt file, it is never parsed. It seems simple enough to modify this but have=\r\n the disallow method return false if honoring type policy is IGNORE.\n\n\n&gt; An=\r\n option for simulating what you want without changing the current \n&gt; IGNORE=\r\n or adding a new policy would be to run your crawl with a CLASSIC \n&gt; robots=\r\n-respecting policy, but set the &#39;calculateRobotsOnly&#39; flag on \n&gt; Preconditi=\r\nonEnforcer. Rather than canceling the fetching of URIs that \n&gt; are robots-p=\r\nrecluded, this setting merely marks them up with an annotation.\n&gt; \n\nThat se=\r\nems a suitable workaround for now. Thanks.\n\n- Kris\n\n\n"}}