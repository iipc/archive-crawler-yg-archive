{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"John Lekashman","from":"John Lekashman &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"6x_AB4sIG5Jxv8RviMi17lInRWRB4dPOzXe-xGcpWSzsUnbGtVCtB9aUjaJ-bQlte2et8joMxR6XRFl5N7m8WZ808BPoPkCx274","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Stucked in 690 million discovered.","postDate":"1344382186","msgId":7740,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwMjFBNEVBLjcwNjA1MDJAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PDUwMjFBMEE2LjUwMDAyMDVAYXJjaGl2ZS5vcmc+","referencesHeader":"PGp1dTkwNyt1Zm1pQGVHcm91cHMuY29tPiA8NTAxMkNCNzguNDA4MDMwNUBiYXlhcmVhLm5ldD4gPDUwMjFBMEE2LjUwMDAyMDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":7739,"nextInTopic":7743,"prevInTime":7739,"nextInTime":7741,"topicId":7731,"numMessagesInTopic":6,"msgSnippet":"Hi Gordon, Yeah, there shouldn t be any such limit. Nonetheless, when I was much younger, and running 8B site crawls with H1, the crawlers always stopped at","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 28696 invoked from network); 7 Aug 2012 23:29:47 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m7.grp.sp2.yahoo.com with QMQP; 7 Aug 2012 23:29:47 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta6.grp.sp2.yahoo.com with SMTP; 7 Aug 2012 23:29:47 -0000\r\nX-Received: from john-lekashmans-macbook-pro.local (173-164-186-205-SFBA.hfc.comcastbusiness.net [173.164.186.205])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id q77NTkvG071074;\n\tTue, 7 Aug 2012 16:29:46 -0700 (PDT)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;5021A4EA.7060502@...&gt;\r\nDate: Tue, 07 Aug 2012 16:29:46 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:14.0) Gecko/20120713 Thunderbird/14.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Gordon Mohr &lt;gojomo@...&gt;\r\nReferences: &lt;juu907+ufmi@...&gt; &lt;5012CB78.4080305@...&gt; &lt;5021A0A6.5000205@...&gt;\r\nIn-Reply-To: &lt;5021A0A6.5000205@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: John Lekashman &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Stucked in 690 million discovered.\r\nX-Yahoo-Group-Post: member; u=90724651; y=CO__w2GJ5qO9MAl5vFqvF_9TQMGRhcXQkneG8IPyLTQ3\r\nX-Yahoo-Profile: lekash\r\n\r\nHi Gordon,\n\nYeah, there shouldn&#39;t be any such limit.\n\nNonetheless, when I was much younger, and running 8B site\ncrawls with H1, the crawlers always stopped at about 700M.\n\nDoubled memory, changed percents, etc.\nStill stopped there.  So, the empirical evidence is\nthat yes, it does.\n\nJohn\n\n\nOn 8/7/12 4:11 PM, Gordon Mohr wrote:\n&gt; There&#39;s no inherent limit in either H1 or H3 of the number of URIs a\n&gt; crawl can discover: if properly configured, with enough disk space, it\n&gt; will keep crawling... just slower and slower, as lookups on ever-larger\n&gt; disk structures take longer.\n&gt;\n&gt; However, in the standard configuration (using BdbUriUniqFilter), this\n&gt; can be get slow indeed, so broader crawls expected to grow into the tens\n&gt; or hundreds of millions of URIs often use the BloomUriUniqFilter\n&gt; instead. Bloom filters only use a fixed amount of main memory... and so\n&gt; can &#39;fill up&#39;: beyond their designed insert-size, their false-positive\n&gt; rate (saying a URI was seen when it wasn&#39;t) will climb, eventually to 100%.\n&gt;\n&gt; If you swapped in our BloomFilter option, and didn&#39;t change its default\n&gt; configuration (which used ~500MB of RAM and is size-optimized for 125\n&gt; million discovered URIs and 1-in-4-million false-positive rate), that\n&gt; might explain what you&#39;re seeing. (I believe even at almost 5x designed\n&gt; size, *some* URIs would still be considered new, but the numbers you&#39;ve\n&gt; included don&#39;t make it clear if the &#39;discovered&#39; count is frozen or just\n&gt; growing very slowly.)\n&gt;\n&gt; If that&#39;s not the problem, other things to consider:\n&gt;\n&gt; � maybe you really have discovered all URIs allowed by your scope and\n&gt; transitively linked from your seeds -- are you sure the expected &quot;2\n&gt; million&quot; domains are both (a) allowed by your scope; and (b) reachable\n&gt; by outlink paths that are at each hop allowed by your scope as well?\n&gt;\n&gt; � some sort of subtle error or data corruption which has allowed\n&gt; crawling to proceed but interfered with the enqueuing of new URIs. I\n&gt; don&#39;t know of any known errors that would cause this, but it&#39;s worth\n&gt; checking the alerts/error-logs/heritrix_out.log for any suspicious\n&gt; Exceptions/Errors.\n&gt;\n&gt; If it is the saturated BloomFilter issue, you would want to relaunch\n&gt; with a filter sized appropriately for your expected crawl size. (It\n&gt; can&#39;t be resized mid-crawl, and unless you were separately redundanly\n&gt; logging outlinks you&#39;ll have to revisit/re-extract many pages to\n&gt; discover the links that were discarded as false duplicates.) In H1, the\n&gt; filter-sizing parameters must be set via system properties -- see\n&gt; BloomUriUniqFilter(). In H3, it&#39;s a bit easier: the component can be\n&gt; swapped out or configured in the same way as others.\n&gt;\n&gt; If your crawl is even larger than your RAM allows a properly-sized\n&gt; filter, you might want to split it into separate non-overlapping crawls,\n&gt; dividing the URI space between separate machines (or serial runs). See\n&gt; discussion on the list and project wiki around &#39;HashCrawlMapper&#39; for\n&gt; some ways to achieve this. (Again, it&#39;s a bit easier in H3.)\n&gt;\n&gt; - Gordon\n&gt;\n&gt; On 7/27/12 10:10 AM, John Lekashman wrote:\n&gt;&gt;\n&gt;&gt; Hi,\n&gt;&gt; You still on Heretrix 1?\n&gt;&gt;\n&gt;&gt; I know that H 1 had a problem of an upper limit of around 700M urls per\n&gt;&gt; crawler.\n&gt;&gt; Split the crawl with a hashmapper.\n&gt;&gt;\n&gt;&gt; Don&#39;t know if H 3 has that problem, I always split those as well, to get\n&gt;&gt; things done\n&gt;&gt; in finite time.\n&gt;&gt;\n&gt;&gt; John\n&gt;&gt;\n&gt;&gt; On 7/27/12 7:35 AM, Elverton wrote:\n&gt;&gt;&gt; Hello everybody.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Well, I&#39;m having a big trouble this time. Before I explain the\n&gt;&gt;&gt; problem, here is the system configuration:\n&gt;&gt;&gt;\n&gt;&gt;&gt; - 24 GB RAM\n&gt;&gt;&gt; - Intel(R) Xeon(R) CPU E5520 @ 2.27GHz\n&gt;&gt;&gt; - 1.8TB hard disk for Heritrix. (I don&#39;t use warc in this crawl. My\n&gt;&gt;&gt; only target is to know how many (approx.) URLs a domain has.) The\n&gt;&gt;&gt; usage of the disk is: used 500GB, free 1.3TB.\n&gt;&gt;&gt; - 16GB java heap size for heritrix.\n&gt;&gt;&gt; - Java 1.7.0_05\n&gt;&gt;&gt;\n&gt;&gt;&gt; Here is the Heritrix configuration that I consider helpful to the\n&gt;&gt;&gt; problem:\n&gt;&gt;&gt;\n&gt;&gt;&gt; - bdb-cache-percent = 25\n&gt;&gt;&gt; - frontier = BdbFrontier\n&gt;&gt;&gt; - max-delay-ms = 10000\n&gt;&gt;&gt; - min-delay-ms = 2000\n&gt;&gt;&gt; - respect-crawl-delay-up-to-secs = 300\n&gt;&gt;&gt; - max-retries = 10\n&gt;&gt;&gt; - retry-delay-seconds = 30\n&gt;&gt;&gt; - timeout-seconds = 1200\n&gt;&gt;&gt; - sotimeout-ms = 20000\n&gt;&gt;&gt;\n&gt;&gt;&gt; % ----------------------------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; So, my problem is: the crawl stucked in 690 million discovered.\n&gt;&gt;&gt; (Queued it&#39;s around 520 million and downloaded is around 170 million).\n&gt;&gt;&gt;\n&gt;&gt;&gt; The strange thing is the download/uri rate.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Docs/s(avg): 53.2(60.77)\n&gt;&gt;&gt; KB/s(avg): 2069(3205)\n&gt;&gt;&gt;\n&gt;&gt;&gt; It continues, in some way, good in theory (about 3 or 4 million uri\n&gt;&gt;&gt; crawled per day if you have 53.2 uri&#39;s during all day), but the real\n&gt;&gt;&gt; crawled per day is below 500.000 (discovered).\n&gt;&gt;&gt;\n&gt;&gt;&gt; Looking at some number in the last five days:\n&gt;&gt;&gt; Queued Downloaded\n&gt;&gt;&gt; 541054381 133121289\n&gt;&gt;&gt; 535322185 138522175\n&gt;&gt;&gt; 530280577 143176680\n&gt;&gt;&gt; 525907149 147086865\n&gt;&gt;&gt; 520568517 151604201\n&gt;&gt;&gt;\n&gt;&gt;&gt; Notice that the queued decreases at the &quot;same&quot; rate that downloaded\n&gt;&gt;&gt; increases. The problem could be getting URIs to the queue. A possible\n&gt;&gt;&gt; is the URIs discovered now had be crawled before, and doesn&#39;t go the\n&gt;&gt;&gt; the queue anymore. But the domain I&#39;m crawling has about 2 million\n&gt;&gt;&gt; domains and I got only 70.000, so there&#39;re many URI&#39;s to be crawled\n&gt;&gt;&gt; yet. :)\n&gt;&gt;&gt;\n&gt;&gt;&gt; Other possibility I thought could be a swap problem (too much I/O).\n&gt;&gt;&gt; For my surprise (using vmstat), the swpd is 0.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Another problem could be know if a URI was crawled already.\n&gt;&gt;&gt; Before the URI goes to the frontier, heritrix verifies it in a queue,\n&gt;&gt;&gt; using the hash technique. If the crawling is big enough, the search\n&gt;&gt;&gt; get slower, even using hash, because there are many URI&#39;s for a key in\n&gt;&gt;&gt; hash table.\n&gt;&gt;&gt;\n&gt;&gt;&gt; But, I really don&#39;t know the exactly problem. Anyone had this problem\n&gt;&gt;&gt; or could point a direction?\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks,\n&gt;&gt;&gt; Elverton.\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}