{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":465980601,"authorName":"Zach Bailey","from":"Zach Bailey &lt;zach.bailey@...&gt;","replyTo":"LIST","senderId":"TeBKLqUG0vdKZ-5uwZLy2oMds6rpUNPBn19jwur8vAFmhXTcKQpFBXIUGHGArK8OcarRfjWYwTChB_Fe0goXSLobEvxH3Gvwp_iRKlI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Simple, single-site crawl &quot;hangs&quot; on last few URIs?","postDate":"1285183266","msgId":6740,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEFBTkxrVGltVUtTZGYwUXZieVc1VzlYel9SLVQxUW1aTHhLR01OVFkwU3RLZkBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PEFBTkxrVGk9eWh1TVpHRk8xNWs0T2FIN1RSUHllVF9ncj1CaUR6ejlRV0ZOd0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGk9eWh1TVpHRk8xNWs0T2FIN1RSUHllVF9ncj1CaUR6ejlRV0ZOd0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6739,"nextInTopic":6741,"prevInTime":6739,"nextInTime":6741,"topicId":6739,"numMessagesInTopic":4,"msgSnippet":"I meant to include this in the original email as well, but it slipped my mind. On some of my more recent crawls with the same configuration, I ve noticed the","rawEmail":"Return-Path: &lt;zach.bailey@...&gt;\r\nX-Sender: zach.bailey@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 60962 invoked from network); 22 Sep 2010 19:21:09 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m15.grp.re1.yahoo.com with QMQP; 22 Sep 2010 19:21:09 -0000\r\nX-Received: from unknown (HELO mail-pw0-f44.google.com) (209.85.160.44)\n  by mta2.grp.re1.yahoo.com with SMTP; 22 Sep 2010 19:21:08 -0000\r\nX-Received: by pwi1 with SMTP id 1so117129pwi.31\n        for &lt;archive-crawler@yahoogroups.com&gt;; Wed, 22 Sep 2010 12:21:08 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.114.74.8 with SMTP id w8mr752481waa.27.1285183267261; Wed, 22\n Sep 2010 12:21:07 -0700 (PDT)\r\nX-Received: by 10.115.92.6 with HTTP; Wed, 22 Sep 2010 12:21:06 -0700 (PDT)\r\nIn-Reply-To: &lt;AANLkTi=yhuMZGFO15k4OaH7TRPyeT_gr=BiDzz9QWFNw@...&gt;\r\nReferences: &lt;AANLkTi=yhuMZGFO15k4OaH7TRPyeT_gr=BiDzz9QWFNw@...&gt;\r\nDate: Wed, 22 Sep 2010 15:21:06 -0400\r\nMessage-ID: &lt;AANLkTimUKSdf0QvbyW5W9Xz_R-T1QmZLxKGMNTY0StKf@...&gt;\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nContent-Type: multipart/alternative; boundary=001636417a811dcc1e0490de0c7d\r\nFrom: Zach Bailey &lt;zach.bailey@...&gt;\r\nSubject: Re: Simple, single-site crawl &quot;hangs&quot; on last few URIs?\r\nX-Yahoo-Group-Post: member; u=465980601\r\n\r\n\r\n--001636417a811dcc1e0490de0c7d\r\nContent-Type: text/plain; charset=ISO-8859-1\r\n\r\nI meant to include this in the original email as well, but it slipped my\nmind. On some of my more recent crawls with the same configuration, I&#39;ve\nnoticed the crawl log displaying entries that I wasn&#39;t expecting: crawl URLs\nfor a completely different domain than the single domain I specified in the\nseed list.\n\nI haven&#39;t removed or moved the SurtPrefixedDecideRule in the scope rule\nsequence, so I&#39;m curious as to why I&#39;m suddenly seeing these pages show up\nin my crawl log. Will the crawler attempt to fetch pages outside of the\ndomain, but then not process them since they don&#39;t match the Surt? If that&#39;s\nthe case that is confusing as I would only expect to see successfully\nprocessed URIs in the crawl log - not ALL crawl URIs that were being\nconsidered.\n\n-Zach\n\nOn Wed, Sep 22, 2010 at 3:05 PM, Zach Bailey &lt;zach.bailey@...&gt;wrote:\n\n&gt; I&#39;ve been proving out our use of heritrix over the past week or so and I&#39;ve\n&gt; noticed the following issue when doing a simple, single-domain crawl.\n&gt;\n&gt; Here is how I have the crawl configured - it is a mostly vanilla crawl\n&gt; configuration save for the following modifications:\n&gt;\n&gt; * metadata.operatorContactUrl specified (obviously)\n&gt; * seeds.textSource.value set to a single domain\n&gt; * Scope chain modifications: TooManyHopsDecideRule.maxHops = 3 and added\n&gt; MatchesFilePatternDecideRule with decision=REJECT and usePreset=ALL right\n&gt; before the PrerequisiteAcceptDecideRule\n&gt; * fetch processor modifications: removed extractorJS, extractorCSS, and\n&gt; extractorSWF\n&gt;\n&gt; The crawl runs fine and when it gets down to the last couple of URLs it\n&gt; &quot;hangs&quot; and does not complete. Looking at the job status page I see:\n&gt;\n&gt; *Totals*\n&gt;    215 downloaded + 4 queued = 219 total\n&gt;    2.5 MiB crawled (2.5 MiB novel, 0 B dup-by-hash, 0 B not-modified)\n&gt;\n&gt; *Frontier*\n&gt;    17 URI queues: 2 active (0 in-process; 0 ready; 2 snoozed); 0 inactive;\n&gt; 0 ineligible; 0 retired; 15 exhausted [RUN: 0 in, 0 out]\n&gt;\n&gt; Examining the frontier report I see the following:\n&gt;\n&gt;  -----===== SNOOZED QUEUES =====-----\n&gt; SNOOZED#0:\n&gt; Queue ssl, (p1)\n&gt;   2 items\n&gt;    wakes in: 7m25s888ms\n&gt;     last enqueued: dns:ssl\n&gt;       last peeked: dns:ssl\n&gt;    total expended: 4 (total budget: -1)\n&gt;    active balance: 2996\n&gt;    last(avg) cost: 1(1)\n&gt;    totalScheduled fetchSuccesses fetchFailures fetchDisregards fetchResponses robotsDenials successBytes totalBytes fetchNonResponses\n&gt;    2 0 0 0 0 0 0 0 5\n&gt;    SimplePrecedenceProvider\n&gt;    1\n&gt;\n&gt; SNOOZED#1:\n&gt; Queue www, (p1)\n&gt;   2 items\n&gt;    wakes in: 7m25s889ms\n&gt;     last enqueued: dns:www\n&gt;       last peeked: dns:www\n&gt;    total expended: 4 (total budget: -1)\n&gt;    active balance: 2996\n&gt;    last(avg) cost: 1(1)\n&gt;    totalScheduled fetchSuccesses fetchFailures fetchDisregards fetchResponses robotsDenials successBytes totalBytes fetchNonResponses\n&gt;    2 0 0 0 0 0 0 0 5\n&gt;    SimplePrecedenceProvider\n&gt;    1\n&gt;\n&gt; So, it looks like there are some weird items being put into these queues\n&gt; that don&#39;t belong there and it&#39;s hanging the crawl job? Is there a\n&gt; configuration option I can tweak to retire these queues or clear them out\n&gt; after a specified idle period? Or, is this the result of a misconfiguration\n&gt; somewhere?\n&gt;\n&gt; Thanks,\n&gt; -Zach\n&gt;\n\r\n--001636417a811dcc1e0490de0c7d\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nI meant to include this in the original email as well, but it slipped my mi=\r\nnd. On some of my more recent crawls with the same configuration, I&#39;ve =\r\nnoticed the crawl log displaying entries that I wasn&#39;t expecting: crawl=\r\n URLs for a completely different domain than the single domain I specified =\r\nin the seed list.&lt;div&gt;\n&lt;br&gt;&lt;/div&gt;&lt;div&gt;I haven&#39;t removed or moved the=A0=\r\nSurtPrefixedDecideRule in the scope rule sequence, so I&#39;m curious as to=\r\n why I&#39;m suddenly seeing these pages show up in my crawl log. Will the =\r\ncrawler attempt to fetch pages outside of the domain, but then not process =\r\nthem since they don&#39;t match the Surt? If that&#39;s the case that is co=\r\nnfusing as I would only expect to see successfully processed URIs in the cr=\r\nawl log - not ALL crawl URIs that were being considered.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/d=\r\niv&gt;&lt;div&gt;-Zach&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Wed, Sep 22,=\r\n 2010 at 3:05 PM, Zach Bailey &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:zach.=\r\nbailey@...&quot;&gt;zach.bailey@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;bl=\r\nockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #=\r\nccc solid;padding-left:1ex;&quot;&gt;I&#39;ve been proving out our use of heritrix =\r\nover the past week or so and I&#39;ve noticed the following issue when doin=\r\ng a simple, single-domain crawl.&lt;div&gt;\n&lt;br&gt;&lt;/div&gt;&lt;div&gt;Here is how I have the=\r\n crawl configured - it is a mostly vanilla crawl configuration save for the=\r\n following modifications:&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;*=A0metadata.operatorCo=\r\nntactUrl specified (obviously)&lt;/div&gt;&lt;div&gt;* seeds.textSource.value set to a =\r\nsingle domain&lt;/div&gt;&lt;div&gt;* Scope chain modifications: TooManyHopsDecideRule.=\r\nmaxHops =3D 3 and added MatchesFilePatternDecideRule with decision=3DREJECT=\r\n and usePreset=3DALL right before the PrerequisiteAcceptDecideRule&lt;/div&gt;\n\n&lt;=\r\ndiv&gt;* fetch processor modifications: removed extractorJS, extractorCSS, and=\r\n extractorSWF&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;The crawl runs fine and when it gets=\r\n down to the last couple of URLs it &quot;hangs&quot; and does not complete=\r\n. Looking at the job status page I see:&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;&lt;font=\r\n face=3D&quot;arial, helvetica, sans-serif&quot;&gt;Totals&lt;/font&gt;&lt;/b&gt;&lt;font face=3D&quot;arial=\r\n, helvetica, sans-serif&quot;&gt;&lt;br&gt;\n=A0=A0 215 downloaded + 4 queued =3D 219 tota=\r\nl=A0&lt;br&gt;=A0=A0 2.5 MiB crawled (2.5 MiB novel, 0 B dup-by-hash, 0 B not-mod=\r\nified)=A0&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;arial, helvetica, sans-serif&quot;&gt;&lt;spa=\r\nn style=3D&quot;font-size:small&quot;&gt;&lt;br&gt;\n&lt;/span&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;&lt;font face=3D&quot;=\r\narial, helvetica, sans-serif&quot;&gt;Frontier&lt;/font&gt;&lt;/b&gt;&lt;font face=3D&quot;arial, helve=\r\ntica, sans-serif&quot;&gt;&lt;br&gt;=A0=A0 17 URI queues: 2 active (0 in-process; 0 ready=\r\n; 2 snoozed); 0 inactive; 0 ineligible; 0 retired; 15 exhausted [RUN: 0 in,=\r\n 0 out]=A0&lt;/font&gt;&lt;/div&gt;\n\n&lt;div&gt;&lt;font face=3D&quot;arial, helvetica, sans-serif&quot;&gt;&lt;=\r\nbr&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;&lt;font face=3D&quot;arial, helvetica, sans-serif&quot;&gt;Examining =\r\nthe frontier report I see the following:&lt;/font&gt;&lt;/div&gt;\n&lt;div&gt;&lt;font face=3D&quot;ar=\r\nial, helvetica, sans-serif&quot;&gt;&lt;span style=3D&quot;font-family:Times;font-size:medi=\r\num&quot;&gt;&lt;pre style=3D&quot;word-wrap:break-word;white-space:pre-wrap&quot;&gt; -----=3D=3D=\r\n=3D=3D=3D SNOOZED QUEUES =3D=3D=3D=3D=3D-----\nSNOOZED#0:\nQueue ssl, (p1)\n  =\r\n2 items\n   wakes in: 7m25s888ms\n    last enqueued: dns:ssl\n      last peeke=\r\nd: dns:ssl\n   total expended: 4 (total budget: -1)\n   active balance: 2996\n=\r\n   last(avg) cost: 1(1)\n   totalScheduled fetchSuccesses fetchFailures fetc=\r\nhDisregards fetchResponses robotsDenials successBytes totalBytes fetchNonRe=\r\nsponses\n   2 0 0 0 0 0 0 0 5\n   SimplePrecedenceProvider\n   1\n\nSNOOZED#1:\nQ=\r\nueue www, (p1)\n  2 items\n   wakes in: 7m25s889ms\n    last enqueued: dns:www=\r\n\n      last peeked: dns:www\n   total expended: 4 (total budget: -1)\n   acti=\r\nve balance: 2996\n   last(avg) cost: 1(1)\n   totalScheduled fetchSuccesses f=\r\netchFailures fetchDisregards fetchResponses robotsDenials successBytes tota=\r\nlBytes fetchNonResponses\n   2 0 0 0 0 0 0 0 5\n   SimplePrecedenceProvider\n =\r\n  1&lt;/pre&gt;&lt;/span&gt;&lt;/font&gt;&lt;/div&gt;&lt;div&gt;So, it looks like there are some weird it=\r\nems being put into these queues that don&#39;t belong there and it&#39;s ha=\r\nnging the crawl job? Is there a configuration option I can tweak to retire =\r\nthese queues or clear them out after a specified idle period? Or, is this t=\r\nhe result of a misconfiguration somewhere?&lt;/div&gt;\n\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Thank=\r\ns,&lt;/div&gt;&lt;div&gt;-Zach&lt;/div&gt;\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;\n\r\n--001636417a811dcc1e0490de0c7d--\r\n\n"}}