{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"pGvWD5XGNfOVRMne1-11PSPwmzXNG4Lkp0p4KcM71M-03cI9PSshVdctf6yX0bAp5Oxh3l5z9d-_RZXM5nk8eLMygVJ2POS54IMyPW5K9A","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Duplicate detection in large scale crawls - thinking out loud","postDate":"1131533250","msgId":2340,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBNzg1NjM1QHNrYXJmdXIuYm9rLmxvY2FsPg==","inReplyToHeader":"PDQ5NGFmZTFjMDUxMTA4MjMyOXE1OTJiOGZjNW01NTVmMDhiNGQ3ZTEwYWQwQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2339,"nextInTopic":2347,"prevInTime":2339,"nextInTime":2341,"topicId":2338,"numMessagesInTopic":7,"msgSnippet":"Hi all, I m currently doing the 4th complete .is crawl. The amount of data gathered has got me thinking about detecting and eliminating duplicate documents in ","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 18694 invoked from network); 9 Nov 2005 10:48:40 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m27.grp.scd.yahoo.com with QMQP; 9 Nov 2005 10:48:40 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta4.grp.scd.yahoo.com with SMTP; 9 Nov 2005 10:48:39 -0000\r\nReceived: (qmail 4470 invoked by uid 100); 9 Nov 2005 10:44:54 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (kris@...@130.208.152.83)\n  by mail-dev.archive.org with SMTP; 9 Nov 2005 10:44:54 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Wed, 9 Nov 2005 10:47:30 -0000\r\nMessage-ID: &lt;0678DB1968EAC7409CC3D0AB7A11B84A785635@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0000_01C5E51A.FD743910&quot;\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nImportance: Normal\r\nIn-Reply-To: &lt;494afe1c0511082329q592b8fc5m555f08b4d7e10ad0@...&gt;\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.2180\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-76.8 required=7.0 tests=AWL,HTML_20_30,HTML_MESSAGE,\n\tUSER_IN_WHITELIST autolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: Duplicate detection in large scale crawls - thinking out loud\r\nX-Yahoo-Group-Post: member; u=163406187; y=YQw0_HLv3XIysN-Rf24XN1ewvvoq2sMXCEYlaGKZUqDOTIjU\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n------=_NextPart_000_0000_01C5E51A.FD743910\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi all,\n \nI&#39;m currently doing the 4th complete .is crawl. The amount of dat=\r\na gathered\nhas got me thinking about detecting and eliminating duplicate do=\r\ncuments in\nsubsequent crawls. \n \nThe following is mostly me thinking out lo=\r\nud. I&#39;d love some feedback on\nthese (very) initial thoughts about the probl=\r\nem.\n \nA basic idea for handling duplicate detection. Add a new processor th=\r\nat\nmaintains its own database (Berkley DB or other). In this database we re=\r\ncord\nthe URL fingerprint (ideally canonicalized but that is more difficult =\r\nsince\nthat is done in the frontier at the moment, Michael any thoughts on t=\r\nhis?)\nand the content hash (plus possibly some additional meta-data such as=\r\n time\nof first discovery and last change). This is indexed by the URL finge=\r\nrprint.\nThe processor is applied after the FetchHTTP processor (and only on=\r\n HTTP\ndocuments). It looks the URI up and compares the content hashes, abor=\r\nts\nfurther processing of unchanged documents. The DB is updated as needed. =\r\nAll\nvery simple and straightforward.\n \nThe only possible downside is the ov=\r\nerhead incurred by doing the lookups\n(hashes are already being generated) a=\r\nnd updates in the DB. \n \nNow some statistics. I&#39;m going to use data from th=\r\ne current on-going crawl.\nIt has been running a little over a week and has =\r\ncovered almost 19 million\ndocuments at this point and about 888GB of uncomp=\r\nressed data which\ntranslates to 674GB of compressed data (about 24% compres=\r\nsion).\n \nLooking at the documents sorted by mime type, text/* are 13.2 mill=\r\nion (70%)\nwhile non-text documents number roughly 5.8 million (30%). \n \nHow=\r\never, text/* accounts for ~265GB while everything else ~615 (there are\nsome=\r\n rounding errors in this, but the scales are clear). We know that text\ncomp=\r\nresses well, typically around 80%. We also know that &#39;everything else&#39;\nmean=\r\ns mostly images and other, already compressed, material that does not\ncompr=\r\ness well. Since this material accounts for 70% of the downloaded data\n(and =\r\nprobably even more of the stored, compressed data) but only 30% of the\ndown=\r\nloaded documents, it is clear that eliminiating duplicates. \n \nThis means t=\r\nhat by setting up some sort of database to record the content\nhash of all b=\r\ninary (i.e. non-text) documents encountered and using that to\nfilter docume=\r\nnts in subsequent crawls, we can potentially save a\nconsiderable amount of =\r\ndisk storage while minimizing the associated overhead\nby focusing our effor=\r\nts.\n \nWhile I don&#39;t have any hard data on it, I think it is fair to assume =\r\nthat\nnon-text data changes far less then text. In fact it probably doesn&#39;t =\r\nchange\nmuch at all if you exclude Word and PDF documents (and other encoded=\r\n text\ndocuments). \n \nPlease share any thoughts you may have on this. Michae=\r\nl, how practical is it\nto maintain a seperate Berkley DB for this (must be =\r\nentirely independand of\nthe Frontier queues)?\n \n- Kris\n\r\n------=_NextPart_000_0000_01C5E51A.FD743910\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n&lt;TITLE&gt;Message&lt;/TITLE&gt;\n&lt;META http-equiv=3DContent-Type content=3D&quot;text/html=\r\n; charset=3Diso-8859-1&quot;&gt;\n&lt;META content=3D&quot;MSHTML 6.00.2900.2769&quot; name=3DGEN=\r\nERATOR&gt;&lt;/HEAD&gt;\n&lt;BODY&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3DAr=\r\nial size=3D2&gt;Hi \nall,&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-0911=\r\n2005&gt;&lt;FONT face=3DArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN cla=\r\nss=3D291463109-09112005&gt;&lt;FONT face=3DArial size=3D2&gt;I&#39;m currently doing \nth=\r\ne 4th complete .is crawl. The amount of data gathered has got me thinking \n=\r\nabout detecting and eliminating duplicate documents in subsequent crawls. \n=\r\n&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3DAri=\r\nal \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-091120=\r\n05&gt;&lt;FONT face=3DArial size=3D2&gt;The following is \nmostly me thinking out lou=\r\nd. I&#39;d love some feedback on these (very) initial \nthoughts about the probl=\r\nem.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3D=\r\nArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-091=\r\n12005&gt;&lt;FONT face=3DArial size=3D2&gt;A basic idea for \nhandling duplicate dete=\r\nction. Add a new processor that maintains its own \ndatabase (Berkley DB or =\r\nother). In this database we record the URL fingerprint \n(ideally canonicali=\r\nzed but that is more difficult since that is done in the \nfrontier at the m=\r\noment, Michael any thoughts on this?) and&nbsp;the content hash \n(plus poss=\r\nibly some additional meta-data such as time of first discovery and \nlast ch=\r\nange). This is indexed by the URL fingerprint. The processor is applied \naf=\r\nter the FetchHTTP processor (and only on HTTP documents). It looks the URI =\r\nup \nand compares the content hashes, aborts further processing of unchanged=\r\n \ndocuments. The DB is updated as needed. All very simple and \nstraightforw=\r\nard.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=\r\n=3DArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-=\r\n09112005&gt;&lt;FONT face=3DArial size=3D2&gt;The only possible \ndownside is the ove=\r\nrhead incurred by doing the lookups (hashes are already being \ngenerated) a=\r\nnd updates in the DB. &lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-091=\r\n12005&gt;&lt;FONT face=3DArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN cl=\r\nass=3D291463109-09112005&gt;&lt;FONT face=3DArial size=3D2&gt;Now some statistics. \n=\r\nI&#39;m going to use data from the current on-going crawl. It has been running =\r\na \nlittle over a week and has covered almost 19 million documents at this p=\r\noint and \nabout 888GB of uncompressed data which translates to 674GB of com=\r\npressed data \n(about 24% compression).&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=\r\n=3D291463109-09112005&gt;&lt;FONT face=3DArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DI=\r\nV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3DArial size=3D2&gt;Lookin=\r\ng at the \ndocuments sorted by mime type, text/* are 13.2 million \n(70%)&nbs=\r\np;while&nbsp;non-text documents&nbsp;number roughly 5.8 million \n(30%).&nbs=\r\np;&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3DA=\r\nrial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-0911=\r\n2005&gt;&lt;FONT face=3DArial size=3D2&gt;However, text/* \naccounts for ~265GB&nbsp;=\r\nwhile everything else ~615 (there are some rounding \nerrors in this, but th=\r\ne scales are clear). We know that text compresses well, \ntypically around 8=\r\n0%. We also know that &#39;everything else&#39; means mostly images \nand other, alr=\r\neady compressed, material that does not compress well. Since this \nmaterial=\r\n accounts for 70% of the downloaded data&nbsp;(and probably even more of \nt=\r\nhe stored, compressed data) but only 30% of the downloaded documents, it is=\r\n \nclear that eliminiating duplicates. &lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=\r\n=3D291463109-09112005&gt;&lt;FONT face=3DArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DI=\r\nV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3DArial size=3D2&gt;This m=\r\neans that by \nsetting up some sort of database to record the content hash o=\r\nf all binary (i.e. \nnon-text) documents encountered and using that to filte=\r\nr documents in subsequent \ncrawls, we can potentially save a considerable a=\r\nmount of disk storage while \nminimizing the associated overhead by focusing=\r\n our efforts.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FO=\r\nNT face=3DArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291=\r\n463109-09112005&gt;&lt;FONT face=3DArial size=3D2&gt;While I don&#39;t have \nany hard da=\r\nta on it, I think it is fair to assume that non-text data changes far \nless=\r\n then text. In fact it probably doesn&#39;t change much at all if you exclude \n=\r\nWord and PDF documents (and other encoded text documents). &lt;/FONT&gt;&lt;/SPAN&gt;&lt;/=\r\nDIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3DArial \nsize=3D2&gt;&lt;/F=\r\nONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3D=\r\nArial size=3D2&gt;Please share any \nthoughts you may have on this. Michael, ho=\r\nw practical is it to maintain a \nseperate Berkley DB for this (must be enti=\r\nrely independand of the Frontier \nqueues)?&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN c=\r\nlass=3D291463109-09112005&gt;&lt;FONT face=3DArial \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;=\r\n&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D291463109-09112005&gt;&lt;FONT face=3DArial size=3D2&gt;- =\r\n\nKris&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_0000_01C5E51A.FD743910--\r\n\n"}}