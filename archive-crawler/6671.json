{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"kristsi25","from":"&quot;kristsi25&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"SmpuJ_CIMAFesr4NqEVu911Gd6vhXUbJbS0A3uCCFtAu-zv3dIcVEN51yAhusbw4K1dL-85WHriPXYFFLTPKuSffhsMa","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Problem with robots.txt IGNORE policy","postDate":"1281629345","msgId":6671,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGk0MTZiMSt2ZWQzQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":6672,"prevInTime":6670,"nextInTime":6672,"topicId":6671,"numMessagesInTopic":9,"msgSnippet":"We wish to ignore robots.txt as far as they exclude us from content BUT we would like to respect the crawl-delay (at least up to some number of seconds).","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 86308 invoked from network); 12 Aug 2010 16:10:37 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m2.grp.sp2.yahoo.com with QMQP; 12 Aug 2010 16:10:37 -0000\r\nX-Received: from unknown (HELO n40b.bullet.mail.sp1.yahoo.com) (66.163.168.154)\n  by mta3.grp.sp2.yahoo.com with SMTP; 12 Aug 2010 16:10:37 -0000\r\nX-Received: from [69.147.65.173] by n40.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2010 16:09:06 -0000\r\nX-Received: from [98.137.34.33] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2010 16:09:06 -0000\r\nDate: Thu, 12 Aug 2010 16:09:05 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;i416b1+ved3@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;kristsi25&quot; &lt;kris@...&gt;\r\nSubject: Problem with robots.txt IGNORE policy\r\nX-Yahoo-Group-Post: member; u=163406187; y=07pli4tM1Ka8DBxJeObz0fgnmIRFFP_Sy3_rpXOBaVwnpxCc\r\nX-Yahoo-Profile: kristsi25\r\n\r\nWe wish to ignore robots.txt as far as they exclude us from content BUT we =\r\nwould like to respect the crawl-delay (at least up to some number of second=\r\ns). However, if you currently select IGNORE as your honoring policy the cra=\r\nwl-delay is never even read, much less enforced.\n\nIs there a way to configu=\r\nre the robots honoring to accomplish this or is this a deficiency in Heritr=\r\nix?\n\nIf this is not possible, does it make sense to amend the IGNORE policy=\r\n or should a new IGNORE_EXCEPT_CRAWLDELAY policy be added?\n\n- Kris\n\n\n"}}