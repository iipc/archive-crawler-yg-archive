{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"8GtPdIg6NLcsDUEAxXaLIk1YBNQWz8_vecUG76TMMno2O1zOZkwOBsD6fItXLO0lCpUjNPeem_46u6CAXYuTFA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] site digest","postDate":"1121796694","msgId":2056,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyREQ0MjU2LjYwMDA1QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGU2MDE2ZWRhMDUwNzE5MTAwNTIxM2Q3NGU4QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGU2MDE2ZWRhMDUwNzE5MTAwNTIxM2Q3NGU4QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2055,"nextInTopic":0,"prevInTime":2055,"nextInTime":2057,"topicId":2055,"numMessagesInTopic":2,"msgSnippet":"... You might start by studying ARCWriterProcessor.  It writes to disk all we ve crawled. You might do an alternate ARCWriter that writes an xml file per","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 89252 invoked from network); 19 Jul 2005 18:21:37 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m9.grp.scd.yahoo.com with QMQP; 19 Jul 2005 18:21:37 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta4.grp.scd.yahoo.com with SMTP; 19 Jul 2005 18:21:37 -0000\r\nReceived: (qmail 19438 invoked by uid 100); 19 Jul 2005 18:21:33 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.8?) (stack@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 19 Jul 2005 18:21:33 -0000\r\nMessage-ID: &lt;42DD4256.60005@...&gt;\r\nDate: Tue, 19 Jul 2005 11:11:34 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.8) Gecko/20050513 Debian/1.7.8-1\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e6016eda0507191005213d74e8@...&gt;\r\nIn-Reply-To: &lt;e6016eda0507191005213d74e8@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-82.0 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] site digest\r\nX-Yahoo-Group-Post: member; u=168599281; y=Vtr2rUFVH8FFxanV-rzDCFL15qsWKu32b0FY4RPop1-SeDa7fzP2jUg9\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nMichael Hansen wrote:\n\n&gt; Hi!\n&gt;\n&gt; ...\n\n&gt;\n&gt; Any help would much appreciated :)\n&gt;\nYou might start by studying ARCWriterProcessor.  It writes to disk all \nwe&#39;ve crawled. You might do an alternate ARCWriter that writes an xml \nfile per &#39;site&#39; appending a digest of every page encountered (This new \nXMLDigestProcessor could be in addition to or in place of current \nARCWriterProcessor).\n\nOr, write a script that processes ARC files offline extracting from them \nthe digest info wanted.\n\nHere are some links that should help:\n\n+ Source code xref: http://crawler.archive.org/xref/index.html.  Use to \nperuse such as the ARCWriterProcessor.\n+ Writing a processor from the developer&#39;s  manual: \nhttp://crawler.archive.org/articles/developer_manual.html#N104C3 (See \nalso the ARC file section in the developer manual for description of \nformat and a selection of Readers).\n+ Study the ExtractorTool class if you&#39;re looking at processing ARCs \noffline.\n\nWriting the XML files as you crawl has the advantage that most of the \ncontext you&#39;d want is to hand (This would likely be the easier of the \ntwo approaches outlined above).  Processing offline you&#39;ll have to \nremanufacture the context but you won&#39;t be interfering with the crawl.\n\nAsk more questions as you get stuck in.\nYours,\nSt.Ack\n\n\n\n&gt; Kind regards\n&gt;\n&gt;   -michael hansen\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}