{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":125829837,"authorName":"Ahnu Nahki","from":"&quot;Ahnu Nahki&quot; &lt;ahnunahki@...&gt;","profile":"ahnunahki","replyTo":"LIST","senderId":"BTbw78zcSnFpv72NnzfekAa34VdPR0eCpcpeIsghA55uldv08TLWB9WfZ-S4gJlwDRMu_U3to8R8F7LvFjTR0QHg7GUxHNTWsRhJDZ5UscvC","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Inserting information to MYSQL during crawl","postDate":"1087496886","msgId":544,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNhc25ybStiZXU4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDEwODY3NzEwNzEuNDBjNmNmN2YxN2VlYkBtYWlsLWRldi5hcmNoaXZlLm9yZz4="},"prevInTopic":526,"nextInTopic":3278,"prevInTime":542,"nextInTime":545,"topicId":507,"numMessagesInTopic":19,"msgSnippet":"In the end I just made a processor class that I imported into heritrix and it works great. Once I took a deep look at the docs and the API it was pretty","rawEmail":"Return-Path: &lt;ahnunahki@...&gt;\r\nX-Sender: ahnunahki@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 11446 invoked from network); 17 Jun 2004 18:29:40 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m25.grp.scd.yahoo.com with QMQP; 17 Jun 2004 18:29:40 -0000\r\nReceived: from unknown (HELO n37.grp.scd.yahoo.com) (66.218.66.105)\n  by mta3.grp.scd.yahoo.com with SMTP; 17 Jun 2004 18:29:39 -0000\r\nReceived: from [66.218.67.159] by n37.grp.scd.yahoo.com with NNFMP; 17 Jun 2004 18:28:07 -0000\r\nDate: Thu, 17 Jun 2004 18:28:06 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;casnrm+beu8@...&gt;\r\nIn-Reply-To: &lt;1086771071.40c6cf7f17eeb@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 3664\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.105\r\nFrom: &quot;Ahnu Nahki&quot; &lt;ahnunahki@...&gt;\r\nSubject: Re: Inserting information to MYSQL during crawl\r\nX-Yahoo-Group-Post: member; u=125829837\r\nX-Yahoo-Profile: ahnunahki\r\n\r\nIn the end I just made a processor class that I imported into heritrix\nand it works great. Once I took a deep look at the docs and the API it\nwas pretty simple. No need to mess with the ArcWriter class...\n\n-Ahnu\n\n--- In archive-crawler@yahoogroups.com, stack@a... wrote:\n&gt; Quoting stack@a...:\n&gt; \n&gt; &gt; Are you clear on where to start making your changes?  That you\nwould put in \n&gt; &gt; place an alterate ARCWriterProcessor, one that did effectively\nwhat the\n&gt; &gt; current \n&gt; &gt; one does but rather than it talk to ARCWriterPool, instead it\nwould keep up \n&gt; &gt; pool of database connections.\n&gt; \n&gt; On further consideration, look at ARCWriterPool.  In particular, the\ninner class\n&gt;  ARCWriterFactory.  See how it is responsible for the manufacture of the\n&gt; &#39;writer&#39;.  An override here, though awkward, might be better because\nthen you&#39;d\n&gt; get the pooling benefits (We should make the insertion of custom\nwriters more\n&gt; amenable by changing the ARCWriterFactory to read a configuration\ndescribing\n&gt; writer class to load).\n&gt; St.Ack \n&gt; \n&gt; \n&gt; &gt; \n&gt; &gt; Would you want to do the document filtering before you did the db\ninsert\n&gt; &gt; (i.e. \n&gt; &gt; the stripping of markup from html and the totexting of word, pdf,\netc.)?\n&gt; &gt; \n&gt; &gt; Keep asking questions.\n&gt; &gt; St.Ack\n&gt; &gt; \n&gt; &gt; Quoting Ahnu Nahki &lt;ahnunahki@t...&gt;:\n&gt; &gt; \n&gt; &gt; &gt; --- In archive-crawler@yahoogroups.com, Andy Boyko &lt;aboy@l...&gt;\nwrote:\n&gt; &gt; &gt; &gt; Ahnu Nahki wrote:\n&gt; &gt; &gt; &gt; &gt; Instead of writing to an arc file, Id like to create a\nmethod that\n&gt; &gt; &gt; &gt; &gt; takes the URI info, Content, headers, ect into a MYSQL\ndatabase. Does\n&gt; &gt; &gt; &gt; &gt; anyone have any suggestion on how to do this , where I\nshould look to\n&gt; &gt; &gt; &gt; &gt; place my methods?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Are you interested in that specifically to get away from ARC,\nor more \n&gt; &gt; &gt; &gt; simply because you&#39;re interested in being able to issue\nqueries on the \n&gt; &gt; &gt; &gt; crawl results in interesting/relational ways?  I ask because\nwe&#39;re \n&gt; &gt; &gt; &gt; looking into a slightly different approach - rather than\nbuilding the \n&gt; &gt; &gt; &gt; database logic into Heritrix, treating the DB import as a \n&gt; &gt; &gt; &gt; post-processing step on the crawl output (ARC files & logs)\nonce the \n&gt; &gt; &gt; &gt; crawl is complete.  I believe Tom Emerson has also talked about \n&gt; &gt; &gt; &gt; populating a DB from ARC files in future versions of his\nlibarc library.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; By keeping the content in ARCs, you get the ability to\nleverage the \n&gt; &gt; &gt; &gt; growing number of tools for dealing with the format coming\nfrom this \n&gt; &gt; &gt; &gt; community, and if the post-processing approach can work for\nyou, code \n&gt; &gt; &gt; &gt; for populating a DB may be forthcoming from a couple of\nsources in the \n&gt; &gt; &gt; &gt; near future.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Can you describe in more detail what you&#39;re envisioning with your \n&gt; &gt; &gt; &gt; planned DB crawl storage?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Regards,\n&gt; &gt; &gt; &gt; Andy Boyko    aboy@l...     Library of Congress\n&gt; &gt; &gt; \n&gt; &gt; &gt; We had considered that aswell initially as a quick way of importing\n&gt; &gt; &gt; the data into the db. Going the arc route after a crawl. But we\nhave a\n&gt; &gt; &gt; search engine we run that is powered by\n&gt; &gt; &gt; lucene(http://jakarta.apache.org/lucene/docs/index.html). We\nhave our\n&gt; &gt; &gt; own crawler which has done all it can and is not nearly as\npowerful as\n&gt; &gt; &gt; heritrix. So we want to incorporate heritrix into out environment\n&gt; &gt; &gt; quickly. We make all our indexes off of content stored in the\ndb, and\n&gt; &gt; &gt; it makes alot of sense for us to just have the crawler populate\nthe db\n&gt; &gt; &gt; at runtime than have some post crawl method were we do it. \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt;  \n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt;  \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; Yahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; \n&gt; &gt;\n\n\n"}}