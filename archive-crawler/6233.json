{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":423327235,"authorName":"欧阳希修","from":"=?GB2312?B?xbfR9M+j0N4=?= &lt;tony871209@...&gt;","profile":"tony871209","replyTo":"LIST","senderId":"hTg_BnqK4YHfp-WPAlDo82WsS9Utr_5XX3b_HQGeKEnE-AUWl83ORnK1FyDTlElIszGaBLHDWlZXGv2WgP2wWO2VOpb4Pm-YG9pE8FLPcRLTrStv6WuW","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Two issues for crawling","postDate":"1261205441","msgId":6233,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDgxYjc1M2EwMDkxMjE4MjI1MHYyZTZhNjBhZXNmZDI2Mjc0MWUyN2I3ZjRhQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDRCMkJBRDc4LjYwMzA1MDZAYXJjaGl2ZS5vcmc+","referencesHeader":"PGhnZmgwZStpMmI3QGVHcm91cHMuY29tPiA8NEIyQkFENzguNjAzMDUwNkBhcmNoaXZlLm9yZz4="},"prevInTopic":6232,"nextInTopic":0,"prevInTime":6232,"nextInTime":6234,"topicId":6230,"numMessagesInTopic":3,"msgSnippet":"thanks a lot :-) i m using 1.14.3 on Ubuntu9.10 2009/12/19 steve@archive.org  ... -- Best regard , TonyOuyang.","rawEmail":"Return-Path: &lt;tony871209@...&gt;\r\nX-Sender: tony871209@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 13310 invoked from network); 19 Dec 2009 06:50:51 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m12.grp.re1.yahoo.com with QMQP; 19 Dec 2009 06:50:51 -0000\r\nX-Received: from unknown (HELO mail-px0-f196.google.com) (209.85.216.196)\n  by mta2.grp.re1.yahoo.com with SMTP; 19 Dec 2009 06:50:50 -0000\r\nX-Received: by pxi34 with SMTP id 34so204601pxi.8\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 18 Dec 2009 22:50:41 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.141.88.5 with SMTP id q5mr3321285rvl.249.1261205441273; Fri, \n\t18 Dec 2009 22:50:41 -0800 (PST)\r\nIn-Reply-To: &lt;4B2BAD78.6030506@...&gt;\r\nReferences: &lt;hgfh0e+i2b7@...&gt; &lt;4B2BAD78.6030506@...&gt;\r\nDate: Sat, 19 Dec 2009 14:50:41 +0800\r\nMessage-ID: &lt;81b753a00912182250v2e6a60aesfd262741e27b7f4a@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=000e0cd1388e50de5e047b0f462e\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: =?GB2312?B?xbfR9M+j0N4=?= &lt;tony871209@...&gt;\r\nSubject: Re: [archive-crawler] Two issues for crawling\r\nX-Yahoo-Group-Post: member; u=423327235; y=1d595ncw_NZDu_J7T961wPzS8VnLjifJxY175sxndHu3d2VKiQ\r\nX-Yahoo-Profile: tony871209\r\n\r\n\r\n--000e0cd1388e50de5e047b0f462e\r\nContent-Type: text/plain; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nthanks a lot :-)\ni&#39;m using 1.14.3 on Ubuntu9.10\n\n2009/12/19 steve@archive.o=\r\nrg &lt;steve@...&gt;\n\n&gt;\n&gt;\n&gt; hi Tony,\n&gt;\n&gt; first, which version of Heritrix=\r\n are you running, and on what platform?\n&gt;\n&gt; re (a) crawl a URL at most once=\r\n, you&#39;ll need to tell subsequent crawls\n&gt; to ignore a specified set of URLs=\r\n. with H3 you can do this by adding\n&gt; the URLs you wish to NOT be crawled t=\r\no the _negative-surts.dump_\n&gt; file i believe, which will be REJECTed from t=\r\nhe scope (via\n&gt; SurtPrefixedDecideRule bean) by the default profile CXML\n&gt; =\r\n(crawler-beans.cxml). i believe you can accomplish the same thing in H1\n&gt; b=\r\ny specifying a &quot;surts-dump-file&quot; with a REJECT SurtPrefixedDecideRule,\n&gt; ho=\r\nwever the default in H1 is to ACCEPT URLs in that file, so you&#39;ll\n&gt; need a =\r\nnew rule. see the Heritrix Glossary for more about SURT prefixes:\n&gt;\n&gt; https=\r\n://webarchive.jira.com/wiki/display/Heritrix/Glossary\n&gt;\n&gt; re (b) adding see=\r\nds after crawl launch. with H3, you can drop a seeds\n&gt; file into the &quot;actio=\r\nn&quot; directory. see the user&#39;s guide for more info:\n&gt;\n&gt; https://webarchive.ji=\r\nra.com/wiki/display/Heritrix/Action+Directory\n&gt;\n&gt; i&#39;m not sure how to do th=\r\nis in H1, and couldn&#39;t find anything about\n&gt; it in a quick search of the ma=\r\niling list and the issue tracker. perhaps\n&gt; someone else on the list has do=\r\nne this?\n&gt;\n&gt; /steve@... &lt;%2Fsteve%40archive.org&gt;\n&gt;\n&gt;\n&gt; On 12/18/09 =\r\n1:11 AM, tony871209 wrote:\n&gt; &gt; how can i\n&gt; &gt; a.crawl a URL at most once ( i=\r\n do not want a update-checking followed\n&gt; &gt; with the 2nd time crawl)\n&gt; &gt; b.=\r\nadd seeds after crawl launch\n&gt;  \n&gt;\n\n\n\n-- \nBest regard , TonyOuyang.\n\r\n--000e0cd1388e50de5e047b0f462e\r\nContent-Type: text/html; charset=windows-1252\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nthanks a lot :-)&lt;br&gt;i&#39;m using 1.14.3 on Ubuntu9.10&lt;br&gt;&lt;br&gt;&lt;div class=3D=\r\n&quot;gmail_quote&quot;&gt;2009/12/19 &lt;a href=3D&quot;mailto:steve@...&quot;&gt;steve@archive=\r\n.org&lt;/a&gt; &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:steve@...&quot;&gt;steve@a=\r\nrchive.org&lt;/a&gt;&gt;&lt;/span&gt;&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;bo=\r\nrder-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding=\r\n-left: 1ex;&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background-color: rgb(255, 255, 25=\r\n5);&quot;&gt;\n&lt;span&gt;=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;h=\r\ni Tony,&lt;br&gt;\n&lt;br&gt;\nfirst, which version of Heritrix are you running, and on w=\r\nhat platform?&lt;br&gt;\n&lt;br&gt;\nre (a) crawl a URL at most once, you&#39;ll need to =\r\ntell subsequent crawls&lt;br&gt;\nto ignore a specified set of URLs. with H3 you c=\r\nan do this by adding&lt;br&gt;\nthe URLs you wish to NOT be crawled to the _negati=\r\nve-surts.dump_&lt;br&gt;\nfile i believe, which will be REJECTed from the scope (v=\r\nia&lt;br&gt;\nSurtPrefixedDecideRule bean) by the default profile CXML&lt;br&gt;\n(crawle=\r\nr-beans.cxml). i believe you can accomplish the same thing in H1&lt;br&gt;\nby spe=\r\ncifying a &quot;surts-dump-file&quot; with a REJECT SurtPrefixedDecideRule,=\r\n&lt;br&gt;\nhowever the default in H1 is to ACCEPT URLs in that file, so you&#39;l=\r\nl&lt;br&gt;\nneed a new rule. see the Heritrix Glossary for more about SURT prefix=\r\nes:&lt;br&gt;\n&lt;br&gt;\n&lt;a href=3D&quot;https://webarchive.jira.com/wiki/display/Heritrix/G=\r\nlossary&quot; target=3D&quot;_blank&quot;&gt;https://webarchive.jira.com/wiki/display/Heritri=\r\nx/Glossary&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\nre (b) adding seeds after crawl launch. with H3, yo=\r\nu can drop a seeds&lt;br&gt;\nfile into the &quot;action&quot; directory. see the =\r\nuser&#39;s guide for more info:&lt;br&gt;\n&lt;br&gt;\n&lt;a href=3D&quot;https://webarchive.jira=\r\n.com/wiki/display/Heritrix/Action+Directory&quot; target=3D&quot;_blank&quot;&gt;https://weba=\r\nrchive.jira.com/wiki/display/Heritrix/Action+Directory&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\ni&#39;m=\r\n not sure how to do this in H1, and couldn&#39;t find anything about&lt;br&gt;\nit=\r\n in a quick search of the mailing list and the issue tracker. perhaps&lt;br&gt;\ns=\r\nomeone else on the list has done this?&lt;br&gt;\n&lt;br&gt;\n&lt;a href=3D&quot;mailto:%2Fsteve%=\r\n40archive.org&quot; target=3D&quot;_blank&quot;&gt;/steve@...&lt;/a&gt;&lt;/p&gt;&lt;div class=3D&quot;im=\r\n&quot;&gt;&lt;br&gt;\n&lt;br&gt;\nOn 12/18/09 1:11 AM, tony871209 wrote:&lt;br&gt;\n&gt; how can i&lt;br&gt;\n&=\r\ngt; a.crawl a URL at most once ( i do not want a update-checking followed&lt;b=\r\nr&gt;\n&gt; with the 2nd time crawl)&lt;br&gt;\n&gt; b.add seeds after crawl launch&lt;br=\r\n&gt;\n&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color: rgb(255, 255, 255=\r\n); min-height: 0pt;&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;br=\r\n clear=3D&quot;all&quot;&gt;&lt;br&gt;-- &lt;br&gt;Best regard , TonyOuyang.&lt;br&gt;\n\r\n--000e0cd1388e50de5e047b0f462e--\r\n\n"}}