{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"david_pane1","from":"&quot;david_pane1&quot; &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"8jYjtc0ezKXcFWPKWtpCv5VCvW03Pt6umZhZa_sHuygRUTyIetkbtsTM9VKjudX4gTux3wWqQZrfJ_9LQtF7HTnIuNBKjCo","spamInfo":{"isSpam":false,"reason":"6"},"subject":"H3 - distributed crawling and memory/cpu utilization","postDate":"1318450084","msgId":7351,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGo3NHMzNCtldWVnQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":7353,"prevInTime":7350,"nextInTime":7352,"topicId":7351,"numMessagesInTopic":7,"msgSnippet":"1) I am using two 8 core 32GB machines for some test crawls.  The machines are connected to the internet on a gigabit connection.  The internal network is also","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 13983 invoked from network); 12 Oct 2011 20:08:05 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m7.grp.sp2.yahoo.com with QMQP; 12 Oct 2011 20:08:05 -0000\r\nX-Received: from unknown (HELO n5-vm6.bullet.mail.sp2.yahoo.com) (67.195.135.101)\n  by mta3.grp.sp2.yahoo.com with SMTP; 12 Oct 2011 20:08:05 -0000\r\nX-Received: from [67.195.134.239] by n5.bullet.mail.sp2.yahoo.com with NNFMP; 12 Oct 2011 20:08:05 -0000\r\nX-Received: from [69.147.65.172] by t4.bullet.mail.sp2.yahoo.com with NNFMP; 12 Oct 2011 20:08:05 -0000\r\nX-Received: from [98.137.34.35] by t14.bullet.mail.sp1.yahoo.com with NNFMP; 12 Oct 2011 20:08:05 -0000\r\nDate: Wed, 12 Oct 2011 20:08:04 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;j74s34+eueg@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;david_pane1&quot; &lt;dpane@...&gt;\r\nSubject: H3 - distributed crawling and memory/cpu utilization\r\nX-Yahoo-Group-Post: member; u=500983475; y=OpC4eU3GORiYaMNNaMAEk8998Danh6L-I83mfBqyONMygKH8JQeJ8A\r\nX-Yahoo-Profile: david_pane1\r\n\r\n\n1) I am using two 8 core 32GB machines for some test crawls.  The machines=\r\n are connected to the internet on a gigabit connection.  The internal netwo=\r\nrk is also a gigabit.  I am writing the data from the crawler to a NAS whic=\r\nh I can copy data to at an average of 70MB/sec transfer rate. I am trying t=\r\no adjust the memory and maxToeThread settings to get the maximum throughput=\r\n.  Currently my settings are: \n\nJAVA_OPTS=3D&quot;-Xmx11000M\n&lt;property name=3D&quot;m=\r\naxToeThreads&quot; value=3D&quot;1200&quot; /&gt;\n\nWith these values, I am seeing about 50% c=\r\npu utilization (about half of the 8 cores) and an average of around 750MB/m=\r\nin or (96Mbps) network activity.  I would like to utilize more of the cpu. =\r\n Is this reasonable?\n\nIncreasing the maxToeThreads to a higher value than 1=\r\n200 causes the java application to fall to minimal to no cpu usage and the =\r\nweb interface to be unresponsive.  Does anyone know why this is happening?\n=\r\n\nHow much of the 32GB of memory should I allocate to JAVA_OPTS?\n\n2) One of =\r\nthe 2 crawlers stopped crawling due to a congestion ratio of infinity.  Wha=\r\nt are some ways to overcome this? What can I do to avoid it happening in th=\r\ne future?  \n\n3) In http://tech.groups.yahoo.com/group/archive-crawler/messa=\r\nge/3846\nGordon stated:\n\n&quot;...\nHashCrawlMapper looks at the queue key of a UR=\r\nI -- here, the SURT\nauthority part, because of the above choice -- and deci=\r\ndes if a URI is\nhandled by the current crawler or one of its siblings. If m=\r\napped to a\nsibling, the URI is dumped to a log rather than crawled locally.=\r\n\nDepending on the character of your crawl, you may want to feed these\nlogs =\r\nto the other crawlers occasionally or it may be OK to ignore them.\n...\n&quot;\n\nH=\r\now does one feed the diverted URIs/logs to a sibling crawler?\n\nAny help is =\r\ngreatly appreciated.\n\n--David\n\n\n"}}