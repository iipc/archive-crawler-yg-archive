{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":401913576,"authorName":"Joel Halbert","from":"Joel Halbert &lt;joel@...&gt;","replyTo":"LIST","senderId":"yBcwXmoSc2SYsBdI_ZbJ6bi2fcwHpbVoshKY-0YaJHKQpAAsJ73-wEnFUX54vK31sjznBrAHdawqRtb4R5Hz0bV3pLJzsESsyvNG","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Limitations on the number of Jobs per\tinstance of Heretrix ?","postDate":"1243852760","msgId":5872,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEyNDM4NTI3NjAuNjk5OS4zNi5jYW1lbEBib2hyPg==","inReplyToHeader":"PDRBMjA3MTYxLjEwMzA2MDVAYXJjaGl2ZS5vcmc+","referencesHeader":"PDEyNDM1OTQxMTAuNjg3NS40OC5jYW1lbEBib2hyPiAgPDRBMjA3MTYxLjEwMzA2MDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":5870,"nextInTopic":5874,"prevInTime":5871,"nextInTime":5873,"topicId":5866,"numMessagesInTopic":6,"msgSnippet":"Thanks for the feedback. Our main goals are: 1. achieving maximum use of commodity hardware. If we want to distribute 200k domains across say, 4 machines, we","rawEmail":"Return-Path: &lt;joel@...&gt;\r\nX-Sender: joel@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 89339 invoked from network); 1 Jun 2009 10:39:24 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m5.grp.re1.yahoo.com with QMQP; 1 Jun 2009 10:39:24 -0000\r\nX-Received: from unknown (HELO mail.roo10.com) (79.170.194.127)\n  by mta1.grp.sp2.yahoo.com with SMTP; 1 Jun 2009 10:39:23 -0000\r\nX-Received: from [192.168.60.91] (78-105-13-3.dsl.cnl.uk.net [78.105.13.3])\n\tby mail.roo10.com (Postfix) with ESMTP id 971454800728\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  1 Jun 2009 11:39:18 +0100 (BST)\r\nTo: heretrix-users &lt;archive-crawler@yahoogroups.com&gt;\r\nIn-Reply-To: &lt;4A207161.1030605@...&gt;\r\nReferences: &lt;1243594110.6875.48.camel@bohr&gt;  &lt;4A207161.1030605@...&gt;\r\nContent-Type: text/plain\r\nOrganization: SU3 Analytics\r\nDate: Mon, 01 Jun 2009 11:39:20 +0100\r\nMessage-Id: &lt;1243852760.6999.36.camel@bohr&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Evolution 2.22.3.1 \r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Joel Halbert &lt;joel@...&gt;\r\nSubject: Re: [archive-crawler] Limitations on the number of Jobs per\n\tinstance of Heretrix ?\r\nX-Yahoo-Group-Post: member; u=401913576\r\n\r\nThanks for the feedback. \n\nOur main goals are:\n\n1. achieving maximum use of commodity hardware. If we want to distribute\n200k domains across say, 4 machines, we would like to ensure that if one\nmachine has already processed 50k domains (because they were smaller\nsites) then we can keep loading up more domains for it to process to\ntake up the slack. \n&gt;From the docs I gather you can edit a seed list for a running job. Can\nyou add to the seed list programatically? Can we see which seeds\n(domains) in the seed list have completed their crawl - can this be\ndetermined, as you suggested, from the job summary and frontier\nreports? \n\n2. Detailed reporting for each domain e.g. number of pages discovered,\npending, queued, errors etc...  You suggested we might be able to gather\nthis information by creating alternate views on the jobs in process from\nthe existing logs & reports? Can we hook into these reports\nprogramatically or would it require us to process the log files?\n\n3. Importantly, we need to be able to use specific crawl scopes for each\ndomain. Some may be DomainScope, others PathScope. How easy is it to\nconstruct the appropriate scopes for multiple domains in a single job?\n\nWe would like to &quot;work with the framework&quot; in achieving our goals,\nrather than fitting a square peg into a round hole, so hopefully  the\nabove is do-able using Heritrix best practice!\n\nThanks,\nJoel\n\n\n-----Original Message-----\nFrom: Gordon Mohr &lt;gojomo@...&gt;\nReply-To: archive-crawler@yahoogroups.com\nTo: archive-crawler@yahoogroups.com\nSubject: Re: [archive-crawler] Limitations on the number of Jobs per\ninstance of Heretrix ?\nDate: Fri, 29 May 2009 16:36:01 -0700\n\n\n\nAt the Internet Archive, we don&#39;t usually split up crawls to \none-site-per-job. We prefer fewer jobs, with more seeds/sites. We then \nrely on postprocessing and indexing to extract individual sites, or\nsite \nstatistics, later.\n\nSome other groups do a job-per-site, with Heritrix, but it&#39;s not very \neasy to run many jobs simultaneously in the same running instance.\n(It&#39;s \npossible, but a little tricky -- unless you shrink some of the usual\njob \nparameters, most importantly in 1.14.x the bdb-cache percentage, \nsimultaneous jobs can exhaust all allocated JVM memory. And I think \nthose who have done so have done more like dozens per instance, rather \nthan hundreds.)\n\nSome potential weaknesses of the one-job-per-site approach:\n\n- offsite inline resources, which our default configuration gets \n(because we want as accurate a rendition of the page as possible), may \nbe duplicated between the jobs -- each has no idea another may have \nalready retrieved that URI\n\n- if there are &#39;deep&#39; areas of a site only discoverable from other \nsites, but not from the home site&#39;s root page, these may not be \ncollected in separate crawls. (In the job they&#39;re discovered, they&#39;re \nout-of-scope; in the job for their own site, they&#39;re not discovered.)\n\n- because of trying to schedule/distribute the jobs, within the \nconstraints of a certain amount of per-job overhead, hardware may often \nbe underutilized\n\nUnless the sites are especially large, a single job on a single\npowerful \nmachine may be plenty to collect 200K sites. A crawler with (for \nexample) 200 crawling &#39;toe&#39; threads is actually making maximal progress \non far more than 200 sites at once, because while some sites are in \n&#39;politeness wait&#39;, threads continue on other sites with pending URIs. \nAnd since many sites are not large, many crawls finish most of their \ntarget sites in the first hours/days of crawling, with later days/weeks \nonly working on the very deep or very slow sites.\n\nWhat are your main goals in splitting the jobs up?\n\nCan those goals be met by postprocessing, or by creating alternate\nviews \nof a single in-progress job&#39;s logs and reports? (For example, you can \nforce dump larger per-host-summary and frontier-queue reports from a \nrunning crawl.)\n\n- Gordon @ IA\n\nJoel Halbert wrote:\n&gt; Hi,\n&gt; \n&gt; I&#39;m using Heretrix, over Nutch, the documentation seems more robust\nand\n&gt; I&#39;m not interested in Nutch&#39;s tight integration with Lucene - I want\nto\n&gt; do quite a lot or pre-processing with the crawl data before indexing\nit\n&gt; in our own format. So Heretrix seems a good flexible fit in this\nregard.\n&gt; \n&gt; I want to crawl about 200k unique domains, using DomainScope. I&#39;d like\n&gt; to distribute this over n machines, with each machine running say 100\n&gt; threads - one thread per domain. \n&gt; \n&gt; I&#39;d also like to be able to track & manage the progress of each domain\n&gt; individually. \n&gt; \n&gt; I&#39;d like to do all of this by assigning one job per domain and have a\n&gt; central management process that tracks each heretrix instance, and\n&gt; assigns new jobs as appropriate e.g. as a job on one machine\ncompletes,\n&gt; create a new one for another domain - keeping the total number of\n&gt; running jobs in each instance at say 200.\n&gt; \n&gt; The docs generally talk of creating a single job with a seed list.\nWill\n&gt; I run into any issues within heretrix with having a seedlist of size 1\n-\n&gt; and instead having multiple jobs e.g. up to 200?\n&gt; \n&gt; I&#39;m guessing this should all be fine, but just want to make sure there\n&gt; are no hidden gotchas.\n&gt; \n&gt; Thx,\n&gt; \n&gt; Joel\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n\n\n\n\n"}}