{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":516735840,"authorName":"mamdev","from":"&quot;mamdev&quot; &lt;aminemersni@...&gt;","profile":"mamdev","replyTo":"LIST","senderId":"RzOiTm6oblfAQGZCXygaCavVBC57E-i6ibxmjviJEXTPQDC3CEpjwewTY_F_VfY79pUoj9kzZKrJlg8w06Z7BjXRV9MrDQ","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Setting parameters to get  deeper section of a domain - Heritrix 1.14.4","postDate":"1356514011","msgId":7876,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGtiZWZzcis1bHBzQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":7877,"prevInTime":7875,"nextInTime":7877,"topicId":7876,"numMessagesInTopic":3,"msgSnippet":"Hi all, I m trying to crawl a domain with a news subsection containing more than 200 hundred pages, linked to each other with Older and Newer buttons. With","rawEmail":"Return-Path: &lt;aminemersni@...&gt;\r\nX-Sender: aminemersni@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 71222 invoked from network); 26 Dec 2012 09:26:55 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m16.grp.sp2.yahoo.com with QMQP; 26 Dec 2012 09:26:55 -0000\r\nX-Received: from unknown (HELO ng4-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.18)\n  by mta5.grp.sp2.yahoo.com with SMTP; 26 Dec 2012 09:26:54 -0000\r\nX-Received: from [98.139.164.125] by ng4.bullet.mail.bf1.yahoo.com with NNFMP; 26 Dec 2012 09:26:54 -0000\r\nX-Received: from [98.137.34.155] by tg6.bullet.mail.bf1.yahoo.com with NNFMP; 26 Dec 2012 09:26:54 -0000\r\nDate: Wed, 26 Dec 2012 09:26:51 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;kbefsr+5lps@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;4-0752964578-3490934024=:9&quot;\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;mamdev&quot; &lt;aminemersni@...&gt;\r\nSubject: Setting parameters to get  deeper section of a domain - Heritrix 1.14.4\r\nX-Yahoo-Group-Post: member; u=516735840; y=7OeqmE6kRANvOMX5wLtedP5pY6AuSxZS5WjQHUeuEc-3g1n5vsM\r\nX-Yahoo-Profile: mamdev\r\n\r\n\r\n--4-0752964578-3490934024=:9\r\nContent-Type: text/plain; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi all,\n\nI&#39;m trying to crawl a domain with a news subsection containing mor=\r\ne than\n200 hundred pages, linked to each other with &quot;Older&quot; and &quot;Newer&quot;\nbut=\r\ntons.\n\nWith the default configuration (max-link-hops=3D25) and the url of n=\r\news\nsubsection as seeds , i only get the first 26 pages and it&#39;s logical.\n\n=\r\nWhat i&#39;m looking for , is a configuration to get all subsection news\npages =\r\n without changing the default number of hops.\n\nIs it possible?\nShould i cra=\r\nwl separatly the subsection news?\n\n\nThanks in advance.\n\n\n\n\n\n\n\r\n--4-0752964578-3490934024=:9\r\nContent-Type: text/html; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi all,&lt;br&gt;&lt;br&gt;I&#39;m trying to crawl a domain with a news subsection containi=\r\nng more than 200 hundred pages, linked to each other with &quot;Older&quot; and &quot;Newe=\r\nr&quot; buttons.&lt;br&gt;&lt;br&gt;With the default configuration (max-link-hops=3D25) and =\r\nthe url of news subsection as seeds , i only get the first 26 pages and it&#39;=\r\ns logical.&lt;br&gt;&lt;br&gt;What i&#39;m looking for , is a configuration to get all subs=\r\nection news pages&nbsp; &lt;span id=3D&quot;result_box&quot; class=3D&quot;short_text&quot; lang=\r\n=3D&quot;en&quot;&gt;&lt;span class=3D&quot;hps&quot;&gt;without changing the default number of hops.&lt;br=\r\n&gt;&lt;br&gt;Is it possible?&lt;br&gt;Should i crawl separatly the subsection news?&lt;br&gt;&lt;b=\r\nr&gt;&lt;br&gt;Thanks in advance.&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/span&gt;&lt;/span&gt;\n\r\n--4-0752964578-3490934024=:9--\r\n\n"}}