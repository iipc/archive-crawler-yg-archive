{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"e79zfIQGNjKc3zDY4_rwlI2q1wSoBYEZ-dTAATAnedhBQO710Xyxx-fwbj_4dJdTYvuxM1vckiq2wm9nGJgaNwNO9H7CyUs","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix Frontier","postDate":"1297153563","msgId":7006,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENTBGRTFCLjcwMTA2QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PEM5NzVEMkE1LjIwMjElbXVyYWxpa3BAYW1hem9uLmNvbT4=","referencesHeader":"PEM5NzVEMkE1LjIwMjElbXVyYWxpa3BAYW1hem9uLmNvbT4="},"prevInTopic":7005,"nextInTopic":7007,"prevInTime":7005,"nextInTime":7007,"topicId":7005,"numMessagesInTopic":11,"msgSnippet":"... You probably won t need a new Frontier for this; the default frontier (BdbFrontier) should work for tens to even hundreds of millions of queued URIs per","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 56706 invoked from network); 8 Feb 2011 08:26:06 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.sp2.yahoo.com with QMQP; 8 Feb 2011 08:26:06 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta1.grp.sp2.yahoo.com with SMTP; 8 Feb 2011 08:26:05 -0000\r\nX-Received: (qmail 41560 invoked by uid 0); 8 Feb 2011 08:26:04 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay00.pair.com with SMTP; 8 Feb 2011 08:26:04 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4D50FE1B.70106@...&gt;\r\nDate: Tue, 08 Feb 2011 00:26:03 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;Krishna, Murali&quot; &lt;muralikp@...&gt;\r\nReferences: &lt;C975D2A5.2021%muralikp@...&gt;\r\nIn-Reply-To: &lt;C975D2A5.2021%muralikp@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix Frontier\r\nX-Yahoo-Group-Post: member; u=137285340; y=v7tQkBv5VAU3u5lVAqPiQCjEC5i84ZtND4NlypBIdIxP\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 2/7/11 3:17 AM, Krishna, Murali wrote:\n&gt;\n&gt;\n&gt; Hi all,\n&gt; We have a list of urls to be crawled, essentially just a fetch and some\n&gt; processing. Assume that the list can be huge and run into billions. So,\n&gt; we are thinking of writing a new Frontier which will accomplish this.\n&gt; Will have multiple heritrix worker boxes, each of the worker�s frontier\n&gt; will get one portion of the centralized url repository (distributed\n&gt; storage) and schedule them for crawling.\n\nYou probably won&#39;t need a new Frontier for this; the default frontier \n(BdbFrontier) should work for tens to even hundreds of millions of \nqueued URIs per node.\n\nI have more confidence in H3 for loading millions of seed URIs at \nstartup (which should be even more efficient in H3 SVN TRUNK and the \nnext H3 release), though you could also feed them in smaller batches via \nthe H1 JMX interface, or in batches via the &#39;action&#39; directory mechanism \nin H3.\n\nIf you simply have a large static list of URIs to crawl -- and don&#39;t \nneed link-extraction and any other running analysis/reporting -- \nHeritrix may be overkill for your purposes.\n\n&gt; 1. Can we achieve this by extending the WorkQueueFrontier ? I couldn�t\n&gt; find much documentation on how WQF handles politeness. I am thinking of\n&gt; grouping the urls into workqueue based on politeness requirement, will\n&gt; it automatically take care of politeness if I group correctly? Can I\n&gt; configure crawl-delay per WorkQueue?\n\nYou can control what is crawled -- whether outlinks from your starting \nURIs are followed, for example, to get inline resources or other linked \npages -- by customizing the scoping rules. If grouping URIs by hostname \ninto queues is insufficient, you can implement a new \nQueueAssignmentPolicy. In H3, politeness delays per URI -- affecting the \nqueue from which the URI came -- are configured outside the frontier, in \nthe DispositionProcessor. So lots of behavioral customization doesn&#39;t \nrequire reimplementing or specializing the frontier.\n\nThe queues are the units of politeness: by default, only one URI from a \nqueue will be in-process at a time. When a URI finishes, a configurable \npause (see the minDelay, maxDelay, delayFactor, and \nrespectCrawlDelayUpToSeconds settings on DispositionProcessor in H3) is \napplied to that queue before any other URIs are tried. Note that URI \ndomain-lookup/connectivity failures cause the same URI to be pushed back \natop the queue, a longer (frontier retryDelaySeconds) pause to be taken, \nand multiple (frontier maxRetries) attempts to be made, before the next \nURI is tried. This means you usually do not want URIs on different \nhosts, where one host might be unreachable, mixed in the same queue -- \none failure will delay them all -- unless you also knock the \nretries/retryDelay way down.\n\nYou can use the settings &#39;sheet overlay&#39; (aka &#39;overrides&#39; in H1) to set \ndifferent politeness values for different URIs by host or other \npatterns; the queue then takes on the delay of the URI that was just \noffered/completed.\n\nYou should also look at previous list traffic about HashCrawlMapper for \nideas on splitting the URI space, and the BloomUriUniqFilter as an \noption for an all in-memory URI-already-seen filter that may be \nappropriate for larger crawls.\n\n&gt; 2. What are inactive queues, retired queues and getURIList here? (sorry,\n&gt; couldn�t find doc)\n\n&#39;inactive&#39; queues are those that are not yet being considered to keep a \nthread busy. All those queues that are &#39;active&#39; round-robin to provide \nURIs to available threads, until the queue exhausts its &#39;session&#39; \nbudget; then it goes to the back of all &#39;inactive&#39; queues. If a thread \ncan&#39;t be kept busy with an available &#39;active&#39; queue, then the top \n&#39;inactive&#39; queue is activated. The intent is for the crawler to \nintensely focus on some queues for a while -- hoping to finish them, or \nat least get a big batch of URIs with as little time-skew as possible �� \nbut then rotate others into activity eventually. (The &#39;budgeting&#39; and \nURI &#39;cost&#39; parameters affect this cycle.)\n\n&#39;retired&#39; queues have already offered up URIs whose total &#39;cost&#39; exceeds \ntheir &#39;totalBudget&#39;, and so they continue to collect newly-discovered \nURIs, but will never b considered for &#39;active&#39; rotation (unless you \nraise their &#39;totalBudget&#39;). You probably don&#39;t need this feature, and \nwon&#39;t notice any &#39;retired&#39; queues unless you set a &#39;totalBudget&#39; and \nnonzero URI cost policy.\n\nI don&#39;t know what you mean by &quot;getURIList&quot;.\n\n&gt; 3. How does checkpointing work, I want to restart from the last crawled\n&gt; state. Is there a callback to do the frequent checkpointing for\n&gt; WorkQueueFrontier�s implementations.\n\nCheckpointing tries to save the whole crawl state at requested points. \nYou can set it to automatically checkpoint at a certain interval. In H1, \nit&#39;s via a system property; see Checkpointer.initialize(). In H3, it&#39;s \nCheckpointService&#39;s checkpointIntervalMinutes property. In H1, the crawl \nmust reach a full pause for a checkpoint to occur; with long downloads \nand connection timeouts, this can mean a slowdown in the tens of \nminutes. In H3, a checkpoint requires a much narrower lock, so may only \ntake a few seconds or a minute or two, and long network fetches may \ncontinue during the checkpoint.\n\nIn both cases, you need to retain the &#39;state&#39; directory files \n(.JDB/.DEL) corresponding to the checkpoints from which you might want \nto resume. (If not needed for a checkpoint, you can freely delete the \n.DELs.) Resuming from an earlier checkpoint may foul any other \nsubsequent-but-unused checkpoints (though future checkpoints should be \nfine).\n\nThe checkpointing system has always been a bit rough but I have more \nconfidence in its flexibility and speed in H3. I would not yet count on \nit for perfect resumability in a large crawl without more experience \nusing it. If not using checkpoints, or checkpoints fail for some reason, \nan approximation of a frontier&#39;s state at the time of a crash can be \nrecreated from the &#39;frontier recovery log&#39; also kept by the crawler. \n(Not all running stats/state can be reconstructed, but essentially all \nthe same pending URIs will be reenqueued.)\n\nI believe there&#39;s a JMX call in H1 to request a checkpoint, and in H3 \nit&#39;s just one of the web UI/web service calls easy to trigger by a web hit:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Heritrix3.0APIGuide-CheckpointJob\n\nHope this helps!\n\n- Gordon @ IA\n\n\n"}}