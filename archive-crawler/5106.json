{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":202034705,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"DolUBN92LkmbUs57qUbyYpffhSFeB2ejGG3YFQJ6461C5ALU81dIk_QcxFrJUHWjVHksx1tZ9ooQcRG-ZRgn","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Nutch or Heritrix?","postDate":"1207590973","msgId":5106,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3RkE2MDNELjUwOTA5MDNAZHVib2NlLm5ldD4=","inReplyToHeader":"PDJlNjdmNWIwMDgwNDA3MDkxOWw4ZGQ2ZTQyaDc0OTBhZWRhMzFmY2JhNTdAbWFpbC5nbWFpbC5jb20+","referencesHeader":"PDJlNjdmNWIwMDgwNDA1MDYzNnU5NDZmMTdjeDIwZDVmZDQ3NDE5M2RiMzdAbWFpbC5nbWFpbC5jb20+CSA8ZnRkZTZnK2hsaTNAZUdyb3Vwcy5jb20+IDwyZTY3ZjViMDA4MDQwNzA5MTlsOGRkNmU0Mmg3NDkwYWVkYTMxZmNiYTU3QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5105,"nextInTopic":0,"prevInTime":5105,"nextInTime":5107,"topicId":5099,"numMessagesInTopic":6,"msgSnippet":"... No.  HCC is just a simple tool for addressing a herd of heritrice as one; start/stop/monitor, etc. If you look back over the heritrix archives, you ll see","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 97677 invoked from network); 7 Apr 2008 17:59:47 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m48.grp.scd.yahoo.com with QMQP; 7 Apr 2008 17:59:47 -0000\r\nX-Received: from unknown (HELO dns.duboce.net) (63.203.238.117)\n  by mta18.grp.scd.yahoo.com with SMTP; 7 Apr 2008 17:59:47 -0000\r\nX-Received: by dns.duboce.net (Postfix, from userid 1008)\n\tid 1A27FC563; Mon,  7 Apr 2008 09:32:36 -0700 (PDT)\r\nX-Spam-Checker-Version: SpamAssassin 3.1.4 (2006-07-26) on dns.duboce.net\r\nX-Spam-Level: \r\nX-Spam-Status: No, score=-3.5 required=5.0 tests=AWL,BAYES_00 autolearn=ham \n\tversion=3.1.4\r\nX-Received: from durruti.desk.hq.powerset.com (durruti.desk.hq.powerset.com [208.84.6.135])\n\tby dns.duboce.net (Postfix) with ESMTP id 10FDBC51B\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  7 Apr 2008 09:31:59 -0700 (PDT)\r\nMessage-ID: &lt;47FA603D.5090903@...&gt;\r\nDate: Mon, 07 Apr 2008 10:56:13 -0700\r\nUser-Agent: Thunderbird 2.0.0.12 (Macintosh/20080213)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;2e67f5b00804050636u946f17cx20d5fd474193db37@...&gt;\t &lt;ftde6g+hli3@...&gt; &lt;2e67f5b00804070919l8dd6e42h7490aeda31fcba57@...&gt;\r\nIn-Reply-To: &lt;2e67f5b00804070919l8dd6e42h7490aeda31fcba57@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: Nutch or Heritrix?\r\nX-Yahoo-Group-Post: member; u=202034705; y=nURtL8F5bH4JP-ZyPV1eNCA-XZHkDnQdI4K6C0JG8MPJQ2TpCltqJfH8\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nSvein Yngvar Willassen wrote:\n&gt;\n&gt; ..\n&gt;\n&gt; &gt; &gt; - Which crawler will best adapt to a distributed crawling system, in\n&gt; &gt; which we\n&gt; &gt; &gt; use many servers conducting crawling together?\n&gt; &gt;\n&gt; &gt; There is an open source project that does distributed control of\n&gt; &gt; Heritrix 1.12. I don&#39;t remember the name of it.\n&gt;\n&gt; Heritrix Cluster Controller (hcc)? I&#39;ve had a look at it, but must\n&gt; admit I didn&#39;t understand how it works from the documentation. Does it\n&gt; allow controlling a cluster of Heritrix instances in such as way that\n&gt; an URL is fetched only once by one computer in the cluster? That seems\n&gt; to be the most important property of clustered crawling. Perhaps you\n&gt; or someone else can fill me in here.\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.  HCC is just a simple tool for addressing a herd of heritrice as \none; start/stop/monitor, etc.\n\nIf you look back over the heritrix archives, you&#39;ll see descriptions of \nhow-to do big crawls using more than one heritrix instance.  The general \nscheme is that the crawl space is divided amongst crawlers.  A crawler \nwill only crawl URLs that fit its subscope.  URLs that fall outside this \nset are saved to disk.  The file on a period is sorted and URLs inserted \nto the appropriate running heritrix instances.\n\nOn Nutch vs. Heritrix, high-level comparison is hard to do in a quick \nemail.   I would suggest you outline what you want of a crawler, then \ndig in to both and ask on both lists particular questions about features \nor functionality you need.  Here are a (few) dimensions that may help \nwith your investigation:\n\n+ The Nutch crawler operates in a stepped, batch fashion.  It runs \nthrough a list of generated URLs fetching each individual URL until the \nlist is done.  You then generate the next list to run after running \nanalysis of the most recent fetch.  Heritrix just runs fetching until it \nruns out of URLs that fit its defined scope.\n+ The Nutch crawler is a MapReduce job.  This means that distribution is \njust a matter of adding nodes and tasks (Tasks are retried if they fail, \netc.).  Heritrix distribution is the divvying up of the crawl-space \noutlined above (but from what I&#39;ve heard, on big crawls folks just don&#39;t \nbother with the sort-and-insert step reasoning that eventually every \nindividual crawler will trip over its URLs if its let run long enough).  \nIf a heritrix instance crashes, recovery is manual involving either the \nrerunning of a pseudo-crawl &#39;journal&#39; or revivification using the last \n&#39;checkpoint&#39;.\n+ Heritrix has more crawling features -- knobs and switches -- than the \ncrawler in Nutch and it is more &#39;dogged&#39; about fetching content than \nNutch given its roots in an Archiving organization.\n\nThe latter feature you may not want and regards the former, its easy \nenough adding whats missing given Nutch is pluggable, open source.\n\nIf you have hadoop in your mix, you might consider crawling straight \ninto DFS using the zevents writer: \nhttp://www.zvents.com/labs/heritrix_hadoop.\n\nSt.Ack\n\n\n\n\n\n\n\n"}}