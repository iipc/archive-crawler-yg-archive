{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"TKml-mufdR2_4HNd1t-5l4ogTKCtRmX09ZJcXgpiBknc9dGFxEJsXOsht0L6nbx3LyldPKv3u2wvmG2QttzubzO0nDxcx3Y","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Improving Crawling Speeds","postDate":"1194451737","msgId":4666,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3MzFFMzE5LjYwNTA4MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDhGMTE3MjJBMDU2MkJCNEY4MEE2ODBFRDRDRkVEMEQ1MEM4MjY5QEVWU0JORzAyLmFkLm9mZmljZS5hb2wuY29tPg==","referencesHeader":"PDhGMTE3MjJBMDU2MkJCNEY4MEE2ODBFRDRDRkVEMEQ1MEM4MjY5QEVWU0JORzAyLmFkLm9mZmljZS5hb2wuY29tPg=="},"prevInTopic":4663,"nextInTopic":0,"prevInTime":4665,"nextInTime":4667,"topicId":4648,"numMessagesInTopic":4,"msgSnippet":"600Kbps isn t very much bandwidth for crawling: it would allow 75KB to be collected per second. We often find content to average near 20KB per URL (larger for","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 86778 invoked from network); 7 Nov 2007 16:08:57 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m52.grp.scd.yahoo.com with QMQP; 7 Nov 2007 16:08:57 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta16.grp.scd.yahoo.com with SMTP; 7 Nov 2007 16:08:57 -0000\r\nX-Received: (qmail 44740 invoked from network); 7 Nov 2007 16:08:56 -0000\r\nX-Received: from unknown (HELO ?10.0.10.102?) (unknown)\n  by unknown with SMTP; 7 Nov 2007 16:08:56 -0000\r\nX-pair-Authenticated: 70.137.138.31\r\nMessage-ID: &lt;4731E319.6050805@...&gt;\r\nDate: Wed, 07 Nov 2007 08:08:57 -0800\r\nUser-Agent: Thunderbird 2.0.0.6 (Windows/20070728)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;8F11722A0562BB4F80A680ED4CFED0D50C8269@...&gt;\r\nIn-Reply-To: &lt;8F11722A0562BB4F80A680ED4CFED0D50C8269@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Improving Crawling Speeds\r\nX-Yahoo-Group-Post: member; u=137285340; y=C6Mco4Bb1nGjObfKwCFafyo957bjBvaZzmOrV9Ys0jdp\r\nX-Yahoo-Profile: gojomo\r\n\r\n600Kbps isn&#39;t very much bandwidth for crawling: it would allow 75KB to \nbe collected per second. We often find content to average near 20KB per \nURL (larger for some crawls) -- on such a crawl 3-4 per second would be \nthe best you could collect.\n\nIf your bandwidth is in fact larger:\n\nOnce the crawl has been running for a while, what does the web UI \nconsole show as the &#39;congestion ratio&#39;?\n\nAlso, what does the 1-line summary of the frontier queues (on the \n&#39;reports&#39; page) show? (That is, the line like: &quot;43 queues: 17 active (10 \nin-process; 3 ready; 4 snoozed); 26 inactive; 0 retired; 0 exhausted&quot;)\n\nYou could be running with more threads -- at least 50 -- but usually \nconcerns that a crawl is &quot;too slow&quot; turn out to be because the \ncombination of number of target sites available plus crawler politeness \nsettings (one URL at a time, with a pause between each) won&#39;t let the \ntarget material be collected any faster. If there are no &#39;inactive&#39; or \n&#39;ready&#39; queues in the above line -- all queues are &#39;in-process&#39; or \n&#39;snoozed&#39; -- then this is the case.\n\nIt&#39;s also possible the target sites are slow, or the content oversized \nfor your bandwidth. Lines in the crawl.log will indicate if this is the \ncase, with the byte size of each resource retrieved and the \n&quot;timestamp+duration&quot; notation indicating if each fetch is taking a long \ntime.\n\nHope this helps,\n\n- Gordon @ IA\n\n\nGoel, Ankur wrote:\n&gt; \n&gt; Hi,\n&gt;      Can anyone suggest a sample configuration and typical crawling \n&gt; speed observed\n&gt; using Heritrix 1.12.1 on a single box setup having 1 GB of RAM trying t \n&gt; crawl multiple\n&gt; domain.\n&gt;  \n&gt; I am finding my instance to be pretty slow at 3-4 URI&#39;s per second given \n&gt; the fact that\n&gt; \n&gt;     *\n&gt;       Network bandwidth in my environment is pretty ample ( averaging\n&gt;       at 600 Kbps).\n&gt;     *\n&gt;       I make sure that no other memory intensive processes are running\n&gt;       on the machine running the crawler.\n&gt;     *\n&gt;       The number of running Toe Threads are 10. \n&gt; \n&gt; Is it a problem with the current architecture where each Toe Thread is \n&gt; doing synchronous I/O\n&gt; or there is something that I am missing which can speed up the crawl ?\n&gt;  \n&gt; Thanks\n&gt; -Ankur\n&gt; \n&gt; *From:* archive-crawler@yahoogroups.com \n&gt; [mailto:archive-crawler@yahoogroups.com] *On Behalf Of *Goel, Ankur\n&gt; *Sent:* Friday, November 02, 2007 12:22 PM\n&gt; *To:* archive-crawler@yahoogroups.com\n&gt; *Subject:* [archive-crawler] Improving Crawling Speeds\n&gt; \n&gt; Hi Folks,\n&gt;             I am using Heritrix 1.12.1 to crawl 500,000 thousand URLs.\n&gt; The setup has 2 machines with 1 GB RAM, each running a crawler\n&gt; instance with 10 Toe Threads.\n&gt;  \n&gt; I am observing an avg crawl speed of 3 - 4 URI&#39;s per second per box\n&gt; which is ok considering the built-in politeness.\n&gt;  \n&gt; If I were to crawl a particular domain RELAXING poiliteness\n&gt; how fast can I crawl ?\n&gt;  \n&gt; Ideally I would like to see a crawl speed of 50 - 100 URIs per second\n&gt; per box.\n&gt;  \n&gt; In other words I would like to know the bottlenecks and known workarounds\n&gt; to fast crawling and if none exist then I would like to share ideas on\n&gt; future works for improving crawling speeds.\n&gt;  \n&gt;  \n&gt; -Ankur\n&gt; \n&gt; \n\n"}}