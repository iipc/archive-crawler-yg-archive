{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"-F82qIgACTD365r76F2AwiRYftHQLnzpXJvYMlcaqoxPDrR4e5gRQCA3R9Ng01BkCqBPvkUpnyPclkc9HCD-4iLAFHd23uNE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Handling identical documents from different sites.","postDate":"1131565882","msgId":2348,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzNzI1MzNBLjIwNDAwQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDQ5NGFmZTFjMDUxMTA5MDY1MWk3ZDI4ZTM5MHZiMGVmOTlhOGM1MmM2NjM1QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGRrdDB1dStlNGEyQGVHcm91cHMuY29tPiA8NDk0YWZlMWMwNTExMDkwNjUxaTdkMjhlMzkwdmIwZWY5OWE4YzUyYzY2MzVAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":2345,"nextInTopic":2350,"prevInTime":2347,"nextInTime":2349,"topicId":2342,"numMessagesInTopic":8,"msgSnippet":"Think in terms of global and local scope. If I want to crawl entire .yu domain, for example, I would set global scope to include all URIs from .yu domain. Each","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 7433 invoked from network); 9 Nov 2005 19:55:33 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m25.grp.scd.yahoo.com with QMQP; 9 Nov 2005 19:55:33 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta5.grp.scd.yahoo.com with SMTP; 9 Nov 2005 19:55:33 -0000\r\nReceived: (qmail 1297 invoked by uid 100); 9 Nov 2005 19:52:52 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.5?) (igor@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 9 Nov 2005 19:52:52 -0000\r\nMessage-ID: &lt;4372533A.20400@...&gt;\r\nDate: Wed, 09 Nov 2005 11:51:22 -0800\r\nUser-Agent: Mozilla Thunderbird 1.0.6 (Windows/20050716)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;dkt0uu+e4a2@...&gt; &lt;494afe1c0511090651i7d28e390vb0ef99a8c52c6635@...&gt;\r\nIn-Reply-To: &lt;494afe1c0511090651i7d28e390vb0ef99a8c52c6635@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-92.3 required=7.0 tests=AWL,DOMAIN_BODY,\n\tUSER_IN_WHITELIST autolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Handling identical documents from different\n sites.\r\nX-Yahoo-Group-Post: member; u=137477665; y=ig0Pnw3DBoxRxxvDBkaISL9akPrseQmPlKVFlPhem9Nj4dMsgQ\r\nX-Yahoo-Profile: iranitovic\r\n\r\nThink in terms of global and local scope.\nIf I want to crawl entire .yu domain, for example, I would set global scope to include all URIs from \n.yu domain. Each machine/node in this multi-machine crawl will have this global scope.\nNow, on the node level there will be a local scope (subscope of .yu domain)\nSo, each node in the crawl is responsible only for the part of the global scope.\nAlso each node knows how to handle URIs that are in the global but not in local scope (see \nhttp://crawler.archive.org/apidocs/org/archive/crawler/processor/CrawlMapper.html)\n\nTherefore, as long as your local scopes don&#39;t overlap, major content duplication is avoid. Same goes \nfor politeness.\ni.\n\n\nVishwesh Thakur wrote:\n&gt; Hello,\n&gt;  Running a set of heritrix instances in parallel introduces additional \n&gt; problems/requirements;\n&gt;   1. How do you remain polite to web sites?\n&gt;   2. How do you avoid doing redundant work in multiple instances? \n&gt; (Fetching the same page in multiple instances)\n&gt; \n&gt; Proposals on enabling Heritrix for multimachine/clustered crawl can be \n&gt; seen on http://crawler.archive.org/cgi-bin/wiki.pl?MultimachineCrawl.\n&gt; \n&gt; If you do not want to have two copies of a document all you need is a \n&gt; &quot;document hash: doc url&quot; collection. You can have a berkely db that \n&gt; stores &quot;document hash: doc url&quot; map and use it to see if you have the \n&gt; same content already.\n&gt; \n&gt; Regards\n&gt; Thakur\n&gt; \n&gt; \n&gt; On 11/9/05, *callforshadab* &lt;callforshadab@... \n&gt; &lt;mailto:callforshadab@...&gt;&gt; wrote:\n&gt; \n&gt;     Hi all,\n&gt; \n&gt;     While performing Incremental Crawling, it seems like Heritrix checks\n&gt;     the current queue, if a document has already been fetched (by checking\n&gt;     its content hash) but does not check if the same document has been\n&gt;     fetched from another website.\n&gt; \n&gt;     How should this be handled in the clustered version (multiple machines\n&gt;     running Heritrix)?\n&gt; \n&gt;     For example: Some mirror sites usually have the same content but\n&gt;     different urls.\n&gt; \n&gt;     These URLs would be waiting in different queues.\n&gt; \n&gt;     Each queue will first fetch a document.\n&gt; \n&gt;     Then, create a hash of their contents and check the value against the\n&gt;     saved hash values in CrawlURI objects only in their respective queues.\n&gt;     It doesnt check if the other queues already contain a CrawlURI object\n&gt;     with the same hashvalue.\n&gt; \n&gt;      \n&gt;     So, how should this be handled ?\n&gt; \n&gt;     With regards,\n&gt;     Shadab\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;     ------------------------------------------------------------------------\n&gt;     YAHOO! GROUPS LINKS\n&gt; \n&gt;         *  Visit your group &quot;archive-crawler\n&gt;           &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;            \n&gt;         *  To unsubscribe from this group, send an email to:\n&gt;             archive-crawler-unsubscribe@yahoogroups.com\n&gt;           &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;            \n&gt;         *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;           Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; \n&gt; \n&gt;     ------------------------------------------------------------------------\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; -- \n&gt; Thakur Vishwesh Singh\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt; \n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; \n&gt; \n&gt; ------------------------------------------------------------------------\n&gt; \n\n\n"}}