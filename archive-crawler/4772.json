{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"ilts4i6agqK0W-cMZB81StathmGlzrjUG8xwx9zg4UALXwf6RbtpdvvephQx4JjAK-ZnwPfkHQdWXjpvZ-UgVz3pxctLsFTA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] crawl just one page","postDate":"1197038585","msgId":4772,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3NTk1QkY5LjkwMDA5MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIwMDcxMjA3MDg0MjQ1LkdHNTIxOEBzbXRwLmNhc2FibGFuY2EuY3o+","referencesHeader":"PDIwMDcxMjA3MDg0MjQ1LkdHNTIxOEBzbXRwLmNhc2FibGFuY2EuY3o+"},"prevInTopic":4771,"nextInTopic":4783,"prevInTime":4771,"nextInTime":4773,"topicId":4771,"numMessagesInTopic":3,"msgSnippet":"Hi Robert, You can simply setup the rejectIfTooManyHops (TooManyHopsDecideRule) to 0. That will ensure that only seeds are fetched. Keep in mind that Heritrix","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 15292 invoked from network); 7 Dec 2007 14:43:17 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m44.grp.scd.yahoo.com with QMQP; 7 Dec 2007 14:43:17 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta17.grp.scd.yahoo.com with SMTP; 7 Dec 2007 14:43:17 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 152EC47971\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri,  7 Dec 2007 06:50:18 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id agdH28+wv1Lm for &lt;archive-crawler@yahoogroups.com&gt;;\n\tFri,  7 Dec 2007 06:50:16 -0800 (PST)\r\nX-Received: from [127.0.0.1] (nor75-24-88-170-99-175.fbx.proxad.net [88.170.99.175])\n\tby mail.archive.org (Postfix) with ESMTP id DD2244797D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri,  7 Dec 2007 06:50:15 -0800 (PST)\r\nMessage-ID: &lt;47595BF9.9000906@...&gt;\r\nDate: Fri, 07 Dec 2007 06:43:05 -0800\r\nUser-Agent: Thunderbird 2.0.0.9 (Windows/20071031)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;20071207084245.GG5218@...&gt;\r\nIn-Reply-To: &lt;20071207084245.GG5218@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] crawl just one page\r\nX-Yahoo-Group-Post: member; u=137477665; y=GkEybah7Oq7ugX2pIDVCoOVLqBXN2p3YB4eF6hKTXsQGI2KWAA\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Robert,\n\nYou can simply setup the rejectIfTooManyHops (TooManyHopsDecideRule) to \n0. That will ensure that only seeds are fetched.\n\nKeep in mind that Heritrix will get dns and robots.txt records. If you \ndon&#39;t want to store those, setup a decide rule on the ARCWriter.\n\nTake care,\ni.\n\n&gt; Hi all,\n&gt; \n&gt; I&#39;d like to be able to start crawl which would basically be\n&gt; restricted to only the seed page.\n&gt; \n&gt; So for example, seed is http://aaa.com/bbb.html. I&#39;d like to\n&gt; be able to get ARC file with only this (bbb.html) document.\n&gt; \n&gt; Is it possible to setup such crawl? I already tried... no\n&gt; success.\n&gt; \n&gt; TIA,\n&gt; Robert\n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}