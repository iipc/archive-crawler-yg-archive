{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"xnAFAvayV-zNOQtiPr58cuZJwz800-837cTldiEH5u5iLTao-uY454gdbW4Cos7JTg2Fb2wF0aT3wS2R_jT2Hw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Best approach to 7M seeds","postDate":"1124304167","msgId":2117,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzMDM4NTI3LjgwMjAxMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDlkMWU0NTI1MDUwODE3MTAyMzQ4Y2Q2MmE4QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PDlkMWU0NTI1MDUwODE3MTAyMzQ4Y2Q2MmE4QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2116,"nextInTopic":2137,"prevInTime":2116,"nextInTime":2118,"topicId":2116,"numMessagesInTopic":25,"msgSnippet":"... Handling big lists of seeds -- in the millions -- needs work on our part (We have an RFE: [944987] Support case where millions of seeds to address this","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 50113 invoked from network); 17 Aug 2005 18:52:29 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m33.grp.scd.yahoo.com with QMQP; 17 Aug 2005 18:52:29 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta3.grp.scd.yahoo.com with SMTP; 17 Aug 2005 18:52:29 -0000\r\nReceived: (qmail 8187 invoked by uid 100); 17 Aug 2005 18:52:03 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.8?) (stack@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 17 Aug 2005 18:52:03 -0000\r\nMessage-ID: &lt;43038527.8020101@...&gt;\r\nDate: Wed, 17 Aug 2005 11:42:47 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.8) Gecko/20050513 Debian/1.7.8-1\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;9d1e4525050817102348cd62a8@...&gt;\r\nIn-Reply-To: &lt;9d1e4525050817102348cd62a8@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-89.0 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Best approach to 7M seeds\r\nX-Yahoo-Group-Post: member; u=168599281; y=YiyGq63KOmlpfE09e65GiLqvRV6PKZp4P19K_Y5NeLX4330A-_uRmGiu\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nMatt Ittigson wrote:\n\n&gt; I have 7 million seeds I&#39;d like to spider.  What&#39;s the best approach?\n\nHandling big lists of seeds -- in the millions -- needs work on our part \n(We have an RFE: &#39;[944987] Support case where millions of seeds&#39; to \naddress this issue).  Its caused us trouble in the past usually in the \nform of OOME soon after startup.\n\n&gt;\n&gt; My current method has been running a SurtPrefixScope with a\n&gt; BdbFrontier.  I use a QueueTotalBudget of 200 (there are usually more\n&gt; than 200 seeds per domain), with a HostnameQueueAssignmentPolicy, and\n&gt; UnitCostAssignmentPolicy.  In order to keep the memory use down and to\n&gt; allow me to get as many seeds spidered as possible, my current\n&gt; approach has been using the JMX client to slowly feed seeds into the\n&gt; system (1500 at a time), watching for when the current speed drops and\n&gt; frontiers are being completed before adding another batch of seeds.\n&gt; In a previous JMX client version, there was a flag somewhat akin to\n&gt; force download of seeds, which allowed me to continue adding seeds for\n&gt; queues that were already retired (at least, I think), though that&#39;s\n&gt; either no longer an option, or no longer working with my current JMX\n&gt; wrapping application (which I haven&#39;t updated for the JMX client\n&gt; 0.10.4 from 0.8).\n\nYour piecemeal addition of seeds is an interesting approach.\n\nYou can just go get the newer cmdline jmx client.  It has some \nimprovements and minor bug fixes.  It has no dependencies on Heritrix so \nyou can just grab it from \nhttp://crawler.archive.org/cmdline-jmxclient/downloads.html.\n\nIf your Heritrix is a recent build -- one from last few days, post build \n687  -- the JMX API has changed (And Heritrix is likely not as stable -- \na recent big commit that allows for the running of multiple instances of \nHeritrix within a single JVM has probably introduced bugs, at least in \nthe UI).  The Hertrix JMX MBean now includes the host -- \n&#39;org.archive.crawler:host=debord,name=Heritrix,type=Service&#39; or \n&#39;org.archive.crawler:host=crawling009.archive.org,name=gordonInstance,type=Service&#39; \n.  The format of the current crawl job bean name has also changed.  Its \nnow &#39;org.archive.crawler:name=20050816230622727,type=CrawlJob&#39; where it \nused be &#39;org.archive.crawler:name=CurrentJob,type=CrawlJob&#39;.  The \noperation API for the insert of seeds should be the same though.  Here&#39;s \ndescription:\n\n importUri: Add passed URL to the frontier\n  Parameters 3, return type=java.lang.Void\n   name=url type=java.lang.String URL to add to the frontier\n   name=forceFetch type=java.lang.Boolean True if URL is to be force fetched\n   name=seed type=java.lang.Boolean True if URL is a seed\n\nOR\n\n importUris: Add file of passed URLs to the frontier\n  Parameters 3, return type=java.lang.String\n   name=pathOrUrl type=java.lang.String Path or URL to file of URLs\n   name=style type=java.lang.String Format \nformat:default|crawlLog|recoveryJournal\n   name=forceFetch type=java.lang.Boolean True if URLs are to be force \nfetched\n\n&gt;\n&gt; I have a custom WriteProcessor which outputs statistics about the\n&gt; downloaded pages needed for my application.  I&#39;m only interested in\n&gt; getting the actual HTML, not in any binary types (I already have\n&gt; various filters in place to faciliate this).  The machine is a dual\n&gt; processor Athlon 64, running CentOS 4, with 2G of ram.  Its sitting on\n&gt; a rack where I have permission to pull down 15Mbs.\n&gt;\n&gt; Just hoping to get some feedback from the Heritrix pros about whether\n&gt; I&#39;m taking the right approach to solving my problem.  My first goal is\n&gt; to spider all 7 million seeds.  The second goal would be to do some\n&gt; spidering of on-domain pages linked to by those seeds (to discover\n&gt; possible new seeds).\n&gt;\nYour approach sounds fine to me (Gordon and Igor might have further \nsuggestions). You might keep the list up to date with your crawls&#39; progress.\n\nYours,\nSt.Ack\n\n&gt; Thanks in advance for your help.\n&gt;\n&gt; -matt\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}