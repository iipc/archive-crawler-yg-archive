{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":465980601,"authorName":"Zach Bailey","from":"Zach Bailey &lt;zach.bailey@...&gt;","replyTo":"LIST","senderId":"dgwLBR8cJQTrKCzb6PPF3zys7jhOFCQXzwrJ_ShvOq93KQ-pUngSwXqJK1XmlO7p1yUv2teaclp6puraoEEKqJpjUrrg_v3cC1QRgRg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Storing Crawl Results in Alternate (non-FS) Store","postDate":"1284744402","msgId":6735,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEFBTkxrVGk9am1FV3c9R0ZGZGZ0S1RYZlk4MzgrUGRfTDVVc2NtVmpzRz1KdkBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PDRDOTA0NzA0LjkwNzAyMDJAYXJjaGl2ZS5vcmc+","referencesHeader":"PEFBTkxrVGk9YWJnRktrTE1FLXlPZWpYYzFFT2hWWmlKQUw5bzExb01IVzZWNEBtYWlsLmdtYWlsLmNvbT4JPDRDOEZEQUEyLjEwNDA0MDdAYXJjaGl2ZS5vcmc+CTxBQU5Ma1RpPWhrdko5UVZTOWNRR1hFeno2QkZZUTBXUDZldj14XzZISnpXN0xAbWFpbC5nbWFpbC5jb20+CTw0QzkwNDcwNC45MDcwMjAyQGFyY2hpdmUub3JnPg=="},"prevInTopic":6727,"nextInTopic":6736,"prevInTime":6734,"nextInTime":6736,"topicId":6723,"numMessagesInTopic":6,"msgSnippet":"Thanks for your invaluable feedback as usual Gordon! Just wanted to throw an update out there for anyone interested. I now successfully have heritrix 3","rawEmail":"Return-Path: &lt;zach.bailey@...&gt;\r\nX-Sender: zach.bailey@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 89036 invoked from network); 17 Sep 2010 17:26:43 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m16.grp.re1.yahoo.com with QMQP; 17 Sep 2010 17:26:43 -0000\r\nX-Received: from unknown (HELO mail-ww0-f42.google.com) (74.125.82.42)\n  by mta2.grp.re1.yahoo.com with SMTP; 17 Sep 2010 17:26:43 -0000\r\nX-Received: by wwb31 with SMTP id 31so1247352wwb.5\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 17 Sep 2010 10:26:43 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.216.234.93 with SMTP id r71mr1039526weq.104.1284744402955;\n Fri, 17 Sep 2010 10:26:42 -0700 (PDT)\r\nX-Received: by 10.216.47.7 with HTTP; Fri, 17 Sep 2010 10:26:42 -0700 (PDT)\r\nIn-Reply-To: &lt;4C904704.9070202@...&gt;\r\nReferences: &lt;AANLkTi=abgFKkLME-yOejXc1EOhVZiJAL9o11oMHW6V4@...&gt;\n\t&lt;4C8FDAA2.1040407@...&gt;\n\t&lt;AANLkTi=hkvJ9QVS9cQGXEzz6BFYQ0WP6ev=x_6HJzW7L@...&gt;\n\t&lt;4C904704.9070202@...&gt;\r\nDate: Fri, 17 Sep 2010 13:26:42 -0400\r\nMessage-ID: &lt;AANLkTi=jmEWw=GFFdftKTXfY838+Pd_L5UscmVjsG=Jv@...&gt;\r\nTo: Gordon Mohr &lt;gojomo@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=00151758a95ec3eca9049077dd16\r\nFrom: Zach Bailey &lt;zach.bailey@...&gt;\r\nSubject: Re: [archive-crawler] Storing Crawl Results in Alternate (non-FS) Store\r\nX-Yahoo-Group-Post: member; u=465980601\r\n\r\n\r\n--00151758a95ec3eca9049077dd16\r\nContent-Type: text/plain; charset=ISO-8859-1\r\n\r\nThanks for your invaluable feedback as usual Gordon!\n\nJust wanted to throw an update out there for anyone interested. I now\nsuccessfully have heritrix 3 crawling and storing data into HDFS - went this\nroute because it is (for my use case) the best solution as we will be\nimmediately wanting to run various M/R jobs over the fetched data.\n\nI suspect this is a rather common use case and as such happy to answer\nquestions about getting it set up in the future. The library I&#39;m using to\nget heritrix putting its crawl data in hadoop&#39;s HDFS is available here:\nhttp://github.com/openplaces/heritrix-hdfs-writer - despite the out of date\ndocumentation it has been updated to work with heritrix 3. I got it working\nlocally with a test crawl running hadoop in pseudo-distributed mode.\n\nBut finally, I had one last question related to setting some crawl\nparameters. For our use case we&#39;re not very interested in making sure we\nhave a full, &quot;deep&quot; crawl of our seed sites. We&#39;re more interested in a\nshallow crawl (e.g. the initial home page and maybe 3-5 &quot;hops&quot; from there).\n\nAm I correct in thinking that to get this behavior all I need to do is\nratchet the TooManyHopsDecideRule.maxHops back to the desired value and that\nwill give me the sort of behavior I&#39;m looking for?\n\nBest,\n-Zach\n\nOn Wed, Sep 15, 2010 at 12:09 AM, Gordon Mohr &lt;gojomo@...&gt; wrote:\n\n&gt; On 9/14/10 3:08 PM, Zach Bailey wrote:\n&gt;\n&gt;&gt; Thanks a lot for your answers, Gordon.\n&gt;&gt;\n&gt;&gt; I am absolutely planning on contributing this back for others to build\n&gt;&gt; on and use, once I figure out how to make it generic enough to be useful\n&gt;&gt; to others. More than likely I will put it up on my public github account.\n&gt;&gt;\n&gt;&gt; A follow-up question - am I correct in assuming my implementation of\n&gt;&gt; Processor#shouldProcess should be pretty much identical to the one in\n&gt;&gt; WriterPoolProcessor?\n&gt;&gt;\n&gt;\n&gt; Sure. In fact I just committed a shorter version getting rid of the\n&gt; nonsensical type check (cruft left over from a previous refactoring).\n&gt;\n&gt; Of course, if you were interested in writing something in your store even\n&gt; for nonpositive fetch codes (such as connection-failed errors), you&#39;d have\n&gt; to loosen the first test. And if there were some theoretical URI type used\n&gt; in your crawls whose successful &#39;fetch&#39; left a zero recorded-content length\n&gt; (perhaps an inline &#39;data:&#39; URI?) but still needed to be written to your\n&gt; store, the second test would need to loosen. But the tests are reasonable\n&gt; for usual needs analogous to WARC writing.\n&gt;\n&gt;\n&gt;  Also, what is the best way to get at the raw page content separate from\n&gt;&gt; the headers (just the content, not the entire response)? Am I correct in\n&gt;&gt; thinking that this is what I would be provided when calling\n&gt;&gt; crawlURI.getRecorder().getRecordedInput().getContentReplayInputStream() ?\n&gt;&gt;\n&gt;\n&gt; For usual HTTP response content, yes -- that will give an InputStream cued\n&gt; up to just past the protocol headers. It does *not* handle &#39;chunked&#39;\n&gt; encodings, which may be a concern. If your later analysis might want the\n&gt; headers, you will want to write them somewhere too. Our goal in the WARC\n&gt; (and earlier ARC) writers was to record verbatim what came over the wire, to\n&gt; support the greatest range of possible future analysis applications. (We do\n&gt; no decoding; no dechunking; no canonicalizing; no header corrections or\n&gt; other alterations; etc.)\n&gt;\n&gt; You can also use getReplayCharSequence(), with an optional specified\n&gt; encoding, to get a charset-decoded CharSequence of the content-body (which\n&gt; uses temp files and a sliding buffer to avoid decoding giant content into\n&gt; memory). It also does not handled dechunking properly.\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt;\n&gt;  Thanks,\n&gt;&gt; -Zach\n&gt;&gt;\n&gt;&gt; On Tue, Sep 14, 2010 at 4:27 PM, Gordon Mohr &lt;gojomo@...\n&gt;&gt; &lt;mailto:gojomo@...&gt;&gt; wrote:\n&gt;&gt;\n&gt;&gt;    Great questions! I know there&#39;s other interest in such writer\n&gt;&gt;    alternatives, so I hope you can make whatever you come up with\n&gt;&gt;    available to other users, or donate it for possible inclusion in the\n&gt;&gt;    official H3 distribution!\n&gt;&gt;\n&gt;&gt;    There also has been previous work to create an HBaseWriter for\n&gt;&gt;    Heritrix; it might already work in H3, or require only a little\n&gt;&gt;    updating. I haven&#39;t yet tried it myself. See:\n&gt;&gt;\n&gt;&gt;    http://code.google.com/p/hbase-writer/\n&gt;&gt;\n&gt;&gt;    On to your questions, interspersed below:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    On 9/14/10 12:39 PM, Zach Bailey wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;        Hi all,\n&gt;&gt;\n&gt;&gt;        I wanted to run some questions by you and maybe get some\n&gt;&gt;        pointers from\n&gt;&gt;        the architects of heritrix hoping I could save a little time before\n&gt;&gt;        attempting this.\n&gt;&gt;\n&gt;&gt;        What I am hoping to do is develop a custom writer to store the\n&gt;&gt;        results\n&gt;&gt;        of my crawls in a distributed database, something like Riak,\n&gt;&gt;        Cassandra,\n&gt;&gt;        or HBase for the Heritrix 3.0 code base.\n&gt;&gt;\n&gt;&gt;        I am very comfortable writing Java code and am very familiar with\n&gt;&gt;        building applications using the Spring Framework so I&#39;m thinking\n&gt;&gt;        this\n&gt;&gt;        should be a relatively easy task to get a proof of\n&gt;&gt; concept/prototype\n&gt;&gt;        working.\n&gt;&gt;\n&gt;&gt;        Looking through the code I had some questions:\n&gt;&gt;\n&gt;&gt;        1.) It looks like I will wire in my custom code inside the\n&gt;&gt;        DispositionChain, essentially replacing where the warcWriter bean\n&gt;&gt; is\n&gt;&gt;        referenced.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    Yes; you could also have both in place for testing. (I&#39;ve often run\n&gt;&gt;    with both our ARCWriterProcessor and WARCWriterProcessor while\n&gt;&gt;    debugging things.) One common gotcha when installing your own\n&gt;&gt;    Processors, as edits to our model CXML, is that if you follow the\n&gt;&gt;    pattern there you need to both declare the bean with a name, then\n&gt;&gt;    add the bean by name to the chain&#39;s ordered list. So: two edits, a\n&gt;&gt;    short distance from each other, to have the intended effect.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;        2.) Looking at the WARCWriterProcessor it appears it uses a pooled\n&gt;&gt;        approach. I&#39;m guessing this is for performance reasons. Assuming\n&gt;&gt; I&#39;m\n&gt;&gt;        using a driver which already implements connection pooling, are\n&gt;&gt;        there\n&gt;&gt;        any other considerations I should think about while implementing\n&gt;&gt;        my own\n&gt;&gt;        Writer?\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    The existing pooling was motivated by the belief that having more\n&gt;&gt;    than one active open file at a time could increase throughput.\n&gt;&gt;    That&#39;s definitely the case if the files are on independent disks;\n&gt;&gt;    I&#39;m not as sure it helps to have more than one file open on the same\n&gt;&gt;    disk. This has undergone some recent simplification (eliminating the\n&gt;&gt;    dependency on Apache commons-pool) in TRUNK that will appear in\n&gt;&gt;    3.0.1, and is likely to change more in the next month as some other\n&gt;&gt;    novel policies for grouping related site captures to different WARCs\n&gt;&gt;    are added.\n&gt;&gt;\n&gt;&gt;    So I&#39;d say: keep an eye on TRUNK for ideas but don&#39;t assume anything\n&gt;&gt;    in the existing WARCWriterProcessor approach is optimal or\n&gt;&gt;    locked-in-place.\n&gt;&gt;\n&gt;&gt;    I would think writing to a remote distributed store might offer nice\n&gt;&gt;    throughput benefits by essentially fanning the IO out over many more\n&gt;&gt;    disks, whether you were writing WARCs into (eg) HDFS or individual\n&gt;&gt;    records/content-bodies into (eg) HBase.\n&gt;&gt;\n&gt;&gt;    The least-straightforward part of the current Writer is the\n&gt;&gt;    special-case handling of deduced duplicates, from headers or\n&gt;&gt;    content-hashes, to change how (or whether) individual records are\n&gt;&gt;    written. There might be an opportunity to factor that decisionmaking\n&gt;&gt;    out of WARCWriterProcessor to be shared with other non-file or even\n&gt;&gt;    non-WARC writers.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;        3.) Once I get my custom code developed, I assume all I need to\n&gt;&gt;        do to\n&gt;&gt;        make it available to heritrix is to jar it up and just drop it in\n&gt;&gt;        $HERITRIX_HOME/lib - anything else I need to be aware of there?\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    That should be all. Then, as I&#39;m sure you know, you just name the\n&gt;&gt;    classes in your crawl-configuration CXML (Spring XML)\n&gt;&gt; bean-declarations.\n&gt;&gt;\n&gt;&gt;    Hope this helps and let me know any other questions!\n&gt;&gt;\n&gt;&gt;    - Gordon @ IA\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;\n&gt;\n\r\n--00151758a95ec3eca9049077dd16\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nThanks for your invaluable feedback as usual Gordon!&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Jus=\r\nt wanted to throw an update out there for anyone interested. I now successf=\r\nully have heritrix 3 crawling and storing data into HDFS - went this route =\r\nbecause it is (for my use case) the best solution as we will be immediately=\r\n wanting to run various M/R jobs over the fetched data.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;I su=\r\nspect this is a rather common use case and as such happy to answer question=\r\ns about getting it set up in the future. The library I&#39;m using to get h=\r\neritrix putting its crawl data in hadoop&#39;s HDFS is available here:=A0&lt;a=\r\n href=3D&quot;http://github.com/openplaces/heritrix-hdfs-writer&quot;&gt;http://github.c=\r\nom/openplaces/heritrix-hdfs-writer&lt;/a&gt; - despite the out of date documentat=\r\nion it has been updated to work with heritrix 3. I got it working locally w=\r\nith a test crawl running hadoop in pseudo-distributed mode.&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;=\r\n&lt;/div&gt;&lt;div&gt;But finally, I had one last question related to setting some cra=\r\nwl parameters. For our use case we&#39;re not very interested in making sur=\r\ne we have a full, &quot;deep&quot; crawl of our seed sites. We&#39;re more =\r\ninterested in a shallow crawl (e.g. the initial home page and maybe 3-5 &qu=\r\not;hops&quot; from there).=A0&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Am I correct in thi=\r\nnking that to get this behavior all I need to do is ratchet the=A0TooManyHo=\r\npsDecideRule.maxHops back to the desired value and that will give me the so=\r\nrt of behavior I&#39;m looking for?&lt;/div&gt;\n&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Best,&lt;/div&gt;&lt;d=\r\niv&gt;-Zach&lt;/div&gt;&lt;div&gt;\n&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Wed, Sep 15, 2010 at =\r\n12:09 AM, Gordon Mohr &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:gojomo@archiv=\r\ne.org&quot; target=3D&quot;_blank&quot;&gt;gojomo@...&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;&lt;block=\r\nquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc=\r\n solid;padding-left:1ex&quot;&gt;\n\n&lt;div&gt;On 9/14/10 3:08 PM, Zach Bailey wrote:&lt;br&gt;\n=\r\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1p=\r\nx #ccc solid;padding-left:1ex&quot;&gt;\nThanks a lot for your answers, Gordon.&lt;br&gt;\n=\r\n&lt;br&gt;\nI am absolutely planning on contributing this back for others to build=\r\n&lt;br&gt;\non and use, once I figure out how to make it generic enough to be usef=\r\nul&lt;br&gt;\nto others. More than likely I will put it up on my public github acc=\r\nount.&lt;br&gt;\n&lt;br&gt;\nA follow-up question - am I correct in assuming my implement=\r\nation of&lt;br&gt;\nProcessor#shouldProcess should be pretty much identical to the=\r\n one in&lt;br&gt;\nWriterPoolProcessor?&lt;br&gt;\n&lt;/blockquote&gt;\n&lt;br&gt;&lt;/div&gt;\nSure. In fact=\r\n I just committed a shorter version getting rid of the&lt;br&gt;\nnonsensical type=\r\n check (cruft left over from a previous refactoring).&lt;br&gt;\n&lt;br&gt;\nOf course, i=\r\nf you were interested in writing something in your store even for nonpositi=\r\nve fetch codes (such as connection-failed errors), you&#39;d have to loosen=\r\n the first test. And if there were some theoretical URI type used in your c=\r\nrawls whose successful &#39;fetch&#39; left a zero recorded-content length =\r\n(perhaps an inline &#39;data:&#39; URI?) but still needed to be written to =\r\nyour store, the second test would need to loosen. But the tests are reasona=\r\nble for usual needs analogous to WARC writing.&lt;div&gt;\n\n&lt;br&gt;\n&lt;br&gt;\n&lt;blockquote =\r\nclass=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #ccc solid=\r\n;padding-left:1ex&quot;&gt;\nAlso, what is the best way to get at the raw page conte=\r\nnt separate from&lt;br&gt;\nthe headers (just the content, not the entire response=\r\n)? Am I correct in&lt;br&gt;\nthinking that this is what I would be provided when =\r\ncalling&lt;br&gt;\ncrawlURI.getRecorder().getRecordedInput().getContentReplayInput=\r\nStream() ?&lt;br&gt;\n&lt;/blockquote&gt;\n&lt;br&gt;&lt;/div&gt;\nFor usual HTTP response content, ye=\r\ns -- that will give an InputStream cued up to just past the protocol header=\r\ns. It does *not* handle &#39;chunked&#39; encodings, which may be a concern=\r\n. If your later analysis might want the headers, you will want to write the=\r\nm somewhere too. Our goal in the WARC (and earlier ARC) writers was to reco=\r\nrd verbatim what came over the wire, to support the greatest range of possi=\r\nble future analysis applications. (We do no decoding; no dechunking; no can=\r\nonicalizing; no header corrections or other alterations; etc.)&lt;br&gt;\n\n\n&lt;br&gt;\nY=\r\nou can also use getReplayCharSequence(), with an optional specified encodin=\r\ng, to get a charset-decoded CharSequence of the content-body (which uses te=\r\nmp files and a sliding buffer to avoid decoding giant content into memory).=\r\n It also does not handled dechunking properly.&lt;br&gt;\n\n\n&lt;br&gt;\n- Gordon @ IA&lt;br&gt;=\r\n\n&lt;br&gt;\n&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;bor=\r\nder-left:1px #ccc solid;padding-left:1ex&quot;&gt;&lt;div&gt;\nThanks,&lt;br&gt;\n-Zach&lt;br&gt;\n&lt;br&gt;\n=\r\nOn Tue, Sep 14, 2010 at 4:27 PM, Gordon Mohr &lt;&lt;a href=3D&quot;mailto:gojomo@a=\r\nrchive.org&quot; target=3D&quot;_blank&quot;&gt;gojomo@...&lt;/a&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;/d=\r\niv&gt;&lt;div&gt;\n&lt;mailto:&lt;a href=3D&quot;mailto:gojomo@...&quot; target=3D&quot;_blank&quot;=\r\n&gt;gojomo@...&lt;/a&gt;&gt;&gt; wrote:&lt;br&gt;\n&lt;br&gt;\n =A0 =A0Great questions! I =\r\nknow there&#39;s other interest in such writer&lt;br&gt;\n =A0 =A0alternatives, so=\r\n I hope you can make whatever you come up with&lt;br&gt;\n =A0 =A0available to oth=\r\ner users, or donate it for possible inclusion in the&lt;br&gt;\n =A0 =A0official H=\r\n3 distribution!&lt;br&gt;\n&lt;br&gt;\n =A0 =A0There also has been previous work to creat=\r\ne an HBaseWriter for&lt;br&gt;\n =A0 =A0Heritrix; it might already work in H3, or =\r\nrequire only a little&lt;br&gt;\n =A0 =A0updating. I haven&#39;t yet tried it myse=\r\nlf. See:&lt;br&gt;\n&lt;br&gt;\n =A0 =A0&lt;a href=3D&quot;http://code.google.com/p/hbase-writer/=\r\n&quot; target=3D&quot;_blank&quot;&gt;http://code.google.com/p/hbase-writer/&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\n =\r\n=A0 =A0On to your questions, interspersed below:&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n =A0 =A0On 9=\r\n/14/10 12:39 PM, Zach Bailey wrote:&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =A0Hi a=\r\nll,&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =A0I wanted to run some questions by you and mayb=\r\ne get some&lt;br&gt;\n =A0 =A0 =A0 =A0pointers from&lt;br&gt;\n =A0 =A0 =A0 =A0the archit=\r\nects of heritrix hoping I could save a little time before&lt;br&gt;\n =A0 =A0 =A0 =\r\n=A0attempting this.&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =A0What I am hoping to do is deve=\r\nlop a custom writer to store the&lt;br&gt;\n =A0 =A0 =A0 =A0results&lt;br&gt;\n =A0 =A0 =\r\n=A0 =A0of my crawls in a distributed database, something like Riak,&lt;br&gt;\n =\r\n=A0 =A0 =A0 =A0Cassandra,&lt;br&gt;\n =A0 =A0 =A0 =A0or HBase for the Heritrix 3.0=\r\n code base.&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =A0I am very comfortable writing Java cod=\r\ne and am very familiar with&lt;br&gt;\n =A0 =A0 =A0 =A0building applications using=\r\n the Spring Framework so I&#39;m thinking&lt;br&gt;\n =A0 =A0 =A0 =A0this&lt;br&gt;\n =A0=\r\n =A0 =A0 =A0should be a relatively easy task to get a proof of concept/prot=\r\notype&lt;br&gt;\n =A0 =A0 =A0 =A0working.&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =A0Looking through=\r\n the code I had some questions:&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =A01.) It looks like =\r\nI will wire in my custom code inside the&lt;br&gt;\n =A0 =A0 =A0 =A0DispositionCha=\r\nin, essentially replacing where the warcWriter bean is&lt;br&gt;\n =A0 =A0 =A0 =A0=\r\nreferenced.&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n =A0 =A0Yes; you could also have both in place fo=\r\nr testing. (I&#39;ve often run&lt;br&gt;\n =A0 =A0with both our ARCWriterProcessor=\r\n and WARCWriterProcessor while&lt;br&gt;\n =A0 =A0debugging things.) One common go=\r\ntcha when installing your own&lt;br&gt;\n =A0 =A0Processors, as edits to our model=\r\n CXML, is that if you follow the&lt;br&gt;\n =A0 =A0pattern there you need to both=\r\n declare the bean with a name, then&lt;br&gt;\n =A0 =A0add the bean by name to the=\r\n chain&#39;s ordered list. So: two edits, a&lt;br&gt;\n =A0 =A0short distance from=\r\n each other, to have the intended effect.&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =A02.)=\r\n Looking at the WARCWriterProcessor it appears it uses a pooled&lt;br&gt;\n =A0 =\r\n=A0 =A0 =A0approach. I&#39;m guessing this is for performance reasons. Assu=\r\nming I&#39;m&lt;br&gt;\n =A0 =A0 =A0 =A0using a driver which already implements co=\r\nnnection pooling, are&lt;br&gt;\n =A0 =A0 =A0 =A0there&lt;br&gt;\n =A0 =A0 =A0 =A0any oth=\r\ner considerations I should think about while implementing&lt;br&gt;\n =A0 =A0 =A0 =\r\n=A0my own&lt;br&gt;\n =A0 =A0 =A0 =A0Writer?&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n =A0 =A0The existing po=\r\noling was motivated by the belief that having more&lt;br&gt;\n =A0 =A0than one act=\r\nive open file at a time could increase throughput.&lt;br&gt;\n =A0 =A0That&#39;s d=\r\nefinitely the case if the files are on independent disks;&lt;br&gt;\n =A0 =A0I&#39=\r\n;m not as sure it helps to have more than one file open on the same&lt;br&gt;\n =\r\n=A0 =A0disk. This has undergone some recent simplification (eliminating the=\r\n&lt;br&gt;\n =A0 =A0dependency on Apache commons-pool) in TRUNK that will appear i=\r\nn&lt;br&gt;\n =A0 =A03.0.1, and is likely to change more in the next month as some=\r\n other&lt;br&gt;\n =A0 =A0novel policies for grouping related site captures to dif=\r\nferent WARCs&lt;br&gt;\n =A0 =A0are added.&lt;br&gt;\n&lt;br&gt;\n =A0 =A0So I&#39;d say: keep a=\r\nn eye on TRUNK for ideas but don&#39;t assume anything&lt;br&gt;\n =A0 =A0in the e=\r\nxisting WARCWriterProcessor approach is optimal or&lt;br&gt;\n =A0 =A0locked-in-pl=\r\nace.&lt;br&gt;\n&lt;br&gt;\n =A0 =A0I would think writing to a remote distributed store m=\r\night offer nice&lt;br&gt;\n =A0 =A0throughput benefits by essentially fanning the =\r\nIO out over many more&lt;br&gt;\n =A0 =A0disks, whether you were writing WARCs int=\r\no (eg) HDFS or individual&lt;br&gt;\n =A0 =A0records/content-bodies into (eg) HBas=\r\ne.&lt;br&gt;\n&lt;br&gt;\n =A0 =A0The least-straightforward part of the current Writer is=\r\n the&lt;br&gt;\n =A0 =A0special-case handling of deduced duplicates, from headers =\r\nor&lt;br&gt;\n =A0 =A0content-hashes, to change how (or whether) individual record=\r\ns are&lt;br&gt;\n =A0 =A0written. There might be an opportunity to factor that dec=\r\nisionmaking&lt;br&gt;\n =A0 =A0out of WARCWriterProcessor to be shared with other =\r\nnon-file or even&lt;br&gt;\n =A0 =A0non-WARC writers.&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n =A0 =A0 =A0 =\r\n=A03.) Once I get my custom code developed, I assume all I need to&lt;br&gt;\n =A0=\r\n =A0 =A0 =A0do to&lt;br&gt;\n =A0 =A0 =A0 =A0make it available to heritrix is to j=\r\nar it up and just drop it in&lt;br&gt;\n =A0 =A0 =A0 =A0$HERITRIX_HOME/lib - anyth=\r\ning else I need to be aware of there?&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n =A0 =A0That should be =\r\nall. Then, as I&#39;m sure you know, you just name the&lt;br&gt;\n =A0 =A0classes =\r\nin your crawl-configuration CXML (Spring XML) bean-declarations.&lt;br&gt;\n&lt;br&gt;\n =\r\n=A0 =A0Hope this helps and let me know any other questions!&lt;br&gt;\n&lt;br&gt;\n =A0 =\r\n=A0- Gordon @ IA&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;br&gt;\n&lt;/blockquote&gt;\n&lt;/b=\r\nlockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;\n\r\n--00151758a95ec3eca9049077dd16--\r\n\n"}}