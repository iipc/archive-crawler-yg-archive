{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264783887,"authorName":"pbaclace","from":"&quot;pbaclace&quot; &lt;pbaclace@...&gt;","profile":"pbaclace","replyTo":"LIST","senderId":"1lDX_3i_41y-pYU6Z6XRWBV9BDAhi46kM6VN2Q5eZ6zwt3C2yv13C0804CreIHJdAhFvveLVnFE8bkxaCd6sfZnUEIUHYA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: lock contention in ServerCache.getServerFor()","postDate":"1234987621","msgId":5687,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGduaHBwNSt2NzBrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGduZnJzNitodGMxQGVHcm91cHMuY29tPg=="},"prevInTopic":5685,"nextInTopic":5704,"prevInTime":5686,"nextInTime":5688,"topicId":5665,"numMessagesInTopic":8,"msgSnippet":"The patch is now available at: http://webteam.archive.org/jira/browse/HER-1609 ... issue for ... the ... two ... cache, ... when ... is not ... files. ... ","rawEmail":"Return-Path: &lt;pbaclace@...&gt;\r\nX-Sender: pbaclace@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 35505 invoked from network); 18 Feb 2009 20:07:04 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m35.grp.scd.yahoo.com with QMQP; 18 Feb 2009 20:07:04 -0000\r\nX-Received: from unknown (HELO n34.bullet.mail.sp1.yahoo.com) (66.163.168.48)\n  by mta17.grp.scd.yahoo.com with SMTP; 18 Feb 2009 20:07:04 -0000\r\nX-Received: from [69.147.65.149] by n34.bullet.mail.sp1.yahoo.com with NNFMP; 18 Feb 2009 20:07:04 -0000\r\nX-Received: from [98.137.34.34] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 18 Feb 2009 20:07:04 -0000\r\nDate: Wed, 18 Feb 2009 20:07:01 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gnhpp5+v70k@...&gt;\r\nIn-Reply-To: &lt;gnfrs6+htc1@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;pbaclace&quot; &lt;pbaclace@...&gt;\r\nSubject: Re: lock contention in ServerCache.getServerFor()\r\nX-Yahoo-Group-Post: member; u=264783887; y=QVPwv1yey7bFTrldF0P40ACmJhTiNF8ZQ51dUMPtuCYQg1E\r\nX-Yahoo-Profile: pbaclace\r\n\r\nThe patch is now available at:\n\nhttp://webteam.archive.org/jira/browse/HER-=\r\n1609\n\n\n\n--- In archive-crawler@yahoogroups.com, &quot;pbaclace&quot; &lt;pbaclace@...&gt; w=\r\nrote:\n&gt;\n&gt; I will use the following bug report for the proposed patch:\n&gt; \n&gt; =\r\n  http://webteam.archive.org/jira/browse/HER-1609\n&gt; \n&gt; The patch is not yet=\r\n ready; I will post a message here when it is.\n&gt; \n&gt; \n&gt; Paul\n&gt; \n&gt; \n&gt; \n&gt; --- =\r\nIn archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Se=\r\nrverCache has been noted as a bottleneck before, so this is a very \n&gt; &gt; wel=\r\ncome result. Can you post a patch either here or to a JIRA\nissue for \n&gt; &gt; o=\r\nthers to review and test?\n&gt; &gt; \n&gt; &gt; As I&#39;d mentioned earlier in our offlist =\r\ndiscussion, I didn&#39;t think\nthe \n&gt; &gt; simple caching approach would help much=\r\n, because there are already\ntwo \n&gt; &gt; levels of caching (CachedBDBMap&#39;s soft=\r\n-reference object-identity\ncache, \n&gt; &gt; and BDB&#39;s byte-array cache) that sho=\r\nuld minimize the IO/lock-time\nwhen \n&gt; &gt; reading the same key multiple times=\r\n in sequence.\n&gt; &gt; \n&gt; &gt; It&#39;s good to see that the deeper lock-untangling off=\r\ners such a big \n&gt; &gt; speedup for your crawl.\n&gt; &gt; \n&gt; &gt; - Gordon @ IA\n&gt; &gt; \n&gt; &gt;=\r\n pbaclace wrote:\n&gt; &gt; &gt; The &quot;un-knotting&quot; performance change worked.  I see =\r\na 2X speedup in\n&gt; &gt; &gt; heritrix v1.14.2:\n&gt; &gt; &gt; \n&gt; &gt; &gt; * 460KB/sec (from 230K=\r\nB/sec) network usage\n&gt; &gt; &gt; * 100% cpu with load between 15-19 (as reported =\r\nby &quot;w&quot; in linux)\n&gt; &gt; &gt; * disk usage at 600KB/sec (from 300KB/sec) \n&gt; &gt; &gt; * =\r\nnumber of established HTTP sockets:  25 (an average from 13\n&gt; netstats)\n&gt; &gt;=\r\n &gt; \n&gt; &gt; &gt; Basically, the same run took half the time.  As long as logging\ni=\r\ns not\n&gt; &gt; &gt; verbose, the TOE worker threads are blocked on writing the WARC=\r\n\nfiles.\n&gt; &gt; &gt;  (Actually, this job writes out both WARC and ARC, so it coul=\r\nd be\n&gt; &gt; &gt; improved.) \n&gt; &gt; &gt; \n&gt; &gt; &gt; The high load number might seem scary t=\r\no some people, but it just\n&gt; &gt; &gt; means the cpu is fully utilized and more c=\r\nores could help.\n&gt; &gt; &gt; \n&gt; &gt; &gt; It requires edits to 3 files plus 3 other fil=\r\nes need trivial\nchanges.\n&gt; &gt; &gt;  The un-knotting has not yet been tested aga=\r\ninst: multi-core,\n&gt; &gt; &gt; multi-processor, checkpointing, and recovery.\n&gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; \n&gt; &gt; &gt; Paul\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; --- In archive-crawler@yahoogroups.=\r\ncom, &quot;pbaclace&quot; &lt;pbaclace@&gt;\nwrote:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; A test run of:\n&gt; &gt; &gt;&gt;   * =\r\nHeritrix 1.14.2 on an AWS/EC2, small instance, with 100 worker\n&gt; &gt; &gt;&gt; threa=\r\nds, 1.3M seeds, 900MB heap\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Has the following resource utiliza=\r\ntion stats:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;   *  230KB/sec of the network\n&gt; &gt; &gt;&gt;   * 100% cpu=\r\n with load between 7 and 13\n&gt; &gt; &gt;&gt;   * disk starts out at 300KB/sec, and 24=\r\n hours later is at 1MB/sec\n&gt; &gt; &gt;&gt;   * number of established HTTP sockets:  =\r\nranges from 1 to 7,\n&gt; &gt; &gt;&gt; occasional spiking to 14\n&gt; &gt; &gt;&gt;   * Full GC ever=\r\ny 10 minutes\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; The limiting resource is the cpu.  A one core ma=\r\nchine should\n&gt; &gt; &gt;&gt; theoretically be able to saturate either the network or=\r\n the disk\n&gt; &gt; &gt;&gt; bandwidth before the cpu hits the wall, unless it has heav=\r\ny lock\n&gt; &gt; &gt;&gt; contention. \n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; See how many and where threads are=\r\n waiting in some jstack thread\n&gt; dumps::\n&gt; &gt; &gt;&gt; # grep &#39;waiting to lock&#39; /m=\r\nnt/Heritrix.9.threaddump  |sort |uniq -c\n&gt; &gt; &gt;&gt;      25         - waiting t=\r\no lock &lt;0x5c357d90&gt; (a\n&gt; &gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierSch=\r\neduler)\n&gt; &gt; &gt;&gt;      61         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt; &gt;&gt; org=\r\n.archive.crawler.datamodel.ServerCache)\n&gt; &gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mn=\r\nt/Heritrix.8.threaddump  |sort |uniq -c\n&gt; &gt; &gt;&gt;       7         - waiting to=\r\n lock &lt;0x5c357d90&gt; (a\n&gt; &gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierSche=\r\nduler)\n&gt; &gt; &gt;&gt;      56         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt; &gt;&gt; org.=\r\narchive.crawler.datamodel.ServerCache)\n&gt; &gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mnt=\r\n/Heritrix.7.threaddump  |sort |uniq -c\n&gt; &gt; &gt;&gt;      31         - waiting to =\r\nlock &lt;0x5c357d90&gt; (a\n&gt; &gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierSched=\r\nuler)\n&gt; &gt; &gt;&gt;      62         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt; &gt;&gt; org.a=\r\nrchive.crawler.datamodel.ServerCache)\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Examination of the Fron=\r\ntierScheduler lock shows that it is held in\n&gt; &gt; &gt;&gt; threaddumps 7 and 9 by a=\r\n thread waiting for ServerCache.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Most threads (about 90) are =\r\nwaiting for a lock on ServerCache\nin the\n&gt; &gt; &gt;&gt; method:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;   pub=\r\nlic synchronized CrawlServer getServerFor(String serverKey)\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; P=\r\nresumably, a simple name to host/server would be fast, but one\n&gt; thread\n&gt; &gt;=\r\n &gt;&gt; holds the lock while doing a relatively long BDB read operation. \n&gt; &gt; &gt;=\r\n&gt; Obviously, having disk io block all cache lookups is not optimal,\n&gt; &gt; &gt;&gt; =\r\nespecially when BDB has a lock per file (FileManager).  In my test\n&gt; &gt; &gt;&gt; c=\r\nase, the bdb data is 3.6GB and there are about 360 *.jdb files\n&gt; in the\n&gt; &gt;=\r\n &gt;&gt; job state directory.  If requests to getServerFor(String) were not\n&gt; &gt; =\r\n&gt;&gt; synchronized, then BDB should be able to read from multiple *.jdb\n&gt; file=\r\n\n&gt; &gt; &gt;&gt; at the same time and threads requesting entries cached in memory by=\r\n\n&gt; &gt; &gt;&gt; CachedBDBMap would not need to wait.  \n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; I think=\r\n the following high gain, small code footprint improvements\n&gt; &gt; &gt;&gt; would he=\r\nlp:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; * superficial thread-local caching (1 affected file)\n&gt; &gt; =\r\n&gt;&gt; **  the ServerCache lookups are done in many code locations, so it\n&gt; &gt; &gt;=\r\n&gt; seems each thread processing a uri might repeatedly do the same\n&gt; lookup\n=\r\n&gt; &gt; &gt;&gt; and get stuck waiting\n&gt; &gt; &gt;&gt; **  a ThreadLocal cache of one key-valu=\r\ne pair could be checked\nbefore\n&gt; &gt; &gt;&gt; the Maps in ServerCache.getServerFor(=\r\nString) before\nsynchronizing on\n&gt; &gt; &gt;&gt; this instance of ServerCache.\n&gt; &gt; &gt;&gt;=\r\n **  this must not interfere with soft reference tracking, of course\n&gt; &gt; &gt;&gt;=\r\n\n&gt; &gt; &gt;&gt; * deep un-knotting by lock-splitting and enabling more\nconcurrency =\r\nin\n&gt; &gt; &gt;&gt; ServerCache, CachedBdbMap, and BDB.\n&gt; &gt; &gt;&gt; ** drop synchronizatio=\r\nn of ServerCache.getServerFor(String)\n&gt; &gt; &gt;&gt; ** drop synchronization of  Ca=\r\nchedBdbMap.get(Object)\n&gt; &gt; &gt;&gt; ** use ConcurrentHashMap for CachedBdbMap.mem=\r\nMap\n&gt; &gt; &gt;&gt; ** drop synchronization of CachedBdbMap.put(K,V) and expose\n&gt; &gt; =\r\n&gt;&gt; putIfAbsent(K,V) if needed.\n&gt; &gt; &gt;&gt; *** ServerCache.createServerFor(Strin=\r\ng) loses synchronization when\n&gt; &gt; &gt;&gt; ServerCache.getServerFor(String) drops=\r\n it.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; My particular crawl job exercises the ServerCache=\r\n more than most\n&gt; jobs,\n&gt; &gt; &gt;&gt; but it is analogous to having a very wide, b=\r\nreadth-first crawl. \n&gt; &gt; &gt;&gt; Characteristics of this performance case are sh=\r\nared by all jobs\nthat\n&gt; &gt; &gt;&gt; crawl many thousands of hosts.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; S=\r\nince full GC was occurring about every 10 minutes, the lock\n&gt; &gt; &gt;&gt; contenti=\r\non was not due to full GC frequency.  A heap histogram\nshowed\n&gt; &gt; &gt;&gt; about =\r\n3700 CrawlServer instances at the end of the run.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; If this un-=\r\nknotting can work, there should be substantially better\n&gt; &gt; &gt;&gt; disk and net=\r\nwork utilization. \n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Paul\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; =\r\n&gt; &gt; \n&gt; &gt; &gt; ------------------------------------\n&gt; &gt; &gt; \n&gt; &gt; &gt; Yahoo! Groups =\r\nLinks\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}