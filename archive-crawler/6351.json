{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"fncaKPXlpe918Ff573m9-CfVV9SjMzwG88hXiGKd8dj8YvvGRozZy0Gn_-NbVzQ_ikOtna3gzglL5r6i5twb6UmjY-j_RcI","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Misc questions","postDate":"1265128878","msgId":6351,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRCNjg1NUFFLjYwNDA3QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PEQyODE4RThCLUUxRDEtNDE0RS1BNEE0LTEzMjlDN0RBNjI4MUBnbWFpbC5jb20+","referencesHeader":"PGhrNnQ5cSs4NXVkQGVHcm91cHMuY29tPiA8NEI2N0I5ODcuNTAyMDJAYXJjaGl2ZS5vcmc+IDxEMjgxOEU4Qi1FMUQxLTQxNEUtQTRBNC0xMzI5QzdEQTYyODFAZ21haWwuY29tPg=="},"prevInTopic":6350,"nextInTopic":6353,"prevInTime":6350,"nextInTime":6352,"topicId":6345,"numMessagesInTopic":9,"msgSnippet":"... In H1, checkpointing should work, though it requires a full pause, and if using custom/contributed components (such as processors) they might not all","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 79308 invoked from network); 2 Feb 2010 16:41:20 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m2.grp.sp2.yahoo.com with QMQP; 2 Feb 2010 16:41:20 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta1.grp.re1.yahoo.com with SMTP; 2 Feb 2010 16:41:20 -0000\r\nX-Received: (qmail 25085 invoked from network); 2 Feb 2010 16:41:19 -0000\r\nX-Received: from 70.137.148.203 (HELO ?192.168.23.128?) (70.137.148.203)\n  by relay01.pair.com with SMTP; 2 Feb 2010 16:41:19 -0000\r\nX-pair-Authenticated: 70.137.148.203\r\nMessage-ID: &lt;4B6855AE.60407@...&gt;\r\nDate: Tue, 02 Feb 2010 08:41:18 -0800\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;hk6t9q+85ud@...&gt; &lt;4B67B987.50202@...&gt; &lt;D2818E8B-E1D1-414E-A4A4-1329C7DA6281@...&gt;\r\nIn-Reply-To: &lt;D2818E8B-E1D1-414E-A4A4-1329C7DA6281@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Misc questions\r\nX-Yahoo-Group-Post: member; u=137285340; y=Vhdz7iaQojsq3Mv-CFmZd34zrgUPBKMjAHhiPU-hbWCa\r\nX-Yahoo-Profile: gojomo\r\n\r\nDerek Pappas wrote:\n&gt; Gordon,\n&gt; \n&gt; We are building a vertical search engine and are using Heritrix\n&gt; \n&gt; 1. As far as checking pointing is concerned is it supposed to work in  \n&gt; 1.xxx.\n&gt; My experience has been that it does not work as advertised.\n\nIn H1, checkpointing should work, though it requires a full pause, and \nif using custom/contributed components (such as processors) they might \nnot all support checkpointing. Can you be more specific about how it has \nfailed?\n\n&gt; 2. Should we switch our production crawl to 3.0?\n\nIt depends on your comfort with H1, your need for H3 features, and your \ncomfort with H3&#39;s changes, especially the changes in configuration.\n\nIf your crawl operators are Java developers or otherwise comfortable \nwith editing XML configuration files, you might prefer some options H3 \noffers; if all&#39;s well with your current crawls and you like the H1 \nguided-forms crawl-configuration, you could stay with H1 a while.\n\nWe&#39;ve started to use H3 at the Internet Archive for our internal crawls \nto expand our own collections, but not most of our partner crawls. That \nwill be changing over the course of 2010.\n\n&gt; 3. I did not receive an invite to the Internet Archive Heritrix  \n&gt; meeting in San Francisco.\n&gt; How do I qualify for an invite to the the meeting?\n\nStep 1 -- if you haven&#39;t already done so -- is to fill out the survey at \n&lt;http://www.surveymonkey.com/s/PWGH959&gt; to express interest.\n\n&gt; 4. We are planning on converting our semi manual flow to a Hadoop  \n&gt; automated\n&gt; flow and would like Hertrix to write directly to HDFS. Is  \n&gt; HDFSWriterProcessor\n&gt; going to be part of the main Heritrix release?\n\nWe&#39;d be happy to include it in the main release, but since at IA we \ndon&#39;t expect to write directly to HDFS from our crawls, it&#39;d be best if \nan outside contributor took the lead on maintaining/supporting it.\n\n&gt; 5. Is it possible to run more than one crawl job in 1.xxx or do we  \n&gt; need to launch\n&gt; new instances of the crawler to do so?\n\nIt is *possible*, using the JMX operations, to launch more than one \n&#39;Heritrix instance&#39; inside one JVM, and then each &#39;Heritrix instance&#39; \nmay run a separate crawl. However, this can be confusing because:\n\n- the web UI only reflects one &#39;Heritrix instance&#39; at a time, making it \nharder to monitor all the crawls\n- you have to use JMX to get into this state\n- until 1.14.3, running even just 2 crawls with default settings was \nhighly likely to result in an OutOfMemoryError, because each would try \nto allocate the default 60% of heap (totaling 120%) to their BDB \nenvironments. Only by resizing crawl parameters based on how many \nsimultaneous crawls were expected would yield good results. (For 1.14.3, \na new BDB shared-cache feature avoids that largest risk, but still most \nof our rules-of-thumb and defaults for crawl settings assume only one \ncrawl is running in the same JVM/machine.)\n\nSo, you can do it if you know what you&#39;re getting into, and if your \ncrawls are not very busy/broad, and you&#39;re able to monitor outside the \nweb UI, but it&#39;s not the preferred mode of operation.\n\nH3 handles multiple simultaneous jobs better; they can be launched and \nmonitored via the UI independently. You would still want to configure \neach crawl to allow for other simultaneous activity, though -- for \nexample fewer threads than if it were the only crawl running.\n\nStill, since the crawl job is the unit on which discovered URIs are \nremembered, there are often efficiencies to running fewer, larger \ncrawls. Can you say more about why you want to run smaller independent \nsimultaneous crawls? (Do the results need to isolated from each other?)\n\n- Gordon @ IA\n\n\n&gt; Best,\n&gt; \n&gt; Derek\n&gt; \n\n"}}