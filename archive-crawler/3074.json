{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"kris@archive.org","from":"kris@...","profile":"kristsi25","replyTo":"LIST","senderId":"orqypcFT0S8QL5ml9sTYVlxiLT8uixTuvulnladXra1hBZJ9LB_JASRhFiaHkSqJCW68YkGYmw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] Advice needed on how to (properly) structure           new Heritrix modify and delete functionality","postDate":"1153211413","msgId":3074,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEyMTQuMTMwLjIwOC4xNTIuODAuMTE1MzIxMTQxMy5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg=="},"prevInTopic":3068,"nextInTopic":3076,"prevInTime":3073,"nextInTime":3075,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"Hey Karl, Thanks for sharing your usage scenario. I think I now understand what your needs are and I may have a thought on how to help you. ... Just to make","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 50661 invoked from network); 18 Jul 2006 08:30:11 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m40.grp.scd.yahoo.com with QMQP; 18 Jul 2006 08:30:11 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta1.grp.scd.yahoo.com with SMTP; 18 Jul 2006 08:30:11 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 47B1214156C09\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Jul 2006 01:30:14 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 27593-01-14 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 18 Jul 2006 01:30:13 -0700 (PDT)\r\nReceived: from mail.archive.org (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 3202F14156A8A\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Jul 2006 01:30:13 -0700 (PDT)\r\nReceived: from 130.208.152.80\n        (SquirrelMail authenticated user kris)\n        by mail.archive.org with HTTP;\n        Tue, 18 Jul 2006 01:30:13 -0700 (PDT)\r\nMessage-ID: &lt;1214.130.208.152.80.1153211413.squirrel@...&gt;\r\nDate: Tue, 18 Jul 2006 01:30:13 -0700 (PDT)\r\nTo: archive-crawler@yahoogroups.com\r\nUser-Agent: SquirrelMail/1.4.6\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nX-Priority: 3 (Normal)\r\nImportance: Normal\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: kris@...\r\nSubject: RE: [archive-crawler] Advice needed on how to (properly) structure \n          new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=163406187; y=EBvprI7H3C_-VQVRooQcLGJRRxBoYY4m4j6vvNH0kut8hX0y\r\nX-Yahoo-Profile: kristsi25\r\n\r\nHey Karl,\n\nThanks for sharing your usage scenario. I think I now understand what your\nneeds are and I may have a thought on how to help you.\n\n\n&gt; &gt; That is a very ambitious project. It would be very\n&gt; interesting if you\n&gt; &gt; could share why you want this functionality.\n&gt; &gt;\n&gt;\n&gt; Ok. We build appliances here that index content for\n&gt; specialized searches. We use a number of tools for ingesting\n&gt; content into the system and removing it when it goes away.\n&gt; Our other tools all work incrementally; they only ingest\n&gt; stuff into the index that is new or has changed, and they\n&gt; delete content from the index when the content has gone away.\n&gt; We use Heritrix right now, but it effectively a one-shot for\n&gt; us; effectively we cannot detect changes without rerunning a\n&gt; job from scratch, and also we cannot detect deletions at all.\n&gt; Several of our clients have attempted to solve these problems\n&gt; by building their own re-fetcher, which looks at the Heritrix\n&gt; crawl logs to determine what to do, but basically this runs\n&gt; afoul of politeness issues and all that stuff. So we\n&gt; concluded that we really did want to build a version of\n&gt; Heritrix that fully understands how to work incrementally.\n\nJust to make sure we are on the same page I&#39;m going to reword your situation.\n\nYou have an index of crawled content and you wish to update it in an\nincremental fashion.\n\nA question here; how long does it take to do a complete crawl? How big is it?\n\n&gt; The adaptive frontier seemed like a possible way to go,\n&gt; except that I was told (by stack) that it did not use a\n&gt; database, and therefore would run out of memory readily. We\n&gt; already have problems even with the bdbfrontier on memory\n&gt; consumption, so I did not think that would be a viable way to proceed.\n\nStack got it wrong (or you misunderstood him) the ARFrontier uses BDB just\nlike the BdbFrontier to serialize data to disk. However, it was designed\nto work with a _limited_ number of hosts. The BdbFrontier uses only one\ndatabase for all its queues while the ARF uses one database *per* queue.\nEach host queue has a some overhead and there are some issues with having\nmultiple Bdb databases in the same environment like the ARF does. And\nsince the ARFrontier has never seen significant use (mostly because of the\nchange detection problem) it has never received much optimization.\n\nIncremental crawling on a truly large scale is bloody hard. As you note\nthe BdbFrontier becomes very heavy during your crawls, yet it only needs\nto keep a small fingerprint of crawled URIs. An incremental frontier needs\nto keep rich objects for each URI (crawled or not) and is going to be\naccessing on-disk content far more randomly (read: higher access latency\nand decreased cache performance) then a snapshot frontier.\n\n\n&gt; We don&#39;t need to be too precise about detecting changes. If\n&gt; content changes only in formatting, say, the worst that can\n&gt; happen is we will reingest needlessly. It&#39;s still a lot\n&gt; better than reingesting *everything*, which would be the alternative.\n\nThat is one problem down.\n\nI&#39;m going to suggest you take a look at the (still unreleased)\nDeDuplicator. A prerelease version can be found here:\nhttp://vefsofnun.bok.hi.is/deduplicator/. The DeDuplicator was developed\nby me and has been tested at netarkivet.dk and is nearing a formal\nrelease.\n\nThe idea behind it is simple. Perform a regular snapshot. Once completed\ncreate a deduplication index (Lucene is used for this). On subsequent\ncrawls the deduplication index is used to discard duplicate data\n(duplicate detection is done by comparing the SHA-1 content digests that\nHeritrix&#39;s FetchHTTP processor creates).\n\nThe only thing I&#39;d be worried about is how well the DeDuplicator scales.\nBecause duplicate detection of HTML documents is so unreliable it is\ntypically used only against non-text documents (using mime types for\ndiscrimination). In that configuration I&#39;ve used (with only minimal\nperformance hit) where I&#39;ve index ~7 million documents and used it to\nfilter duplicates in a crawl of 25 million documents where ~7 million\nwhere non-text documents. I.e. the lookup was done on about one in three\ndocuments. As the index gets larger the performance will get worse. In\nparticular the Lucene index and the BdbFrontier will begin to contend for\nI/O. Keeping the Bdb scratch space and the Lucene dedup index on separate\ndisks would undoubtedly be wise for large crawls.\n\n&gt; &gt;&gt;These conditions would have to be signaled in some way to\n&gt; all Writers.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; URIs always pass through all processor unless their processing is\n&gt; &gt; interrupted so once the determination has been made it is\n&gt; simply a matter\n&gt; &gt; of recording the meta-data in the CrawlURI and having the\n&gt; writers pick up\n&gt; &gt; on that. You&#39;d clearly need your own writers but that was a\n&gt; given in any\n&gt; &gt; case.\n&gt;\n&gt; Yes, we have our own writer. What I didn&#39;t know (or didn&#39;t think of)\n&gt; was that the writer would be called for a 404 error. If that&#39;s the\n&gt; case, the delete path issue is &quot;solved&quot;.\n\nBy filtering repeat crawls with the DeDuplicator only changed URIs would\nreach your writers. A 404 will have a different content digest to the\noriginal document and will be signaled as a change.\n\n&gt; &gt;\n&gt; &gt;&gt;Alternatively, since backwards compatibility might be a\n&gt; problem, it&#39;s\n&gt; &gt;&gt;potentially possible that a new kind of Writer-like module\n&gt; would need\n&gt; &gt;&gt;to be created. I was going to look at the adaptive frontier\n&gt; to see how\n&gt; &gt;&gt;exactly it handled changes, if indeed it does anything\n&gt; special at all.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; It issues URIs, the URIs pass through the processing chain\n&gt; where stuff\n&gt; &gt; happens and then they return to the frontier with updated\n&gt; state and are\n&gt; &gt; reinserted. The ARFrontier doesn&#39;t worry to much about\n&gt; &#39;change&#39; as such.\n&gt; &gt; It relies on the fact that change can only occur when a URI is being\n&gt; &gt; processed. There are no external &#39;events&#39; that can trigger a change.\n&gt; &gt;\n&gt;\n&gt; I understand that the change detecting comes only as a result of\n&gt; refetching a url. What I don&#39;t get is how I could detect a change\n&gt; without saving some indication somewhere of what the data looked like\n&gt; last time through.\n\nWhat the AR stuff did was that the SHA-1 fingerprint of a URI was stored\nin the CrawlURI during fetching. This information survives reentry into\nthe ARFrontier and is still intact when the CrawlURI is reissues\n(recrawled) and can then be compared against the new fingerprint by the\nChangeEvaluator.\n\nThe DeDuplicator stores this information in a separate index that is\ncreated &#39;offline&#39; (between snapshot crawls) and is looked up as needed.\n\n&gt; One thing that would help in this regard is simply keeping a checksum\n&gt; around of the contents of the url as it was fetched last time\n&gt; round, and\n&gt; the checksum of the data for the current fetch. Then, if they differ,\n&gt; we&#39;d presume that the data had changed.\n\nWhich is exactly what existing strategies do.\n\n\n&gt; I would definitely plan on doing this as an entirely new\n&gt; frontier, based\n&gt; on both the Bdb guy and the adaptive guy.\n\nA large scale incremental frontier would definitely be interesting, but\nwould also be hard to achieve for reasons cited above.\n\nIf you decide to go this route I wish you all the luck in the world but I\ndo note the following:\n\n1. If you do not use some type of adaptive strategy to optimize your\ncrawling you are no better off then doing repeated snapshot crawls with\nthe DeDuplicator\n\n2. If you do use some adaptive strategies the complexity of the frontier\nwill go up making it harder to scale it to multimillion document crawls.\n\nI&#39;m not trying to discourage you, just trying to point out the main\nproblems you&#39;ll face. I&#39;ve been down this road before and I found my\nsolution outside the Frontier (the DeDuplicator) but my goal was subtly\ndifferent (reduce amount of duplicate data stored in an archive).\n\n&gt; I hear you. I will look further at both frontiers. In the meantime,\n&gt; have you considered how you might have proceeded if you\n&gt; needed to make\n&gt; the adaptive frontier store its queue on disk? That&#39;s\n&gt; basically what I\n&gt; am trying to build.\n\nI used BDB. The way I used it could be optimized, but I&#39;m unaware of a\nbetter choice then BDB for storing data on disk.\n\nIf you could make the ARFrontier effectively scale to multi-million\ndocument crawls I believe you&#39;d have what you need. I also believe that\nthat is no easy task.\n\n- Kris\n\n\n"}}