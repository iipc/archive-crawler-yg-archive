{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":299179219,"authorName":"mjjjhjemj","from":"&quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;","profile":"mjjjhjemj","replyTo":"LIST","senderId":"jju91FuOMSbHFyjYokHC5-hB9Z1atSUhzQrpAssV3xDq5jjxqQ8jg6mUWOJBtA11aB9_B2X13Kxz2S6Hx2Q4_jy7ggQOU4bKK0c","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Large domain crawl best practice depth of crawl","postDate":"1199913884","msgId":4888,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZtM2Uycytmb2hrQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":4889,"prevInTime":4887,"nextInTime":4889,"topicId":4888,"numMessagesInTopic":2,"msgSnippet":"I am crawling a domain with a very large number of hosts and content. It appears that due to time contraints we may not be able to gather all content. Is there","rawEmail":"Return-Path: &lt;bosoxchamps@...&gt;\r\nX-Sender: bosoxchamps@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 60186 invoked from network); 9 Jan 2008 21:24:45 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m47.grp.scd.yahoo.com with QMQP; 9 Jan 2008 21:24:45 -0000\r\nX-Received: from unknown (HELO n36a.bullet.mail.sp1.yahoo.com) (66.163.168.130)\n  by mta18.grp.scd.yahoo.com with SMTP; 9 Jan 2008 21:24:45 -0000\r\nX-Received: from [216.252.122.219] by n36.bullet.mail.sp1.yahoo.com with NNFMP; 09 Jan 2008 21:24:45 -0000\r\nX-Received: from [66.218.69.5] by t4.bullet.sp1.yahoo.com with NNFMP; 09 Jan 2008 21:24:45 -0000\r\nX-Received: from [66.218.66.88] by t5.bullet.scd.yahoo.com with NNFMP; 09 Jan 2008 21:24:45 -0000\r\nDate: Wed, 09 Jan 2008 21:24:44 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;fm3e2s+fohk@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;\r\nSubject: Large domain crawl best practice depth of crawl\r\nX-Yahoo-Group-Post: member; u=299179219; y=0gTj2jrqnTceLtakZg8lDuHA4Uf0wORiq_O-rzOah5UOffbF\r\nX-Yahoo-Profile: mjjjhjemj\r\n\r\nI am crawling a domain with a very large number of hosts and content.\nIt ap=\r\npears that due to time contraints we may not be able to gather\nall content.=\r\n Is there a recommended hops from seed that is considered\nbest practice if =\r\none does have to limit the crawl? *I understand that\nfor the number of hops=\r\n to be consistent across all hosts within a\ncrawl that I would need to make=\r\n sure there was a seed entry per\ndistinct host. As I discover new hosts I w=\r\nill pause, enter in new\nseed, and resume crawl. \n\nAny help is greatly appre=\r\nciated,\nMike\n\n\n"}}