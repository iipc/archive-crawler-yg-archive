{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":332889988,"authorName":"Travis Jensen","from":"&quot;Travis Jensen&quot; &lt;travis.jensen@...&gt;","profile":"tajensen72","replyTo":"LIST","senderId":"JEXvsvgymbCrsCTn2GZFy0LvADaFQNyIZPKgULvTCTAukLuiBqtqvPYCZdDt8M6DtAmhfsixl_AiAWC5GSFjoJY-DHvEXsoE8UoCZsx6iEQnFel2Zg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [archive-crawler] Best approach question","postDate":"1200686955","msgId":4908,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEEwNzdERjdDRjc3MzM1NERCMjg1Mjc5MDMwRjZBMUE1MzFCODE5QE9YWUdFTi5zaXJzaS5wdnQ+","inReplyToHeader":"PDQ3OTBGRDM0LjYwMTAzMDRAYXJjaGl2ZS5vcmc+"},"prevInTopic":4907,"nextInTopic":4910,"prevInTime":4907,"nextInTime":4909,"topicId":4906,"numMessagesInTopic":6,"msgSnippet":"Hi, Thanks for your reply, Igor. Would this still be a preferred way if I have 1500 of these URLs?  I would worry about the performance hit of every crawled","rawEmail":"Return-Path: &lt;Travis.Jensen@...&gt;\r\nX-Sender: Travis.Jensen@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 90450 invoked from network); 18 Jan 2008 20:09:49 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m48.grp.scd.yahoo.com with QMQP; 18 Jan 2008 20:09:49 -0000\r\nX-Received: from unknown (HELO email.sirsi.com) (192.150.149.242)\n  by mta16.grp.scd.yahoo.com with SMTP; 18 Jan 2008 20:09:49 -0000\r\nX-MimeOLE: Produced By Microsoft Exchange V6.5\r\nContent-class: urn:content-classes:message\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nDate: Fri, 18 Jan 2008 13:09:15 -0700\r\nMessage-ID: &lt;A077DF7CF773354DB285279030F6A1A531B819@...&gt;\r\nIn-Reply-To: &lt;4790FD34.6010304@...&gt;\r\nX-MS-Has-Attach: \r\nX-MS-TNEF-Correlator: \r\nthread-topic: [archive-crawler] Best approach question\r\nthread-index: AchaB++GtHsD3wu7Sb2UdjX9QPGnGgABaKlA\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nX-Received-SPF: none\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Travis Jensen&quot; &lt;travis.jensen@...&gt;\r\nSubject: RE: [archive-crawler] Best approach question\r\nX-Yahoo-Group-Post: member; u=332889988; y=b2BPebwQS68Zn1ZwlePPZ-cdFQ9qUpPpOMtbS-RjWkG9Sd7zcg\r\nX-Yahoo-Profile: tajensen72\r\n\r\n\n\nHi,\n\nThanks for your reply, Igor.  \n\nWould this still be a preferred way =\r\nif I have 1500 of these URLs?  I\nwould worry about the performance hit of e=\r\nvery crawled URL needing to go\nthrough an average of 750 regex matches to f=\r\nind the one that will match.\n\ntj\n\n-----Original Message-----\nFrom: archive-=\r\ncrawler@yahoogroups.com\n[mailto:archive-crawler@yahoogroups.com] On Behalf =\r\nOf Igor Ranitovic\nSent: Friday, January 18, 2008 12:26 PM\nTo: archive-crawl=\r\ner@yahoogroups.com\nSubject: Re: [archive-crawler] Best approach question\n\n\n=\r\nI don&#39;t think that you need to run separate jobs for each seed.\n\nFor exampl=\r\ne, is you have two seeds as:\n\nhttp://www.foo.com/baz/bar2.html\nhttp://www.f=\r\noo.com/zab/bar2.html\n\nyou can have scope as:\n\n+http://www.foo.com/baz/bar2.=\r\nhtml\n+http://www.foo.com/zab/bar2.html\n\nand this decide rule:\n\n&lt;newObject n=\r\name=3D&quot;seedsCurrentFolderOnly&quot; \nclass=3D&quot;org.archive.crawler.deciderules.Ma=\r\ntchesListRegExpDecideRule&quot;&gt;\n  &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n  &lt;=\r\nstring name=3D&quot;list-logic&quot;&gt;OR&lt;/string&gt;\n  &lt;stringList name=3D&quot;regexp-list&quot;&gt;\n=\r\n     &lt;string&gt;(?i)http://www.foo.com/baz/.*/&lt;/string&gt;\n     &lt;string&gt;(?i)http:=\r\n//www.foo.com/zab/.*/&lt;/string&gt;\n   &lt;/stringList&gt;\n&lt;/newObject&gt;\n\n\nCreating the=\r\n scope and MatchesListRegExpDecideRule can be done easily \nwith little scri=\r\npting.\n\nI hope this helps.\n\nTake care,\ni.\n\nTravis Jensen wrote:\n&gt; Hi,\n&gt; \n&gt; =\r\n \n&gt; \n&gt; I&#39;m looking for some feedback as to a best approach to a crawling \n&gt;=\r\n problem I need solve.  I have a list of URLs, some of which I only\nwant \n&gt;=\r\n to crawl that URL, some of which I want to crawl that whole domain \n&gt; (&quot;fo=\r\no.com&quot;), some I want to crawl only that server (&quot;www.foo.com&quot;),\nsome \n&gt; I w=\r\nant to crawl anything below the given URL, and some I want to crawl\n\n&gt; ever=\r\nything in the URL&#39;s folder.\n&gt; \n&gt;  \n&gt; \n&gt; The first four are out-of-the-box f=\r\nrom my understanding of Heritrix 2,\n\n&gt; which is great.  The last one doesn&#39;=\r\nt seem to be, so I&#39;ve been looking\n\n&gt; at options on how to implement it.  J=\r\nust to clearly define the\nproblem, \n&gt; if I&#39;m given a seed URL of:\n&gt; \n&gt;  \n&gt; =\r\n\n&gt; http://www.foo.com/baz/bar.html\n&gt; \n&gt;  \n&gt; \n&gt; then I want to match:\n&gt; \n&gt;  =\r\n\n&gt; \n&gt; http://www.foo.com/baz/bar2.html,\nhttp://www.foo.com/baz/somethingels=\r\ne.html\n&gt; \n&gt;  \n&gt; \n&gt; but I don&#39;t want to match:\n&gt; \n&gt;  \n&gt; \n&gt; http://www.foo.co=\r\nm/apage.html,\nhttp://www.foo.com/baz/bang/anotherpage.html\n&gt; \n&gt;  \n&gt; \n&gt; The =\r\ntwo methods I&#39;ve found to do this would be to define a regex of\nthe \n&gt; matc=\r\nhing URL and use a MatchesRegExpDecideRule or to create my own \n&gt; DecideRul=\r\ne (call it FolderOnlyDecideRule).\n&gt; \n&gt;  \n&gt; \n&gt; If I use the MatchesRegExpDec=\r\nideRule, will I have to create a\ndifferent \n&gt; job for each seed URL (becaus=\r\ne the regex will be different)?  Or is it\n\n&gt; possible to say &quot;this job uses=\r\n this seed URL with this regex&quot;?\nDealing \n&gt; with a different job for each U=\r\nRL seems painful.\n&gt; \n&gt;  \n&gt; \n&gt; If the FolderOnlyDecideRule is the way to go,=\r\n can I get some pointers\non \n&gt; how to go about implementing it.  I&#39;ve been =\r\nlooking through the \n&gt; DecideRules and it hasn&#39;t &quot;clicked&quot; yet. :)\n&gt; \n&gt;  \n&gt;=\r\n \n&gt; Thanks.\n&gt; \n&gt;  \n&gt; \n&gt; tj\n&gt; \n&gt; \n\n\n\n \nYahoo! Groups Links\n\n\n\n\n"}}