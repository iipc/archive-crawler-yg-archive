{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"yOBZ37SUGYj0eTvSHZQs5tlAz4qTxCjq2qkMhMJRuGsdtJ9YhxnCY_GN6j6QcJhuyGAVCTeAQQdvlW551ZPITDfexEExaWsN","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] writing a crawl.log per processed domain","postDate":"1165868699","msgId":3590,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1N0RCRTlCLjYwMDAyMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGVsaHFodCs2OTRiQGVHcm91cHMuY29tPg==","referencesHeader":"PGVsaHFodCs2OTRiQGVHcm91cHMuY29tPg=="},"prevInTopic":3587,"nextInTopic":0,"prevInTime":3589,"nextInTime":3591,"topicId":3587,"numMessagesInTopic":2,"msgSnippet":"... Hey Olaf: Postprocessing crawl.log w/ perl/python/awk/etc. seems much easier than modifying Heritrix but if you insist, one suggestion for how to change ","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 13323 invoked from network); 11 Dec 2006 20:20:21 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m23.grp.scd.yahoo.com with QMQP; 11 Dec 2006 20:20:21 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta2.grp.scd.yahoo.com with SMTP; 11 Dec 2006 20:20:21 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 123521416C0DC;\n\tMon, 11 Dec 2006 12:18:00 -0800 (PST)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 11311-03-56; Mon, 11 Dec 2006 12:17:58 -0800 (PST)\r\nReceived: from [192.168.1.204] (c-71-198-60-165.hsd1.ca.comcast.net [71.198.60.165])\n\tby mail.archive.org (Postfix) with ESMTP id 18B211416C05C;\n\tMon, 11 Dec 2006 12:17:57 -0800 (PST)\r\nMessage-ID: &lt;457DBE9B.6000208@...&gt;\r\nDate: Mon, 11 Dec 2006 12:24:59 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.2) Gecko/20060405 SeaMonkey/1.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;elhqht+694b@...&gt;\r\nIn-Reply-To: &lt;elhqht+694b@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] writing a crawl.log per processed domain\r\nX-Yahoo-Group-Post: member; u=168599281; y=dJEi8ovKhkMYEgjm8BuXBnJ1TA8nMsDPiKYujxXpVZOG3VFZOHCkPba1\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\npandae667 wrote:\n&gt;\n&gt; Hi everyone,\n&gt;\n&gt; I&#39;m currently having a problem that I&#39;m not able to solve.\n&gt; I am currently spidering a rather big set of seeds and thus my\n&gt; crawl.log gets really big. Unfortunatly I have to post-process the\n&gt; crawl.log to retrieve some additional informations for some ULRs I&#39;m\n&gt; interested in. (Basically the linkpath from a seed to the URLs in\n&gt; question).\n&gt;\n&gt; To ease this process I&#39;d like to split up the crawl.log into multiple\n&gt; files (one per domain). But I don&#39;t wanna do this after my crawl is\n&gt; done, I want heritrix to do this on the fly (either additional to the\n&gt; normal crawl.log or as an optional way to output the contained data).\n&gt;\n&gt; I traced down the place the crawl.log gets written to the &quot;void\n&gt; log(CrawlURI curi)&quot; method in AbstractFrontier.java, so I know _where_\n&gt; it happens - but I have no clue how to change the normal behavior to\n&gt; what I want/need.\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHey Olaf:\n\nPostprocessing crawl.log w/ perl/python/awk/etc. seems much easier than \nmodifying Heritrix but if you insist, one suggestion for how to change \nHeritrix follows.  Regards tracing path from seed, have you see the \nhoppath.pl script in $HERITRIX_HOME/bin?\n\nSt.Ack\n\nLogging configuration is done in code inside in CrawlController so its \nhard for you to provide an alternate or supplemental crawl.log logger \njust by changing configuration.  You could do something like subclass \nfrontier so you could catch the call to log but better might be adding a \nprocessor that does nothing but pass URIs but that implements \nCrawlURIDispositionListener.  Your processor&#39;s initialTask, would look \nsomething like:\n\n    @Override\n    protected void initialTasks() {\n        super.initialTasks();\n        getController().addCrawlURIDispositionListener(this);\n    }\n\n\nThen in your implementation of \nCrawlURIDispositionListener#crawledURISuccessful, you&#39;d do your logging.\n\nTo log to a file per domain, you could write your own Handler and then \nwith configuration in heritrix.properties, set your Processor up to use \nyour custom Handler.  Inside in your Handler, you&#39;d do the sorting of \nURIs per domain log file, etc.\n\n\n\n\n&gt;\n&gt; Does anyone have some useful ideas?\n&gt;\n&gt; Regards\n&gt; Olaf Freyer\n&gt;\n&gt;  \n\n\n"}}