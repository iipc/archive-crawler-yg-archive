{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":126110077,"authorName":"Saurabh Pathak","from":"&quot;Saurabh Pathak&quot; &lt;sau_pathak@...&gt;","profile":"sau_pathak","replyTo":"LIST","senderId":"3QJqw7CE_li1oHSLADr2GLQNtxO2MMpuPetPg5b08BVRCjljRTmd-XPBIHuHF61HZSD0VdHaByHyMlZklnGxYhyBSJMo4_D1WUzn_8kMpXI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"BroadScope Crawl and out of memory error","postDate":"1113506599","msgId":1724,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGQzbWZ2NytoMnE1QGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":1725,"prevInTime":1723,"nextInTime":1725,"topicId":1724,"numMessagesInTopic":14,"msgSnippet":"Hi, I ran Heritrix 1.2.0 from command line on a debian machine to do a broadscope crawl on a set of 200 seed urls. I used 15 therads and allocated 1280 Megs of","rawEmail":"Return-Path: &lt;sau_pathak@...&gt;\r\nX-Sender: sau_pathak@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 97213 invoked from network); 14 Apr 2005 19:24:34 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m2.grp.scd.yahoo.com with QMQP; 14 Apr 2005 19:24:34 -0000\r\nReceived: from unknown (HELO n19a.bulk.scd.yahoo.com) (66.94.237.48)\n  by mta5.grp.scd.yahoo.com with SMTP; 14 Apr 2005 19:24:34 -0000\r\nDomainKey-Signature: \r\nReceived: from [66.218.69.1] by n19.bulk.scd.yahoo.com with NNFMP; 14 Apr 2005 19:23:21 -0000\r\nReceived: from [66.218.66.84] by mailer1.bulk.scd.yahoo.com with NNFMP; 14 Apr 2005 19:23:21 -0000\r\nDate: Thu, 14 Apr 2005 19:23:19 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;d3mfv7+h2q5@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 3686\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: &quot;Saurabh Pathak&quot; &lt;sau_pathak@...&gt;\r\nSubject: BroadScope Crawl and out of memory error\r\nX-Yahoo-Group-Post: member; u=126110077\r\nX-Yahoo-Profile: sau_pathak\r\n\r\n\nHi, I ran Heritrix 1.2.0 from command line on a debian machine to do a\nbroadscope crawl on a set of 200 seed urls. I used 15 therads and\nallocated 1280 Megs of memory. I am using the HostQueueFrontier.\nCrawler came to a halt after crawling some 200000 urls (I am not\ncounting URLs containing robot.txt files). I took the thread dump and\nfound out that I ran out of memory. I checked the log files and there\nare no runtime errors and no unusual local-errors. I am pasting a\nsection of progress-statistics.log file, \n\n........\n.......\n20050413133414 CRAWL WAITING TO PAUSE\n20050413133428     1074163     1076251       231446      1.0(4.07)   \n    0(21)          6060            10      1300672\n20050413133448     1074163     1076251       231446      0.0(4.07)   \n    0(21)          6060            10      1300672\n20050413133508     1074163     1076251       231446      0.0(4.07)   \n    0(21)          6060            10      1300672\n20050413133528     1074163     1076251       231446      0.0(4.07)   \n    0(21)          6060            10      1300672\n20050413133548     1074163     1076251       231446      0.0(4.07)   \n    0(21)          6060            10      1300672\n............\n........(no progress starting from above point and the number of busy\nthreads remain 10 and docs/s remains 0)\n\nhere is the thread dump,\n\n&lt;&lt;&lt;\njava.lang.OutOfMemoryError\n&lt;&lt;&lt;\njava.lang.OutOfMemoryError\n&lt;&lt;&lt;\njava.lang.OutOfMemoryError\n&lt;&lt;&lt;\njava.lang.OutOfMemoryError\n     #11   dns:redlandsdailyfacts.interest.com (0 attempts)\n        LLLRLPLLLP http://redlandsdailyfacts.interest.com/calculators.asp\n        Current processor: DNS\n        ACTIVE for 13s303ms\n        Where: ABOUT_TO_BEGIN_PROCESSOR for 13302ms\n\njava.lang.OutOfMemoryError\n&gt;&gt;&gt;\n     #4   \nhttp://gcirm.laregionalonline.com/RealMedia/ads/adstream_nx.ads/realestate.thenewsstar.com/@Right2\n(0 attempts)\n        LLLLLLLE http://www.homefinder.com/thenewsstar/index_map.jhtml\n        Current processor: HTTP\n        ACTIVE for 13s493ms\n        Where: ABOUT_TO_BEGIN_PROCESSOR for 13492ms\n\njava.lang.OutOfMemoryError\n&gt;&gt;&gt;\n     #2    dns:www.local-new-york-information.com (0 attempts)\n        LLLRLPLLLP http://www.local-new-york-information.com/\n        Current processor: Archiver\n        ACTIVE for 15s234ms\n        Where: ABOUT_TO_BEGIN_PROCESSOR for 14981ms\n\njava.lang.OutOfMemoryError\n&gt;&gt;&gt;\n     #15   dns:www.velociwo (1 attempts)\n        LLLLLLLLXP http://www.velociwo/\n        Current processor:\n        ACTIVE for 14s632ms\n        Where: ABOUT_TO_RETURN_URI for 14550ms\n\njava.lang.OutOfMemoryError\n&gt;&gt;&gt;\n\nI need to do a broadScope crawl and I dont know how scalable is\nheritrix when it comes to broadScope crawling. I will be glad if\nsomebody can point me out to a solution. I can allocate more memory\nand reduce the per host settings but that will only prolong the crawl\nbefore it crashes again. I will love to get a more permanent solution\n which will enable me to crawl the complete collection for my seed\nURLs rather then just getting a slightly bigger crawl. \n\nAlso can somebody please tell me if this will work, If I stop the\ncrawl once it runs out of memory and restart it using recover.gz. Also\nhow does one restarts the crawl using recover.gz?\n\nI was thinking of one more temprory solution which I think will give\nme a bigger crawl collection. I can break the seed set into chunks and\nrun crawler seprately for each of them till crawler runs out of memory\nand then merge the results in the end followed by removing the\noverlap. Will this approach work?   \n\nIn the end can somebody please point me to the documentation which\ndescribes how Heritrix manages memory and the data structures it uses. \n\nThanks in advance,\n\n-saurabh \n\n\n\n\n\n"}}