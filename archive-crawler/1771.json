{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (@Internet Archive)","from":"&quot;Gordon Mohr (@Internet Archive)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"SmGqLAb3xQHXVLHYx7rFKUan902x3AL79UUgjAIeWWMSv6Z_rhNUMLwsneR9_qRdGQU6yKtNo6pYBSee3hN0vwKM2DHPZtGYKC-7W5Uj_tOVfPaKfiJ4enIvgSkE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Help! What always NOTCRAWLED","postDate":"1114754573","msgId":1771,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyNzFDRTBELjEwNzAxMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGQ0czlrZys3dGh0QGVHcm91cHMuY29tPg==","referencesHeader":"PGQ0czlrZys3dGh0QGVHcm91cHMuY29tPg=="},"prevInTopic":1770,"nextInTopic":1773,"prevInTime":1770,"nextInTime":1772,"topicId":1760,"numMessagesInTopic":10,"msgSnippet":"... I would guess that this is the problem. You should make sure the actual job you launch has the http-proxy-host and http-proxy-port expert settings in the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 52662 invoked from network); 29 Apr 2005 06:03:01 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m27.grp.scd.yahoo.com with QMQP; 29 Apr 2005 06:03:01 -0000\r\nReceived: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta2.grp.scd.yahoo.com with SMTP; 29 Apr 2005 06:03:00 -0000\r\nReceived: (qmail 91185 invoked from network); 29 Apr 2005 06:02:53 -0000\r\nReceived: from unknown (HELO ?10.0.10.13?) (unknown)\n  by unknown with SMTP; 29 Apr 2005 06:02:53 -0000\r\nX-pair-Authenticated: 67.119.24.131\r\nMessage-ID: &lt;4271CE0D.1070106@...&gt;\r\nDate: Thu, 28 Apr 2005 23:02:53 -0700\r\nUser-Agent: Mozilla Thunderbird 1.0 (X11/20041206)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;d4s9kg+7tht@...&gt;\r\nIn-Reply-To: &lt;d4s9kg+7tht@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: &quot;Gordon Mohr (@Internet Archive)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Help! What always NOTCRAWLED\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\ncash_05 wrote:\n&gt; Dear Gordon,\n&gt;  \n&gt; \n&gt;&gt;The server(s) may be completely unreachable -- so Heritrix isn&#39;t\n&gt;&gt;even getting a 404 for /robots.txt. From the same machine on which\n&gt;&gt;Heritrix is running, can you access the sites with a web browser?\n&gt;&gt;(Do you need to use a web proxy to do so?)\n&gt; \n&gt; \n&gt; The same machine running heritrix can access those sites with web\n&gt; browser. However it is behind a squid proxy server. Proxy setting was\n&gt; defined in global profile.\n&gt; \n&gt; Is it the proxy that causing this problem? If so any work around?\n\nI would guess that this is the problem. You should make sure the\nactual job you launch has the http-proxy-host and http-proxy-port\nexpert settings in the FetchHTTP processor set properly.\n\n&gt;&gt;Alternative, something about the crawl configuration may be preventing\n&gt;&gt;/robots.txt from being fetched. If you have changed the scope, added\n&gt;&gt;filters, or reduced the allowable &#39;max-trans-hops&#39; for your crawl,\n&gt;&gt;this might be the case.\n&gt; \n&gt; \n&gt; I didnt change much on default setting, only reduce those delay time\n&gt; to half the default setting. Instead, for the last try i increase\n&gt; max-trans-hop and link to 5000. So craw job will not stary if no\n&gt; robots.txt been fetch at the first hand? If so how about those site\n&gt; that didn&#39;t contain a robots.txt file?\n\nIf there is no robots.txt at the server, but the server is still reachable,\nit will at least return a 404-not-found, letting the crawler know that\nno robot exclusion rules are in effect, allowing the crawl to continue.\n\n&gt;&gt;Does the crawl show as &#39;finished&#39; very quickly? If so, your crawl\n&gt;&gt;configuration is probably to blame -- the /robots.txt URLs were never\n&gt;&gt;tried.\n&gt;&gt;\n&gt;&gt;If the crawl shows as running for a while, but makes no progress, it&#39;s\n&gt;&gt;retrying the /robots.txt multiple times with pauses between, but\n&gt; \n&gt; failing --\n&gt; \n&gt;&gt;in which case it&#39;s probably your network. (After it is done with\n&gt; \n&gt; retries,\n&gt; \n&gt;&gt;the /robots.txt URLs will show in the crawl.log with whatever error\n&gt;&gt;caused their last failure.)\n&gt;&gt;\n&gt; \n&gt; \n&gt; The craw takes some times to finish, about 7-10 minutes. It show\n&gt; finished on the status.\n&gt; \n&gt; Again, thanks for help.\n\nIf you still have a problem after checking the proxy settings, also\ncheck the logs/crawl.log after the crawl shows as finished -- there\nmay be additional lines/errors for the /robots.txt fetches with\nfurther clues.\n\n- Gordon @ IA\n\n"}}