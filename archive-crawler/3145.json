{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"sVm6NYKJcqOdLns3BjK4JfBDT8bR9oHuy1f-qqsIX1eE_BQ2HCPxVfrBdOzBn06NIGmAAnsZC_2KRKaLubQKt59iQCPr5RMzbyznoA0-","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to configure a bigger BloomUriUniqFilter","postDate":"1154479813","msgId":3145,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVhb3NzNStlNzV0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0Q0ZFN0ZCLjMwNTAzMDlAYXJjaGl2ZS5vcmc+"},"prevInTopic":3142,"nextInTopic":3146,"prevInTime":3144,"nextInTime":3146,"topicId":3138,"numMessagesInTopic":4,"msgSnippet":"... rate ... Actually that might be good enough. My current idea is to have all 8 crawlers (total 8) download 1B pages in total. Assume ideal page distribution","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 94300 invoked from network); 2 Aug 2006 00:51:14 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m28.grp.scd.yahoo.com with QMQP; 2 Aug 2006 00:51:14 -0000\r\nReceived: from unknown (HELO n31.bullet.scd.yahoo.com) (66.94.237.25)\n  by mta6.grp.scd.yahoo.com with SMTP; 2 Aug 2006 00:51:14 -0000\r\nReceived: from [66.218.66.58] by n31.bullet.scd.yahoo.com with NNFMP; 02 Aug 2006 00:50:14 -0000\r\nReceived: from [66.218.66.83] by t7.bullet.scd.yahoo.com with NNFMP; 02 Aug 2006 00:50:14 -0000\r\nDate: Wed, 02 Aug 2006 00:50:13 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;eaoss5+e75t@...&gt;\r\nIn-Reply-To: &lt;44CFE7FB.3050309@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: How to configure a bigger BloomUriUniqFilter\r\nX-Yahoo-Group-Post: member; u=132996324; y=1IwtL31cv1UAOS7AwV9_56jPbiExsQ_5iu_45nYe1ASNqVEphw\r\nX-Yahoo-Profile: joehung302\r\n\r\n\n&gt; &gt; \n&gt; &gt; What does 125M inserts mean? Does that mean I can have 125M URL \n=\r\n&gt; &gt; downloaded with 1 in 4M error rate? (that&#39;s about 30 dups in 125M?)\n&gt; \n=\r\n&gt; Yes -- it means that the predicted false-positive rate inherent to a \n&gt; b=\r\nloom filter won&#39;t go over 1-in-4million (1 in 2^22) up through \n&gt; 125millio=\r\nn inserts.\n&gt; \n&gt; (It will be even lower than that at the beginning, and even=\r\n after \n&gt; 125million it won&#39;t rocket up, but edge up over that target error=\r\n \nrate \n&gt; over time.)\n&gt; \n\nActually that might be good enough. My current id=\r\nea is to have all 8 \ncrawlers (total 8) download 1B pages in total. Assume =\r\nideal page \ndistribution (this is why a CHF-based CrawlMapper is much bette=\r\nr than \nthe current alphabetical CrawlMapper), each crawler would be \nrespo=\r\nnsible for 125M URLs.\n\nLet&#39;s make sure we&#39;re not talking about &quot;queue&#39;ed&quot; U=\r\nRLs \nbut &quot;downloaded&quot; URLs.\n\nPlease confirm. Thanks a lot,\n\n-Joe\n\n\n\n\n\n\n\n\n"}}