{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"LgclkNF8qJedDrx-YoiwNmUnGjNCbTdNLk0dmApU27veDWWq51Vg4VVrPKEgJtCI33mAwRyEdYcVv5I11KDnGqPEBgMvypM","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Storing Crawl Results in Alternate (non-FS) Store","postDate":"1284818692","msgId":6736,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOTRDNzA0LjQwMjA1MDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEFBTkxrVGk9am1FV3c9R0ZGZGZ0S1RYZlk4MzgrUGRfTDVVc2NtVmpzRz1KdkBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGk9YWJnRktrTE1FLXlPZWpYYzFFT2hWWmlKQUw5bzExb01IVzZWNEBtYWlsLmdtYWlsLmNvbT4JPDRDOEZEQUEyLjEwNDA0MDdAYXJjaGl2ZS5vcmc+CTxBQU5Ma1RpPWhrdko5UVZTOWNRR1hFeno2QkZZUTBXUDZldj14XzZISnpXN0xAbWFpbC5nbWFpbC5jb20+CTw0QzkwNDcwNC45MDcwMjAyQGFyY2hpdmUub3JnPiA8QUFOTGtUaT1qbUVXdz1HRkZkZnRLVFhmWTgzOCtQZF9MNVVzY21WanNHPUp2QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":6735,"nextInTopic":0,"prevInTime":6735,"nextInTime":6737,"topicId":6723,"numMessagesInTopic":6,"msgSnippet":"Great to hear of your success! I d forgotten about Doug Judd s HDFS writer and didn t realize it was up-to-date for H3. Yes -- setting a lower threshold on","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 12416 invoked from network); 18 Sep 2010 14:04:53 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m3.grp.sp2.yahoo.com with QMQP; 18 Sep 2010 14:04:53 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta2.grp.sp2.yahoo.com with SMTP; 18 Sep 2010 14:04:53 -0000\r\nX-Received: (qmail 94428 invoked from network); 18 Sep 2010 14:04:51 -0000\r\nX-Received: from 188.22.169.209 (HELO silverbook.local) (188.22.169.209)\n  by relay03.pair.com with SMTP; 18 Sep 2010 14:04:51 -0000\r\nX-pair-Authenticated: 188.22.169.209\r\nMessage-ID: &lt;4C94C704.4020503@...&gt;\r\nDate: Sat, 18 Sep 2010 07:04:52 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.9) Gecko/20100825 Thunderbird/3.1.3\r\nMIME-Version: 1.0\r\nTo: Zach Bailey &lt;zach.bailey@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;AANLkTi=abgFKkLME-yOejXc1EOhVZiJAL9o11oMHW6V4@...&gt;\t&lt;4C8FDAA2.1040407@...&gt;\t&lt;AANLkTi=hkvJ9QVS9cQGXEzz6BFYQ0WP6ev=x_6HJzW7L@...&gt;\t&lt;4C904704.9070202@...&gt; &lt;AANLkTi=jmEWw=GFFdftKTXfY838+Pd_L5UscmVjsG=Jv@...&gt;\r\nIn-Reply-To: &lt;AANLkTi=jmEWw=GFFdftKTXfY838+Pd_L5UscmVjsG=Jv@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Storing Crawl Results in Alternate (non-FS)\n Store\r\nX-Yahoo-Group-Post: member; u=137285340; y=YoNnSrCxXPsfNJoFdapOi0Th7CvJgLGmryLdRZIOz2Jo\r\nX-Yahoo-Profile: gojomo\r\n\r\nGreat to hear of your success! I&#39;d forgotten about Doug Judd&#39;s HDFS \nwriter and didn&#39;t realize it was up-to-date for H3.\n\nYes -- setting a lower threshold on TooManyHopsDecideRule will cause all \nURIs more than that number of hops to get a REJECT scope decision.\n\nFor your purposes you might also want to trim or eliminate the \nTransclusionDecideRule, as well -- because that adds another one or more \n&#39;grace&#39; hops along outlinks that may be necessary to render \nearlier-discovered pages. That&#39;s mostly things that your prior messages \nsuggest aren&#39;t of interest to your project -- JS, CSS, images -- but may \ninclude some things that are of interest -- frames, redirects.\n\n- Gordon @ IA\n\nOn 9/17/10 10:26 AM, Zach Bailey wrote:\n&gt; Thanks for your invaluable feedback as usual Gordon!\n&gt;\n&gt; Just wanted to throw an update out there for anyone interested. I now\n&gt; successfully have heritrix 3 crawling and storing data into HDFS - went\n&gt; this route because it is (for my use case) the best solution as we will\n&gt; be immediately wanting to run various M/R jobs over the fetched data.\n&gt;\n&gt; I suspect this is a rather common use case and as such happy to answer\n&gt; questions about getting it set up in the future. The library I&#39;m using\n&gt; to get heritrix putting its crawl data in hadoop&#39;s HDFS is available\n&gt; here: http://github.com/openplaces/heritrix-hdfs-writer - despite the\n&gt; out of date documentation it has been updated to work with heritrix 3. I\n&gt; got it working locally with a test crawl running hadoop in\n&gt; pseudo-distributed mode.\n&gt;\n&gt; But finally, I had one last question related to setting some crawl\n&gt; parameters. For our use case we&#39;re not very interested in making sure we\n&gt; have a full, &quot;deep&quot; crawl of our seed sites. We&#39;re more interested in a\n&gt; shallow crawl (e.g. the initial home page and maybe 3-5 &quot;hops&quot; from there).\n&gt;\n&gt; Am I correct in thinking that to get this behavior all I need to do is\n&gt; ratchet the TooManyHopsDecideRule.maxHops back to the desired value and\n&gt; that will give me the sort of behavior I&#39;m looking for?\n&gt;\n&gt; Best,\n&gt; -Zach\n&gt;\n&gt; On Wed, Sep 15, 2010 at 12:09 AM, Gordon Mohr &lt;gojomo@...\n&gt; &lt;mailto:gojomo@...&gt;&gt; wrote:\n&gt;\n&gt;     On 9/14/10 3:08 PM, Zach Bailey wrote:\n&gt;\n&gt;         Thanks a lot for your answers, Gordon.\n&gt;\n&gt;         I am absolutely planning on contributing this back for others to\n&gt;         build\n&gt;         on and use, once I figure out how to make it generic enough to\n&gt;         be useful\n&gt;         to others. More than likely I will put it up on my public github\n&gt;         account.\n&gt;\n&gt;         A follow-up question - am I correct in assuming my implementation of\n&gt;         Processor#shouldProcess should be pretty much identical to the\n&gt;         one in\n&gt;         WriterPoolProcessor?\n&gt;\n&gt;\n&gt;     Sure. In fact I just committed a shorter version getting rid of the\n&gt;     nonsensical type check (cruft left over from a previous refactoring).\n&gt;\n&gt;     Of course, if you were interested in writing something in your store\n&gt;     even for nonpositive fetch codes (such as connection-failed errors),\n&gt;     you&#39;d have to loosen the first test. And if there were some\n&gt;     theoretical URI type used in your crawls whose successful &#39;fetch&#39;\n&gt;     left a zero recorded-content length (perhaps an inline &#39;data:&#39; URI?)\n&gt;     but still needed to be written to your store, the second test would\n&gt;     need to loosen. But the tests are reasonable for usual needs\n&gt;     analogous to WARC writing.\n&gt;\n&gt;\n&gt;         Also, what is the best way to get at the raw page content\n&gt;         separate from\n&gt;         the headers (just the content, not the entire response)? Am I\n&gt;         correct in\n&gt;         thinking that this is what I would be provided when calling\n&gt;         crawlURI.getRecorder().getRecordedInput().getContentReplayInputStream()\n&gt;         ?\n&gt;\n&gt;\n&gt;     For usual HTTP response content, yes -- that will give an\n&gt;     InputStream cued up to just past the protocol headers. It does *not*\n&gt;     handle &#39;chunked&#39; encodings, which may be a concern. If your later\n&gt;     analysis might want the headers, you will want to write them\n&gt;     somewhere too. Our goal in the WARC (and earlier ARC) writers was to\n&gt;     record verbatim what came over the wire, to support the greatest\n&gt;     range of possible future analysis applications. (We do no decoding;\n&gt;     no dechunking; no canonicalizing; no header corrections or other\n&gt;     alterations; etc.)\n&gt;\n&gt;     You can also use getReplayCharSequence(), with an optional specified\n&gt;     encoding, to get a charset-decoded CharSequence of the content-body\n&gt;     (which uses temp files and a sliding buffer to avoid decoding giant\n&gt;     content into memory). It also does not handled dechunking properly.\n&gt;\n&gt;     - Gordon @ IA\n&gt;\n&gt;\n&gt;         Thanks,\n&gt;         -Zach\n&gt;\n&gt;         On Tue, Sep 14, 2010 at 4:27 PM, Gordon Mohr &lt;gojomo@...\n&gt;         &lt;mailto:gojomo@...&gt;\n&gt;         &lt;mailto:gojomo@... &lt;mailto:gojomo@...&gt;&gt;&gt; wrote:\n&gt;\n&gt;             Great questions! I know there&#39;s other interest in such writer\n&gt;             alternatives, so I hope you can make whatever you come up with\n&gt;             available to other users, or donate it for possible\n&gt;         inclusion in the\n&gt;             official H3 distribution!\n&gt;\n&gt;             There also has been previous work to create an HBaseWriter for\n&gt;             Heritrix; it might already work in H3, or require only a little\n&gt;             updating. I haven&#39;t yet tried it myself. See:\n&gt;\n&gt;         http://code.google.com/p/hbase-writer/\n&gt;\n&gt;             On to your questions, interspersed below:\n&gt;\n&gt;\n&gt;             On 9/14/10 12:39 PM, Zach Bailey wrote:\n&gt;\n&gt;\n&gt;\n&gt;                 Hi all,\n&gt;\n&gt;                 I wanted to run some questions by you and maybe get some\n&gt;                 pointers from\n&gt;                 the architects of heritrix hoping I could save a little\n&gt;         time before\n&gt;                 attempting this.\n&gt;\n&gt;                 What I am hoping to do is develop a custom writer to\n&gt;         store the\n&gt;                 results\n&gt;                 of my crawls in a distributed database, something like Riak,\n&gt;                 Cassandra,\n&gt;                 or HBase for the Heritrix 3.0 code base.\n&gt;\n&gt;                 I am very comfortable writing Java code and am very\n&gt;         familiar with\n&gt;                 building applications using the Spring Framework so I&#39;m\n&gt;         thinking\n&gt;                 this\n&gt;                 should be a relatively easy task to get a proof of\n&gt;         concept/prototype\n&gt;                 working.\n&gt;\n&gt;                 Looking through the code I had some questions:\n&gt;\n&gt;                 1.) It looks like I will wire in my custom code inside the\n&gt;                 DispositionChain, essentially replacing where the\n&gt;         warcWriter bean is\n&gt;                 referenced.\n&gt;\n&gt;\n&gt;             Yes; you could also have both in place for testing. (I&#39;ve\n&gt;         often run\n&gt;             with both our ARCWriterProcessor and WARCWriterProcessor while\n&gt;             debugging things.) One common gotcha when installing your own\n&gt;             Processors, as edits to our model CXML, is that if you\n&gt;         follow the\n&gt;             pattern there you need to both declare the bean with a name,\n&gt;         then\n&gt;             add the bean by name to the chain&#39;s ordered list. So: two\n&gt;         edits, a\n&gt;             short distance from each other, to have the intended effect.\n&gt;\n&gt;\n&gt;                 2.) Looking at the WARCWriterProcessor it appears it\n&gt;         uses a pooled\n&gt;                 approach. I&#39;m guessing this is for performance reasons.\n&gt;         Assuming I&#39;m\n&gt;                 using a driver which already implements connection\n&gt;         pooling, are\n&gt;                 there\n&gt;                 any other considerations I should think about while\n&gt;         implementing\n&gt;                 my own\n&gt;                 Writer?\n&gt;\n&gt;\n&gt;             The existing pooling was motivated by the belief that having\n&gt;         more\n&gt;             than one active open file at a time could increase throughput.\n&gt;             That&#39;s definitely the case if the files are on independent\n&gt;         disks;\n&gt;             I&#39;m not as sure it helps to have more than one file open on\n&gt;         the same\n&gt;             disk. This has undergone some recent simplification\n&gt;         (eliminating the\n&gt;             dependency on Apache commons-pool) in TRUNK that will appear in\n&gt;             3.0.1, and is likely to change more in the next month as\n&gt;         some other\n&gt;             novel policies for grouping related site captures to\n&gt;         different WARCs\n&gt;             are added.\n&gt;\n&gt;             So I&#39;d say: keep an eye on TRUNK for ideas but don&#39;t assume\n&gt;         anything\n&gt;             in the existing WARCWriterProcessor approach is optimal or\n&gt;             locked-in-place.\n&gt;\n&gt;             I would think writing to a remote distributed store might\n&gt;         offer nice\n&gt;             throughput benefits by essentially fanning the IO out over\n&gt;         many more\n&gt;             disks, whether you were writing WARCs into (eg) HDFS or\n&gt;         individual\n&gt;             records/content-bodies into (eg) HBase.\n&gt;\n&gt;             The least-straightforward part of the current Writer is the\n&gt;             special-case handling of deduced duplicates, from headers or\n&gt;             content-hashes, to change how (or whether) individual\n&gt;         records are\n&gt;             written. There might be an opportunity to factor that\n&gt;         decisionmaking\n&gt;             out of WARCWriterProcessor to be shared with other non-file\n&gt;         or even\n&gt;             non-WARC writers.\n&gt;\n&gt;\n&gt;                 3.) Once I get my custom code developed, I assume all I\n&gt;         need to\n&gt;                 do to\n&gt;                 make it available to heritrix is to jar it up and just\n&gt;         drop it in\n&gt;                 $HERITRIX_HOME/lib - anything else I need to be aware of\n&gt;         there?\n&gt;\n&gt;\n&gt;             That should be all. Then, as I&#39;m sure you know, you just\n&gt;         name the\n&gt;             classes in your crawl-configuration CXML (Spring XML)\n&gt;         bean-declarations.\n&gt;\n&gt;             Hope this helps and let me know any other questions!\n&gt;\n&gt;             - Gordon @ IA\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;         \n&gt;\n&gt;\n\n"}}