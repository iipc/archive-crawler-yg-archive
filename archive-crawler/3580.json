{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":187404531,"authorName":"astar_t","from":"&quot;astar_t&quot; &lt;astar_t@...&gt;","profile":"astar_t","replyTo":"LIST","senderId":"DbvOuRsg8egdQNjyPXbrIBhYTxdNkB-1ae4QDjiqwUMNvIzcJyk3f7YsIOEyaga5sjOzdZruQ2fNwZ-FY1Z_H9RgWxk","spamInfo":{"isSpam":false,"reason":"6"},"subject":"How to stop filtering of already seen URIs based on previous fetch status code?","postDate":"1165603508","msgId":3580,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVsY2JyaytmN2g2QGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":3581,"prevInTime":3579,"nextInTime":3581,"topicId":3580,"numMessagesInTopic":5,"msgSnippet":"Hi, I m running a crawl where initially some URIs were rejected by a DecideRule that was added by mistake.  So the URIs show up in the crawl.log as having a","rawEmail":"Return-Path: &lt;astar_t@...&gt;\r\nX-Sender: astar_t@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 87679 invoked from network); 8 Dec 2006 18:50:07 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m22.grp.scd.yahoo.com with QMQP; 8 Dec 2006 18:50:07 -0000\r\nReceived: from unknown (HELO n7c.bullet.sp1.yahoo.com) (69.147.64.167)\n  by mta5.grp.scd.yahoo.com with SMTP; 8 Dec 2006 18:50:07 -0000\r\nReceived: from [216.252.122.218] by n7.bullet.sp1.yahoo.com with NNFMP; 08 Dec 2006 18:45:08 -0000\r\nReceived: from [66.218.69.5] by t3.bullet.sp1.yahoo.com with NNFMP; 08 Dec 2006 18:45:08 -0000\r\nReceived: from [66.218.66.91] by t5.bullet.scd.yahoo.com with NNFMP; 08 Dec 2006 18:45:08 -0000\r\nDate: Fri, 08 Dec 2006 18:45:08 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;elcbrk+f7h6@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;astar_t&quot; &lt;astar_t@...&gt;\r\nSubject: How to stop filtering of already seen URIs based on previous fetch status code?\r\nX-Yahoo-Group-Post: member; u=187404531; y=7ITduPGZFt7-AJF3-d87Pnne17Rtl7TKkCXmaNm9IuPHkw\r\nX-Yahoo-Profile: astar_t\r\n\r\nHi, I&#39;m running a crawl where initially some URIs were rejected by a \nDecid=\r\neRule that was added by mistake.  So the URIs show up in the \ncrawl.log as =\r\nhaving a -5000 fetch code.  However, I have realized that \nI would actually=\r\n like to crawl a subset of these URIs that have already \nbeen rejected.  \n\n=\r\nSo I tried to import the URIs back into the frontier with force fetch \nvia =\r\nJMX cmdline but they are not being fetched.  I&#39;m assuming this is \nbecause =\r\nthey are &quot;already seen&quot; URIs so the BdbUriUniqFilter is \nignoring them.\n\nIs=\r\n there a way I can force Heritrx to retry a fetch of the URIs that \npreviou=\r\nsly had a -5000 code ?\n\nThanks!\nAdam T.\n\n\n"}}