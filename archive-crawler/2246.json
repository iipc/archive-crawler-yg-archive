{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":83282930,"authorName":"Jay","from":"&quot;Jay&quot; &lt;bighead007us@...&gt;","profile":"bighead007us","replyTo":"LIST","senderId":"heo8xq2uG239r6cEDhOa9FtHxvIGcxo0_A5ua2pcXH10CBnITBUSMSf_aPYBWKiVHIA7YzsWQmX1uVm0_pkwCh-KbJs2","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Seed has not been processed -- Out of Clues","postDate":"1129660767","msgId":2246,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRqM2ZnditpcGpmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGRqMTA0cCtwaTd1QGVHcm91cHMuY29tPg=="},"prevInTopic":2237,"nextInTopic":0,"prevInTime":2245,"nextInTime":2247,"topicId":2231,"numMessagesInTopic":6,"msgSnippet":"Hi Gordon, I get the head from CVS and doing test crawl and so far, I didn t see the problem yet.  But Delete function from View or Edit Frontier URIs is","rawEmail":"Return-Path: &lt;bighead007us@...&gt;\r\nX-Sender: bighead007us@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 14928 invoked from network); 18 Oct 2005 18:39:54 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m23.grp.scd.yahoo.com with QMQP; 18 Oct 2005 18:39:54 -0000\r\nReceived: from unknown (HELO n27.bulk.scd.yahoo.com) (66.94.237.56)\n  by mta6.grp.scd.yahoo.com with SMTP; 18 Oct 2005 18:39:54 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.6] by n27.bulk.scd.yahoo.com with NNFMP; 18 Oct 2005 18:39:29 -0000\r\nReceived: from [66.218.66.81] by mailer6.bulk.scd.yahoo.com with NNFMP; 18 Oct 2005 18:39:29 -0000\r\nDate: Tue, 18 Oct 2005 18:39:27 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;dj3fgv+ipjf@...&gt;\r\nIn-Reply-To: &lt;dj104p+pi7u@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Jay&quot; &lt;bighead007us@...&gt;\r\nSubject: Re: Seed has not been processed -- Out of Clues\r\nX-Yahoo-Group-Post: member; u=83282930; y=7BF2BTuGu8dZGzpQkOe-tbIFx7yLvQkpYEfUll_b49sYrDqan07V\r\nX-Yahoo-Profile: bighead007us\r\n\r\nHi Gordon,\n\nI get the head from CVS and doing test crawl and so far, I didn=\r\n&#39;t see\nthe problem yet.  But Delete function from &quot;View or Edit Frontier\nUR=\r\nIs&quot; is broken, I think.  I pause the crawl and I go into the\nFrontier URI a=\r\nnd do search on &quot;.*&quot; or &quot;.*xyz.com.*&quot;, I get the list of\nurls that I wanted=\r\n to delete, at that time if I click delete, I got\nthis error message.\n\nAn e=\r\nrror occured\n\njava.lang.UnsupportedOperationException\n\njava.lang.Unsupporte=\r\ndOperationException\n\tat org.archive.util.CachedBdbMap.entrySet(CachedBdbMap=\r\n.java:332)\n\tat java.util.AbstractMap$4.(AbstractMap.java:432)\n\tat java.util=\r\n.AbstractMap$3.iterator(AbstractMap.java:431)\n\tat\njava.util.Collections$Syn=\r\nchronizedCollection.iterator(Collections.java:1553)\n\tat\norg.archive.crawler=\r\n.frontier.WorkQueueFrontier.deleteURIs(WorkQueueFrontier.java:917)\n\tat\norg.=\r\narchive.crawler.admin.CrawlJob.deleteURIsFromPending(CrawlJob.java:1085)\n\ta=\r\nt\norg.archive.crawler.admin.CrawlJobHandler.deleteURIsFromPending(CrawlJobH=\r\nandler.java:1244)\n\tat\norg.archive.crawler.jspc.admin.console.frontier_jsp._=\r\njspService(Unknown\nSource)\n\tat org.apache.jasper.runtime.HttpJspBase.servic=\r\ne(HttpJspBase.java:137)\n\tat javax.servlet.http.HttpServlet.service(HttpServ=\r\nlet.java:853)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHol=\r\nder.java:358)\n\tat\norg.mortbay.jetty.servlet.WebApplicationHandler$Chain.doF=\r\nilter(WebApplicationHandler.java:342)\n\tat org.archive.crawler.admin.ui.Root=\r\nFilter.doFilter(RootFilter.java:67)\n\tat\norg.mortbay.jetty.servlet.WebApplic=\r\nationHandler$Chain.doFilter(WebApplicationHandler.java:334)\n\tat\norg.mortbay=\r\n.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:28=\r\n6)\n\tat\norg.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:=\r\n567)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1807)\n\tat\norg=\r\n.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.j=\r\nava:525)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1757)\n\tat=\r\n org.mortbay.http.HttpServer.service(HttpServer.java:879)\n\tat org.mortbay.h=\r\nttp.HttpConnection.service(HttpConnection.java:789)\n\tat org.mortbay.http.Ht=\r\ntpConnection.handleNext(HttpConnection.java:960)\n\tat org.mortbay.http.HttpC=\r\nonnection.handle(HttpConnection.java:806)\n\tat\norg.mortbay.http.SocketListen=\r\ner.handleConnection(SocketListener.java:218)\n\tat org.mortbay.util.ThreadedS=\r\nerver.handle(ThreadedServer.java:300)\n\tat org.mortbay.util.ThreadPool$PoolT=\r\nhread.run(ThreadPool.java:511)\n\n\nSince I am actively using this functionali=\r\nty, I would be gladly\nupgrade my crawlers if this function is working.\n\n\n\nT=\r\nhanks a bunch,\n\nJay\n\n\n\n\n\n--- In archive-crawler@yahoogroups.com, &quot;Jay&quot; &lt;big=\r\nhead007us@y...&gt; wrote:\n&gt;\n&gt; Hello Gordon,\n&gt; \n&gt; Thanks for your suggestion.  =\r\nI have and just tried everything except\n&gt; updating the code which I will tr=\r\ny in momentarily.  My comments\n&gt; following yours.\n&gt; \n&gt; &gt; (2) At a time when=\r\n all but 1 of the URLs have succeeded, call\n&gt; &gt; up a &#39;Frontier Report&#39;. Wha=\r\nt is the status of the queue holding\n&gt; &gt; the problem URL? If it&#39;s in a mult=\r\ni-minute &#39;snooze&#39;, the topmost\n&gt; &gt; URL (which may be one of your URL&#39;s prer=\r\nequisites) was tried and\n&gt; &gt; failed and is awaiting retry. The default sett=\r\nings take many\n&gt; &gt; hours to finally give up on an URL, since transient netw=\r\nork\n&gt; &gt; problems are common, and once an URL is makred as &#39;failed&#39;,\n&gt; &gt; it =\r\nis no longer retried.\n&gt; \n&gt; I just tried with 17 url seed file and 4 of them=\r\n got that message\n&gt; &quot;Seeds has not been processed&quot;.  If i terminate the job=\r\n and rerun\n&gt; again, I got all of them crawled.  And those 4 problemtic seed=\r\n url are\n&gt; not in any of the log file and as well as not in any of the queu=\r\nes in\n&gt; &#39;Frontier Report&#39;.\n&gt; \n&gt; \n&gt; &gt; (3) Look in the &#39;local-errors.log&#39;. Ma=\r\nny transient errors which\n&gt; &gt; do not mark a URI as &#39;failed for good&#39; appear=\r\n here. Many resolve\n&gt; &gt; with a later retry, but seeing what sort of error i=\r\ns affecting\n&gt; &gt; the URL(s) which remain unprocessed could help determine wh=\r\nat&#39;s\n&gt; &gt; happening.\n&gt; \n&gt; I did grep on those url in the log directory, not =\r\nin any of the files\n&gt; even after I stop the crawl.\n&gt; \n&gt; &gt; Similarly, you co=\r\nuld run a test crawl with very small\n&gt; &gt; Frontier &#39;max-retries&#39; and &#39;retry-=\r\ndelay-seconds&#39; settings --\n&gt; &gt; perhaps &#39;4&#39; and &#39;30&#39;. Then, unless the probl=\r\nem is some sort\n&gt; &gt; of long network hang, all URLs should either succeed or=\r\n fail\n&gt; &gt; within a few minutes -- at least getting you error information\n&gt; =\r\n&gt; in the &#39;crawl.log&#39; about everything.\n&gt; \n&gt; my current max-retries is 3 and=\r\n retry-delay-sec is 2000 and\n&gt; delay-factor is 0.  \n&gt; \n&gt; &gt; (4) Check to see=\r\n if a substantially similar URL has been crawled.\n&gt; &gt; For example, if you a=\r\nre having problems with &#39;http://example.com&#39;\n&gt; &gt; showing as never-crawled, =\r\nsee if &#39;http://www.example.com&#39; has been\n&gt; &gt; crawled. Our default canonical=\r\nization rules will consider these\n&gt; &gt; the same URL in most situations, and =\r\nuntil recently (and perhaps\n&gt; &gt; still) certain combinations of URL seed/dis=\r\ncovery ordering,\n&gt; &gt; redirections, and scope configuration can inadvertentl=\r\ny keep the\n&gt; &gt; &#39;right&#39; variant from being crawled, once the &#39;wrong&#39; variant=\r\n\n&gt; &gt; has already marked the URL as done.\n&gt; \n&gt; I tried as well.  All of my s=\r\needs are fully qualified\n&gt; http://www.xyz.com and as I explained above, the=\r\ny are not consistent\n&gt; on one url.\n&gt; \n&gt; And the error is not consistent, I =\r\ncouldn&#39;t guess when it will happen\n&gt; or on what URL.  But mostly it is righ=\r\nt after the crawler is started.\n&gt;  Pretty rare in the re-run without shutti=\r\nng down the crawler.\n&gt; \n&gt; \n&gt; Thanks for any of the input.  I&#39;ll try the lat=\r\nest head and update as\nwell.\n&gt; \n&gt; -- Jay\n&gt;\n\n\n\n\n"}}