{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"10rgXvTwbKGGDync1N9bTUDxspG7y2j4rdgqhzgcdetJ_Kyky1nN2QDxGDHpMhNK7cTivBrbYiW06aeDYm7MR5j9mk37cTzW","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Parallelizing crawler","postDate":"1152900917","msgId":3052,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QjdERjM1LjMwNDA4MDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGU5OGlxZCtpMXMzQGVHcm91cHMuY29tPg==","referencesHeader":"PGU5OGlxZCtpMXMzQGVHcm91cHMuY29tPg=="},"prevInTopic":3051,"nextInTopic":3060,"prevInTime":3051,"nextInTime":3053,"topicId":3043,"numMessagesInTopic":16,"msgSnippet":"... Here s a few items you d have to contend with if you split Heritrix: + You ll have to spend alot of resources serializing and deserializing passing rich","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 93328 invoked from network); 14 Jul 2006 18:15:37 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m41.grp.scd.yahoo.com with QMQP; 14 Jul 2006 18:15:37 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta5.grp.scd.yahoo.com with SMTP; 14 Jul 2006 18:15:36 -0000\r\nReceived: from [192.168.1.106] ([192.168.1.106])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k6EGxEk32278\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 14 Jul 2006 09:59:14 -0700\r\nMessage-ID: &lt;44B7DF35.3040801@...&gt;\r\nDate: Fri, 14 Jul 2006 11:15:17 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.4) Gecko/20060516 SeaMonkey/1.0.2\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e98iqd+i1s3@...&gt;\r\nIn-Reply-To: &lt;e98iqd+i1s3@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: Parallelizing crawler\r\nX-Yahoo-Group-Post: member; u=168599281; y=Dz0PMwECe4rvfBoY6WnHWaCM0eiJW9MA-Ft8rUNZctnOADgeuQojjm0k\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nmolzbh wrote:\n&gt;\n&gt; Cool. Anyways my thoughts on splitting the architecture were to split\n&gt; the Processors across various machines. Have a Statistics server\n&gt; maintain statistics, and a single frontier.\n&gt;\n\n\n\n\nHere&#39;s a few items you&#39;d have to contend with if you split Heritrix:\n\n+ You&#39;ll have to spend alot of resources serializing and deserializing \npassing rich URLs and configurations.\n+ Certain processors, as written, expect to find on local disk the \ndownloaded resources (You could share them using NFS.  NFS won&#39;t take \ntoo kindly to the crawler&#39;s heavy I/O).\n\n\n&gt; My idea was to have\n&gt; frontier emit a batch of URLS to the processore boxes, hence instead\n&gt; of next() you have nextBatch(). Anyways, I guess this involves a lot\n&gt; of work, hence I am going in for a Peer2Peer setup, with fully loaded\n&gt; agents doing the URL splits and emmitting batches to the others. I am\n&gt; wondering however on the efficiency of JMX with RMI connectors to do\n&gt; this Job. Do you think going the plain RMI way would be faster?\n&gt;\n\n\n\n\n\n\n\n\nWe haven&#39;t measured.  JMX is heavyweight but works.\n\nOne suggestion has been to use JGroups.  Friends of Heritrix were \nexperimenting making a &#39;cluster&#39; subclass of Frontier.  On \ninitialization, it joined a JGroups group and listened in for URL \nbroadcasts.  Those that fit its portion of the URL space, it picked off \nthe bus and added to the local Frontier.  Those URLs meant for another \ncrawler, were broadcast (They&#39;ve said they&#39;ll write the list if findings \nprove promising).\n\nBut depends on your requirements.  Joe Hungs&#39; group didn&#39;t bother \nswapping URLs across the cluster.  Their experience was that the \ncrawlers had sufficient work without exchanging URLs and figured that \neach crawler would discover its URL-segment important pages anyways \nwithout having to have injection from peers (I don&#39;t know if they \n&#39;proved&#39; this assertion).\n\n\n&gt; Sorry\n&gt; for pounding you with questions but I am still in the design phase of\n&gt; the system looking for a billion plus crawl, and since it won&#39;t be a\n&gt; one time thing I am also looking at scalability and agent join/leave\n&gt; mechanisms.\n&gt;\n\n\n\n\n\n\nCan we collaborate?  This is a problem we need to solve ourselves.\n\nSt.Ack\n\n&gt; --- In archive-crawler@yahoogroups.com \n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Michael Stack &lt;stack@...&gt; \n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Anmol Bhasin wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Thanks! I am wondering however if we leave the central frontier\n&gt; &gt; &gt; machine concept out for a bit, would there be anybenfit to split URL\n&gt; &gt; &gt; space in place of Split Architechture.\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; &gt; I am trying to do quick big crawl, hence wondering which approaches\n&gt; &gt; &gt; are the best ways to get moving.\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; If you&#39;re in a hurry, split URL space (and throw hardware at it). Some\n&gt; &gt; fairly large crawls have been achieved using this technique both by us\n&gt; &gt; -- 200million plus -- and by others (See the testimonial cited in the\n&gt; &gt; previous where Joe Hung and his compaï¿½eros did a 1Billion+ pages).\n&gt; &gt;\n&gt; &gt; What were you thinking regards splitting the architecture?\n&gt; &gt;\n&gt; &gt; Yours,\n&gt; &gt; St.Ack\n&gt; &gt;\n&gt;\n&gt;  \n\n\n"}}