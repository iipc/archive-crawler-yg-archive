{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":180958640,"authorName":"nickbirren","from":"&quot;nickbirren&quot; &lt;nickbirren@...&gt;","profile":"nickbirren","replyTo":"LIST","senderId":"cLrHZ-UREEH3_vr1FdClCzWxd_XyW6ayiiP4cTShQEy1j26xyP0Kt-9j7efwA9y53LA4FA0AI9ambAITA5iU8Dwf0bfSlAzCSyU","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: &quot;Balanced&quot; crawl frontier","postDate":"1215289699","msgId":5347,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGc0b2xoMys2M28yQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ4NkMxRDQ3LjUwODA4MDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":5343,"nextInTopic":5350,"prevInTime":5346,"nextInTime":5348,"topicId":5342,"numMessagesInTopic":5,"msgSnippet":"My problem is that when I use 20 seeds (each for a different site), the ones that have many sub-domains seem to get more crawler attention . If I understand","rawEmail":"Return-Path: &lt;nickbirren@...&gt;\r\nX-Sender: nickbirren@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 80713 invoked from network); 5 Jul 2008 20:28:21 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m36.grp.scd.yahoo.com with QMQP; 5 Jul 2008 20:28:21 -0000\r\nX-Received: from unknown (HELO n4.bullet.mail.re1.yahoo.com) (69.147.103.131)\n  by mta15.grp.scd.yahoo.com with SMTP; 5 Jul 2008 20:28:21 -0000\r\nX-Received: from [68.142.237.90] by n4.bullet.mail.re1.yahoo.com with NNFMP; 05 Jul 2008 20:28:20 -0000\r\nX-Received: from [66.218.69.3] by t6.bullet.re3.yahoo.com with NNFMP; 05 Jul 2008 20:28:20 -0000\r\nX-Received: from [66.218.66.73] by t3.bullet.scd.yahoo.com with NNFMP; 05 Jul 2008 20:28:20 -0000\r\nDate: Sat, 05 Jul 2008 20:28:19 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g4olh3+63o2@...&gt;\r\nIn-Reply-To: &lt;486C1D47.5080805@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;nickbirren&quot; &lt;nickbirren@...&gt;\r\nSubject: Re: &quot;Balanced&quot; crawl frontier\r\nX-Yahoo-Group-Post: member; u=180958640; y=-uLp7ETKCmPYMydjqSm0gdpMWPvAnJAklZI1Frw-afW3UGbivQ\r\nX-Yahoo-Profile: nickbirren\r\n\r\nMy problem is that when I use 20 seeds (each for a different site),\nthe one=\r\ns that have many sub-domains seem to get more crawler\n&quot;attention&quot;. If I und=\r\nerstand correctly, each sub-domain will have its\nown queue?\n\n--- In archive=\r\n-crawler@yahoogroups.com, Noah Levitt &lt;nlevitt@...&gt; wrote:\n&gt;\n&gt; Heritrix doe=\r\ns a pretty good job of running in a &quot;balanced&quot; manner by \n&gt; default. The li=\r\nst of urls yet to be crawled (the frontier) is divided \n&gt; into queues by ho=\r\nst, and there is a delay between fetches of each\nurl in \n&gt; a single queue, =\r\nso hosts/queues generally aren&#39;t starved out. Have you \n&gt; encountered this =\r\nproblem in a real crawl?\n&gt; \n&gt; Noah\n&gt; \n&gt; nickbirren wrote:\n&gt; &gt; Is there a wa=\r\ny with Heritrix to define a job with many starting\n&gt; &gt; points, one per site=\r\n, then have the job run in a &quot;balanced&quot; manner so\n&gt; &gt; that each site gets i=\r\nts share of bandwidth / processing. I&#39;m trying to\n&gt; &gt; &quot;avoid starvation&quot; of=\r\n sites that do not have many links in them on\n&gt; &gt; each page by other sites =\r\nwho do, and if I&#39;m not mistaken, will &quot;take\n&gt; &gt; over&quot; the crawl frontier af=\r\nter a few iterations.\n&gt; &gt;\n&gt; &gt; My other option is to run many crawl engines,=\r\n one per site, but this\n&gt; &gt; seems cumbersome and is probably less scaleable=\r\n.\n&gt; &gt;\n&gt; &gt; Thanks,\n&gt; &gt; -Nick\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; -----------------------------------=\r\n-\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}