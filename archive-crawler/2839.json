{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"6ec7pt27jcHKTyKLiO_ze-r4Ah62TrLKVKAs6maqbYFslswzaiQYQiCkr9WNHjpJP3ZfgjQN9gRCpW5au8Pp5ItXT6hFJoo","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Crawling whole czech domain","postDate":"1147116570","msgId":2839,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0NUY5QzFBLjcwNzAxMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIwMDYwNTA2MTAzNC4xMTQxNUBjZW50cnVtLmN6Pg==","referencesHeader":"PDIwMDYwNTA2MTAzNC4xMTQxNUBjZW50cnVtLmN6Pg=="},"prevInTopic":2837,"nextInTopic":2841,"prevInTime":2837,"nextInTime":2840,"topicId":2837,"numMessagesInTopic":5,"msgSnippet":"... Some suggestions regarding the OOME: - If you are using the ExtractorSWF, use the latest release (1.8.0, officially out just today) or disable the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 90110 invoked from network); 8 May 2006 19:30:57 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m24.grp.scd.yahoo.com with QMQP; 8 May 2006 19:30:57 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta7.grp.scd.yahoo.com with SMTP; 8 May 2006 19:30:57 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 6EDC914154E58\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  8 May 2006 12:29:26 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 18567-02-28 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 8 May 2006 12:29:26 -0700 (PDT)\r\nReceived: from [192.168.1.9] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 03C23140D8694\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  8 May 2006 12:29:25 -0700 (PDT)\r\nMessage-ID: &lt;445F9C1A.7070102@...&gt;\r\nDate: Mon, 08 May 2006 12:29:30 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;200605061034.11415@...&gt;\r\nIn-Reply-To: &lt;200605061034.11415@...&gt;\r\nContent-Type: text/plain; charset=windows-1250; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Crawling whole czech domain\r\nX-Yahoo-Group-Post: member; u=137285340; y=FrdM0h65KOjfPhm2RyxcrPc3TT4pcwk40x_8Xl--8-uc\r\nX-Yahoo-Profile: gojomo\r\n\r\nAdam Brokes wrote:\n&gt; Hallo,\n&gt; I know the OutOfMemory problem was there mentioned many times, but i could not find any topic that can solve my problem.\n&gt; I am working on crawling whole czech domain. I have got list of 220 000 seeds (2.level domains) and I do it by SurtPrefixScope.\n&gt; Machine specs:\n&gt; 900 MHz PIII\n&gt; 4GB RAM\n&gt; 4GB swap\n&gt; Debian Linux\n&gt; 32b JVM\n&gt; I started java with JAVA_OPTS=&quot; -Xms1500m -Xmx1500m -Xss512k&quot;.\n&gt; But after few hours i have got alerts with OOM exceptions and crawl stoped. I set 5 toethreads and I hope the crawl will go on without errors.\n&gt; If it will be necessary I will copy my order file.\n&gt; So question is: what is the most important settings in so big crawl?\n&gt; Thanks a lot. Adam B.\n\nSome suggestions regarding the OOME:\n\n  - If you are using the ExtractorSWF, use the latest release (1.8.0, \nofficially out just today) or disable the extractor. The 3rd-party \nlibrary used can inflate small bitmaps (&lt;100K) in the source SWF to \n100MB or more of heap space momentarily, which has been implicated in \nsome OOMEs we&#39;ve seen. (By the time the OOME is caught and the heap \nexamined, the temporary giant inflated maps are gone, which made this \npretty hard to track down.)\n\n  - See this blog entry at Sleepycat:\n    http://blog.sleepycat.com/2006/04/adler32-vs-gc.html\n    In Heritrix 1.8, our launcher script sets this workaround flag \nalways just to be safe. According to Sun, the underlying bug has been \nfixed in Java 1.4.2_11 and the latest 1.6 beta (&quot;mustang&quot;) releases... \nbut not yet any 1.5.0 release. Unfortunately, I believe there is a risk \nthat our heavy use of Java&#39;s gzip/deflate functionality would be another \nway to trigger this JVM bug, and there is no easy workaround. So if \nmysterious OOMEs continue and you are using a 1.5.0 release, consider \nrolling back to 1.4.2_11. (We use 1.5.0_06, though.)\n\n  - There are several kinds of OOMEs -- some have to do with exhaustion \nof native resources (sometimes not even memory) other than the Java \nheap. Always make note of the exact message. (Though Java 1.6 beta is \nnot yet officially supported for Heritrix, if you have a reliably \nreproduceable OOME its improved OOME stacks and \n-XX:+HeapDumpOnOutOfMemoryError are valuable for a Java head to get to \nthe bottom of the problem).\n\n- Is there a reason you&#39;ve set a non-default -Xss stack size? (We&#39;ve \nonly seen StackOverflowErrors very rarely, on pathologically odd \ncontent, and there&#39;s handling code around the places where they might \noccur that should recover cleanly.) A large number here increases the \noverhead per thread, and this comes out of non-heap memory.. so can \nsometimes lead to non-heap OOMEs.\n\nOther comments:\n\n- That should be enough memory to run 200 threads or more, though that \nmight be an excessive number for that processor. (The point of \ndiminishing or negative returns to a larger number of threads might \nbegin much lower than 200 on your system: it&#39;s still just trial and \nerror to discover the best tally for good throughput.)\n\n- If you are certain you will end your crawl at a specific size, you \nmight prefer to use a Bloom-filter based &#39;already-seen&#39; structure \n(UriUniqFilter) rather than the default BDB approach. See this prior \nmessage for details:\n   http://groups.yahoo.com/group/archive-crawler/message/2450\n(If you do, you should also decrease the BDB cache percentage, which is \nby default 60% of heap, to 30% or lower because it&#39;s no longer doing one \nof its biggest jobs and the Bloom filter needs the space instead.)\n\nHope this helps,\n\n- Gordon\n\n"}}