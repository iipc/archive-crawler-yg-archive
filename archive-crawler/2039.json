{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":160886731,"authorName":"Marco Baroni","from":"Marco Baroni &lt;baroni@...&gt;","profile":"kumaraja2000","replyTo":"LIST","senderId":"Z3Nvq2XI1kiLTzmdNFp82DmwAa8oVBSk6dHRFpQ80OyoYfgsjQOxAsMZofBRJpcjdJ-_Fo7eP3MgfxCehn5VLSjRsrqa86jJ_HTOUA","spamInfo":{"isSpam":false,"reason":"4"},"subject":"minimizing overlap","postDate":"1120736287","msgId":2039,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFBpbmUuTE5YLjQuNTguMDUwNzA3MTMyODQ3MC4yMTU4NkBlaW5zdGVpbi5zc2xtaXQudW5pYm8uaXQ+"},"prevInTopic":0,"nextInTopic":2042,"prevInTime":2038,"nextInTime":2040,"topicId":2039,"numMessagesInTopic":3,"msgSnippet":"Dear all, I was happily crawling the web, when I ve got an out-of-memory error and heritrix hanged up. I tried to restart the crawl through the recovery","rawEmail":"Return-Path: &lt;baroni@...&gt;\r\nX-Sender: baroni@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 7090 invoked from network); 7 Jul 2005 11:38:15 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m24.grp.scd.yahoo.com with QMQP; 7 Jul 2005 11:38:15 -0000\r\nReceived: from unknown (HELO einstein.sslmit.unibo.it) (137.204.200.1)\n  by mta6.grp.scd.yahoo.com with SMTP; 7 Jul 2005 11:38:14 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby einstein.sslmit.unibo.it (Postfix) with ESMTP id 142FC1AA35\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  7 Jul 2005 13:38:12 +0200 (CEST)\r\nReceived: from einstein.sslmit.unibo.it ([127.0.0.1])\n by localhost (einstein.sslmit.unibo.it [127.0.0.1]) (amavisd-new, port 10024)\n with ESMTP id 19404-07 for &lt;archive-crawler@yahoogroups.com&gt;;\n Thu,  7 Jul 2005 13:38:08 +0200 (CEST)\r\nReceived: from localhost (localhost [127.0.0.1])\n\t(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))\n\t(No client certificate requested)\n\tby einstein.sslmit.unibo.it (Postfix) with ESMTP id 079AE4AD5\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  7 Jul 2005 13:38:07 +0200 (CEST)\r\nDate: Thu, 7 Jul 2005 13:38:07 +0200 (CEST)\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;Pine.LNX.4.58.0507071328470.21586@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: TEXT/PLAIN; charset=US-ASCII\r\nX-Virus-Scanned: by amavisd-new at sslmit.unibo.it\r\nX-eGroups-Msg-Info: 2:4:8\r\nFrom: Marco Baroni &lt;baroni@...&gt;\r\nSubject: minimizing overlap\r\nX-Yahoo-Group-Post: member; u=160886731; y=H6Vn0kgEKLF6e9smTNUXPabKdZ7Ka0ePVBbVfpaL8m4_ZkA36c0B\r\nX-Yahoo-Profile: kumaraja2000\r\n\r\nDear all,\n\nI was happily crawling the web, when I&#39;ve got an out-of-memory error and \nheritrix hanged up.\n\nI tried to restart the crawl through the recovery option, but heritrix\ndied again with an error (pasted at the bottom of this mail) complaining\nabout the recover log file being corrupted.\n\nNow, my question is: given that it looks like I have to start the crawl \nagain, is there a way to do so that would minimize overlap with the \nprevious crawl?\n\nFor example, can I use the list of urls in the previous crawl&#39;s crawl.log\nfile as a list of pages that heritrix should NOT download this time? If \nso, how?\n\nFor my purposes, I don&#39;t care if the crawl is &quot;complete&quot;, i.e., I prefer\nto avoid crawling the same pages twice, even if this means that I will\nalso fail to download pages I originally intended to download.\n\nAll this, as part of a surt-prefix crawl based on two simple patterns \n(pages from .de and .at top domains).\n\nAny hint greatly appreciated.\n\nThanks in advance.\n\nRegards,\n\nMarco\n\n************************************\nError Message:\n\nTitle:   \t \norg.archive.crawler.framework.exceptions.FatalConfigurationException on \ncrawl: delarger2\nTime:  \tJul. 6, 2005 11:15:54 GMT\nLevel:  \tCONFIG\nMessage:  \t\n\nUnable to setup crawl modules: \norg.archive.crawler.framework.exceptions.FatalConfigurationException: \nRecover.log problem: java.io.IOException: Corrupt GZIP trailer\n\nAssociated Throwable: \norg.archive.crawler.framework.exceptions.FatalConfigurationException: \nRecover.log problem: java.io.IOException: Corrupt GZIP trailer\n\n  Message:\n    Recover.log problem: java.io.IOException: Corrupt GZIP trailer\n\n  Cause:\n    java.io.IOException: Corrupt GZIP trailer\n\n  Stacktrace:\norg.archive.crawler.framework.exceptions.FatalConfigurationException: \nRecover.log problem: java.io.IOException: Corrupt GZIP trailer\n\tat \norg.archive.crawler.framework.CrawlController.setupCrawlModules(CrawlController.java:585)\n\tat \norg.archive.crawler.framework.CrawlController.initialize(CrawlController.java:336)\n\tat \norg.archive.crawler.admin.CrawlJobHandler.startNextJobInternal(CrawlJobHandler.java:1050)\n\tat \norg.archive.crawler.admin.CrawlJobHandler$2.run(CrawlJobHandler.java:1016)\n\tat java.lang.Thread.run(Thread.java:534)\nCaused by: java.io.IOException: Corrupt GZIP trailer\n\tat \njava.util.zip.GZIPInputStream.readTrailer(GZIPInputStream.java:174)\n\tat java.util.zip.GZIPInputStream.read(GZIPInputStream.java:89)\n\tat \nsun.nio.cs.StreamDecoder$CharsetSD.readBytes(StreamDecoder.java:408)\n\tat \nsun.nio.cs.StreamDecoder$CharsetSD.implRead(StreamDecoder.java:450)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:182)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:167)\n\tat java.io.BufferedReader.fill(BufferedReader.java:136)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:299)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:362)\n\tat \norg.archive.crawler.frontier.RecoveryJournal.importRecoverLog(RecoveryJournal.java:179)\n\tat \norg.archive.crawler.frontier.AbstractFrontier.importRecoverLog(AbstractFrontier.java:695)\n\tat \norg.archive.crawler.framework.CrawlController.setupCrawlModules(CrawlController.java:582)\n\t... 4 more\n\n\n\n-- \nMarco Baroni\nSSLMIT, University of Bologna\nhttp://sslmit.unibo.it/~baroni\n\n"}}