{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"oZ3xH7lOIgQnLu5IMxeq6WXa7-F_vKT_qTZkeKp-xRxd77OT-43-fZ3OSCjso_x2SHIkzcSHG0j9vcQh1aGeeb2fmu09pTQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Few questions about Heritrix 1 [1 Attachment]","postDate":"1303859159","msgId":7130,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREQjc0RkQ3LjgwMjAwQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PEJBTkxrVGluYk5kellKQmc1elFwY2lOX29hPXBnakJwZHFRQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PEJBTkxrVGluYk5kellKQmc1elFwY2lOX29hPXBnakJwZHFRQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":7129,"nextInTopic":7131,"prevInTime":7129,"nextInTime":7131,"topicId":7129,"numMessagesInTopic":3,"msgSnippet":"Hey! Lots of questions! Comments below inline... ... These first 3 issues seem similar -- overrides not sticking -- so may be ... The key here is to narrow","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 68571 invoked from network); 26 Apr 2011 23:06:06 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.sp2.yahoo.com with QMQP; 26 Apr 2011 23:06:06 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.sp2.yahoo.com with SMTP; 26 Apr 2011 23:06:06 -0000\r\nX-Received: (qmail 93511 invoked by uid 0); 26 Apr 2011 23:06:00 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 26 Apr 2011 23:06:00 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4DB74FD7.80200@...&gt;\r\nDate: Tue, 26 Apr 2011 16:05:59 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.15) Gecko/20110303 Thunderbird/3.1.9\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: =?UTF-8?B?QWRhbSBCcm9rZcWh?= &lt;adam.brokes@...&gt;\r\nReferences: &lt;BANLkTinbNdzYJBg5zQpciN_oa=pgjBpdqQ@...&gt;\r\nIn-Reply-To: &lt;BANLkTinbNdzYJBg5zQpciN_oa=pgjBpdqQ@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Few questions about Heritrix 1 [1 Attachment]\r\nX-Yahoo-Group-Post: member; u=137285340; y=fwpUhoayTDUNV0GJMmkUMp5WUVP1Qm3XcAV4JRTJEP4V\r\nX-Yahoo-Profile: gojomo\r\n\r\nHey! Lots of questions! Comments below inline...\n\nOn 4/26/11 1:20 AM, Adam Brokeš wrote:\n&gt; Hi all,\n&gt;\n&gt; we have found few strange issues when we have been building our new\n&gt; crawl strategy. Basically what we want to do is having three separate\n&gt; MatchesListRegExpDecideRules, all with different purpose. One is for\n&gt; global excludes which are set by publishers (e.g. eshop on their\n&gt; website), second is for global traps (like calendars, cms generated\n&gt; forms) and local excludes which are set only for particular website\n&gt; (if the trap is on one domain but could not be global because it could\n&gt; prevent from downloading meaningful stuff).\n&gt;\n&gt; The process is as follow:\n&gt; 1] we export global excludes from our database and insert into the\n&gt; last job based on desired profile (if we would do that on profile we\n&gt; have to copy every change in settings to that profile)\n&gt; 2] we export global traps from document and insert them into order.xml\n&gt; like the step above.\n&gt; 3] for every domain which have local trap we set override and set the\n&gt; traps in localTrapsRegexpList (this list is on the top level empty)\n&gt;\n&gt; So far I think it is not something complex and should work fine, but\n&gt; we encounter few problems:\n&gt;\n\nThese first 3 issues seem similar -- overrides not sticking -- so may be \nsome previously unrecognized bug (or related bugs):\n\n&gt; 1] when we have job with some expressions in localTrapsRegexpList and\n&gt; we build new job based on it, when I go to override which should have\n&gt; regexp, it is there, but the checkbox next to the listbox is unchecked\n&gt; and thus if I do not check it, the list is set as empty (operators\n&gt; often forgot about it, but I think it is not the desired behavior).\n&gt; 2] when the job is running and we change for example total budget\n&gt; limit (it is the value which is always changed during the crawl) and\n&gt; rise it from 10k to 15k, some of overrides get lost\n&gt; 3] sometimes the overrides get lost without any obvious reason, we are\n&gt; still figuring it out and narrowing the possible reasons for it.\n\nThe key here is to narrow down the steps to reproduce. I&#39;ll try, but \nalso please let us know any other patterns you&#39;ve notices, and \nespecially if you&#39;re ever editing the on-disk order.xml/overrides \noutside the web UI during a crawl. (There are a few situations where \nit&#39;s OK to do so, but it could be part of the trigger situation.)\n\nI&#39;ve created an issue to track this investigation:\n\nhttps://webarchive.jira.com/browse/HER-1886\n\n&gt; 4] our budgeting policy is set on\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Frontier+queue+budgets\n&gt; - when we do the midcrawl adjustments in some cases the change is not\n&gt; exteriorized to the frontier and the retired queue which has increased\n&gt; budget is still retired and the old number of total budget is shown in\n&gt; frontier report.\n\nThis sounds like some separate problem.\n\nAre these adjustments being done to the global &#39;total-budget&#39; value, or \nan override?\n\nWhen the change does not appear to take effect, does a repeated attempt \never/eventually work?\n\nAre there times that a change works for some domains/queues, but not \nothers?\n\nA few important but potentially confusing things to note about overrides \nand queues:\n\n(1) overrides apply to URIs (and especially in H1, URI \ndomains/hostnames), whereas queues are often (but not always) \nhostname-related. (For example, overrides to the queue-assignment-policy \ncan force URIs from many hostnames into any arbitrarily-named queue.) \nQueues then essentially take on the settings of the URI they are \nenqueuing/dequeueing at the moment the setting is consulted. So if you \nhave a queue with URIs that have different settings, the queue&#39;s \nbehavior is going to vary somewhat based on which URI was consulted for \nsettings-cues last.\n\n(2) Some of the settings changes are only noticed at certain points in \nthe queue&#39;s lifecycle – for example when it becomes &#39;active&#39; (and thus \ngets a new session budget).\n\n(3) Changes via the web UI should trigger the &#39;kickUpdate&#39; method which, \njust to be sure, un-retires all queues so that they can be reevaluated. \nNext time they come up for &#39;active&#39; crawling, if their effective \nsettings are unchanged, they should instantly re-retire. If their \nsettings have changed allowing more crawling, they should &#39;activate&#39; and \nshow proper budget numbers then.\n\nWe may want to create a separate tracking issue for this as soon as it \nis more clear when changes do or don&#39;t take effect.\n\n&gt; I have few more general question.\n&gt; 1] about the budgeting policy - redirects and 404 are counted to the\n&gt; expenditure with UnitCost policy? From browsing the source code, I\n&gt; assume it is counted. If it is, it is good for discovering traps in\n&gt; early stage of the crawl, but not exactly the best approach to set\n&gt; limits for objects on one domain. Could I combine this rule with\n&gt; QuotaEnforcer (I think this one counts only successful downloads)\n\nThe frontier-budgeting mechanism was originally designed as a rough way \nto allocate relative frontier-attention, and ration (or even stop) \neffort on queues of decreasing value. It doesn&#39;t strictly map to any \nsort of &#39;exact document count&#39; or &#39;content quantity&#39;; rather it&#39;s more \nclosely based on protocol/queue-operations, which (as far as the crawler \nis concerned) are the same for a &#39;200 OK&#39; as a &#39;404 Not Found&#39; or &#39;30X \nMoved&#39;. So while it can be used as a rough cap for metering other \ncollection goals, even with policy customization its tallies are \nunlikely to match those wanted for other purposes.\n\nThe QuotaEnforcer mechanism uses other stats kept outside the Frontier \nto potentially either (1) reject all additional URIs after a threshold \nis met; or (2) mark a URI with a flag that forces the frontier to retire \nthe enclosing queue. So it may offer a more precise cutoff for some \nneeds. However, what it calls a &#39;success&#39; is still just any completed \nfetch (including for example a &#39;404&#39; response).\n\nYou can use either or both, though using the QuotaEnforcer in its &#39;start \nrejecting extra URIs&#39; mode actually empties the frontier of URIs, \nlogging them in the crawl.log as &#39;over quota&#39;, so if you then change \nyour mind you&#39;d have to manually reload those URIs to the crawler.\n\n&gt; 2] sometimes when the crawl is near the finish (the budget on almost\n&gt; every queue is spent), but there are few queues (up to ten, mainly\n&gt; large sites with big files) where the snooze time gets really long:\n&gt;\n&gt; Queue cz,czhops,\n&gt;    148 items\n&gt;     wakes in: 9h17m33s296ms\n&gt;      last enqueued:\n&gt; http://czhops.cz/index.php/component/content/article/37-pravni-predpisy/78-zakon-971996-sb\n&gt;        last peeked:\n&gt; http://czhops.cz/index.php/cs/hop-growing?tmpl=component&print=1&page=\n&gt;     total expended: 1115 (total budget: 15000)\n&gt;     active balance: 2630\n&gt;     last(avg) cost: 1(1)\n&gt;\n&gt; Even though the max delay is set to short time. How is exactly snooze\n&gt; time computed and could I affect it?\n\nIn a default configuration, the &#39;snooze&#39; time will never be this long. \nThe default &#39;retry-delay-seconds&#39; of 900, meaning wait 15 minutes before \nretrying certain kinds of errors that may indicate a network/host-down \nsituation, is effectively the longest &#39;snooze&#39; that might occur. (It can \ntake longer for a thread to get back to the queue, but that wouldn&#39;t \nappear as waiting-to-wake time.)\n\nSo I&#39;m not sure what might have caused this, but two guesses would be:\n\n(1) perhaps resetting the clock on the machine mid-crawl might cause this\n\n(2) your &#39;max-per-host-bandwidth-usage-KB-sec&#39; setting of 50. We can&#39;t \neasily directly throttle individual connections, so instead we just see \nif we&#39;ve used more bandwidth during the crawl so far than the setting \nprefers, and then &#39;snooze&#39; the queue long enough to get back under the \ntarget usage. If you&#39;d successfully received a multi-GB resource from \nthis host very quickly, it could take 10-hours-plus before the average \nwould go back below 50KB/sec. (If this is the case, it should be obvious \nfrom some of the czhops.cz entries in the crawl.log.)\n\n&gt; 3] could be some of the issues mentioned above solved by using\n&gt; Heritrix 3. If so, is there any automated or semiautomated way to\n&gt; convert profiles from Heritrix 1?\n\nAfter the learning curve, some might be helped.\n\nThe &#39;overrides&#39; system is very different. While it&#39;s currently harder to \napply overrides mid-crawl (requiring injecting a customized script), but \ntheir operation once active is a bit more simple, and (especially after \na number of fixes applied for the upcoming H3.1 release) I&#39;m confident \nthey take immediate effect, especially for queue-budgeting/lifecycle \npurposes.\n\nCustomizing behavior with either scripts or Java is a bit easier, so if \nyou needed a very-specific quota system, I think it&#39;d be easier to add \nas a loosely-coupled drop-in component.\n\n&gt; I am sorry, for such long email full of questions, but I get a little\n&gt; bit confused by Heritrix behaviour and I would like to make sure if\n&gt; there is problem on our side or if there could be bug in Heritrix.\n\nNo worries! It does sound like there&#39;s some bug or bugs with override \npersistence.\n\nOf course feel free to send off questions one-by-one as they come up too.\n\n- Gordon @ IA\n\n&gt; The order.xml is in attachment.\n&gt;\n&gt; Cheers,\n&gt;\n&gt; Adam\n&gt; --\n&gt; Adam Brokeš\n&gt; http://www.brokes.net\n&gt; adam@...\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}