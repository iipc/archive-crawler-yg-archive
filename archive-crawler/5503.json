{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":360569707,"authorName":"happyxinglele","from":"&quot;happyxinglele&quot; &lt;happyxinglele@...&gt;","profile":"happyxinglele","replyTo":"LIST","senderId":"xcHcKfkJ3osQVzpMRut_hXZuxh0xe21QdnHJaWC0k4cmPdkHJoLHLR7AoEqimVdjNPHHhnjZ26k8529a1WeNlbfAVhzzVkx-4Ry2sY0UvkGqmU8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to let Heritrix do not crawl robots.txt and DNS","postDate":"1223455046","msgId":5503,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdjaHJnNitkZmU1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGQ2MWM1NzMwMDgxMDA4MDEzNHAzYzUxMTZhMXgzNWVkZDQ2YTJjOWYwZjViQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5502,"nextInTopic":5504,"prevInTime":5502,"nextInTime":5504,"topicId":5501,"numMessagesInTopic":7,"msgSnippet":"Oh, I know. But could anyone tell me how to strip the robots.txt -Thanks ... server. ... robots and ... Heritrix to","rawEmail":"Return-Path: &lt;happyxinglele@...&gt;\r\nX-Sender: happyxinglele@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 83545 invoked from network); 8 Oct 2008 08:37:28 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m57.grp.scd.yahoo.com with QMQP; 8 Oct 2008 08:37:28 -0000\r\nX-Received: from unknown (HELO n51a.bullet.mail.sp1.yahoo.com) (66.163.168.145)\n  by mta15.grp.scd.yahoo.com with SMTP; 8 Oct 2008 08:37:28 -0000\r\nX-Received: from [69.147.65.148] by n51.bullet.mail.sp1.yahoo.com with NNFMP; 08 Oct 2008 08:37:28 -0000\r\nX-Received: from [66.218.66.78] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 08 Oct 2008 08:37:28 -0000\r\nDate: Wed, 08 Oct 2008 08:37:26 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gchrg6+dfe5@...&gt;\r\nIn-Reply-To: &lt;d61c57300810080134p3c5116a1x35edd46a2c9f0f5b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;happyxinglele&quot; &lt;happyxinglele@...&gt;\r\nSubject: Re: How to let Heritrix do not crawl robots.txt and DNS\r\nX-Yahoo-Group-Post: member; u=360569707; y=oYCw8TOmyVKU3QzhSQcgHE9jk9pkumb9MIY_qTnVHpNp0zRALfsoRw\r\nX-Yahoo-Profile: happyxinglele\r\n\r\nOh, I know.\nBut could anyone tell me how to strip the robots.txt\n\n-Thanks \n=\r\n\n--- In archive-crawler@yahoogroups.com, &quot;Jean-No=C3=ABl Rivasseau&quot; \n&lt;elvan=\r\nor@...&gt; wrote:\n&gt;\n&gt; I dont know for the robots.txt part, but you will still =\r\nbe forced to\n&gt; &quot;crawl&quot; (eg, contact) the DNS to obtain the IP address of th=\r\ne \nserver.\n&gt; This is mandatory.\n&gt; \n&gt; \n&gt; On Wed, Oct 8, 2008 at 10:29 AM, ha=\r\nppyxinglele\n&gt; &lt;happyxinglele@...&gt; wrote:\n&gt; &gt; I only crawl test URLs of myse=\r\nlf. And Heritrix need to crawl \nrobots and\n&gt; &gt; DNS firstly, which is cost l=\r\nots of time. I donot need the \nHeritrix to\n&gt; &gt; crawl the robots file and th=\r\ne DNS.\n&gt; &gt;\n&gt; &gt; Could you tell me how can I let it work?\n&gt; &gt; p.s. I use Heri=\r\ntrix-2.0.0\n&gt; &gt;\n&gt; &gt; Thanks a lot!\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}