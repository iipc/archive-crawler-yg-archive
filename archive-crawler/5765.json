{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"LSnS9g4V1BpYkKo9wzf9ZqlC84qidHjl49OQdfxxnmfqDc9a_PgX0tTc6qanEqQZuBO4YU0JO1Z4UNYnGhvKY5DnI6xlxN8","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Crawl failure details","postDate":"1239080990","msgId":5765,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5REFFMDFFLjQwMjAzMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGdyZWhqbis1b2V2QGVHcm91cHMuY29tPg==","referencesHeader":"PGdyZWhqbis1b2V2QGVHcm91cHMuY29tPg=="},"prevInTopic":5764,"nextInTopic":0,"prevInTime":5764,"nextInTime":5766,"topicId":5764,"numMessagesInTopic":2,"msgSnippet":"You should do a test crawl with all defaults first to confirm a basic crawl works, then make changes one-by-one. We don t recommend use of the old","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 29255 invoked from network); 7 Apr 2009 05:10:10 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m3.grp.re1.yahoo.com with QMQP; 7 Apr 2009 05:10:10 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta2.grp.sp2.yahoo.com with SMTP; 7 Apr 2009 05:10:10 -0000\r\nX-Received: (qmail 52844 invoked from network); 7 Apr 2009 05:09:50 -0000\r\nX-Received: from 70.137.147.22 (HELO ?10.0.13.7?) (70.137.147.22)\n  by relay02.pair.com with SMTP; 7 Apr 2009 05:09:50 -0000\r\nX-pair-Authenticated: 70.137.147.22\r\nMessage-ID: &lt;49DAE01E.4020305@...&gt;\r\nDate: Mon, 06 Apr 2009 22:09:50 -0700\r\nUser-Agent: Thunderbird 2.0.0.21 (Windows/20090302)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;grehjn+5oev@...&gt;\r\nIn-Reply-To: &lt;grehjn+5oev@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Crawl failure details\r\nX-Yahoo-Group-Post: member; u=137285340; y=HcNteHwVcJAF0pOM7uRRxAiy80xqCP2V6jA1QdsgCuSz\r\nX-Yahoo-Profile: gojomo\r\n\r\nYou should do a test crawl with all defaults first to confirm a basic \ncrawl works, then make changes one-by-one.\n\nWe don&#39;t recommend use of the old BroadScope/HostScope/DomainScope/etc. \nscope classes anymore. They should work, but DecidingScope, starting \nwith the default set of rules, is the better way to go.\n\nThe preprocessor PreconditionEnforcer is absolutely required for \nnecessary DNS and robots fetches to be triggered before other URIs are \ntried. This appears to be the fatal flaw in your config.\n\nThe &#39;-2&#39; error probably also appears in your crawl.log. It means &quot;HTTP \nconnect failed&quot;, per this list:\n\nhttp://crawler.archive.org/articles/user_manual/glossary.html#statuscodes\n\nHowever, the absence of the necessary DNS precondition fetch is the root \ncause.\n\nWe&#39;ve never tested with IcedTea/OpenJDK -- only Sun&#39;s Java. So I \nrecommend that, though given the above that&#39;s not the problem, and \nthings might work fine with IcedTea/OpenJDK.\n\n&quot;${HOSTNAME}&quot;, which causes the crawl-machine&#39;s hostname (or failing \nthat, IP address) to be included in the output ARC file names, is \ntraditional to help understand the origin of ARC files. I think your \nreplacement, &quot;${Rough-Writer}&quot; might very well cause those &#39;$&#39;/&#39;{&#39;/&#39;}&#39; \ncharacters to be in the ARC filenames, which may not be what you want.\n\nBut primarily: always launch your first crawl with the defaults, plus \nthe minimum required changes. (Add a user-agent/from and your own \nseed(s).) Then you can view a working crawl.\n\nAfter that, start tinkering with swapped classes, adding/removing rules \nor processors, etc.\n\nAt least in such a case, you&#39;ll know when a change (like removing \nPreconditionEnforcer) breaks things.\n\n- Gordon @ IA\n\nbowser.richard wrote:\n&gt; Thanks for responding, Gordon.\n&gt; \n&gt; I am running Ubuntu *.10 (Intrepid) and Java 1.6.0_0 IcedTea6 1.3.1 (6b12-0ununtu6.4) Runtime Environment (build 1.6.0_0-b12) OpenJDK 64-Bit Server VM (build 1.6.0_0-b12, mixed-mode)\n&gt; \n&gt; I am using Heritrix 1.14.3\n&gt; \n&gt; I selected a new job with defaults.  I specified the seed http://www.cs.nmt.edu (my school&#39;s CS Dept. web site)\n&gt; I chose org.archive.crawler.scope.BroadScope \n&gt; Then org.archive.crawler.frontier.BdbFrontier\n&gt; I used NO preprocessor to prevent such domain elimination.\n&gt; Fetchers: org.archive.crawler.fetcher.FetchDNS\n&gt;       and org.archive.crawler.fetcher.FetchHTTP\n&gt; I used several Extractors:\n&gt;    org.archive.crawler.extractor.ExtractorHTTP\n&gt;    org.archive.crawler.extractor.ExtractorHTML\n&gt;    org.archive.crawler.extractor.ExtractorCSS\n&gt;    org.archive.crawler.extractor.ExtractorJS\n&gt;    org.archive.crawler.extractor.ExtractorSWF\n&gt; I used one Writer:\n&gt;    org.archive.crawler.writer.ARCWriterProcessor\n&gt; I used three Post Processors:\n&gt;    org.archive.crawler.postprocessor.CrawlStateUpdater\t\n&gt;    org.archive.crawler.postprocessor.LinksScoper\n&gt;    org.archive.crawler.postprocessor.FrontierScheduler\n&gt; Finally, I selected one Statistics Tracker:\n&gt;    org.archive.crawler.admin.StatisticsTracker\n&gt; I then specified nothing on the Submodules Page\n&gt; \n&gt; On the Setting page, I specified valid user-agent: and from: fields\n&gt; Then I altered the default Archiver#decide-rules suffix from ${HOSTNAME} to ${Rough-Writer} - which is correct.\n&gt; At that point I submitted the job & I had a new pending job.\n&gt; \n&gt; Next I started the job.  It immediately completed and the totals on the screen showed 0 downloaded but one job queued.  The terminal shows no files of form *.arc\n&gt; \n&gt; I looked into a produced file (local-errors.log) and saw a top line:\n&gt; 2009-04-07T03:16:31.659Z    -2          - http://www.cs.nmt.edu/ - - no-type #045 - - - le:IOException@HTTP\n&gt; 2nd line was: java.io.IOException: Failed to get host www.cs.nmt.edu address from ServerCache\n&gt; These were followed by a slew of seemingly induced  internal errors.\n&gt; \n&gt; So I wonder how is the address of my seed supposed to get INTO the cache?\n&gt; \n&gt; Can you help me decode any of this?  I am saving all my results for later examination.\n&gt; \n&gt; Thanks\n&gt; -Rich B.\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}