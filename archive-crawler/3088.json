{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigur√∞sson","from":"=?iso-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"k_Mg4PoSE6TUIbS0kz-loR6UWZkpk0aB_X88SKekyHfX_emr78C6KvT4QLFZVisiNremWjyu6DRVT-C7XWIhFpC6nbTdFTcj9ocHlXLL9Lyp7Fl5xgYgt49ifG4Fh26R","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] Advice needed on how to (properly) structure new Heritrix modify and delete functionality","postDate":"1153313612","msgId":3088,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDYwNzE5MTI1MzQwLjAyMEE3MTQxNTZDOTVAbWFpbC5hcmNoaXZlLm9yZz4=","inReplyToHeader":"PDQ0QkUxRUQ5LjEwNjAxMDJAbWV0YWNhcnRhLmNvbT4="},"prevInTopic":3087,"nextInTopic":3097,"prevInTime":3087,"nextInTime":3089,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"Replying to Karl s latest post... ... No, it was aimed at Stack (who s post I was replying to). ... Pretty much. The idea is to limit the impact on the actual","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 78876 invoked from network); 19 Jul 2006 12:55:33 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m40.grp.scd.yahoo.com with QMQP; 19 Jul 2006 12:55:33 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta6.grp.scd.yahoo.com with SMTP; 19 Jul 2006 12:55:32 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id BD77914156C8F\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 19 Jul 2006 05:53:41 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 15132-01-4 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 19 Jul 2006 05:53:41 -0700 (PDT)\r\nReceived: from FORRITUN1 (forritun-1.bok.hi.is [130.208.152.80])\n\tby mail.archive.org (Postfix) with ESMTP id 020A714156C95\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 19 Jul 2006 05:53:39 -0700 (PDT)\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Wed, 19 Jul 2006 12:53:32 -0000\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Mailer: Microsoft Office Outlook, Build 11.0.5510\r\nThread-Index: AcarLuJK1FEyxqxgQnGGRY4SrvAyTAAA3Fgg\r\nIn-Reply-To: &lt;44BE1ED9.1060102@...&gt;\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.2869\r\nMessage-Id: &lt;20060719125340.020A714156C95@...&gt;\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: =?iso-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] Advice needed on how to (properly) structure new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=163406187; y=D06IGAv8WK1jsEF-49JsuQMfnksM8SGF00Fds4emy3T-TA5i\r\nX-Yahoo-Profile: kristsi25\r\n\r\nReplying to Karl&#39;s latest post...\n\n&gt; &gt;&gt;We&#39;ve been talking here at the Archive on how to broach the topics \n&gt; &gt;&gt;under discussion here: duplicate/near-duplicate detection, \n&gt; &gt;&gt;change-detection, junk-avoidance, etc. Here are a few notes on what \n&gt; &gt;&gt;we&#39;ve been thinking.\n&gt; &gt;&gt;They pertain to Karl and Kris&#39;s discussion so I&#39;ll add them in here \n&gt; &gt;&gt;under the same subject (Implementation is probably too far off to \n&gt; &gt;&gt;solve Karl&#39;s needs).\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Just out of curiosity, what kind of timeframe are you looking at?\n&gt; &gt; \n&gt; \n&gt; If this question is for me, I need to begin my work within the next \n&gt; couple of weeks.\n\nNo, it was aimed at Stack (who&#39;s post I was replying to).\n\n&gt; I am curious as to why you would not try to build the second index \n&gt; (with Lucene etc) as URIs are crawled, rather than at snapshot time. \n&gt; Are you simply concerned about disk contention?\n\nPretty much. The idea is to limit the impact on the actual crawl. Partly\nthis is because disk contention would mean that the additional work would\ncause a disproportionate amount of delay in the crawling. There is also\nlittle apparent gain from building the index at crawl time. You only crawl\neach resource once so it does nothing to eliminate duplicates in the ongoing\ncrawl (unless you are thinking about mirrors, but that is a whole other\nstory). The data isn&#39;t needed until next time we do that crawl.\n\nBuilding the index between snapshots means better utilization of available\nresources if (like me) you have hardware dedicated for web crawling. Those\nmachines tend to do little in between crawl cycles, might as well give them\nsomething to chew on. In practice though it takes only a few hours to create\nthe dedup index for a crawl that might run for two weeks. Building the index\nused by my weekly crawls (runs for one day and covers about half a million\ndocuments) takes only a few minutes.\n\n&gt; At some point one really has to notice that you are in effect building \n&gt; features of a true database out of berkeley db plus lucene; if you \n&gt; used something like postgresql with the proper indexes you may be \n&gt; surprised at its performance, esp. if you can contrive to query \n&gt; multiple uri&#39;s out of it at once. I&#39;ve done this in (separate) \n&gt; crawling work by fetching everything that fits a given schedule slice, \n&gt; for instance, where if the slice is longer the query simply returns \n&gt; more uri&#39;s. The real point is that someone else spent lots of time \n&gt; optimizing disk access, so you may not have to.\n\nYes and no. The crawl state is being maintained by the Bdb databases in the\nfrontier. \nThe Lucene index should be thought of more as a service that the relevant\nprocessor uses to filter URLs being processed. The choice of implementing\nthis service in Lucene was arbitrary. The original idea was to make this an\nactual web service hosted on another computer, either backed by BDB or a\nregular transactional DB. This also called for crawl time updates. The\napproach (while viable) seemed unduly complicated for our needs.\n\nOf course you could build something that did both (maintain crawl state and\ndo duplicate reduction) but that is more complicated.\n\n- Kris\n\n\n"}}