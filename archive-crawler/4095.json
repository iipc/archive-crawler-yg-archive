{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"nhEa-Xiy6F6JopQaXYpcNmzaf0q23GgBa6K9o95nlPAUJQps-JQcFrOTqCic5CYVYi4li-siAK61UzpW3BeLCHw7sZ5unBE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Constructing a web graph","postDate":"1176328915","msgId":4095,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2MUQ1QUQzLjEwMTA2MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDVhMWMzNGFhMDcwNDExMTEzM3AyODVjZjQzNnEzZjkzMzE0YmRhOTA2NWFlQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGV2OGxxOCt1YWZqQGVHcm91cHMuY29tPiA8ZXZpbW1sK3NiczdAZUdyb3Vwcy5jb20+IDw1YTFjMzRhYTA3MDQxMTExMzNwMjg1Y2Y0MzZxM2Y5MzMxNGJkYTkwNjVhZUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":4091,"nextInTopic":4096,"prevInTime":4094,"nextInTime":4096,"topicId":4059,"numMessagesInTopic":8,"msgSnippet":"If I understand correctly, this analysis requires even the redundant discovered links to be saved somewhere, since they are not scheduled for redundant","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 44780 invoked from network); 11 Apr 2007 21:59:28 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m42.grp.scd.yahoo.com with QMQP; 11 Apr 2007 21:59:28 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta11.grp.scd.yahoo.com with SMTP; 11 Apr 2007 21:59:27 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id E9B881415FFA8\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 11 Apr 2007 14:59:23 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 07994-05-92 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 11 Apr 2007 14:59:23 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 905621415FF97\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 11 Apr 2007 14:59:23 -0700 (PDT)\r\nMessage-ID: &lt;461D5AD3.1010602@...&gt;\r\nDate: Wed, 11 Apr 2007 15:01:55 -0700\r\nUser-Agent: Thunderbird 1.5.0.10 (X11/20070306)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ev8lq8+uafj@...&gt; &lt;evimml+sbs7@...&gt; &lt;5a1c34aa0704111133p285cf436q3f93314bda9065ae@...&gt;\r\nIn-Reply-To: &lt;5a1c34aa0704111133p285cf436q3f93314bda9065ae@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Constructing a web graph\r\nX-Yahoo-Group-Post: member; u=137285340; y=YgMXte49FioS2t1G1I9KcsUPzHNr27DFj6ITgRi_SDR6\r\nX-Yahoo-Profile: gojomo\r\n\r\nIf I understand correctly, this analysis requires even the redundant \ndiscovered links to be saved somewhere, since they are not scheduled for \nredundant visitation within the same crawl.\n\nOne option would be to perform a post-crawl analysis on your ARC files \nto re-extract the links.\n\nAnother would be to insert a processor that logs the outlinks before \nscoping and/or scheduling have whittled them down.\n\nThe current experimental WARC-writing processor uses the discovered \noutlinks as example metadata for the new &#39;metadata&#39; record, so a side \neffect of its operation is to save aside the data you want. Both the \nformat of WARCs and the content of specific records is going to change, \nso I can&#39;t recommend depending on the current processor for \nfunctionality, but it may provide a model for other code to save aside \nthis info.\n\n- Gordon @ IA\n\nAndrea Goethals wrote:\n&gt; If I understand the original post correctly - this is something we also \n&gt; want\n&gt; to implement (logging of &quot;intra&quot;-harvest duplicates not downloaded).\n&gt; \n&gt; The heritrix 1.12 supports deduping *between* crawls but older heritrixs\n&gt; dedupe *within* crawls already. Ideally we would like to see logging \n&gt; options\n&gt; for both kinds of deduplication. We want this intra-harvest dedupe logging\n&gt; so that we can know all the parents seen for downloaded resources - not \n&gt; just\n&gt; the one first parent that currently gets logged in crawl.log.\n&gt; \n&gt; I haven&#39;t yet looked into where the extra logging should go - just \n&gt; wanted to\n&gt; add to this thread that this is something we want too & are willing to\n&gt; implement (if there&#39;s not already a way to log this) because it will effect\n&gt; how we implement our harvest q/a and takedown request handling.\n&gt; \n&gt; Andrea\n&gt; \n&gt; On 11 Apr 2007 06:12:44 -0700, mbarlotta &lt;barlotta_michael@...&gt; wrote:\n&gt;&gt;\n&gt;&gt;   &gt; I&#39;ve tried playing around with the setting for herdrix 1.1.2 but I&#39;m\n&gt;&gt; &gt; getting nowhere; I&#39;ve read that older version of heredrix does not\n&gt;&gt; &gt; have the ability to filter out duplicate pages, so perhaps I should\n&gt;&gt; &gt; try older versions of heredrix?\n&gt;&gt;\n&gt;&gt; The current version of Heritrix does not dedupe pages by default you\n&gt;&gt; would have to configure the job with additional processors to get it to\n&gt;&gt; do that. If you crawl sites without dedupe you get what ever your\n&gt;&gt; decide rules allow.\n&gt;&gt;\n&gt;&gt; Read more about it here:\n&gt;&gt; http://webteam.archive.org/confluence/display/Heritrix/Feature+Notes+-\n&gt;&gt; +1.12.0\n&gt;&gt;\n&gt;&gt; What are you using to visualize your Web Graph?\n&gt;&gt;\n&gt;&gt; HTH,\n&gt;&gt; Mike\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt; \n\n\n"}}