{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"juiSu1BfSAPwp-R_K6oGYGKxL5BzVynU3O9vVAhQZ728PVcvGHgNn7dPM-qJ-WCFQc1aQ5bet_odFAOv0kp_Mzq2jPanCPne","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Parallelizing crawler","postDate":"1152724924","msgId":3044,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QjUyRkJDLjIwMTA5QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGU5MzdxbStrcnRpQGVHcm91cHMuY29tPg==","referencesHeader":"PGU5MzdxbStrcnRpQGVHcm91cHMuY29tPg=="},"prevInTopic":3043,"nextInTopic":3046,"prevInTime":3043,"nextInTime":3045,"topicId":3043,"numMessagesInTopic":16,"msgSnippet":"... Not to do the above but splitting the URL space across machines has been discussed on the list in the past.  Currently its a manual, up-front process (To","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 76480 invoked from network); 12 Jul 2006 17:22:19 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m35.grp.scd.yahoo.com with QMQP; 12 Jul 2006 17:22:19 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta7.grp.scd.yahoo.com with SMTP; 12 Jul 2006 17:22:19 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id CDF3A14156BC3;\n\tWed, 12 Jul 2006 10:21:59 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 18134-01-15; Wed, 12 Jul 2006 10:21:59 -0700 (PDT)\r\nReceived: from [192.168.1.204] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 7103E14156A74;\n\tWed, 12 Jul 2006 10:21:59 -0700 (PDT)\r\nMessage-ID: &lt;44B52FBC.20109@...&gt;\r\nDate: Wed, 12 Jul 2006 10:22:04 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.2) Gecko/20060405 SeaMonkey/1.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e937qm+krti@...&gt;\r\nIn-Reply-To: &lt;e937qm+krti@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Parallelizing crawler\r\nX-Yahoo-Group-Post: member; u=168599281; y=_YLLionap-i4n6P79KfzlXbUgKF3ZjBf5ja9CGBsBGV9-5kqwwg7knli\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nmolzbh wrote:\n&gt;\n&gt; Hi,\n&gt;\n&gt; I am sure this has been posed as a question before, about\n&gt; parallelizing the crawler, i.e. running Frontier and Processor Chains\n&gt; on different address spaces. Any guides or links?\n&gt;\n\n\n\n\n\n\n\n\n\nNot to do the above but splitting the URL space across machines has been \ndiscussed on the list in the past.  Currently its a manual, up-front \nprocess (To be rectified).  Here are sample related messages: \nhttp://groups.yahoo.com/group/archive-crawler/message/2402 and \nhttp://groups.yahoo.com/group/archive-crawler/message/2909.  Search the \narchive for &#39;distributed crawling&#39; and &#39;multimachine&#39; for lots more\n\nYours,\nSt.Ack\n\n"}}