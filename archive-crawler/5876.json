{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":401913576,"authorName":"Joel Halbert","from":"Joel Halbert &lt;joel@...&gt;","replyTo":"LIST","senderId":"24bbJbOSN0pYv7Aaf1gGMmkpWbZ2qzBC5l1-DuqBSP84SUxwkIc33UdXjgpgBtlrT-USuBaO4yvnjg_xOIIA16xQdbdIttoJ_5lo","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Limitations on the number of Jobs\tper\tinstance of Heretrix ?","postDate":"1244018084","msgId":5876,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEyNDQwMTgwODQuNjk2OC4xLmNhbWVsQGJvaHI+","inReplyToHeader":"PDRBMjVDRDE5LjEwNTA2QGFyY2hpdmUub3JnPg==","referencesHeader":"PDEyNDM1OTQxMTAuNjg3NS40OC5jYW1lbEBib2hyPiAgPDRBMjA3MTYxLjEwMzA2MDVAYXJjaGl2ZS5vcmc+CSA8MTI0Mzg1Mjc2MC42OTk5LjM2LmNhbWVsQGJvaHI+ICA8NEEyNUNEMTkuMTA1MDZAYXJjaGl2ZS5vcmc+"},"prevInTopic":5875,"nextInTopic":0,"prevInTime":5875,"nextInTime":5877,"topicId":5866,"numMessagesInTopic":6,"msgSnippet":"Thanks Gordon. This really helps. ... From: Gordon Mohr  Reply-To: archive-crawler@yahoogroups.com To: archive-crawler@yahoogroups.com ","rawEmail":"Return-Path: &lt;joel@...&gt;\r\nX-Sender: joel@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 674 invoked from network); 3 Jun 2009 08:39:48 -0000\r\nX-Received: from unknown (69.147.108.201)\n  by m4.grp.sp2.yahoo.com with QMQP; 3 Jun 2009 08:39:48 -0000\r\nX-Received: from unknown (HELO mail.roo10.com) (79.170.194.127)\n  by mta2.grp.re1.yahoo.com with SMTP; 3 Jun 2009 08:39:47 -0000\r\nX-Received: from [192.168.60.91] (78-105-13-3.dsl.cnl.uk.net [78.105.13.3])\n\tby mail.roo10.com (Postfix) with ESMTP id DA5614800728\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  3 Jun 2009 09:34:37 +0100 (BST)\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;4A25CD19.10506@...&gt;\r\nReferences: &lt;1243594110.6875.48.camel@bohr&gt;  &lt;4A207161.1030605@...&gt;\n\t &lt;1243852760.6999.36.camel@bohr&gt;  &lt;4A25CD19.10506@...&gt;\r\nContent-Type: text/plain\r\nOrganization: SU3 Analytics\r\nDate: Wed, 03 Jun 2009 09:34:44 +0100\r\nMessage-Id: &lt;1244018084.6968.1.camel@bohr&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Evolution 2.22.3.1 \r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Joel Halbert &lt;joel@...&gt;\r\nSubject: Re: [archive-crawler] Limitations on the number of Jobs\n\tper\tinstance of Heretrix ?\r\nX-Yahoo-Group-Post: member; u=401913576\r\n\r\nThanks Gordon. This really helps.\n\n-----Original Message-----\nFrom: Gordon Mohr &lt;gojomo@...&gt;\nReply-To: archive-crawler@yahoogroups.com\nTo: archive-crawler@yahoogroups.com\nSubject: Re: [archive-crawler] Limitations on the number of Jobs per\ninstance of Heretrix ?\nDate: Tue, 02 Jun 2009 18:08:41 -0700\n\n\n\nJoel Halbert wrote:\n&gt; Thanks for the feedback. \n&gt; \n&gt; Our main goals are:\n&gt; \n&gt; 1. achieving maximum use of commodity hardware. If we want to\ndistribute\n&gt; 200k domains across say, 4 machines, we would like to ensure that if\none\n&gt; machine has already processed 50k domains (because they were smaller\n&gt; sites) then we can keep loading up more domains for it to process to\n&gt; take up the slack. \n\nThis goal (considered alone) is best served by a large crawl with all\nseeds entered at the very start. Then the only things limiting the\ncrawler will be thread availability, IO throughput, and the politeness\nsettings. (As long as there are URIs pending and eligible, it will try\nthem ASAP.)\n\nA number of the &#39;cost&#39; or &#39;budget&#39; related settings affect how Heritrix\nrotates thread attention among queues with pending URIs. I&#39;ve put some\nnotes about these settings on the project wiki here:\n\nhttp://webarchive.jira.com/wiki/display/Heritrix/Frontier+queue+budgets\n\n&gt; From the docs I gather you can edit a seed list for a running job. Can\n&gt; you add to the seed list programatically? Can we see which seeds\n&gt; (domains) in the seed list have completed their crawl - can this be\n&gt; determined, as you suggested, from the job summary and frontier\n&gt; reports? \n\nWhenever you can avoid it, you&#39;d rather not change the seeds/scope \nduring a crawl. It can make coverage dependent on arbitrary \norder-of-discovery/change differences.\n\nFor example, let&#39;s say you want to crawl site A. One of the pages on A \nincludes the root page of site B in an IFrame, so that one page (but\nnot \nall of site B) is crawled by inline inclusion. Some time later, you add \nto that same crawl the root page of B, and expand the crawl&#39;s scope to \ninclude site B. However, since the crawl already visited root-B, it \nwon&#39;t be revisited. And because when it was visited, the rest of site B \nwasn&#39;t in-scope, it may not be discovered and crawled.\n\nSo while you *can* add seeds, and adjust scope, during a crawl, in \ngeneral you want to do that to adjust for things that couldn&#39;t have\nbeen \nforeseen, rather than as a matter of course.\n\nSome notes on how to add URIs mid-crawl are now at:\n\nhttp://webarchive.jira.com/wiki/display/Heritrix/Adding+URIs+mid-crawl\n\nYou can deduce if a host-centric queue is still in progress from the \n&#39;frontier report&#39;. When viewed in the Web UI, only a manageably-sized \nexcerpt of this report is shown (so it might be missing info on some \nqueues of interest). However, when you use the UI&#39;s link to &#39;Force \ngeneration of end-of-crawl Reports&#39;, you get full reports, and the \nfrontier report is in a more compact format with one line per \nnonempty-queue. (Generating these reports can be time-consuming in a \nlarge crawl.)\n\nIf a queue did have URIs, but is now empty, then in some sense that\nhost \nis &#39;finished&#39;. Of course, more previously-unknown material could be \ndiscovered via other paths. Whether the heuristic &quot;we&#39;ve crawled the \nroot page, and everything reachable from it, and now the queue is\nempty&quot; \nis a good enough definition of &quot;completed&quot; depends on your project.\n\n&gt; 2. Detailed reporting for each domain e.g. number of pages discovered,\n&gt; pending, queued, errors etc... You suggested we might be able to\ngather\n&gt; this information by creating alternate views on the jobs in process\nfrom\n&gt; the existing logs & reports? Can we hook into these reports\n&gt; programatically or would it require us to process the log files?\n\nThe forced-dump of reports mid-crawl might be enough for you; it \nincludes host-centric reports and the aforementioned frontier-report \nthat might be plenty of info for you.\n\nWe don&#39;t use the forced-dumps very often in our operations, and \ngenerating the full reports on a large, in-process crawl is \ntime-consuming and memory-stressing. So, if you were doing it many\ntimes \nper day in an unpaused crawl, there could be problems. (Or maybe not; \ntesting is suggested.) But doing in once per day in a temporarily\npaused \ncrawl should be safe.\n\nAnd, if all else fails, generating reports from the crawl.log isn&#39;t too \nhard. It won&#39;t include pending URI counts -- but those are in the \nfrontier report, or could even be deduced by crunching the recovery.gz\nlog.\n\n&gt; 3. Importantly, we need to be able to use specific crawl scopes for\neach\n&gt; domain. Some may be DomainScope, others PathScope. How easy is it to\n&gt; construct the appropriate scopes for multiple domains in a single job?\n\nIt&#39;s possible via the per-domain-overrides mechanism, but not \nsuper-easy, and might get unwieldy if you have many thousands of \ndifferent scopes (as opposed to specializations for a few big/important \nsites).\n\nWe don&#39;t recommend either DomainScope or PathScope for current crawls; \nthe bundled default DecidingScope with a SurtPrefixedDecideRule (and \nwhatever other edits you need) is more efficient and offers \nfunctionality that is a superset of both those older scopes.\n\nFor example, it may be all the specific scoping you need -- for example \nyou can have it do A.COM and all its subdomains; B.COM exactly without \nsubdomains, and C.COM/SUBPATH/ without getting other areas of C.COM.\nThe \nkey idea is to rearrange URIs into what we call &#39;SURT form&#39;, where the \ndomain-name-segments also become more specific left-to-right like \npath-segments, and then turns most scoping decisions into shared-prefix \ncomparisons.\n\nIf you do wind up needing to use per-domain overrides, a tactic to keep \nin mind is that overriding is easiest when either enabling/disabling \ncomponents, or changing primitive values. So if you think you&#39;ll need a \nspecial scope rule for some domains, it&#39;s best to add to the global \nsettings, but mark it disabled -- then override the &#39;enabled&#39; flag on \ntarget domains. Or, include a rule for blocking URIs by regular \nexpression which is empty for the global settings -- but is overridden \nwith domain-specific blocks for certain domains.\n\n&gt; We would like to &quot;work with the framework&quot; in achieving our goals,\n&gt; rather than fitting a square peg into a round hole, so hopefully the\n&gt; above is do-able using Heritrix best practice!\n\nHope this helps!\n\n- Gordon @ IA\n\n&gt; Thanks,\n&gt; Joel\n&gt; \n&gt; \n&gt; -----Original Message-----\n&gt; From: Gordon Mohr &lt;gojomo@...&gt;\n&gt; Reply-To: archive-crawler@yahoogroups.com\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Subject: Re: [archive-crawler] Limitations on the number of Jobs per\n&gt; instance of Heretrix ?\n&gt; Date: Fri, 29 May 2009 16:36:01 -0700\n&gt; \n&gt; \n&gt; \n&gt; At the Internet Archive, we don&#39;t usually split up crawls to \n&gt; one-site-per-job. We prefer fewer jobs, with more seeds/sites. We\nthen \n&gt; rely on postprocessing and indexing to extract individual sites, or\n&gt; site \n&gt; statistics, later.\n&gt; \n&gt; Some other groups do a job-per-site, with Heritrix, but it&#39;s not very \n&gt; easy to run many jobs simultaneously in the same running instance.\n&gt; (It&#39;s \n&gt; possible, but a little tricky -- unless you shrink some of the usual\n&gt; job \n&gt; parameters, most importantly in 1.14.x the bdb-cache percentage, \n&gt; simultaneous jobs can exhaust all allocated JVM memory. And I think \n&gt; those who have done so have done more like dozens per instance,\nrather \n&gt; than hundreds.)\n&gt; \n&gt; Some potential weaknesses of the one-job-per-site approach:\n&gt; \n&gt; - offsite inline resources, which our default configuration gets \n&gt; (because we want as accurate a rendition of the page as possible),\nmay \n&gt; be duplicated between the jobs -- each has no idea another may have \n&gt; already retrieved that URI\n&gt; \n&gt; - if there are &#39;deep&#39; areas of a site only discoverable from other \n&gt; sites, but not from the home site&#39;s root page, these may not be \n&gt; collected in separate crawls. (In the job they&#39;re discovered, they&#39;re \n&gt; out-of-scope; in the job for their own site, they&#39;re not discovered.)\n&gt; \n&gt; - because of trying to schedule/distribute the jobs, within the \n&gt; constraints of a certain amount of per-job overhead, hardware may\noften \n&gt; be underutilized\n&gt; \n&gt; Unless the sites are especially large, a single job on a single\n&gt; powerful \n&gt; machine may be plenty to collect 200K sites. A crawler with (for \n&gt; example) 200 crawling &#39;toe&#39; threads is actually making maximal\nprogress \n&gt; on far more than 200 sites at once, because while some sites are in \n&gt; &#39;politeness wait&#39;, threads continue on other sites with pending URIs. \n&gt; And since many sites are not large, many crawls finish most of their \n&gt; target sites in the first hours/days of crawling, with later\ndays/weeks \n&gt; only working on the very deep or very slow sites.\n&gt; \n&gt; What are your main goals in splitting the jobs up?\n&gt; \n&gt; Can those goals be met by postprocessing, or by creating alternate\n&gt; views \n&gt; of a single in-progress job&#39;s logs and reports? (For example, you can \n&gt; force dump larger per-host-summary and frontier-queue reports from a \n&gt; running crawl.)\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; Joel Halbert wrote:\n&gt;&gt; Hi,\n&gt;&gt;\n&gt;&gt; I&#39;m using Heretrix, over Nutch, the documentation seems more robust\n&gt; and\n&gt;&gt; I&#39;m not interested in Nutch&#39;s tight integration with Lucene - I want\n&gt; to\n&gt;&gt; do quite a lot or pre-processing with the crawl data before indexing\n&gt; it\n&gt;&gt; in our own format. So Heretrix seems a good flexible fit in this\n&gt; regard.\n&gt;&gt; I want to crawl about 200k unique domains, using DomainScope. I&#39;d\nlike\n&gt;&gt; to distribute this over n machines, with each machine running say 100\n&gt;&gt; threads - one thread per domain. \n&gt;&gt;\n&gt;&gt; I&#39;d also like to be able to track & manage the progress of each\ndomain\n&gt;&gt; individually. \n&gt;&gt;\n&gt;&gt; I&#39;d like to do all of this by assigning one job per domain and have a\n&gt;&gt; central management process that tracks each heretrix instance, and\n&gt;&gt; assigns new jobs as appropriate e.g. as a job on one machine\n&gt; completes,\n&gt;&gt; create a new one for another domain - keeping the total number of\n&gt;&gt; running jobs in each instance at say 200.\n&gt;&gt;\n&gt;&gt; The docs generally talk of creating a single job with a seed list.\n&gt; Will\n&gt;&gt; I run into any issues within heretrix with having a seedlist of size\n1\n&gt; -\n&gt;&gt; and instead having multiple jobs e.g. up to 200?\n&gt;&gt;\n&gt;&gt; I&#39;m guessing this should all be fine, but just want to make sure\nthere\n&gt;&gt; are no hidden gotchas.\n&gt;&gt;\n&gt;&gt; Thx,\n&gt;&gt;\n&gt;&gt; Joel\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n\n\n\n\n\n"}}