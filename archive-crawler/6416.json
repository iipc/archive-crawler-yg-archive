{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":435601129,"authorName":"Bernd","from":"&quot;Bernd&quot; &lt;bernd.fehling@...&gt;","profile":"bernd.fehling","replyTo":"LIST","senderId":"4U7DvHoDJs8CW84VGw9uleG118e8IPaLHmZJGmpzY7DB2eHHeInCWgN434xJEXV8lnr-cmuYKBBw5cO-b0pzqIYudVD2iqM0viitWN0PVg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Heritrix - crawl job for single pages referenced by URL","postDate":"1267619253","msgId":6416,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhtbGtqbCttYjc2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRCODMxMTY4LjQwMDAyMDlAYXJjaGl2ZS5vcmc+"},"prevInTopic":6398,"nextInTopic":6420,"prevInTime":6415,"nextInTime":6417,"topicId":6394,"numMessagesInTopic":5,"msgSnippet":"This explains some odd behavior. While crawling http://bar.foo.org/ I also saw URLs crawled from http://www.foo.org/images/. This is because bar.foo.org has","rawEmail":"Return-Path: &lt;bernd.fehling@...&gt;\r\nX-Sender: bernd.fehling@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 18899 invoked from network); 3 Mar 2010 12:27:36 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m11.grp.re1.yahoo.com with QMQP; 3 Mar 2010 12:27:36 -0000\r\nX-Received: from unknown (HELO n44d.bullet.mail.sp1.yahoo.com) (66.163.169.158)\n  by mta3.grp.re1.yahoo.com with SMTP; 3 Mar 2010 12:27:36 -0000\r\nX-Received: from [69.147.65.172] by n44.bullet.mail.sp1.yahoo.com with NNFMP; 03 Mar 2010 12:27:34 -0000\r\nX-Received: from [98.137.35.12] by t14.bullet.mail.sp1.yahoo.com with NNFMP; 03 Mar 2010 12:27:34 -0000\r\nDate: Wed, 03 Mar 2010 12:27:33 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;hmlkjl+mb76@...&gt;\r\nIn-Reply-To: &lt;4B831168.4000209@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Bernd&quot; &lt;bernd.fehling@...&gt;\r\nSubject: Re: Heritrix - crawl job for single pages referenced by URL\r\nX-Yahoo-Group-Post: member; u=435601129; y=veUAIPQx_8p-Bb8143UY1Zh3OGLRNKlMkHmOrHP-TARDfQg47dJXNQ\r\nX-Yahoo-Profile: bernd.fehling\r\n\r\nThis explains some odd behavior.\nWhile crawling http://bar.foo.org/ I also =\r\nsaw URLs crawled from http://www.foo.org/images/. \nThis is because bar.foo.=\r\norg has some IMG tags pointing to www.foo.org/images/.\n\nNot very pleased ab=\r\nout content from sites which I don&#39;t want to have crawled at all.\n\nIs there=\r\n a strict or pedantic mode for the crawler?\n\nRegards, Bernd\n\n\n--- In archiv=\r\ne-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; swschilke wr=\r\note:\n&gt; &gt; Dear All,\n&gt; &gt; \n&gt; &gt; Heritrix: I was reading the manual and I have a=\r\n little problem\n&gt; &gt; understanding how I can set up a crawl job. My  task wo=\r\nuld be to archive\n&gt; &gt; only certain pages in a crawl job, i.e., I want to gi=\r\nve Heritrix a list of URLs referring to one page each and I want them to be=\r\n collected (including all components of that page (e.g., PDF files, images,=\r\n ...). Anybody here which could give me a hint / sample job definition?\n&gt; &gt;=\r\n \n&gt; &gt; Thank you very much in advance\n&gt; \n&gt; Roughy, this can be achieved by:\n=\r\n&gt; \n&gt; - using the bundled default configuration\n&gt; \n&gt; - including all your UR=\r\nLs of interest as &#39;seeds&#39;\n&gt; \n&gt; - setting the &#39;maxHops&#39; value of the TooMany=\r\nHopsDecideRule to &#39;0&#39;, \n&gt; meaning &quot;don&#39;t follow links more than 0 hops from=\r\n seeds&quot;\n&gt; \n&gt; The following TransclusionDecideRule will still rule-in items =\r\nthat are \n&gt; obviously required to inline-render the page (such as \n&gt; FRAME/=\r\nIMG/STYLE/SCRIPT SRC links) or even possibly-required (in the case \n&gt; of UR=\r\nL-looking strings in certain form elements and scripts).\n&gt; \n&gt; So, the overa=\r\nll effect is: get exactly the URLs provided as seeds, plus \n&gt; those things =\r\ndefinitely necessary to render them.\n&gt; \n&gt; Note: &quot;A HREF&quot; links, whether to =\r\nresources on the same site or different \n&gt; sites, won&#39;t be ruled-in. (They&#39;=\r\nll all be considered &#39;1&#39; hop out, more \n&gt; than &#39;0&#39; hops.) So if you have a =\r\npage full of links to PDF files or \n&gt; photos or whatever, those links won&#39;t=\r\n be followed.\n&gt; \n&gt; Some options to consider if you want to get those:\n&gt; \n&gt; =\r\n- increase the &#39;maxHops&#39; to 1 -- this will at least follow links into \n&gt; th=\r\ne same sites/site-prefixes implied by the seeds\n&gt; \n&gt; - add an extra Matches=\r\nRegexPatternDecideRule, late in the scope, to \n&gt; ACCEPT all URLs with certa=\r\nin desired patterns (such as ending &quot;.pdf&quot;)\n&gt; \n&gt; Hope this helps,\n&gt; \n&gt; - Go=\r\nrdon @ IA\n&gt;\n\n\n\n"}}