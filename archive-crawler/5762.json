{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":191387937,"authorName":"pandae667","from":"&quot;pandae667&quot; &lt;aaron667@...&gt;","profile":"pandae667","replyTo":"LIST","senderId":"8AFJkRYdI_CcUz8jQ1ovEMrDjiHe7b4awUFOzc5D2_gZYvYSvjzzpomR5fNLTla-DsGdvvMTNmoeiXbK_IXXS1XVIYM6","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Best way of filtering of URLs with RegExp?","postDate":"1239053914","msgId":5762,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdyZHNvcStsbWp1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGdyZHBrNit2dTJpQGVHcm91cHMuY29tPg=="},"prevInTopic":5761,"nextInTopic":0,"prevInTime":5761,"nextInTime":5763,"topicId":5753,"numMessagesInTopic":4,"msgSnippet":"Back when I started using Heritrix one of the best resources I could find about limitig the scope of my crawl to certain document types was the following wiki","rawEmail":"Return-Path: &lt;aaron667@...&gt;\r\nX-Sender: aaron667@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 12008 invoked from network); 6 Apr 2009 21:38:53 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m2.grp.sp2.yahoo.com with QMQP; 6 Apr 2009 21:38:53 -0000\r\nX-Received: from unknown (HELO n17c.bullet.sp1.yahoo.com) (69.147.64.126)\n  by mta3.grp.sp2.yahoo.com with SMTP; 6 Apr 2009 21:38:53 -0000\r\nX-Received: from [69.147.65.148] by n17.bullet.sp1.yahoo.com with NNFMP; 06 Apr 2009 21:38:36 -0000\r\nX-Received: from [98.137.34.36] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 06 Apr 2009 21:38:36 -0000\r\nDate: Mon, 06 Apr 2009 21:38:34 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;grdsoq+lmju@...&gt;\r\nIn-Reply-To: &lt;grdpk6+vu2i@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;pandae667&quot; &lt;aaron667@...&gt;\r\nSubject: Re: Best way of filtering of URLs with RegExp?\r\nX-Yahoo-Group-Post: member; u=191387937; y=Y-_DA_rOnS2grh5OVIUFAMIvwQg4oU6omte-AZ5edNqzrZKU\r\nX-Yahoo-Profile: pandae667\r\n\r\nBack when I started using Heritrix one of the best resources I could find a=\r\nbout limitig the scope of my crawl to certain document types was the follow=\r\ning wiki entry:\nhttp://web.archive.org/web/20071218182807/http://www.dreame=\r\nrsrealm.net/~tree/blog/?s=3Dtext/html&submit=3DGO\n\n--- In archive-crawler@y=\r\nahoogroups.com, &quot;pbaclace&quot; &lt;pbaclace@...&gt; wrote:\n&gt;\n&gt; The &quot;fetch-processor -=\r\n&gt; midfetch-decide-rules&quot;\n&gt; are good for rejecting or accepting by MIME type=\r\n before fully fetching a file because that is the only point where the MIME=\r\n type is officially known (although it could be wrong, but that is another =\r\nmatter).\n&gt; \n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;go=\r\njomo@&gt; wrote:\n&gt; &gt;\n&gt; &gt; The &#39;midfetch-decide-rules&#39; are used only to abort a =\r\nfetch midway (after \n&gt; &gt; the headers have been retrieved).\n&gt; &gt; \n&gt; &gt; The pro=\r\nper way to control which URIs are even attempted is to adjust the \n&gt; &gt; craw=\r\nl&#39;s &#39;scope&#39;. You could add a late rule to the set of scoping \n&gt; &gt; decide-ru=\r\nles that REJECTs URIs matching your expression. Then, those \n&gt; &gt; URIs won&#39;t=\r\n even be queued for processing.\n&gt; &gt; \n&gt; &gt; - Gordon @ IA\n&gt; &gt; \n&gt; &gt; felizimm wr=\r\note:\n&gt; &gt; &gt; Hi,\n&gt; &gt; &gt; \n&gt; &gt; &gt; I like to filter specific URLs and everything w=\r\norks really fine with RegExp in &quot;fetch-processor -&gt; midfetch-decide-rules&quot;.=\r\n But I noticed in the logs that heritrix gets the http-header of every &quot;unw=\r\nanted&quot; page.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Is there a possibility to prevent crawling &quot;unwan=\r\nted&quot; pages in advance, so that heritrix does not get any piece of the file?=\r\n I tried the &quot;pre-fetch-processors&quot; but it didn&#39;t work with the same RegExp=\r\n.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Thanks!\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; -----------=\r\n-------------------------\n&gt; &gt; &gt; \n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; =\r\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}