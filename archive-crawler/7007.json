{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":475160622,"authorName":"Krishna, Murali","from":"&quot;Krishna, Murali&quot; &lt;muralikp@...&gt;","profile":"muralikpbhat","replyTo":"LIST","senderId":"Tj3AepBXQNYRLiN1QXwEPcLkBl-O7o8zBBfql0yqmek6kAonOG9aemdTUMsMPe2E9lYuPprzOJ5c5yaXptRp3ZRZYPYlEihdmnHJof5z","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix Frontier","postDate":"1297173352","msgId":7007,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM5Nzc0OTQwLjIxMUUlbXVyYWxpa3BAYW1hem9uLmNvbT4=","inReplyToHeader":"PDRENTBGRTFCLjcwMTA2QGFyY2hpdmUub3JnPg=="},"prevInTopic":7006,"nextInTopic":7008,"prevInTime":7006,"nextInTime":7008,"topicId":7005,"numMessagesInTopic":11,"msgSnippet":"Thanks Gordon for the detailed response. We don t have all the urls upfront, it is a continuous stream of urls and we don t want to wait for the previous","rawEmail":"Return-Path: &lt;muralikp@...&gt;\r\nX-Sender: muralikp@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 19990 invoked from network); 8 Feb 2011 13:56:03 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m15.grp.re1.yahoo.com with QMQP; 8 Feb 2011 13:56:03 -0000\r\nX-Received: from unknown (HELO smtp-fw-4101.amazon.com) (72.21.198.25)\n  by mta3.grp.re1.yahoo.com with SMTP; 8 Feb 2011 13:56:03 -0000\r\nX-IronPort-AV: E=Sophos;i=&quot;4.60,442,1291593600&quot;; \n   d=&quot;scan&#39;208&quot;;a=&quot;348073897&quot;\r\nX-Received: from smtp-in-0105.sea3.amazon.com ([10.224.19.45])\n  by smtp-border-fw-out-4101.iad4.amazon.com with ESMTP/TLS/DHE-RSA-AES256-SHA; 08 Feb 2011 13:55:58 +0000\r\nX-Received: from ex-hub-12010.ant.amazon.com (ex-hub-12010.ant.amazon.com [10.32.49.103])\n\tby smtp-in-0105.sea3.amazon.com (8.13.8/8.13.8) with ESMTP id p18Dtuja012437\n\t(version=TLSv1/SSLv3 cipher=AES128-SHA bits=128 verify=FAIL)\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 8 Feb 2011 13:55:57 GMT\r\nX-Received: from ex-cas-hyd1-1.ant.amazon.com (10.43.31.64) by\n ex-hub-12010.ant.amazon.com (10.32.49.103) with Microsoft SMTP Server (TLS)\n id 8.2.254.0; Tue, 8 Feb 2011 13:55:55 +0000\r\nX-Received: from ex-mail-hyd1-1.ant.amazon.com ([fe80::556:4cdd:76c1:d940]) by\n ex-cas-hyd1-1.ant.amazon.com ([fe80::e497:35d3:f367:9a0%11]) with mapi; Tue,\n 8 Feb 2011 19:25:53 +0530\r\nTo: &quot;archive-crawler@yahoogroups.com&quot; &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Tue, 8 Feb 2011 19:25:52 +0530\r\nThread-Topic: [archive-crawler] Heritrix Frontier\r\nThread-Index: AcvHadjaWeHUJ+DjSoCU4jUOF0aGDAALgz7R\r\nMessage-ID: &lt;C9774940.211E%muralikp@...&gt;\r\nIn-Reply-To: &lt;4D50FE1B.70106@...&gt;\r\nAccept-Language: en-US\r\nContent-Language: en-US\r\nX-MS-Has-Attach: \r\nX-MS-TNEF-Correlator: \r\nuser-agent: Microsoft-Entourage/13.6.0.100712\r\nacceptlanguage: en-US\r\nContent-Type: text/plain; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nMIME-Version: 1.0\r\nFrom: &quot;Krishna, Murali&quot; &lt;muralikp@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix Frontier\r\nX-Yahoo-Group-Post: member; u=475160622; y=61PC7td3oqYresQAzpU6M4ZvZSr3NmeyHKhsxQQgD0W6klchtCE1\r\nX-Yahoo-Profile: muralikpbhat\r\n\r\nThanks Gordon for the detailed response.\n    We don&#39;t have all the urls upf=\r\nront, it is a continuous stream of urls\nand we don&#39;t want to wait for the p=\r\nrevious heritrix jobs to finish.\nEssentially, we want to schedule them as a=\r\nnd when is possible (honoring\npoliteness). Also some of the urls have highe=\r\nr priority for crawling.\n   The problem with BDB frontier is that it is tie=\r\nd to the box and is a\nreliability concern if the machine goes down. We are =\r\nthinking of having the\nurls in reliable queue service in a different cluste=\r\nr and make the heritrix\nread from that queue. This makes heritrix instance =\r\nstateless (the crawled\ncontent goes to another cluster) and easy to replace=\r\n with another box. Of\ncourse this calls for a new checkpointing mechanism o=\r\nutside the box.\n   I understand it is an overkill to use heritrix, but In f=\r\nuture, we might\nneed depth crawl which we can easily implement by schedulin=\r\ng the newly\ndetected urls back into the reliable queue service. We are just=\r\n trying to\nleverage the politeness, threading and pluggable processor frame=\r\nworks of\nheritrix.\n\nThoughts?\n\nThanks,\nMurali\n\nOn 2/8/11 1:56 PM, &quot;Gordon M=\r\nohr&quot; &lt;gojomo@...&gt; wrote:\n\n&gt; On 2/7/11 3:17 AM, Krishna, Murali wrot=\r\ne:\n&gt;&gt; \n&gt;&gt; \n&gt;&gt; Hi all,\n&gt;&gt; We have a list of urls to be crawled, essentially =\r\njust a fetch and some\n&gt;&gt; processing. Assume that the list can be huge and r=\r\nun into billions. So,\n&gt;&gt; we are thinking of writing a new Frontier which wi=\r\nll accomplish this.\n&gt;&gt; Will have multiple heritrix worker boxes, each of th=\r\ne worker=B9s frontier\n&gt;&gt; will get one portion of the centralized url reposi=\r\ntory (distributed\n&gt;&gt; storage) and schedule them for crawling.\n&gt; \n&gt; You prob=\r\nably won&#39;t need a new Frontier for this; the default frontier\n&gt; (BdbFrontie=\r\nr) should work for tens to even hundreds of millions of\n&gt; queued URIs per n=\r\node.\n&gt; \n&gt; I have more confidence in H3 for loading millions of seed URIs at=\r\n\n&gt; startup (which should be even more efficient in H3 SVN TRUNK and the\n&gt; n=\r\next H3 release), though you could also feed them in smaller batches via\n&gt; t=\r\nhe H1 JMX interface, or in batches via the &#39;action&#39; directory mechanism\n&gt; i=\r\nn H3.\n&gt; \n&gt; If you simply have a large static list of URIs to crawl -- and d=\r\non&#39;t\n&gt; need link-extraction and any other running analysis/reporting --\n&gt; H=\r\neritrix may be overkill for your purposes.\n&gt; \n&gt;&gt; 1. Can we achieve this by =\r\nextending the WorkQueueFrontier ? I couldn=B9t\n&gt;&gt; find much documentation o=\r\nn how WQF handles politeness. I am thinking of\n&gt;&gt; grouping the urls into wo=\r\nrkqueue based on politeness requirement, will\n&gt;&gt; it automatically take care=\r\n of politeness if I group correctly? Can I\n&gt;&gt; configure crawl-delay per Wor=\r\nkQueue?\n&gt; \n&gt; You can control what is crawled -- whether outlinks from your =\r\nstarting\n&gt; URIs are followed, for example, to get inline resources or other=\r\n linked\n&gt; pages -- by customizing the scoping rules. If grouping URIs by ho=\r\nstname\n&gt; into queues is insufficient, you can implement a new\n&gt; QueueAssign=\r\nmentPolicy. In H3, politeness delays per URI -- affecting the\n&gt; queue from =\r\nwhich the URI came -- are configured outside the frontier, in\n&gt; the Disposi=\r\ntionProcessor. So lots of behavioral customization doesn&#39;t\n&gt; require reimpl=\r\nementing or specializing the frontier.\n&gt; \n&gt; The queues are the units of pol=\r\niteness: by default, only one URI from a\n&gt; queue will be in-process at a ti=\r\nme. When a URI finishes, a configurable\n&gt; pause (see the minDelay, maxDelay=\r\n, delayFactor, and\n&gt; respectCrawlDelayUpToSeconds settings on DispositionPr=\r\nocessor in H3) is\n&gt; applied to that queue before any other URIs are tried. =\r\nNote that URI\n&gt; domain-lookup/connectivity failures cause the same URI to b=\r\ne pushed back\n&gt; atop the queue, a longer (frontier retryDelaySeconds) pause=\r\n to be taken,\n&gt; and multiple (frontier maxRetries) attempts to be made, bef=\r\nore the next\n&gt; URI is tried. This means you usually do not want URIs on dif=\r\nferent\n&gt; hosts, where one host might be unreachable, mixed in the same queu=\r\ne --\n&gt; one failure will delay them all -- unless you also knock the\n&gt; retri=\r\nes/retryDelay way down.\n&gt; \n&gt; You can use the settings &#39;sheet overlay&#39; (aka =\r\n&#39;overrides&#39; in H1) to set\n&gt; different politeness values for different URIs =\r\nby host or other\n&gt; patterns; the queue then takes on the delay of the URI t=\r\nhat was just\n&gt; offered/completed.\n&gt; \n&gt; You should also look at previous lis=\r\nt traffic about HashCrawlMapper for\n&gt; ideas on splitting the URI space, and=\r\n the BloomUriUniqFilter as an\n&gt; option for an all in-memory URI-already-see=\r\nn filter that may be\n&gt; appropriate for larger crawls.\n&gt; \n&gt;&gt; 2. What are ina=\r\nctive queues, retired queues and getURIList here? (sorry,\n&gt;&gt; couldn=B9t fin=\r\nd doc)\n&gt; \n&gt; &#39;inactive&#39; queues are those that are not yet being considered t=\r\no keep a\n&gt; thread busy. All those queues that are &#39;active&#39; round-robin to p=\r\nrovide\n&gt; URIs to available threads, until the queue exhausts its &#39;session&#39;\n=\r\n&gt; budget; then it goes to the back of all &#39;inactive&#39; queues. If a thread\n&gt; =\r\ncan&#39;t be kept busy with an available &#39;active&#39; queue, then the top\n&gt; &#39;inacti=\r\nve&#39; queue is activated. The intent is for the crawler to\n&gt; intensely focus =\r\non some queues for a while -- hoping to finish them, or\n&gt; at least get a bi=\r\ng batch of URIs with as little time-skew as possible =AD=AD\n&gt; but then rota=\r\nte others into activity eventually. (The &#39;budgeting&#39; and\n&gt; URI &#39;cost&#39; param=\r\neters affect this cycle.)\n&gt; \n&gt; &#39;retired&#39; queues have already offered up URI=\r\ns whose total &#39;cost&#39; exceeds\n&gt; their &#39;totalBudget&#39;, and so they continue to=\r\n collect newly-discovered\n&gt; URIs, but will never b considered for &#39;active&#39; =\r\nrotation (unless you\n&gt; raise their &#39;totalBudget&#39;). You probably don&#39;t need =\r\nthis feature, and\n&gt; won&#39;t notice any &#39;retired&#39; queues unless you set a &#39;tot=\r\nalBudget&#39; and\n&gt; nonzero URI cost policy.\n&gt; \n&gt; I don&#39;t know what you mean by=\r\n &quot;getURIList&quot;.\n&gt; \n&gt;&gt; 3. How does checkpointing work, I want to restart from=\r\n the last crawled\n&gt;&gt; state. Is there a callback to do the frequent checkpoi=\r\nnting for\n&gt;&gt; WorkQueueFrontier=B9s implementations.\n&gt; \n&gt; Checkpointing trie=\r\ns to save the whole crawl state at requested points.\n&gt; You can set it to au=\r\ntomatically checkpoint at a certain interval. In H1,\n&gt; it&#39;s via a system pr=\r\noperty; see Checkpointer.initialize(). In H3, it&#39;s\n&gt; CheckpointService&#39;s ch=\r\neckpointIntervalMinutes property. In H1, the crawl\n&gt; must reach a full paus=\r\ne for a checkpoint to occur; with long downloads\n&gt; and connection timeouts,=\r\n this can mean a slowdown in the tens of\n&gt; minutes. In H3, a checkpoint req=\r\nuires a much narrower lock, so may only\n&gt; take a few seconds or a minute or=\r\n two, and long network fetches may\n&gt; continue during the checkpoint.\n&gt; \n&gt; I=\r\nn both cases, you need to retain the &#39;state&#39; directory files\n&gt; (.JDB/.DEL) =\r\ncorresponding to the checkpoints from which you might want\n&gt; to resume. (If=\r\n not needed for a checkpoint, you can freely delete the\n&gt; .DELs.) Resuming =\r\nfrom an earlier checkpoint may foul any other\n&gt; subsequent-but-unused check=\r\npoints (though future checkpoints should be\n&gt; fine).\n&gt; \n&gt; The checkpointing=\r\n system has always been a bit rough but I have more\n&gt; confidence in its fle=\r\nxibility and speed in H3. I would not yet count on\n&gt; it for perfect resumab=\r\nility in a large crawl without more experience\n&gt; using it. If not using che=\r\nckpoints, or checkpoints fail for some reason,\n&gt; an approximation of a fron=\r\ntier&#39;s state at the time of a crash can be\n&gt; recreated from the &#39;frontier r=\r\necovery log&#39; also kept by the crawler.\n&gt; (Not all running stats/state can b=\r\ne reconstructed, but essentially all\n&gt; the same pending URIs will be reenqu=\r\neued.)\n&gt; \n&gt; I believe there&#39;s a JMX call in H1 to request a checkpoint, and=\r\n in H3\n&gt; it&#39;s just one of the web UI/web service calls easy to trigger by a=\r\n web hit:\n&gt; \n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3=\r\n.0+API+Guide#Herit\n&gt; rix3.0APIGuide-CheckpointJob\n&gt; \n&gt; Hope this helps!\n&gt; \n=\r\n&gt; - Gordon @ IA\n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo!=\r\n Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}