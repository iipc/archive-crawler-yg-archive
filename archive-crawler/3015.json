{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":39397245,"authorName":"Eric","from":"Eric &lt;ej@...&gt;","profile":"mar1ow2003","replyTo":"LIST","senderId":"U0ehdq9qTiuEPAAYy6c1H2C_RbF5ImBlT8ye_r02lSUzE3fv7J-PQRZIXT-OR5UpUz2Dt1xTb5k3cfs","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] robots.txt from every directory?","postDate":"1152244470","msgId":3015,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDYwNzA3MDM1NDMwLkdBMjU3ODRAZHV2ZWwuaXIuaWl0LmVkdT4=","inReplyToHeader":"PDQ0QUQ4QzU5LjYwODA4MDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PDIwMDYwNzA2MjIxMTQ4LkdBMTE4NjNAZHV2ZWwuaXIuaWl0LmVkdT4gPDQ0QUQ4QzU5LjYwODA4MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":3013,"nextInTopic":0,"prevInTime":3014,"nextInTime":3016,"topicId":3012,"numMessagesInTopic":3,"msgSnippet":"You are correct, these errant robots.txt URL s are coming from speculative embeds (from javascript).  I was fooled because I had a crawl finish with all of its","rawEmail":"Return-Path: &lt;ej@...&gt;\r\nX-Sender: ej@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 49823 invoked from network); 7 Jul 2006 03:54:46 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m27.grp.scd.yahoo.com with QMQP; 7 Jul 2006 03:54:46 -0000\r\nReceived: from unknown (HELO duvel.ir.iit.edu) (216.47.134.15)\n  by mta2.grp.scd.yahoo.com with SMTP; 7 Jul 2006 03:54:45 -0000\r\nReceived: from ej by duvel.ir.iit.edu with local (Exim 4.54)\n\tid 1FyhQE-0006rM-Fi\n\tfor archive-crawler@yahoogroups.com; Thu, 06 Jul 2006 22:54:30 -0500\r\nDate: Thu, 6 Jul 2006 22:54:30 -0500\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;20060707035430.GA25784@...&gt;\r\nReferences: &lt;20060706221148.GA11863@...&gt; &lt;44AD8C59.6080807@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Disposition: inline\r\nIn-Reply-To: &lt;44AD8C59.6080807@...&gt;\r\nX-PGP-Key: http://ir.iit.edu/~ej/ericjensen.asc\r\nUser-Agent: Mutt/1.5.11\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Eric &lt;ej@...&gt;\r\nSubject: Re: [archive-crawler] robots.txt from every directory?\r\nX-Yahoo-Group-Post: member; u=39397245; y=cQkVj5SIrkiebA6q0txQhHbj8nhotc_UR4kdt_uWxdhCkBaQxQ\r\nX-Yahoo-Profile: mar1ow2003\r\n\r\nYou are correct, these errant robots.txt URL&#39;s are coming from\nspeculative embeds (from javascript).  I was fooled because I had a\ncrawl finish with all of its URI&#39;s ending up in the crawl.log with -63\nstatus and then I saw all these errant robots.txt requests.  \n\nAt first, I suspected that I got the -63&#39;s because the real robots\nfile (http://www.somewhere.com/robots.txt) might have been interrupted\nby my mid-fetch filter to only get text/html (since it appears in the\narc as only the headers and not the body).  However, it no longer\nappears that&#39;s the problem, and I guess I should have gotten -61 if it\ncouldn&#39;t grab robots.txt\n\nRather, it was simply that my connection was interrupted and the DNS\nrequest failed so that was the prereq that couldn&#39;t be satisfied.\n\nThanks,\neric.\n\nOn Thu, Jul 06, 2006 at 03:19:05PM -0700, Gordon Mohr wrote:\n&gt; Eric wrote:\n&gt; &gt; In examining my crawl logs, I find that heritrix is trying to download\n&gt; &gt; robots.txt from every directory I access on the server, i.e.\n&gt; &gt; \n&gt; &gt; http://www.somewhere.com/a/robots.txt\n&gt; &gt; http://www.somewhere.com/b/robots.txt\n&gt; &gt; \n&gt; &gt; even though it&#39;s able to fetch the root\n&gt; &gt; http://www.somewhere.com/robots.txt just fine (it&#39;s in the arcs)\n&gt; &gt; \n&gt; &gt; This is a problem because some of my crawl has CGI&#39;s which use the\n&gt; &gt; path as their argument list, so I have many different &quot;directories&quot;\n&gt; &gt; that are really just CGI parameters.  How can I turn this off?\n&gt; &gt; \n&gt; &gt; I tried changing\n&gt; &gt; \n&gt; &gt; PreconditionEnforcer.java:176\n&gt; &gt; String prereq = curi.getUURI().resolve(&quot;/robots.txt&quot;).toString();\n&gt; &gt; \n&gt; &gt; But it doesn&#39;t seem to have done the trick.  \n&gt; \n&gt; The robots.txt standard only provides for root/host-level robots.txt \n&gt; files, so that&#39;s the only URI automatically checked by Heritrix.\n&gt; \n&gt; I suspect the pages you are crawling must include some reference to \n&gt; these other URIs. (Or, a redirect is occuring from some other URI to these.)\n&gt; \n&gt; For the crawl.log lines showing these other fetches, what are the \n&gt; &#39;hops-path&#39; (string of capital letters like &#39;P&#39; or &#39;LLE&#39;) and &#39;via&#39; \n&gt; (precedent URI) shown?\n&gt; \n&gt; For the Heritrix auto-fetch, the hops-path would end with &#39;P&#39;.\n&gt; \n&gt; - Gordon @ IA\n\n-- \nhttp://ir.iit.edu/~ej\n\n"}}