{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":149686414,"authorName":"Judy Ma","from":"Judy Ma &lt;jma@...&gt;","profile":"jma0112","replyTo":"LIST","senderId":"kVBGPNgvkmq2X5LQA0XVIF2NsFB8-D00Y8h58otShy1nMP_Aqj8kDpvJIgB6yGXp-GzfTu-I6Vi8uwUPVq2mdw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: manifest crawl","postDate":"1059420431","msgId":125,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFBpbmUuTE5YLjQuMzMuMDMwNzI4MTIyMjQzMC42MTI3LTEwMDAwMEBob21lc2VydmVyLmFyY2hpdmUub3JnPg==","inReplyToHeader":"PDAwMGIwMWMzNTUzYSQxYzFlMDc2MCQ0OGYwZWRkMUBXT1JLU1RBVElPTjIxPg=="},"prevInTopic":124,"nextInTopic":0,"prevInTime":124,"nextInTime":126,"topicId":106,"numMessagesInTopic":6,"msgSnippet":"... It s getting all excluded areas.  But I think you re right in that the two User-Agent: * directives in robots.txt file is causing the problem. Judy","rawEmail":"Return-Path: &lt;jma@...&gt;\r\nX-Sender: jma@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 39416 invoked from network); 28 Jul 2003 19:27:12 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m5.grp.scd.yahoo.com with QMQP; 28 Jul 2003 19:27:12 -0000\r\nReceived: from unknown (HELO homeserver.archive.org) (209.237.233.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 28 Jul 2003 19:27:12 -0000\r\nReceived: from localhost (jma@localhost)\n\tby homeserver.archive.org (8.11.6/8.11.6) with ESMTP id h6SJRBI32520\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 28 Jul 2003 12:27:11 -0700\r\nDate: Mon, 28 Jul 2003 12:27:11 -0700 (PDT)\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nSubject: Re: [archive-crawler] Re: manifest crawl\r\nIn-Reply-To: &lt;000b01c3553a$1c1e0760$48f0edd1@WORKSTATION21&gt;\r\nMessage-ID: &lt;Pine.LNX.4.33.0307281222430.6127-100000@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: TEXT/PLAIN; charset=US-ASCII\r\nX-eGroups-From: Judy Ma &lt;jma@...&gt;\r\nFrom: Judy Ma &lt;jma@...&gt;\r\nReply-To: &lt;jma@...&gt;\r\nX-Yahoo-Group-Post: member; u=149686414\r\nX-Yahoo-Profile: jma0112\r\n\r\n&gt; &gt; Also, heritrix is getting robots.txt excluded pages & directories. \n&gt; \n&gt; Is it getting all excluded areas, or just some? If just some, is\n&gt; there any pattern to those it gets and those it doesn&#39;t?\n&gt; \n&gt; I see that there is still a bug in the EasyGarden robots.txt:\n&gt; there should be only one &quot;User-Agent: *&quot; directive, with\n&gt; mutliple &quot;Disallow:&quot;s after it. Could this be the issue?\n&gt; \n\nIt&#39;s getting all excluded areas.  But I think you&#39;re right in that the \ntwo &quot;User-Agent: *&quot; directives in robots.txt file is causing the problem.\n\n\nJudy\n\n\n"}}