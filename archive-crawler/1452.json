{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":210551673,"authorName":"John R. Frank","from":"&quot;John R. Frank&quot; &lt;jrf@...&gt;","profile":"tamarind473","replyTo":"LIST","senderId":"RqmC7y3ItSS9BBVVbVCQXhXQgCOcROYioyKwsvdWZwnpsO5OUkll9UT3_qt1fkvBhj8ii1rYRDQyql-UJrJGIlvKyXk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"continuous crawling proposal","postDate":"1107241310","msgId":1452,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFBpbmUuTE5YLjQuNTYuMDUwMjAxMDA1ODUxMC4xMzQ0OEBwaWtlc3BlYWsubWV0YWNhcnRhLmNvbT4="},"prevInTopic":0,"nextInTopic":1453,"prevInTime":1451,"nextInTime":1453,"topicId":1452,"numMessagesInTopic":24,"msgSnippet":"Stack, What do you think of these three steps as a possible way to implement continuous crawling in Heritrix?  Details for each of these three are discussed","rawEmail":"Return-Path: &lt;jrf@...&gt;\r\nX-Sender: jrf@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 61548 invoked from network); 1 Feb 2005 07:05:48 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m7.grp.scd.yahoo.com with QMQP; 1 Feb 2005 07:05:48 -0000\r\nReceived: from unknown (HELO pikespeak.metacarta.com) (66.92.95.164)\n  by mta1.grp.scd.yahoo.com with SMTP; 1 Feb 2005 07:05:48 -0000\r\nReceived: from jrf (helo=localhost)\n\tby pikespeak.metacarta.com with local-esmtp (Exim 3.35 #1 (Debian))\n\tid 1Cvs2o-0003Yo-00\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 01 Feb 2005 02:01:50 -0500\r\nDate: Tue, 1 Feb 2005 02:01:50 -0500 (EST)\r\nX-X-Sender: jrf@...\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;Pine.LNX.4.56.0502010058510.13448@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: TEXT/PLAIN; charset=US-ASCII\r\nSender:  &lt;jrf@...&gt;\r\nX-eGroups-Remote-IP: 66.92.95.164\r\nFrom: &quot;John R. Frank&quot; &lt;jrf@...&gt;\r\nSubject: continuous crawling proposal\r\nX-Yahoo-Group-Post: member; u=210551673\r\nX-Yahoo-Profile: tamarind473\r\n\r\nStack,\n\nWhat do you think of these three steps as a possible way to implement\ncontinuous crawling in Heritrix?  Details for each of these three are\ndiscussed below.\n\n1) extend the work queue&#39;s logic to only dequeue &quot;ready&quot; URLs\n\n2) decide when to recheck a given page based on a simple model\n\n3) detect substantive page changes and store the info in AlreadySeen\n\n\n\n1) To do this, we need a queueing mechanism that blocks the dequeuing of a\nCrawlURI until we are past its assigned next &quot;okay to crawl&quot; moment.\nWhere should we store this timestamp?  As you said, the BDB keys are\noverloaded with too much meaning already, so putting a seconds since the\nepoch in there is not so good:\nhttp://crawler.archive.org/xref/org/archive/crawler/frontier/BdbMultipleWorkQueues.html#246\n\nPerhaps this timestamp belongs in CandidateURI where the\nschedulingDirective pieces are?  For example, if the schedulingDirective\nis negative, then it could be interpreted as a timestamp.  So for a\ndocument next Wednesday, it would get:\n\njrf@localhost~$ date +%s -d &quot;Feb 9 01:20:29 EST 2005&quot;\n1107930029\n\nwith a minus sign in front:  -1107930029\n\nAdding `date +%s -d now` now to that will give a positive number when it\nis okay to crawl.  This would require modifications to the four-bit\ninterpretation of priority in the BDB key computation.  An escape value\nof 1111 might tell the queue that it needs to look in the object to find\nthe value of the timestamp.\n\nThe problem with this is that suddenly the queue is not a queue, because\nthe keys are the sorting mechanism on the queue in the BDB implementation,\nright?  Do you see a way fix to this besides cycling through all the curi\nwith 1111 looking at the full value of the timestamp?\n\n\n\n2) To compute the time at which to refetch, we can predict the likelihood\nthat the page has been modified as a function of time since last modified.\nSee below for discussion of detecting last modified.  An easy function to\nuse for this modeling comes from the Michaelis-Menton model of enzyme\nkinetics:\n\n\tt is time elapsed after a modification of the content\n\n\tP(t) is the probability that the page has changed by time t\n\n                         a\n                      k t\n       \tP(t) =  ---------------\n                           a\n                   1  + k t\n\nAt small t, P is small.  At large t, P tends to 1, i.e. certainty of\nchange.  It is easiest to choose a=2, which is the smallest integer value\nthat gives step-like behavior.  We can then set k by fitting this function\nto any given page&#39;s history (details below).\n\nGiven k, we can use P(t) to predict when to refetch a document.  We pick a\nthreshold probability above which we want to refetch.  If we set it low,\nthen we want to recheck more often.  If we set it high, that means we&#39;re\nwilling to tolerate more stale content in order to not recheck as often.\n\nWhen P(t) exceeds the chosen threshold, then we want to recheck it.  By\ninverting the probability function, this threshold gives us a time to\nwait before rechecking the page:\n\n                 k                (-1/a)\n       t = ( ----------   -   k )\n             threshold\n\n   For clarity that is:\n   t = (((k / threshold) - k ) ^ (-1/a))\n\nt is the time interval between the most recent known modification event\nand that time in the future when the expected probability of change is\njust above threshold.  For robustness, we should define a time within\nwhich all pages must be rechecked.  In python, the function might look\nlike:\n\ndef delay(k, threshold):\n    Days = 60 * 60 * 24\n    maxRecheckDelay = 20 * Days\n    thresholdDelay = (((k / threshold) - k ) ^ (-1/alpha))\n    return minimum(maxRecheckDelay, thresholdDelay)\n\n\n3. Since last-modified information is not universally provided by document\nrepositories, we need a mechanism to detect substantive content changes\nand record them in the BDB.  I&#39;m looking into tools for this kind of\ncontent hashing that could be run in the extractor chain and stored in the\nobject that gets into BDB.\n\nSuppose we have such a content hash, then the interval of time between two\nknown modification events is an upper bound because there could have been\na modification event that we did not observe between these observed\nevents.  If these upper bounds are not far off the real value, then we can\naccurately approximate the probability of modification at half the\nobserved interval as being 50%.  That is, we use the previously observed\nmodification intervals to estimate what the next actual interval will be.\nIf the average of the last, say, five observed intervals is T, then we\nestimate that at (T/2) in the future, the probability of modification will\nbe 50%.  This let&#39;s us estimate a value for k based on previously observed\nmodification intervals:\n\n               -a\n       k = (T/2)    which for a=2 is (two over T) squared.\n\n\nSome logic is required to make sure that only the last few, say five,\nmeaningful modification intervals are averaged to make T.  This moving\nwindow average allows Heritrix to more rapidly adapt to pages that have\nvarying update frequencies, which is most pages.\n\n\nThoughts?  Reactions?\n\nJohn\n\n"}}