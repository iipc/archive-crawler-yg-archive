{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":496150545,"authorName":"Markus Mirsberger","from":"Markus Mirsberger &lt;markus.mirsberger@...&gt;","profile":"mirschi74","replyTo":"LIST","senderId":"UneIcfVm93rN_UCrqFHz1aoBcpvk_jca-Jbh3GgZtlWFijTCYNjbZSPcSdpn995p7pJ6Na-eiKlJFVgt01LumAdT5w0vmzGtSVVsILzLGLNO0S4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Limit the crawls to e.g. 100 URLs/Host","postDate":"1348033305","msgId":7776,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwNTk1QjE5LjcwNDAwMDNAZ214LmRlPg==","inReplyToHeader":"PDM3MjBFNDUyLThGNDktNEJGQi1BNDBGLUQ2MEJDMjA5MTQ1REBzdGF0c2JpYmxpb3Rla2V0LmRrPg==","referencesHeader":"PGsxcXFyOSthZm9pQGVHcm91cHMuY29tPiA8NTA1MjY3QUQuNjAyMDgwNkBhcmNoaXZlLm9yZz4gPDUwNTk1MzM0LjYwNDA2MDhAZ214LmRlPiA8MzcyMEU0NTItOEY0OS00QkZCLUE0MEYtRDYwQkMyMDkxNDVEQHN0YXRzYmlibGlvdGVrZXQuZGs+"},"prevInTopic":7774,"nextInTopic":7777,"prevInTime":7775,"nextInTime":7777,"topicId":7755,"numMessagesInTopic":8,"msgSnippet":"Yes I think that too. But with one job I m only crawling one host and I not follow external hosts (with a custom decide rule). So in every queue should be the","rawEmail":"Return-Path: &lt;markus.mirsberger@...&gt;\r\nX-Sender: markus.mirsberger@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 31046 invoked from network); 19 Sep 2012 05:41:51 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m6.grp.sp2.yahoo.com with QMQP; 19 Sep 2012 05:41:51 -0000\r\nX-Received: from unknown (HELO mailout-de.gmx.net) (213.165.64.23)\n  by mta5.grp.sp2.yahoo.com with SMTP; 19 Sep 2012 05:41:51 -0000\r\nX-Received: (qmail invoked by alias); 19 Sep 2012 05:41:49 -0000\r\nX-Received: from mx-ll-14.207.114-14.dynamic.3bb.co.th (EHLO [192.168.1.11]) [14.207.114.14]\n  by mail.gmx.net (mp019) with SMTP; 19 Sep 2012 07:41:49 +0200\r\nX-Authenticated: #10074639\r\nX-Provags-ID: V01U2FsdGVkX19Drs5TYN9DEikNlAeQFxhze0XuHbVJMxLONrLGSO\n\tv0MiUScjhI4xVm\r\nMessage-ID: &lt;50595B19.7040003@...&gt;\r\nDate: Wed, 19 Sep 2012 12:41:45 +0700\r\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:15.0) Gecko/20120827 Thunderbird/15.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;k1qqr9+afoi@...&gt; &lt;505267AD.6020806@...&gt; &lt;50595334.6040608@...&gt; &lt;3720E452-8F49-4BFB-A40F-D60BC209145D@...&gt;\r\nIn-Reply-To: &lt;3720E452-8F49-4BFB-A40F-D60BC209145D@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------040605030302060907000403&quot;\r\nX-Y-GMX-Trusted: 0\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Markus Mirsberger &lt;markus.mirsberger@...&gt;\r\nSubject: Re: [archive-crawler] Limit the crawls to e.g. 100 URLs/Host\r\nX-Yahoo-Group-Post: member; u=496150545; y=LDtb-ar_jzEVSH3VMNeY2RYwSQRoFjCysN3YVCf6rPGWhQEJmCVkfNKmcr4-jm3TfmHHwQo1YKV-H4M\r\nX-Yahoo-Profile: mirschi74\r\n\r\n\r\n--------------040605030302060907000403\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\n\r\nYes I think that too.\nBut with one job I&#39;m only crawling one host and I not follow external \nhosts (with a custom decide rule).\nSo in every queue should be the same hostname since I think the external \nhosts should not be moved to the queue when filtered out with a decide rule.\n\nRegards,\nMarkus\n\n\nOn 09/19/2012 12:23 PM, Bjarne Andersen wrote:\n&gt; IF your queues does not exactly match hostnames the queuetotalbudget \n&gt; does not really make sense in your setup. This setting limits the \n&gt; number of URIs taken off each queue so if a queue can have URIs from \n&gt; more than one host, the limit just means that every URI could be from \n&gt; the same host as long as the total budget is not yet spent. If URIs \n&gt; from the same host gets randomly put into different queues it makes \n&gt; even less sense in your setting\n&gt;\n&gt; AFAIK there is no existing solution for you with heritrix :(\n&gt; You could write your own limiter module off cause that holds an \n&gt; alternative datastructure side by side the current queues\n&gt;\n&gt; Best\n&gt; Bjarne Andersen\n&gt; Netarchive.dk &lt;http://Netarchive.dk&gt;\n&gt;\n&gt;\n&gt; Sendt fra min iPhone\n&gt;\n&gt; Den 19/09/2012 kl. 07.08 skrev &quot;Markus Mirsberger&quot; \n&gt; &lt;markus.mirsberger@... &lt;mailto:markus.mirsberger@...&gt;&gt;:\n&gt;\n&gt;&gt; Hello Noah,\n&gt;&gt;\n&gt;&gt; thanks for your reply. This works well when I only use one thread for \n&gt;&gt; one host.\n&gt;&gt; Unfortunately I am crawling most hosts with parallel queues and this \n&gt;&gt; setting affects every queue.\n&gt;&gt; I thought first .... ok no problem..just part the amount of sites to \n&gt;&gt; the queues .. e.g. if I like to crawl 10.000 URLs with 10 parallel \n&gt;&gt; queues so I set queueTotalBudget to 1000.\n&gt;&gt;\n&gt;&gt; This worked in tests with small hosts but now I tried it with a \n&gt;&gt; bigger host and the result is completely different from what I expected.\n&gt;&gt; With 30 parallelQueues I tried to get a maximum of 250.000 URIs (out \n&gt;&gt; of 2.000.000) from one host. So I set the queueTotalBudgt to 8334 but \n&gt;&gt; the result are only about 55.000 crawled URIs.\n&gt;&gt;\n&gt;&gt; Did I use this in a wrong way or do I have to use another setting \n&gt;&gt; when I use parallel queues?\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Thanks and regarads,\n&gt;&gt; Markus\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; On 09/14/2012 06:09 AM, Noah Levitt wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt; Hello Markus,\n&gt;&gt;&gt;\n&gt;&gt;&gt; You can set the value of queueTotalBudget to 100 on your frontier. \n&gt;&gt;&gt; Since\n&gt;&gt;&gt; by default each queue corresponds to one host, the effect is what you\n&gt;&gt;&gt; describe.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Noah\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 08/31/2012 10:04 AM, mirschi74 wrote:\n&gt;&gt;&gt; &gt; Hi,\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt; I have a seed file filled with hosts, but want only crawl e.g. 100 \n&gt;&gt;&gt; URLs from each host.\n&gt;&gt;&gt; &gt; Can you please give me a hint where I can configure that?\n&gt;&gt;&gt; &gt; I think it should be somewhere in the BDBFrontier, but I cant find \n&gt;&gt;&gt; any documentation about that.\n&gt;&gt;&gt; &gt; There is another setting that limits the maxdocuments. But this is \n&gt;&gt;&gt; a global setting and limits the crawls for a jobrun and not meant to \n&gt;&gt;&gt; limit crawls by host.\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt; Thanks in advance,\n&gt;&gt;&gt; &gt; Markus\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt; ------------------------------------\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt; Yahoo! Groups Links\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt; \n\n\r\n--------------040605030302060907000403\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: 8bit\r\n\r\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta content=&quot;text/html; charset=UTF-8&quot; http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#FFFFFF&quot; text=&quot;#000000&quot;&gt;\n    Yes I think that too. &lt;br&gt;\n    But with one job I&#39;m only crawling one host and I not follow\n    external hosts (with a custom decide rule).&lt;br&gt;\n    So in every queue should be the same hostname since I think the\n    external hosts should not be moved to the queue when filtered out\n    with a decide rule.&lt;br&gt;\n    &lt;br&gt;\n    Regards,&lt;br&gt;\n    Markus&lt;br&gt;\n    &lt;br&gt;\n    &lt;br&gt;\n    &lt;div class=&quot;moz-cite-prefix&quot;&gt;On 09/19/2012 12:23 PM, Bjarne Andersen\n      wrote:&lt;br&gt;\n    &lt;/div&gt;\n    &lt;blockquote\n      cite=&quot;mid:3720E452-8F49-4BFB-A40F-D60BC209145D@...&quot;\n      type=&quot;cite&quot;&gt;\n      &lt;span style=&quot;display:none&quot;&gt; &lt;/span&gt;\n      \n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;div&gt;IF your queues does not exactly match hostnames the\n              queuetotalbudget does not really make sense in your setup.\n              This setting limits the number of URIs taken off each\n              queue so if a queue can have URIs from more than one host,\n              the limit just means that every URI could be from the same\n              host as long as the total budget is not yet spent. If URIs\n              from the same host gets randomly put into different queues\n              it makes even less sense in your setting&lt;/div&gt;\n            &lt;div&gt;&lt;br&gt;\n            &lt;/div&gt;\n            &lt;div&gt;AFAIK there is no existing solution for you with\n              heritrix :( &lt;/div&gt;\n            &lt;div&gt;You could write your own limiter module off cause that\n              holds an alternative datastructure side by side the\n              current queues&lt;/div&gt;\n            &lt;div&gt;&lt;br&gt;\n            &lt;/div&gt;\n            &lt;div&gt;Best&lt;/div&gt;\n            &lt;div&gt;Bjarne Andersen&lt;/div&gt;\n            &lt;div&gt;&lt;a moz-do-not-send=&quot;true&quot; href=&quot;http://Netarchive.dk&quot;&gt;Netarchive.dk&lt;/a&gt;&lt;/div&gt;\n            &lt;div&gt;&lt;br&gt;\n              &lt;br&gt;\n              Sendt fra min iPhone&lt;/div&gt;\n            &lt;div&gt;&lt;br&gt;\n              Den 19/09/2012 kl. 07.08 skrev &quot;Markus Mirsberger&quot; &lt;&lt;a\n                moz-do-not-send=&quot;true&quot;\n                href=&quot;mailto:markus.mirsberger@...&quot;&gt;markus.mirsberger@...&lt;/a&gt;&gt;:&lt;br&gt;\n              &lt;br&gt;\n            &lt;/div&gt;\n            &lt;blockquote type=&quot;cite&quot;&gt;\n              &lt;div&gt;\n                &lt;span&gt; &lt;/span&gt;\n                &lt;div id=&quot;ygrp-text&quot;&gt;\n                  &lt;p&gt; Hello Noah,&lt;br&gt;\n                    &lt;br&gt;\n                    thanks for your reply. This works well when I only\n                    use one thread for one host.&lt;br&gt;\n                    Unfortunately I am crawling most hosts with parallel\n                    queues and this setting affects every queue.&lt;br&gt;\n                    I thought first .... ok no problem..just part the\n                    amount of sites to the queues .. e.g. if I like to\n                    crawl 10.000 URLs with 10 parallel queues so I set\n                    queueTotalBudget to 1000.&lt;br&gt;\n                    &lt;br&gt;\n                    This worked in tests with small hosts but now I\n                    tried it with a bigger host and the result is\n                    completely different from what I expected. &lt;br&gt;\n                    With 30 parallelQueues I tried to get a maximum of\n                    250.000 URIs (out of 2.000.000) from one host. So I\n                    set the queueTotalBudgt to 8334 but the result are\n                    only about 55.000 crawled URIs.&lt;br&gt;\n                    &lt;br&gt;\n                    Did I use this in a wrong way or do I have to use\n                    another setting when I use parallel queues?&lt;br&gt;\n                    &lt;br&gt;\n                    &lt;br&gt;\n                    Thanks and regarads,&lt;br&gt;\n                    Markus&lt;br&gt;\n                    &lt;br&gt;\n                    &lt;br&gt;\n                  &lt;/p&gt;\n                  &lt;div class=&quot;moz-cite-prefix&quot;&gt;On 09/14/2012 06:09 AM,\n                    Noah Levitt wrote:&lt;br&gt;\n                  &lt;/div&gt;\n                  &lt;blockquote cite=&quot;mid:505267AD.6020806@...&quot;\n                    type=&quot;cite&quot;&gt; &lt;span&gt; &lt;/span&gt;\n                    &lt;div id=&quot;ygrp-text&quot;&gt;\n                      &lt;p&gt;Hello Markus,&lt;br&gt;\n                        &lt;br&gt;\n                        You can set the value of queueTotalBudget to 100\n                        on your frontier. Since &lt;br&gt;\n                        by default each queue corresponds to one host,\n                        the effect is what you &lt;br&gt;\n                        describe.&lt;br&gt;\n                        &lt;br&gt;\n                        Noah&lt;br&gt;\n                        &lt;br&gt;\n                        On 08/31/2012 10:04 AM, mirschi74 wrote:&lt;br&gt;\n                        &gt; Hi,&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt; I have a seed file filled with hosts, but\n                        want only crawl e.g. 100 URLs from each host.&lt;br&gt;\n                        &gt; Can you please give me a hint where I can\n                        configure that?&lt;br&gt;\n                        &gt; I think it should be somewhere in the\n                        BDBFrontier, but I cant find any documentation\n                        about that.&lt;br&gt;\n                        &gt; There is another setting that limits the\n                        maxdocuments. But this is a global setting and\n                        limits the crawls for a jobrun and not meant to\n                        limit crawls by host.&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt; Thanks in advance,&lt;br&gt;\n                        &gt; Markus&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt; ------------------------------------&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt; Yahoo! Groups Links&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &gt;&lt;br&gt;\n                        &lt;br&gt;\n                      &lt;/p&gt;\n                    &lt;/div&gt;\n                    &lt;!-- end group email --&gt; &lt;/blockquote&gt;\n                  &lt;br&gt;\n                &lt;/div&gt;\n                &lt;!-- end group email --&gt;\n              &lt;/div&gt;\n            &lt;/blockquote&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- end group email --&gt;\n    &lt;/blockquote&gt;\n    &lt;br&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------040605030302060907000403--\r\n\n"}}