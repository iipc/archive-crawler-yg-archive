{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (archive.org)","from":"&quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"u9HjzZkReW2p4-aVqtDjYrZZebMlnDqxeLVDCb3sZqZkCW5lcthwXfFtKPgHKKhWYFVwB3D6dN1c_DY6n4mKw0yVj-uhEVkJZv2sRR0Z3KGCQcayAeW5","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Robots.txt parsing problem?","postDate":"1141898558","msgId":2742,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0MEZGRDNFLjQwNzA2MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ0MEYyMkNBLjUwMjAzMDBAbWV0YWNhcnRhLmNvbT4=","referencesHeader":"PDQ0MEYxQkE3LjIwNTAyMDNAbWV0YWNhcnRhLmNvbT4gPDQ0MEYxRTQ2LjgwMzAwMDJAYXJjaGl2ZS5vcmc+IDw0NDBGMjJDQS41MDIwMzAwQG1ldGFjYXJ0YS5jb20+"},"prevInTopic":2739,"nextInTopic":2743,"prevInTime":2741,"nextInTime":2743,"topicId":2736,"numMessagesInTopic":7,"msgSnippet":"... If you are saving crawl results to ARCs, the robots.txt that was consulted will be in the crawl ARCs -- as will be each daily refetch. ... Such bugs are","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60950 invoked from network); 9 Mar 2006 10:02:44 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m32.grp.scd.yahoo.com with QMQP; 9 Mar 2006 10:02:44 -0000\r\nReceived: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta10.grp.scd.yahoo.com with SMTP; 9 Mar 2006 10:02:44 -0000\r\nReceived: (qmail 28480 invoked from network); 9 Mar 2006 10:02:37 -0000\r\nReceived: from unknown (HELO ?192.168.1.102?) (unknown)\n  by unknown with SMTP; 9 Mar 2006 10:02:37 -0000\r\nX-pair-Authenticated: 70.112.50.223\r\nMessage-ID: &lt;440FFD3E.4070608@...&gt;\r\nDate: Thu, 09 Mar 2006 02:02:38 -0800\r\nUser-Agent: Thunderbird 1.5 (Windows/20051201)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;440F1BA7.2050203@...&gt; &lt;440F1E46.8030002@...&gt; &lt;440F22CA.5020300@...&gt;\r\nIn-Reply-To: &lt;440F22CA.5020300@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Robots.txt parsing problem?\r\nX-Yahoo-Group-Post: member; u=137285340; y=7YLnY7Kd8-xzbEHt6kIjqJ34dWUYRhNoyDxGWfhlq85V\r\nX-Yahoo-Profile: gojomo\r\n\r\nKarl Wright wrote:\n&gt; I don&#39;t know what the robots.txt looked like prior to that, of course, \n&gt; since this is not my website.\n\nIf you are saving crawl results to ARCs, the robots.txt that was \nconsulted will be in the crawl ARCs -- as will be each daily refetch.\n\n&gt; What do you think about the fact that the final line is EOF-terminated, \n&gt; not newline terminated?  Would that upset Heritrix&#39; parser?\n\nSuch bugs are not out of the question, but I&#39;ve just tried a test \ncrawl of the URI from your log -- and Heritrix with a default \nconfig refused to fetch it because of the robots rules (which \nstill lack a final newline).\n\nAlso, I&#39;ve never noticed (nor heard reports) that the \nBufferedReader.readLine() method, used in the robots.txt parsing \n(see org.archive.crawler.datamodel.Robotstxt), mishandles a final \nEOF-terminated line.\n\nSo I strongly suspect Igor&#39;s theory is correct: the robots.txt \nbecame more restrictive during the crawl, and the crawler was \nstill consulting the older version at the time the page was fetched.\n\nThe robots expiration can be made very small, but there will \nalways be some window where the consulted robots rules could be \nout-of-date by the time a followup fetch is executed.\n\n- Gordon @ IA\n\n&gt; Karl\n&gt; \n&gt;&gt;&gt; Hi,\n&gt;&gt;&gt;\n&gt;&gt;&gt; We got dinged again by using Heritrix in that a crawlee complained that \n&gt;&gt;&gt; we were ignoring their robots.txt file.  On the face of it, they look \n&gt;&gt;&gt; like they are correct in complaining:\n&gt;&gt;&gt;\n&gt;&gt;&gt; http://uaelp.pennnet.com/robots.txt\n&gt;&gt;&gt; ===================================\n&gt;&gt;&gt;\n&gt;&gt;&gt; # pennwell robots.txt\n&gt;&gt;&gt; # updated 3/6/06 by bwn\n&gt;&gt;&gt;\n&gt;&gt;&gt; User-agent: Googlebot\n&gt;&gt;&gt; Disallow: /Search/\n&gt;&gt;&gt; Disallow: /search/\n&gt;&gt;&gt; Disallow: /Userreg/\n&gt;&gt;&gt; Disallow: /userreg/\n&gt;&gt;&gt; Disallow: /Nav/\n&gt;&gt;&gt; Disallow: /nav/\n&gt;&gt;&gt; Disallow: /js\n&gt;&gt;&gt; Disallow: /JS\n&gt;&gt;&gt; Disallow: /whitepapers/wp_redirect.cfm\n&gt;&gt;&gt; Disallow: /*.js$\n&gt;&gt;&gt;\n&gt;&gt;&gt; User-agent: *\n&gt;&gt;&gt; Disallow: /Search/\n&gt;&gt;&gt; Disallow: /search/\n&gt;&gt;&gt; Disallow: /Userreg/\n&gt;&gt;&gt; Disallow: /userreg/\n&gt;&gt;&gt; Disallow: /Nav/\n&gt;&gt;&gt; Disallow: /nav/\n&gt;&gt;&gt; Disallow: /js\n&gt;&gt;&gt; Disallow: /JS\n&gt;&gt;&gt; Disallow: /whitepapers/wp_redirect.cfm\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; heritrix crawl log portion\n&gt;&gt;&gt; ==========================\n&gt;&gt;&gt;\n&gt;&gt;&gt; 2006-03-08T16:11:41.295Z   200      17266 \n&gt;&gt;&gt; http://uaelp.pennnet.com/whitepapers/wp_redirect.cfm?id=305 LLL \n&gt;&gt;&gt; http://uaelp.pennnet.com/whitepapers/wp.cfm?id=305 text/html #075 \n&gt;&gt;&gt; 20060308161137362+1062 R5C73R3EPNNO4UYMWRPBTUJD3TWW32X7 2t\n&gt;&gt;\n&gt;&gt;&gt; According to our reading of the robots.txt spec, this URL should not \n&gt;&gt;&gt; have been crawled.  The only reason I can find for the failure may be \n&gt;&gt;&gt; that the last line is not terminated with a newline, but rather just an EOF.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Any thoughts?\n&gt;&gt;&gt;\n&gt;&gt;&gt; Karl\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n\n"}}