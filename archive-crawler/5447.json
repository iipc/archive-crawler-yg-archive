{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":358418055,"authorName":"liangjie.hong","from":"&quot;liangjie.hong&quot; &lt;liangjie.hong@...&gt;","profile":"liangjie.hong","replyTo":"LIST","senderId":"Klw3lozd73waHwsB50j8EOSoB2-u27gmFS6XtJjBXljpItnUYSboaIc4sqHGVR85V2TyfAAB3bnLxI4a4JNoFYkaUgg8iZIlNoUR6iUPxcY","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Crawling Stopped after the Seeds completed","postDate":"1220630877","msgId":5447,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PGc5cmxndCs0ZnZyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ4QkY0RTYxLjYwNjA4MDRAYXJjaGl2ZS5vcmc+"},"prevInTopic":5445,"nextInTopic":0,"prevInTime":5446,"nextInTime":5448,"topicId":5438,"numMessagesInTopic":3,"msgSnippet":"Great Thanks! ... the ... code ... understand. ... browser, and ... doesn t ... the ... HTTP Accept ... any ... smallest, ... add to ... the ... because it ","rawEmail":"Return-Path: &lt;liangjie.hong@...&gt;\r\nReceived: (qmail 17534 invoked from network); 5 Sep 2008 16:17:59 -0000\r\nReceived: from unknown (66.218.67.94)\n  by m46.grp.scd.yahoo.com with QMQP; 5 Sep 2008 16:17:59 -0000\r\nReceived: from unknown (HELO n37a.bullet.mail.sp1.yahoo.com) (66.163.168.131)\n  by mta15.grp.scd.yahoo.com with SMTP; 5 Sep 2008 16:17:59 -0000\r\nReceived: from [69.147.65.172] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 05 Sep 2008 16:17:59 -0000\r\nReceived: from [66.218.69.4] by t14.bullet.mail.sp1.yahoo.com with NNFMP; 05 Sep 2008 16:17:59 -0000\r\nReceived: from [66.218.67.201] by t4.bullet.scd.yahoo.com with NNFMP; 05 Sep 2008 16:17:59 -0000\r\nX-Sender: liangjie.hong@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 44598 invoked from network); 5 Sep 2008 16:07:58 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m49.grp.scd.yahoo.com with QMQP; 5 Sep 2008 16:07:58 -0000\r\nX-Received: from unknown (HELO n17.bullet.sp1.yahoo.com) (69.147.64.214)\n  by mta16.grp.scd.yahoo.com with SMTP; 5 Sep 2008 16:07:58 -0000\r\nX-Received: from [69.147.65.173] by n17.bullet.sp1.yahoo.com with NNFMP; 05 Sep 2008 16:07:58 -0000\r\nX-Received: from [209.73.164.83] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 05 Sep 2008 16:07:58 -0000\r\nX-Received: from [66.218.66.87] by t7.bullet.scd.yahoo.com with NNFMP; 05 Sep 2008 16:07:58 -0000\r\nDate: Fri, 05 Sep 2008 16:07:57 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g9rlgt+4fvr@...&gt;\r\nIn-Reply-To: &lt;48BF4E61.6060804@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;liangjie.hong&quot; &lt;liangjie.hong@...&gt;\r\nSubject: Re: Crawling Stopped after the Seeds completed\r\nX-Yahoo-Group-Post: member; u=358418055; y=bfwPDxHTH7KjlfvkZT5QhGsmZ40bRrx9G051RnP8CB3uS8W2_MukKA\r\nX-Yahoo-Profile: liangjie.hong\r\nX-eGroups-Approved-By: stearcorg &lt;steve@...&gt; via web; 05 Sep 2008 16:17:59 -0000\r\n\r\nGreat Thanks!\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@=\r\n...&gt; \nwrote:\n&gt;\n&gt; It&#39;s not an issue with your scoping/prefixes.\n&gt; \n&gt; I tried=\r\n crawling &lt;http://www.genealogy.ams.org/id.php?id=3D123&gt;, and \nthe \n&gt; resul=\r\nt line in the crawl.log was:\n&gt; \n&gt; 2008-09-04T00:16:11.218Z   400        312=\r\n \n&gt; http://www.genealogy.ams.org/id.php?id=3D123 - - text/html #004 \n&gt; 2008=\r\n0904001610953+265 sha1:UNXDSRUEFN4HMW5UFGQJQMM6VTBUYLY7 - 3t\n&gt; \n&gt; Note the =\r\n&#39;400&#39; status code: that&#39;s an HTTP failure code. The same \ncode \n&gt; was shown=\r\n for the fetch of /robots.txt, and examining the ARC file \n&gt; showed an erro=\r\nr from the server which included the message:\n&gt; \n&gt; &quot;&lt;h1&gt;Bad Request&lt;/h1&gt;\n&gt; =\r\n&lt;p&gt;Your browser sent a request that this server could not \nunderstand.&quot;\n&gt; \n=\r\n&gt; That indicates the server is for some reason rejecting the Heritrix \n&gt; re=\r\nquest. However, the same URL can easily be visited with a \nbrowser, and \n&gt; =\r\neven setting the browser User-Agent to the same as the crawler \ndoesn&#39;t \n&gt; =\r\ncause the same &#39;400&#39; error -- so they&#39;re not specifically blocking \nthe \n&gt; =\r\ncrawler.\n&gt; \n&gt; Via trial and error, I discovered this site requires an \nHTTP=\r\n &#39;Accept&#39; \n&gt; header on the request. The &#39;Accept&#39; header isn&#39;t required by H=\r\nTTP \n&gt; specifications, and when it&#39;s absent a site is supposed to assume \na=\r\nny \n&gt; return content-type is OK. See...\n&gt; \n&gt;    http://www.w3.org/Protocols=\r\n/rfc2616/rfc2616-sec14.html#sec14.1\n&gt; \n&gt; We leave it off crawler requests b=\r\necause we want to make the \nsmallest, \n&gt; most simple requests necessary.\n&gt; =\r\n\n&gt; However, you can add an &#39;Accept&#39; header (or other headers) via the \n&gt; cr=\r\nawler configuration. Use the &#39;accept-headers&#39; setting under the \n&gt; FetchHTT=\r\nP processor -- you can specify a list of full headers to \nadd to \n&gt; request=\r\ns (not just &#39;accept&#39; headers) here. (In Heritrix 2.0, go to \nthe \n&gt; &#39;detail=\r\ns&#39; for that setting to add to the list.)\n&gt; \n&gt; I added the header...\n&gt; \n&gt; &quot;A=\r\nccept: */*&quot;\n&gt; \n&gt; ...and then had no problems crawling the URL you gave. (An=\r\nd, \nbecause it \n&gt; could then discover the included links, it continued to c=\r\nrawl other \n&gt; pages on the same site.)\n&gt; \n&gt; If in fact there are other site=\r\ns that throw an error on requests \nwithout \n&gt; and &#39;Accept&#39; header, we&#39;ll ch=\r\nange the default in Heritrix to \ninclude \n&gt; this header. I&#39;ve made this iss=\r\nue to track the matter:\n&gt; \n&gt;    http://webteam.archive.org/jira/browse/HER-=\r\n1547\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; \n&gt; liangjie.hong wrote:\n&gt; &gt; Hi:\n&gt; &gt; \n&gt; &gt; I dow=\r\nnloaded and installed Heritrix 2 on my machine. I followed \nthe \n&gt; &gt; guide =\r\nfor Version 2 and only changed contact url and email.\n&gt; &gt; \n&gt; &gt; My goal is t=\r\no crawl urls like:http://www.genealogy.ams.org/id.php?\nid=3DXXX\n&gt; &gt; \n&gt; &gt; So=\r\n I setup the seed url as:http://www.genealogy.ams.org/id.php?\nid=3D123. \n&gt; =\r\n&gt; However, no matter I setup the prefix associates to \n&gt; &gt; &quot;http://(org,ams=\r\n,genealogy,www,)/&quot; or &quot;http://\n&gt; &gt; (org,ams,genealogy,www,)/id.php&quot; or I do=\r\nn&#39;t have the prefix, the \n&gt; &gt; program only crawl ONE file, which is the see=\r\nd url and then stops.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Shall I change what configuration? I am=\r\n a newbie to Heritrix.\n&gt; &gt; \n&gt; &gt; Thank you very much.\n&gt;\n\n\n\n"}}