{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"H031hUyrn7gW1XLNjxhlz9OsNEq6ywjAXCJJDxA7l1_i9QitzWRP3mBFODrNG0iC7PLOOKPe2KK0b25-VrDkTBPv1bA2V9EWvfmleJFjXw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [archive-crawler] Handling identical documents from different sites.","postDate":"1131633441","msgId":2354,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBNzg1NjNDQHNrYXJmdXIuYm9rLmxvY2FsPg==","inReplyToHeader":"PDQzNzI3MkFDLjcwMDA5MDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":2353,"nextInTopic":2358,"prevInTime":2353,"nextInTime":2355,"topicId":2342,"numMessagesInTopic":8,"msgSnippet":"The following is a very rough idea of how we could implement duplicate content detection. -DataStore- Think of it as a Java class for the sake of this example ","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 62709 invoked from network); 10 Nov 2005 14:41:50 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m13.grp.scd.yahoo.com with QMQP; 10 Nov 2005 14:41:50 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta4.grp.scd.yahoo.com with SMTP; 10 Nov 2005 14:41:50 -0000\r\nReceived: (qmail 4368 invoked by uid 100); 10 Nov 2005 14:34:42 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (kris@...@130.208.152.83)\n  by mail-dev.archive.org with SMTP; 10 Nov 2005 14:34:42 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Thu, 10 Nov 2005 14:37:21 -0000\r\nMessage-ID: &lt;0678DB1968EAC7409CC3D0AB7A11B84A78563C@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0000_01C5E604.4392A9B0&quot;\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nImportance: Normal\r\nIn-Reply-To: &lt;437272AC.7000908@...&gt;\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.2180\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-78.5 required=7.0 tests=AWL,HTML_20_30,\n\tHTML_FONTCOLOR_BLUE,HTML_MESSAGE,USER_IN_WHITELIST autolearn=no \n\tversion=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] Handling identical documents from different sites.\r\nX-Yahoo-Group-Post: member; u=163406187; y=_A4is2XPANsE65DqKMRu7kge6eQN3_h3tnL7rRrPr1k5ysXm\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n------=_NextPart_000_0000_01C5E604.4392A9B0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nThe following is a very rough idea of how we could implement duplicate\ncont=\r\nent detection.\n \n-DataStore-\nThink of it as a Java class for the sake of th=\r\nis example\n \nContains a Berkley database:\nKey: Content hash\nData: Object (l=\r\nets call it Data) that contains a series of Entries. Each\nEntry consists of=\r\n a URI and the last time the content hash in question was\ndiscovered at tha=\r\nt URI. One of the entries is marked as being the &#39;first\none.&#39; I.e. the loca=\r\ntion (and time) of initial discovery of the document that\nthe key hash desi=\r\ngnates.\n \nMaking the following method possible:\npublic Entry process(String=\r\n URI, String contentHash, Date timeOfDiscovery,\nboolean disableMirrorDetect=\r\nion)\n \nMethod does the following:\n1. Looks up the Data object for the conte=\r\nnt hash. If none exists it creates\na new one and returns null (a return val=\r\nue of null means no duplicate\ndetected).\n2. If Data object is found it is s=\r\ncanned for an entry from the current URI.\nIf it isn&#39;t found then it is adde=\r\nd (if mirror detection is disabled then the\nfailure to find an entry for th=\r\ne current URI should lead to the method\nreturning null after creating the e=\r\nntry). If an entry is found then its time\nof last encounter should be updat=\r\ned.\n3. The method returns the &#39;original&#39; entry (as designated by the Data\no=\r\nbject) unless mirror detection is disabled in which case it returns the\nent=\r\nry for the current URI.\n \n-\n\nThis could be simplified somewhat by not allow=\r\ning the disabling of mirror\ndetection, but since current ARC formats do not=\r\n allow for recording &#39;we got\nthis and it was the same as this other object&#39;=\r\n it would seem to make sense\nto include it for now. Note that having run th=\r\ne DB with mirror detection\ndisabled it is possible to switch it to mirror d=\r\netection and immediately\ntake advantage of the gathered data.\n \nIt does not=\r\n record which of the &#39;hits&#39; was actually stored but simply\nassumes that at =\r\nleast the initial hit was stored. A simplified version of\nthis would ONLY k=\r\neep the initial hit. While OK, this (in addition to letting\nus disable mirr=\r\nor detection) provides much richer information about each\ndocument. This co=\r\nuld possibly be used (between crawls) to identify fully\nmirrored sites etc.=\r\n by doing some data mining.\n \n\nThe DataStore would have to be invoked by a =\r\nprocessor soon after the Fetcher\nprocessor has run. It could be embedded in=\r\n Heritrix, but I believe that it\nmight be better if it is run seperately an=\r\nd is accessed via some form of web\nservice. This would (for instance) make =\r\nit possible to run multiple\ndatastores segmented in some way based on the U=\r\nRIs or other suitable\nfactors.\n \nBetween crawls it would also be possible t=\r\no run maintenance on the DataStore\nto improve it (condensing the Bdb logs) =\r\nor otherwise manipulate it (adding\ndata from existing web archives, deletin=\r\ng old entries etc.)\n \nI have (in another post) made the case that (in terms=\r\n of saving on storage)\nit is sufficient to apply this to non-text documents=\r\n. Doing so reduces the\nnumber of documents by approx. 70% in typical broad =\r\ncrawls (while still\ntackling 70% of the uncompressed data, factor in that t=\r\next/* documents\ncompress very well and you realize that by focusing on the =\r\n30% non-text\ndocuments we can address duplices in 80-90% of the stored, com=\r\npressed data).\n \nIf applied to text documents it could be argued that we wi=\r\nll also save on\nbandwidth since we wont extract links from those documents =\r\n(and lets face it\nnon-text documents - at the moment - don&#39;t really provide=\r\n any links even\nthough we know that SWF and PDFs do contain some links) and=\r\n thus wont try to\ndownload those linked documents. It could be argued in re=\r\nturn that just\nbecause document A has not changed, that does not mean that =\r\ndocument B\n(which is only linked to from A) has not changed. Thus we risk m=\r\nissing valid\ncontent just because a &#39;gateway&#39; page has not changed.\n \nPract=\r\nical example of this. Consider a blogging site (not a big one, just one\nwit=\r\nh a few hundred people). We have its root domain in our seeds and rely on\nf=\r\ninding the individual blogs via a list of blogs provided near the root.\nThi=\r\ns list does not change often ... you see where I&#39;m going. The\nAdaptiveRevis=\r\niting avoided this situation by always checking all the URIs it\nhad encount=\r\nered without worrying about the documents that led to them. That\nisn&#39;t an o=\r\nption here unless we want to start using multi-million entry seed\nlists for=\r\n each crawl.\n \n\nMy opinion: We need to get WARCs into use ASAP to be able t=\r\no make maximum\nuse of this.\n \n \n- Kris \n\n-----Original Message-----\nFrom: a=\r\nrchive-crawler@yahoogroups.com\n[mailto:archive-crawler@yahoogroups.com] On =\r\nBehalf Of stack\nSent: 9. n=F3vember 2005 22:06\nTo: archive-crawler@yahoogro=\r\nups.com\nSubject: Re: [archive-crawler] Handling identical documents from di=\r\nfferent\nsites.\n\n\ncallforshadab wrote:\n\n&gt; How should this be handled in the =\r\nclustered version (multiple machines\n&gt; running Heritrix)?\n&gt;\nWhat would you =\r\nsuggest?\n\nOne notion we&#39;ve been kicking around is having a fast and dumb se=\r\nrver \nsitting in the middle of a coordinated crawling cluster.  A processor=\r\n \nhosted by alll machines would ask it if the content (hash) had already \nb=\r\neen seen. Such a server might also be used to check if an URL is \nalready-s=\r\neen.  Though the crawl space is split between crawlers, we \nmight run a cen=\r\ntral server to guard against repeat fetches when queues \nare moved between =\r\nmachines, either because one machine is overloaded so \nwe narrow range its =\r\nresponsible for off-loading to adjacent crawlers or \nbecause a crawler is p=\r\nicking up for a crashed crawler (If I remember \ncorrectly, the ubicrawler f=\r\nolks in their paper allow that a few URLs may \nbe mutiply fetched during th=\r\ne coming and going of crawlers flux).\n\nSt.Ack\n\n\n  _____  \n\nYAHOO! GROUPS LI=\r\nNKS \n\n\n\t\n*\t Visit your group &quot;archive-crawler\n&lt;http://groups.yahoo.com/grou=\r\np/archive-crawler&gt; &quot; on the web.\n  \n\n*\t To unsubscribe from this group, sen=\r\nd an email to:\n archive-crawler-unsubscribe@yahoogroups.com\n&lt;mailto:archive=\r\n-crawler-unsubscribe@yahoogroups.com?subject=3DUnsubscribe&gt; \n  \n\n*\t Your us=\r\ne of Yahoo! Groups is subject to the Yahoo! Terms of Service\n&lt;http://docs.y=\r\nahoo.com/info/terms/&gt; . \n\n\n  _____  \n\n\n\r\n------=_NextPart_000_0000_01C5E604.4392A9B0\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n&lt;TITLE&gt;Message&lt;/TITLE&gt;\n&lt;META http-equiv=3DContent-Type content=3D&quot;text/html=\r\n; charset=3Diso-8859-1&quot;&gt;\n&lt;META content=3D&quot;MSHTML 6.00.2900.2769&quot; name=3DGEN=\r\nERATOR&gt;&lt;/HEAD&gt;\n&lt;BODY&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;The =\r\nfollowing is a very rough idea of \nhow we could implement duplicate content=\r\n detection.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;&lt;=\r\n/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial&gt;&lt;FONT color=3D#0000ff&gt;&lt;FONT size=\r\n=3D2&gt;-DataStore-&lt;BR&gt;&lt;SPAN \nclass=3D154142614-10112005&gt;Think of it as a Java=\r\n class for the sake of this \nexample&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV=\r\n&gt;&lt;FONT face=3DArial&gt;&lt;FONT color=3D#0000ff&gt;&lt;FONT size=3D2&gt;&lt;SPAN \nclass=3D154=\r\n142614-10112005&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3D=\r\nArial&gt;&lt;FONT color=3D#0000ff&gt;&lt;FONT size=3D2&gt;&lt;SPAN \nclass=3D154142614-1011200=\r\n5&gt;Contains a &lt;/SPAN&gt;Berkley database:&lt;BR&gt;Key: Content \nhash&lt;BR&gt;Data: Object=\r\n (lets call it Data) that contains a series of&nbsp;&lt;SPAN \nclass=3D15414261=\r\n4-10112005&gt;E&lt;/SPAN&gt;ntries. Each&nbsp;&lt;SPAN \nclass=3D154142614-10112005&gt;E&lt;/S=\r\nPAN&gt;ntry consists of a URI and the last time the \ncontent hash in question =\r\nwas discovered at that URI. One of the entries is \nmarked as being the &#39;fir=\r\nst one.&#39; I.e. the location (and time) of initial \ndiscovery of the document=\r\n that the key hash \ndesignates.&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=\r\n=3DArial color=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DAri=\r\nal color=3D#0000ff size=3D2&gt;Making the following method \npossible:&lt;/FONT&gt;&lt;/=\r\nDIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;public Entry process(=\r\nString URI, \nString contentHash, Date timeOfDiscovery, boolean \ndisableMirr=\r\norDetection)&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;=\r\n&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;Method=\r\n does the following:&lt;BR&gt;1. \nLooks up the Data object for the content hash. =\r\nIf none exists it creates a new \none and returns null (a return value of nu=\r\nll means no duplicate detected).&lt;BR&gt;2. \nIf Data object is found it is scann=\r\ned for an entry from the current URI. If it \nisn&#39;t found then it is added (=\r\nif mirror detection is disabled then the failure \nto find an entry for the =\r\ncurrent URI should lead to the method returning null \nafter creating the en=\r\ntry). If an entry is found then its time of last encounter \nshould be updat=\r\ned.&lt;BR&gt;3. The method returns the &#39;original&#39; entry (as designated \nby the Da=\r\nta object) unless mirror detection is disabled in which case it returns \nth=\r\ne entry for the current URI.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#=\r\n0000ff size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D154142614-10112005&gt;&lt;=\r\nFONT face=3DArial color=3D#0000ff \nsize=3D2&gt;-&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;BR&gt;=\r\n&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;This&nbsp;could be simplified \n=\r\nsomewhat by not allowing the disabling of mirror detection, but since curre=\r\nnt \nARC formats do not allow for recording &#39;we got this and it was the same=\r\n as this \nother object&#39; it would seem to make sense to include it for now. =\r\nNote that \nhaving run the DB with mirror detection disabled it is possible =\r\nto switch it to \nmirror detection and immediately take advantage of the gat=\r\nhered \ndata.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;=\r\n&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;It doe=\r\ns not record which of the \n&#39;hits&#39; was actually stored but simply assumes th=\r\nat at least the initial hit was \nstored. A simplified version of this would=\r\n ONLY keep the initial hit. While OK, \nthis (in addition to letting us disa=\r\nble mirror detection) provides much richer \ninformation about each document=\r\n. This could possibly be used (between crawls) to \nidentify fully mirrored =\r\nsites etc. by doing some data mining.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial =\r\ncolor=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;BR&gt;&lt;FONT face=3DArial co=\r\nlor=3D#0000ff size=3D2&gt;The DataStore would have to be \ninvoked by a process=\r\nor soon after the Fetcher processor has run. It could be \nembedded in Herit=\r\nrix, but I believe that it might be better if it is run \nseperately and is =\r\naccessed via some form of web service. This would (for \ninstance) make it p=\r\nossible to run multiple datastores segmented in some&lt;SPAN \nclass=3D15414261=\r\n4-10112005&gt; &lt;/SPAN&gt;way based on the URIs or other suitable \nfactors.&lt;/FONT&gt;=\r\n&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;=\r\n\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;Between crawls it would a=\r\nlso be \npossible to run maintenance on the DataStore to improve it (condens=\r\ning the Bdb \nlogs) or otherwise manipulate it (adding data from existing we=\r\nb archives, \ndeleting old entries etc.)&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DAria=\r\nl color=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial colo=\r\nr=3D#0000ff size=3D2&gt;I have (&lt;SPAN \nclass=3D154142614-10112005&gt;in another p=\r\nost&lt;/SPAN&gt;) made the case that (in terms of \nsaving on storage) it is suffi=\r\ncient to apply this to non-text documents. Doing \nso reduces the number of =\r\ndocuments by approx. 70% in typical broad crawls (while \nstill tackling 70%=\r\n of the uncompressed data, factor in that text/* documents \ncompress very w=\r\nell and you realize that by focusing on the 30% non-text \ndocuments we can =\r\naddress duplices in 80-90% of the stored, compressed \ndata).&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;=\r\nDIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;S=\r\nPAN class=3D154142614-10112005&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;=\r\nIf \napplied to text documents it could be argued that we will also save on =\r\nbandwidth \nsince we wont extract links from those documents (and lets face =\r\nit non-text \ndocuments - at the moment - don&#39;t really provide any links eve=\r\nn though we know \nthat SWF and PDFs do contain some links) and thus wont tr=\r\ny to download those \nlinked documents. It could be argued in return that ju=\r\nst because document A has \nnot changed, that does not mean that document B =\r\n(which is only linked to from A) \nhas not changed. Thus we risk missing val=\r\nid content just because a &#39;gateway&#39; \npage has not changed.&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/D=\r\nIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D154142614-10112005&gt;&lt;FONT face=3DArial color=3D#0000f=\r\nf \nsize=3D2&gt;&lt;/FONT&gt;&lt;/SPAN&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D154142614-1011200=\r\n5&gt;&lt;FONT face=3DArial color=3D#0000ff \nsize=3D2&gt;Practical example of this. C=\r\nonsider a blogging site (not a big one, just \none with a few hundred people=\r\n). We have its root domain in our seeds and rely on \nfinding the individual=\r\n blogs via a list of blogs provided near the root. This \nlist does not chan=\r\nge often ... you see where I&#39;m going. The AdaptiveRevisiting \navoided this =\r\nsituation by always checking all the URIs it had encountered \nwithout worry=\r\ning about the documents that led to them. That isn&#39;t an option here \nunless=\r\n we want to start using multi-million entry seed lists for each \ncrawl.&lt;/FO=\r\nNT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&n=\r\nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;BR&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;My opinion:=\r\n We need to get WARCs \ninto use ASAP to be able to make maximum use of this=\r\n.&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&nbs=\r\np;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;&lt;/FONT&gt;&nbsp;&lt;/DI=\r\nV&gt;\n&lt;DIV&gt;&lt;SPAN class=3D154142614-10112005&gt;&lt;FONT face=3DArial color=3D#0000ff=\r\n size=3D2&gt;- Kris \n&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/DIV&gt;\n&lt;BLOCKQUOTE \nstyle=3D&quot;PADDING-LEFT: =\r\n5px; MARGIN-LEFT: 5px; BORDER-LEFT: #0000ff 2px solid; MARGIN-RIGHT: 0px&quot;&gt;\n=\r\n  &lt;DIV&gt;&lt;/DIV&gt;\n  &lt;DIV class=3DOutlookMessageHeader lang=3Den-us dir=3Dltr al=\r\nign=3Dleft&gt;&lt;FONT \n  face=3DTahoma size=3D2&gt;-----Original Message-----&lt;BR&gt;&lt;B=\r\n&gt;From:&lt;/B&gt; \n  archive-crawler@yahoogroups.com [mailto:archive-crawler@yahoo=\r\ngroups.com] &lt;B&gt;On \n  Behalf Of &lt;/B&gt;stack&lt;BR&gt;&lt;B&gt;Sent:&lt;/B&gt; 9. n=F3vember 2005=\r\n 22:06&lt;BR&gt;&lt;B&gt;To:&lt;/B&gt; \n  archive-crawler@yahoogroups.com&lt;BR&gt;&lt;B&gt;Subject:&lt;/B&gt; =\r\nRe: [archive-crawler] \n  Handling identical documents from different \n  sit=\r\nes.&lt;BR&gt;&lt;BR&gt;&lt;/FONT&gt;&lt;/DIV&gt;&lt;TT&gt;callforshadab wrote:&lt;BR&gt;&lt;BR&gt;&gt; How should \n  =\r\nthis be handled in the clustered version (multiple machines&lt;BR&gt;&gt; running=\r\n \n  Heritrix)?&lt;BR&gt;&gt;&lt;BR&gt;What would you suggest?&lt;BR&gt;&lt;BR&gt;One notion we&#39;ve b=\r\neen \n  kicking around is having a fast and dumb server &lt;BR&gt;sitting in the m=\r\niddle of a \n  coordinated crawling cluster.&nbsp; A processor &lt;BR&gt;hosted by=\r\n alll machines \n  would ask it if the content (hash) had already &lt;BR&gt;been s=\r\neen. Such a server \n  might also be used to check if an URL is &lt;BR&gt;already-=\r\nseen.&nbsp; Though the \n  crawl space is split between crawlers, we &lt;BR&gt;mig=\r\nht run a central server to \n  guard against repeat fetches when queues &lt;BR&gt;=\r\nare moved between machines, \n  either because one machine is overloaded so =\r\n&lt;BR&gt;we narrow range its \n  responsible for off-loading to adjacent crawlers=\r\n or &lt;BR&gt;because a crawler is \n  picking up for a crashed crawler (If I reme=\r\nmber &lt;BR&gt;correctly, the ubicrawler \n  folks in their paper allow that a few=\r\n URLs may &lt;BR&gt;be mutiply fetched during \n  the coming and going of crawlers=\r\n flux).&lt;BR&gt;&lt;BR&gt;St.Ack&lt;BR&gt;&lt;/TT&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_0000_01C5E604.4392A9B0--\r\n\n"}}