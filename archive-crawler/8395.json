{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"lKY646uk5as87C9Oq0edGCkDh99pYzXM3uDW_UmY5NFzIzuvwEX155JXPeVtr2n8R0vwJaDhBuPE6jxlAjbyqI3KBAv-zFk","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] 50k pending URLs but crawler is almost idle","postDate":"1384389306","msgId":8395,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUyODQxQUJBLjQwMTAwMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGw1dmttYSttYjJjcTlAWWFob29Hcm91cHMuY29tPg==","referencesHeader":"PGw1dmttYSttYjJjcTlAWWFob29Hcm91cHMuY29tPg=="},"prevInTopic":8394,"nextInTopic":0,"prevInTime":8394,"nextInTime":8396,"topicId":8394,"numMessagesInTopic":2,"msgSnippet":"... When all the fast and small sites have finished, what s left are the large and slow sites...  so to some extent this could be normal. It s then a matter of","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 72616 invoked by uid 102); 14 Nov 2013 00:35:18 -0000\r\nX-Received: from unknown (HELO mtaq6.grp.bf1.yahoo.com) (10.193.84.37)\n  by m10.grp.bf1.yahoo.com with SMTP; 14 Nov 2013 00:35:18 -0000\r\nX-Received: (qmail 22496 invoked from network); 14 Nov 2013 00:35:18 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mtaq6.grp.bf1.yahoo.com with SMTP; 14 Nov 2013 00:35:18 -0000\r\nX-Received: (qmail 27380 invoked by uid 0); 14 Nov 2013 00:35:10 -0000\r\nX-Received: from 65.169.82.60 (HELO probook.local) (65.169.82.60)\n  by relay01.pair.com with SMTP; 14 Nov 2013 00:35:10 -0000\r\nX-pair-Authenticated: 65.169.82.60\r\nMessage-ID: &lt;52841ABA.4010006@...&gt;\r\nDate: Wed, 13 Nov 2013 16:35:06 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.1.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;l5vkma+mb2cq9@...&gt;\r\nIn-Reply-To: &lt;l5vkma+mb2cq9@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] 50k pending URLs but crawler is almost idle\r\nX-Yahoo-Group-Post: member; u=137285340; y=R9-yzYT0NxMqoCFQ4ithaVKYe40emTZHY0CBjK2mdMcl\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 11/13/13, 2:41 AM, james@... wrote:\n&gt; Hello again\n&gt;\n&gt; Our crawl job consist of about 50&#39;000 seeds with around 4 M urls. The\n&gt; crawl gets extremely slow at the end if only a couple of sites need to\n&gt; be crawled.\n&gt; I extracted the pending URLs (with this very helpful script\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Heritrix3+Useful+Scripts#Heritrix3UsefulScripts-listpendingurls&gt;)\n&gt; and found out that there are 12 domains left to be crawled (which are\n&gt; webshops with many, many sites) with around 50k pending urls. Although\n&gt; there still is a lot to do the crawl rate is almost zero (around 2 URLs\n&gt; per minute). I do not understand why the left over queues get always\n&gt; snoozedagain for many seconds after one page has been crawled. I use the\n&gt; standard &quot;politness&quot; settings for the disposition chain and don&#39;t see\n&gt; any obvious reason why the same server is not crawled faster. What&#39;s the\n&gt; reason for this slow crawling rate?\n\nWhen all the fast and small sites have finished, what&#39;s left are the \nlarge and slow sites...  so to some extent this could be normal. It&#39;s \nthen a matter of why those exact sites are slow, which requires a closer \nlook at the crawl.log & other logs/reports, per below.\n\n&gt; There are als o some URLs in queues where I cannot find out why they do\n&gt; not get crawled, but get stuck in the queue. The site is available and\n&gt; can be fetched form the same machine on which heritrix runs. What&#39;s the\n&gt; best way to find out why an URL hasn&#39;t been fetched and the queue gets\n&gt; snoozed?\n\nWhen the last URI tried from a queue finishes into the crawl.log \n(success or failure with a result code), the queue is snoozed a short \nduration, influenced by how long the last fetch took and robots.txt \ncrawl delay.\n\nIf instead the last URI tried from the top of the queue fails in a way \nthat might be a transient network/server issue - like host \nunreachable/unconnectable - the URi is returned to the top of the queue \nfor retrying, and a longer retryDelaySeconds snooze happens. Such \nconnection failures are logged in one of the other logs â€“ check for \nexample nonfatal-errors.log.\n\nLooking at the length of the snooze, the results and timings of most \nrecent URIs tried/failed in crawl.log/nonfatal-errors.log, or the \nspecific URIs atop the queue (or recently added) should give a good idea \nwhy progress is slow.\n\nIf in fact the exact URI atop the queue triggers a repeatable connection \nerror (while others on the queue would not), that problem URI will be \nretried, with long snoozes between, for the configured maxRetries before \nany other URIs on the same queue are tried. (There&#39;s no &quot;skipping \nahead&quot;.) That can be hours of slow-retrying without progress, depending \non your settings.\n\nThere was also a report a while ago of unexplained intermittent network \nerrors with certain Java/Heritrix/OS combinations -- look for the thread \nwith subject line &quot;Intermittent issue running Heritrix with Java 7&quot;. The \nsymptom was fetches failing in batches with connection errors, as if the \nnetwork was unavailable, when most other tests (including successful \nHTTP fetches outside Java/Heritrix) indicating no network problems. It \nseemed to me like a potential new JVM/Java socket bug, or perhaps \nstrange interaction with local network idiosyncracies, but I don&#39;t know \nany definitive resolution. That *might* be a factor as well if a close \nlook at the individual URIs/queues/hosts doesn&#39;t explain your \nslower-than-expected progress.\n\n- Gordon\n\n"}}