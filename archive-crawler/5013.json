{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":112974261,"authorName":"Mathew Nik Foscarini","from":"Mathew Nik Foscarini &lt;nfoscarini@...&gt;","profile":"nfoscarini","replyTo":"LIST","senderId":"4XuxHTg1RMJ2eE0_dmOZUdVppc2B3dmnozRQpseshE3wiSyQKPGSp7Kf-jUycVKN3augnRlky3ECvKqLZVgKj3dwBSWUywfsymVeAeqdelOHNg","spamInfo":{"isSpam":false,"reason":"2"},"subject":"Re: [archive-crawler] Scalability in Heritrix","postDate":"1204210950","msgId":5013,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDM4MTAzLjg4NjYwLnFtQHdlYjU0NjA2Lm1haWwucmUyLnlhaG9vLmNvbT4="},"prevInTopic":5012,"nextInTopic":0,"prevInTime":5012,"nextInTime":5014,"topicId":5012,"numMessagesInTopic":2,"msgSnippet":"Your crawl rules are accepting every URL found that contains xyz.com  ","rawEmail":"Return-Path: &lt;nfoscarini@...&gt;\r\nX-Sender: nfoscarini@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 88288 invoked from network); 28 Feb 2008 15:02:31 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m54.grp.scd.yahoo.com with QMQP; 28 Feb 2008 15:02:31 -0000\r\nX-Received: from unknown (HELO web54606.mail.re2.yahoo.com) (206.190.49.176)\n  by mta16.grp.scd.yahoo.com with SMTP; 28 Feb 2008 15:02:31 -0000\r\nX-Received: (qmail 88732 invoked by uid 60001); 28 Feb 2008 15:02:31 -0000\r\nX-YMail-OSG: wvMI1GoVM1krAkpjUyq1iwZRYQ8GtoAEI3XPYUflcQDHLkWAQip0cLEMXMPAB0O0D4YoxvBVDQ0OQHJAuoyCBgAm1MVAqkrNYnZ1BlhnvgN62twCBAkNp9nn2.lGN8I0yw_AS5d8zkU60mdHy8nyoJCDYQ--\r\nX-Received: from [38.99.150.66] by web54606.mail.re2.yahoo.com via HTTP; Thu, 28 Feb 2008 07:02:30 PST\r\nX-Mailer: YahooMailRC/818.31 YahooMailWebService/0.7.162\r\nDate: Thu, 28 Feb 2008 07:02:30 -0800 (PST)\r\nTo: archive-crawler@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-1627611880-1204210950=:88660&quot;\r\nMessage-ID: &lt;38103.88660.qm@...&gt;\r\nX-eGroups-Msg-Info: 2:2:2:0:2\r\nFrom: Mathew Nik Foscarini &lt;nfoscarini@...&gt;\r\nReply-To: Mathew Nik Foscarini &lt;nfoscarini@...&gt;\r\nSubject: Re: [archive-crawler] Scalability in Heritrix\r\nX-Yahoo-Group-Post: member; u=112974261; y=YREYuLHaYgu9dpYEdawCJh-H9hOg6S1BiP-EsHx3psF37iqW3Q\r\nX-Yahoo-Profile: nfoscarini\r\n\r\n\r\n--0-1627611880-1204210950=:88660\r\nContent-Type: text/plain; charset=us-ascii\r\n\r\nYour crawl rules are accepting every URL found that contains xyz.com\n\n\n        &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;newObject name=&quot;regexMatch&quot;\n\nclass=&quot;org.archive. crawler.decideru les.MatchesRegEx pDecideRule&quot; &gt;\n\n            &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;\n\n            &lt;string name=&quot;regexp&quot; &gt;.*xyz.com. *&lt;/string&gt;\n\n          &lt;/newObject&gt;\n\nIs there a reason you added this?\n\nAlso, this will accept any URL that contains xyz.com\n\nfor example;\n\nhttp://www.someotherdoamin.com/link?d=http://www.unlated_xyz.com\n\nThe above would be accepted.\n\nSo, what I think is happening is that your archiving every URL found in the domain xyz.com, and every URL found where xyz.com is found in the URL. It is not being limited to the /members branches or the site, or the others.\n\nTry using the default profile that comes with Heritrix, because it is setup to limit to the scope of the domain, but also cleans the URLs to reduce repeats.\n\n----- Original Message ----\nFrom: waisovsky &lt;miro786@...&gt;\nTo: archive-crawler@yahoogroups.com\nSent: Thursday, February 28, 2008 3:31:30 AM\nSubject: [archive-crawler] Scalability in Heritrix\n\n\n\n\n\n\n\n\n\n\n  \n\n\n    \n            Hi\n\n\n\ni was wondering if Heritrix is scalable. I suppose it surely is as\n\nit&#39;s used so widely. My problem was however that I ran a crawl on a\n\nsite but t took really long time-or showed at least &quot;150 days left&quot;\n\nand it was still growing. Then I thought it might be job configuration\n\nproblem.\n\n\n\nI am therefore going to briefly describe what i want to crawl:\n\nsite name: www.xyz.com\n\npages of interest: \n\n- www.xyz.com/ members.* (i.e. prefixed with www.xyz.com/ members)\n\n- www.xyz.com/ journal.*\n\n- www.xyz.com/ location. *\n\n\n\n-----seed file:------- --------- ------\n\nhttp://www.xyz. com/members\n\n+http://(com, xyz,www,) /journal\n\n+http://(com, xyz,www,) /memebers\n\n+http://(com, xyz,www,) /location\n\n------------ --------- --------- -------\n\n\n\nSo the idea was to crawl this site and whenever I find a url which\n\nmatch those mentioned, scrape the information contained in that page\n\n(by a writer processor).\n\n\n\nMy problem was however after 2 weeks, that there were no space left\n\nfrom 20 GB and it showed really long time to complete the crawl.\n\n\n\nWhat do you gys think, is that a problem of job/profile configuartion?\n\n\n\nbtw, here is the order.xml:\n\n\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF- 8&quot;?&gt;&lt;crawl- order\n\nxmlns:xsi=&quot;http://www.w3. org/2001/ XMLSchema- instance&quot;\n\nxsi:noNamespaceSche maLocation= &quot;heritrix_ settings. xsd&quot;&gt;\n\n  &lt;meta&gt;\n\n    &lt;name&gt;XYZ&lt;/name&gt;\n\n    &lt;description&gt; XYZ&lt;/description &gt;\n\n    &lt;operator&gt;Admin&lt; /operator&gt;\n\n    &lt;organization/ &gt;\n\n    &lt;audience/&gt;\n\n    &lt;date&gt;2008012910545 8&lt;/date&gt;\n\n  &lt;/meta&gt;\n\n  &lt;controller&gt;\n\n    &lt;string name=&quot;settings- directory&quot; &gt;settings&lt; /string&gt;\n\n    &lt;string name=&quot;disk-path&quot; /&gt;\n\n    &lt;string name=&quot;logs-path&quot; &gt;logs&lt;/string&gt;\n\n    &lt;string name=&quot;checkpoints- path&quot;&gt;checkpoint s&lt;/string&gt;\n\n    &lt;string name=&quot;state- path&quot;&gt;state&lt; /string&gt;\n\n    &lt;string name=&quot;scratch- path&quot;&gt;scratch&lt; /string&gt;\n\n    &lt;long name=&quot;max-bytes- download&quot; &gt;0&lt;/long&gt;\n\n    &lt;long name=&quot;max-document- download&quot; &gt;0&lt;/long&gt;\n\n    &lt;long name=&quot;max-time- sec&quot;&gt;0&lt;/long&gt;\n\n    &lt;integer name=&quot;max-toe- threads&quot;&gt; 100&lt;/integer&gt;\n\n    &lt;integer name=&quot;recorder- out-buffer- bytes&quot;&gt;4096&lt; /integer&gt;\n\n    &lt;integer name=&quot;recorder- in-buffer- bytes&quot;&gt;65536&lt; /integer&gt;\n\n    &lt;integer name=&quot;bdb-cache- percent&quot;&gt; 0&lt;/integer&gt;\n\n    &lt;newObject name=&quot;scope&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecidingScop e&quot;&gt;\n\n      &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n      &lt;string name=&quot;seedsfile&quot; &gt;seeds.txt&lt; /string&gt;\n\n      &lt;boolean name=&quot;reread- seeds-on- config&quot;&gt;true&lt; /boolean&gt;\n\n      &lt;newObject name=&quot;decide- rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n        &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;newObject name=&quot;regexMatch&quot;\n\nclass=&quot;org.archive. crawler.decideru les.MatchesRegEx pDecideRule&quot; &gt;\n\n            &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;\n\n            &lt;string name=&quot;regexp&quot; &gt;.*xyz.com. *&lt;/string&gt;\n\n          &lt;/newObject&gt;\n\n          &lt;newObject name=&quot;Surt&quot;\n\nclass=&quot;org.archive. crawler.decideru les.SurtPrefixed DecideRule&quot; &gt;\n\n            &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;\n\n            &lt;string name=&quot;surts- source-file&quot; /&gt;\n\n            &lt;boolean name=&quot;seeds- as-surt-prefixes &quot;&gt;true&lt;/boolean&gt;\n\n            &lt;string name=&quot;surts- dump-file&quot; /&gt;\n\n            &lt;boolean name=&quot;also-check- via&quot;&gt;false&lt; /boolean&gt;\n\n            &lt;boolean name=&quot;rebuild- on-reconfig&quot; &gt;true&lt;/boolean&gt;\n\n          &lt;/newObject&gt;\n\n        &lt;/map&gt;\n\n      &lt;/newObject&gt;\n\n    &lt;/newObject&gt;\n\n    &lt;map name=&quot;http-headers&quot; &gt;\n\n      &lt;string name=&quot;user-agent&quot; &gt;Mozilla/ 5.0 (compatible;\n\nheritrix/1.12. 1+http://www.www. com)&lt;/string&gt;\n\n      &lt;string name=&quot;from&quot;&gt;some@...&lt;/string&gt;\n\n    &lt;/map&gt;\n\n    &lt;newObject name=&quot;robots- honoring- policy&quot;\n\nclass=&quot;org.archive. crawler.datamode l.RobotsHonoring Policy&quot;&gt;\n\n      &lt;string name=&quot;type&quot;&gt; ignore&lt;/string&gt;\n\n      &lt;boolean name=&quot;masquerade&quot; &gt;false&lt;/boolean&gt;\n\n      &lt;text name=&quot;custom- robots&quot;/&gt;\n\n      &lt;stringList name=&quot;user-agents&quot; &gt;\n\n      &lt;/stringList&gt;\n\n    &lt;/newObject&gt;\n\n    &lt;newObject name=&quot;frontier&quot;\n\nclass=&quot;org.archive. crawler.frontier .BdbFrontier&quot; &gt;\n\n      &lt;float name=&quot;delay- factor&quot;&gt;4. 0&lt;/float&gt;\n\n      &lt;integer name=&quot;max-delay- ms&quot;&gt;20000&lt; /integer&gt;\n\n      &lt;integer name=&quot;min-delay- ms&quot;&gt;2000&lt; /integer&gt;\n\n      &lt;integer name=&quot;max-retries&quot; &gt;30&lt;/integer&gt;\n\n      &lt;long name=&quot;retry- delay-seconds&quot; &gt;900&lt;/long&gt;\n\n      &lt;integer name=&quot;preference- embed-hops&quot; &gt;1&lt;/integer&gt;\n\n      &lt;integer name=&quot;total- bandwidth- usage-KB- sec&quot;&gt;0&lt;/integer&gt;\n\n      &lt;integer name=&quot;max-per- host-bandwidth- usage-KB- sec&quot;&gt;0&lt;/integer&gt;\n\n      &lt;string\n\nname=&quot;queue- assignment- policy&quot;&gt;org. archive.crawler. frontier. HostnameQueueAss ignmentPolicy&lt; /string&gt;\n\n      &lt;string name=&quot;force- queue-assignment &quot;/&gt;\n\n      &lt;boolean name=&quot;pause- at-start&quot; &gt;false&lt;/boolean&gt;\n\n      &lt;boolean name=&quot;pause- at-finish&quot; &gt;false&lt;/boolean&gt;\n\n      &lt;boolean name=&quot;source- tag-seeds&quot; &gt;false&lt;/boolean&gt;\n\n      &lt;boolean name=&quot;recovery- log-enabled&quot; &gt;true&lt;/boolean&gt;\n\n      &lt;boolean name=&quot;hold-queues&quot; &gt;true&lt;/boolean&gt;\n\n      &lt;integer name=&quot;balance- replenish- amount&quot;&gt;3000&lt; /integer&gt;\n\n      &lt;integer name=&quot;error- penalty-amount&quot; &gt;100&lt;/integer&gt;\n\n      &lt;long name=&quot;queue- total-budget&quot; &gt;-1&lt;/long&gt;\n\n      &lt;string\n\nname=&quot;cost-policy&quot; &gt;org.archive. crawler.frontier .UnitCostAssignm entPolicy&lt; /string&gt;\n\n      &lt;long name=&quot;snooze- deactivate- ms&quot;&gt;300000&lt; /long&gt;\n\n      &lt;integer name=&quot;target- ready-backlog&quot; &gt;50&lt;/integer&gt;\n\n      &lt;string\n\nname=&quot;uri-included- structure&quot; &gt;org.archive. crawler.util. BdbUriUniqFilter &lt;/string&gt;\n\n    &lt;/newObject&gt;\n\n    &lt;map name=&quot;uri-canonical ization-rules&quot; &gt;\n\n      &lt;newObject name=&quot;lc&quot;\n\nclass=&quot;org.archive. crawler.url. canonicalize. LowercaseRule&quot; &gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n      &lt;/newObject&gt;\n\n      &lt;newObject name=&quot;WWW&quot;\n\nclass=&quot;org.archive. crawler.url. canonicalize. StripWWWRule&quot; &gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n      &lt;/newObject&gt;\n\n    &lt;/map&gt;\n\n    &lt;map name=&quot;pre-fetch- processors&quot; &gt;\n\n      &lt;newObject name=&quot;Preselector&quot;\n\nclass=&quot;org.archive. crawler.prefetch .Preselector&quot; &gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;Preselector# decide-rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n        &lt;boolean name=&quot;override- logger&quot;&gt;false&lt; /boolean&gt;\n\n        &lt;boolean name=&quot;recheck- scope&quot;&gt;false&lt; /boolean&gt;\n\n        &lt;boolean name=&quot;block- all&quot;&gt;false&lt; /boolean&gt;\n\n        &lt;string name=&quot;block- by-regexp&quot; /&gt;\n\n        &lt;string name=&quot;allow- by-regexp&quot; &gt;.*xyz.*&lt; /string&gt;\n\n      &lt;/newObject&gt;\n\n      &lt;newObject name=&quot;Preprocessor&quot;\n\nclass=&quot;org.archive. crawler.prefetch .PreconditionEnf orcer&quot;&gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;Preprocessor# decide-rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n        &lt;integer name=&quot;ip-validity- duration- seconds&quot;&gt; 21600&lt;/integer&gt;\n\n        &lt;integer name=&quot;robot- validity- duration- seconds&quot;&gt; 86400&lt;/integer&gt;\n\n        &lt;boolean name=&quot;calculate- robots-only&quot; &gt;false&lt;/boolean&gt;\n\n      &lt;/newObject&gt;\n\n    &lt;/map&gt;\n\n    &lt;map name=&quot;fetch- processors&quot; &gt;\n\n      &lt;newObject name=&quot;HTTP&quot;\n\nclass=&quot;org.archive. crawler.fetcher. FetchHTTP&quot; &gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;HTTP#decide- rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n            &lt;newObject name=&quot;regex&quot;\n\nclass=&quot;org.archive. crawler.decideru les.MatchesRegEx pDecideRule&quot; &gt;\n\n              &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;\n\n              &lt;string name=&quot;regexp&quot; &gt;.*xyz.*&lt; /string&gt;\n\n            &lt;/newObject&gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n        &lt;newObject name=&quot;midfetch- decide-rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n        &lt;integer name=&quot;timeout- seconds&quot;&gt; 1200&lt;/integer&gt;\n\n        &lt;integer name=&quot;sotimeout- ms&quot;&gt;20000&lt; /integer&gt;\n\n        &lt;integer name=&quot;fetch- bandwidth&quot; &gt;0&lt;/integer&gt;\n\n        &lt;long name=&quot;max-length- bytes&quot;&gt;0&lt; /long&gt;\n\n        &lt;boolean name=&quot;ignore- cookies&quot;&gt; false&lt;/boolean&gt;\n\n        &lt;boolean name=&quot;use-bdb- for-cookies&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;string name=&quot;load-cookies- from-file&quot; /&gt;\n\n        &lt;string name=&quot;save-cookies- to-file&quot;/ &gt;\n\n        &lt;string name=&quot;trust- level&quot;&gt;open&lt; /string&gt;\n\n        &lt;stringList name=&quot;accept- headers&quot;&gt;\n\n        &lt;/stringList&gt;\n\n        &lt;string name=&quot;http-proxy- host&quot;/&gt;\n\n        &lt;string name=&quot;http-proxy- port&quot;/&gt;\n\n        &lt;string name=&quot;default- encoding&quot; &gt;ISO-8859- 1&lt;/string&gt;\n\n        &lt;boolean name=&quot;digest- content&quot;&gt; true&lt;/boolean&gt;\n\n        &lt;string name=&quot;digest- algorithm&quot; &gt;sha1&lt;/string&gt;\n\n        &lt;boolean name=&quot;send-if- modified- since&quot;&gt;true&lt; /boolean&gt;\n\n        &lt;boolean name=&quot;send-if- none-match&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;boolean name=&quot;send-connecti on-close&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;boolean name=&quot;send-referer&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;boolean name=&quot;send-range&quot; &gt;false&lt;/boolean&gt;\n\n        &lt;string name=&quot;bind-address&quot; /&gt;\n\n      &lt;/newObject&gt;\n\n      &lt;newObject name=&quot;DNS&quot; class=&quot;org.archive. crawler.fetcher. FetchDNS&quot; &gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;DNS#decide- rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n        &lt;boolean name=&quot;accept- non-dns-resolves &quot;&gt;false&lt;/ boolean&gt;\n\n        &lt;boolean name=&quot;digest- content&quot;&gt; true&lt;/boolean&gt;\n\n        &lt;string name=&quot;digest- algorithm&quot; &gt;sha1&lt;/string&gt;\n\n      &lt;/newObject&gt;\n\n    &lt;/map&gt;\n\n    &lt;map name=&quot;extract- processors&quot; &gt;\n\n      &lt;newObject name=&quot;ExtractorHTML &quot;\n\nclass=&quot;org.archive. crawler.extracto r.ExtractorHTML&quot; &gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;ExtractorHTML #decide-rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n        &lt;boolean name=&quot;extract- javascript&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;boolean name=&quot;treat- frames-as- embed-links&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;boolean name=&quot;ignore- form-action- urls&quot;&gt;false&lt; /boolean&gt;\n\n        &lt;boolean name=&quot;overly- eager-link- detection&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;boolean name=&quot;ignore- unexpected- html&quot;&gt;true&lt; /boolean&gt;\n\n      &lt;/newObject&gt;\n\n    &lt;/map&gt;\n\n    &lt;map name=&quot;write- processors&quot; &gt;\n\n\t&lt;!- this is an ad-hoc writer for this crawl-&gt;\n\n      &lt;newObject name=&quot;Sql&quot; class=&quot;org.archive. crawler.writer. sqlWriter&quot; &gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;Sql#decide- rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n      &lt;/newObject&gt;\n\n    &lt;/map&gt;\n\n    &lt;map name=&quot;post-processo rs&quot;&gt;\n\n      &lt;newObject name=&quot;Updater&quot;\n\nclass=&quot;org.archive. crawler.postproc essor.CrawlState Updater&quot;&gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;Updater# decide-rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n      &lt;/newObject&gt;\n\n      &lt;newObject name=&quot;LinksScoper&quot;\n\nclass=&quot;org.archive. crawler.postproc essor.LinksScope r&quot;&gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;LinksScoper# decide-rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n        &lt;boolean name=&quot;override- logger&quot;&gt;false&lt; /boolean&gt;\n\n        &lt;boolean name=&quot;seed-redirect s-new-seed&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;integer name=&quot;preference- depth-hops&quot; &gt;-1&lt;/integer&gt;\n\n        &lt;newObject name=&quot;scope- rejected- url-rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n      &lt;/newObject&gt;\n\n      &lt;newObject name=&quot;FrontierSched uler&quot;\n\nclass=&quot;org.archive. crawler.postproc essor.FrontierSc heduler&quot;&gt;\n\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;\n\n        &lt;newObject name=&quot;FrontierSched uler#decide- rules&quot;\n\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;\n\n          &lt;map name=&quot;rules&quot; &gt;\n\n          &lt;/map&gt;\n\n        &lt;/newObject&gt;\n\n      &lt;/newObject&gt;\n\n    &lt;/map&gt;\n\n    &lt;map name=&quot;loggers&quot; &gt;\n\n    &lt;/map&gt;\n\n    &lt;string name=&quot;recover- path&quot;&gt;... &#92;logs&#92;recover. gz&lt;/string&gt;\n\n    &lt;boolean name=&quot;checkpoint- copy-bdbje- logs&quot;&gt;true&lt; /boolean&gt;\n\n    &lt;boolean name=&quot;recover- retain-failures&quot; &gt;false&lt;/boolean&gt;\n\n    &lt;newObject name=&quot;credential- store&quot;\n\nclass=&quot;org.archive. crawler.datamode l.CredentialStor e&quot;&gt;\n\n      &lt;map name=&quot;credentials&quot; &gt;\n\n      &lt;/map&gt;\n\n    &lt;/newObject&gt;\n\n  &lt;/controller&gt;\n\n&lt;/crawl-order&gt;\n\n\n\n\n\n    \n  \n\n    \n    \n\n\n\n\n&lt;!--\n\n#ygrp-mkp{\nborder:1px solid #d8d8d8;font-family:Arial;margin:14px 0px;padding:0px 14px;}\n#ygrp-mkp hr{\nborder:1px solid #d8d8d8;}\n#ygrp-mkp #hd{\ncolor:#628c2a;font-size:85%;font-weight:bold;line-height:122%;margin:10px 0px;}\n#ygrp-mkp #ads{\nmargin-bottom:10px;}\n#ygrp-mkp .ad{\npadding:0 0;}\n#ygrp-mkp .ad a{\ncolor:#0000ff;text-decoration:none;}\n--&gt;\n\n\n\n&lt;!--\n\n#ygrp-sponsor #ygrp-lc{\nfont-family:Arial;}\n#ygrp-sponsor #ygrp-lc #hd{\nmargin:10px 0px;font-weight:bold;font-size:78%;line-height:122%;}\n#ygrp-sponsor #ygrp-lc .ad{\nmargin-bottom:10px;padding:0 0;}\n--&gt;\n\n\n\n&lt;!--\n\n#ygrp-mlmsg {font-size:13px;font-family:arial, helvetica, clean, sans-serif;}\n#ygrp-mlmsg table {font-size:inherit;font:100%;}\n#ygrp-mlmsg select, input, textarea {font:99% arial, helvetica, clean, sans-serif;}\n#ygrp-mlmsg pre, code {font:115% monospace;}\n#ygrp-mlmsg * {line-height:1.22em;}\n#ygrp-text{\nfont-family:Georgia;\n}\n#ygrp-text p{\nmargin:0 0 1em 0;}\n#ygrp-tpmsgs{\nfont-family:Arial;\nclear:both;}\n#ygrp-vitnav{\npadding-top:10px;font-family:Verdana;font-size:77%;margin:0;}\n#ygrp-vitnav a{\npadding:0 1px;}\n#ygrp-actbar{\nclear:both;margin:25px 0;white-space:nowrap;color:#666;text-align:right;}\n#ygrp-actbar .left{\nfloat:left;white-space:nowrap;}\n.bld{font-weight:bold;}\n#ygrp-grft{\nfont-family:Verdana;font-size:77%;padding:15px 0;}\n#ygrp-ft{\nfont-family:verdana;font-size:77%;border-top:1px solid #666;\npadding:5px 0;\n}\n#ygrp-mlmsg #logo{\npadding-bottom:10px;}\n\n#ygrp-vital{\nbackground-color:#e0ecee;margin-bottom:20px;padding:2px 0 8px 8px;}\n#ygrp-vital #vithd{\nfont-size:77%;font-family:Verdana;font-weight:bold;color:#333;text-transform:uppercase;}\n#ygrp-vital ul{\npadding:0;margin:2px 0;}\n#ygrp-vital ul li{\nlist-style-type:none;clear:both;border:1px solid #e0ecee;\n}\n#ygrp-vital ul li .ct{\nfont-weight:bold;color:#ff7900;float:right;width:2em;text-align:right;padding-right:.5em;}\n#ygrp-vital ul li .cat{\nfont-weight:bold;}\n#ygrp-vital a{\ntext-decoration:none;}\n\n#ygrp-vital a:hover{\ntext-decoration:underline;}\n\n#ygrp-sponsor #hd{\ncolor:#999;font-size:77%;}\n#ygrp-sponsor #ov{\npadding:6px 13px;background-color:#e0ecee;margin-bottom:20px;}\n#ygrp-sponsor #ov ul{\npadding:0 0 0 8px;margin:0;}\n#ygrp-sponsor #ov li{\nlist-style-type:square;padding:6px 0;font-size:77%;}\n#ygrp-sponsor #ov li a{\ntext-decoration:none;font-size:130%;}\n#ygrp-sponsor #nc{\nbackground-color:#eee;margin-bottom:20px;padding:0 8px;}\n#ygrp-sponsor .ad{\npadding:8px 0;}\n#ygrp-sponsor .ad #hd1{\nfont-family:Arial;font-weight:bold;color:#628c2a;font-size:100%;line-height:122%;}\n#ygrp-sponsor .ad a{\ntext-decoration:none;}\n#ygrp-sponsor .ad a:hover{\ntext-decoration:underline;}\n#ygrp-sponsor .ad p{\nmargin:0;}\no{font-size:0;}\n.MsoNormal{\nmargin:0 0 0 0;}\n#ygrp-text tt{\nfont-size:120%;}\nblockquote{margin:0 0 0 4px;}\n.replbq{margin:4;}\n--&gt;\n\n\n\n\n\n\n\n\n      ____________________________________________________________________________________\nBe a better friend, newshound, and \nknow-it-all with Yahoo! Mobile.  Try it now.  http://mobile.yahoo.com/;_ylt=Ahu06i62sR8HDtDypao8Wcj9tAcJ \n\r\n--0-1627611880-1204210950=:88660\r\nContent-Type: text/html; charset=us-ascii\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;style type=&quot;text/css&quot;&gt;&lt;!-- DIV {margin:0px;} --&gt;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div style=&quot;font-family:times new roman, new york, times, serif;font-size:12pt&quot;&gt;&lt;div style=&quot;font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;Your crawl rules are accepting every URL found that contains xyz.com&lt;br&gt;&lt;br&gt;\n        &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;newObject name=&quot;regexMatch&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.MatchesRegEx pDecideRule&quot; &gt;&lt;br&gt;\n            &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;&lt;br&gt;\n            &lt;string name=&quot;regexp&quot; &gt;.*xyz.com. *&lt;/string&gt;&lt;br&gt;\n          &lt;/newObject&gt;&lt;br&gt;&lt;br&gt;Is there a reason you added this?&lt;br&gt;&lt;br&gt;Also, this will accept any URL that contains xyz.com&lt;br&gt;&lt;br&gt;for example;&lt;br&gt;&lt;br&gt;&lt;span&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://www.someotherdoamin.com/link?d=http://www.unlated_xyz.com&quot;&gt;http://www.someotherdoamin.com/link?d=http://www.unlated_xyz.com&lt;/a&gt;&lt;/span&gt;&lt;br&gt;&lt;br&gt;The above would be accepted.&lt;br&gt;&lt;br&gt;So, what I think is happening is that your archiving every URL found in the domain xyz.com, and every URL found where xyz.com is found in the URL. It is not being limited to the /members branches or the site, or the others.&lt;br&gt;&lt;br&gt;Try using the default profile that comes with Heritrix, because it is setup to limit to the scope of the domain, but also cleans the URLs to reduce repeats.&lt;br&gt;&lt;br&gt;&lt;div style=&quot;border-left: 2px solid rgb(16, 16, 255); margin: 5px 0px 5px 5px; padding-left: 5px; font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;----- Original Message\n ----&lt;br&gt;From: waisovsky &lt;miro786@...&gt;&lt;br&gt;To: archive-crawler@yahoogroups.com&lt;br&gt;Sent: Thursday, February 28, 2008 3:31:30 AM&lt;br&gt;Subject: [archive-crawler] Scalability in Heritrix&lt;br&gt;&lt;br&gt;\n\n\n\n\n\n\n\n\n    &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;Hi&lt;br&gt;\n&lt;br&gt;\ni was wondering if Heritrix is scalable. I suppose it surely is as&lt;br&gt;\nit&#39;s used so widely. My problem was however that I ran a crawl on a&lt;br&gt;\nsite but t took really long time-or showed at least &quot;150 days left&quot;&lt;br&gt;\nand it was still growing. Then I thought it might be job configuration&lt;br&gt;\nproblem.&lt;br&gt;\n&lt;br&gt;\nI am therefore going to briefly describe what i want to crawl:&lt;br&gt;\nsite name: &lt;a target=&quot;_blank&quot; href=&quot;http://www.xyz.com&quot;&gt;www.xyz.com&lt;/a&gt;&lt;br&gt;\npages of interest: &lt;br&gt;\n- &lt;a target=&quot;_blank&quot; href=&quot;http://www.xyz.com/&quot;&gt;www.xyz.com/&lt;/a&gt; members.* (i.e. prefixed with &lt;a target=&quot;_blank&quot; href=&quot;http://www.xyz.com/&quot;&gt;www.xyz.com/&lt;/a&gt; members)&lt;br&gt;\n- &lt;a target=&quot;_blank&quot; href=&quot;http://www.xyz.com/&quot;&gt;www.xyz.com/&lt;/a&gt; journal.*&lt;br&gt;\n- &lt;a target=&quot;_blank&quot; href=&quot;http://www.xyz.com/&quot;&gt;www.xyz.com/&lt;/a&gt; location. *&lt;br&gt;\n&lt;br&gt;\n-----seed file:------- --------- ------&lt;br&gt;\n&lt;a rel=&quot;nofollow&quot; target=&quot;_blank&quot; href=&quot;http://www.xyz.com/members&quot;&gt;http://www.xyz. com/members&lt;/a&gt;&lt;br&gt;&lt;span&gt;\n+&lt;a target=&quot;_blank&quot; href=&quot;http://%28com&quot;&gt;http://(com&lt;/a&gt;, xyz,www,) /journal&lt;/span&gt;&lt;br&gt;&lt;span&gt;\n+&lt;a target=&quot;_blank&quot; href=&quot;http://%28com&quot;&gt;http://(com&lt;/a&gt;, xyz,www,) /memebers&lt;/span&gt;&lt;br&gt;&lt;span&gt;\n+&lt;a target=&quot;_blank&quot; href=&quot;http://%28com&quot;&gt;http://(com&lt;/a&gt;, xyz,www,) /location&lt;/span&gt;&lt;br&gt;\n------------ --------- --------- -------&lt;br&gt;\n&lt;br&gt;\nSo the idea was to crawl this site and whenever I find a url which&lt;br&gt;\nmatch those mentioned, scrape the information contained in that page&lt;br&gt;\n(by a writer processor).&lt;br&gt;\n&lt;br&gt;\nMy problem was however after 2 weeks, that there were no space left&lt;br&gt;\nfrom 20 GB and it showed really long time to complete the crawl.&lt;br&gt;\n&lt;br&gt;\nWhat do you gys think, is that a problem of job/profile configuartion?&lt;br&gt;\n&lt;br&gt;\nbtw, here is the order.xml:&lt;br&gt;\n&lt;br&gt;\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF- 8&quot;?&gt;&lt;crawl- order&lt;br&gt;\nxmlns:xsi=&quot;&lt;a rel=&quot;nofollow&quot; target=&quot;_blank&quot; href=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;http://www.w3. org/2001/ XMLSchema- instance&lt;/a&gt;&quot;&lt;br&gt;\nxsi:noNamespaceSche maLocation= &quot;heritrix_ settings. xsd&quot;&gt;&lt;br&gt;\n  &lt;meta&gt;&lt;br&gt;\n    &lt;name&gt;XYZ&lt;/name&gt;&lt;br&gt;\n    &lt;description&gt; XYZ&lt;/description &gt;&lt;br&gt;\n    &lt;operator&gt;Admin&lt; /operator&gt;&lt;br&gt;\n    &lt;organization/ &gt;&lt;br&gt;\n    &lt;audience/&gt;&lt;br&gt;\n    &lt;date&gt;2008012910545 8&lt;/date&gt;&lt;br&gt;\n  &lt;/meta&gt;&lt;br&gt;\n  &lt;controller&gt;&lt;br&gt;\n    &lt;string name=&quot;settings- directory&quot; &gt;settings&lt; /string&gt;&lt;br&gt;\n    &lt;string name=&quot;disk-path&quot; /&gt;&lt;br&gt;\n    &lt;string name=&quot;logs-path&quot; &gt;logs&lt;/string&gt;&lt;br&gt;\n    &lt;string name=&quot;checkpoints- path&quot;&gt;checkpoint s&lt;/string&gt;&lt;br&gt;\n    &lt;string name=&quot;state- path&quot;&gt;state&lt; /string&gt;&lt;br&gt;\n    &lt;string name=&quot;scratch- path&quot;&gt;scratch&lt; /string&gt;&lt;br&gt;\n    &lt;long name=&quot;max-bytes- download&quot; &gt;0&lt;/long&gt;&lt;br&gt;\n    &lt;long name=&quot;max-document- download&quot; &gt;0&lt;/long&gt;&lt;br&gt;\n    &lt;long name=&quot;max-time- sec&quot;&gt;0&lt;/long&gt;&lt;br&gt;\n    &lt;integer name=&quot;max-toe- threads&quot;&gt; 100&lt;/integer&gt;&lt;br&gt;\n    &lt;integer name=&quot;recorder- out-buffer- bytes&quot;&gt;4096&lt; /integer&gt;&lt;br&gt;\n    &lt;integer name=&quot;recorder- in-buffer- bytes&quot;&gt;65536&lt; /integer&gt;&lt;br&gt;\n    &lt;integer name=&quot;bdb-cache- percent&quot;&gt; 0&lt;/integer&gt;&lt;br&gt;\n    &lt;newObject name=&quot;scope&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecidingScop e&quot;&gt;&lt;br&gt;\n      &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n      &lt;string name=&quot;seedsfile&quot; &gt;seeds.txt&lt; /string&gt;&lt;br&gt;\n      &lt;boolean name=&quot;reread- seeds-on- config&quot;&gt;true&lt; /boolean&gt;&lt;br&gt;\n      &lt;newObject name=&quot;decide- rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n        &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;newObject name=&quot;regexMatch&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.MatchesRegEx pDecideRule&quot; &gt;&lt;br&gt;\n            &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;&lt;br&gt;\n            &lt;string name=&quot;regexp&quot; &gt;.*xyz.com. *&lt;/string&gt;&lt;br&gt;\n          &lt;/newObject&gt;&lt;br&gt;\n          &lt;newObject name=&quot;Surt&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.SurtPrefixed DecideRule&quot; &gt;&lt;br&gt;\n            &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;&lt;br&gt;\n            &lt;string name=&quot;surts- source-file&quot; /&gt;&lt;br&gt;\n            &lt;boolean name=&quot;seeds- as-surt-prefixes &quot;&gt;true&lt;/boolean&gt;&lt;br&gt;\n            &lt;string name=&quot;surts- dump-file&quot; /&gt;&lt;br&gt;\n            &lt;boolean name=&quot;also-check- via&quot;&gt;false&lt; /boolean&gt;&lt;br&gt;\n            &lt;boolean name=&quot;rebuild- on-reconfig&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n          &lt;/newObject&gt;&lt;br&gt;\n        &lt;/map&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n    &lt;/newObject&gt;&lt;br&gt;\n    &lt;map name=&quot;http-headers&quot; &gt;&lt;br&gt;\n      &lt;string name=&quot;user-agent&quot; &gt;Mozilla/ 5.0 (compatible;&lt;br&gt;\nheritrix/1.12. 1+&lt;a rel=&quot;nofollow&quot; target=&quot;_blank&quot; href=&quot;http://www.www.com&quot;&gt;http://www.www. com&lt;/a&gt;)&lt;/string&gt;&lt;br&gt;\n      &lt;string name=&quot;from&quot;&gt;&lt;a rel=&quot;nofollow&quot; ymailto=&quot;mailto:some%40email.com&quot; target=&quot;_blank&quot; href=&quot;mailto:some%40email.com&quot;&gt;some@...&lt;/a&gt;&lt;/string&gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;newObject name=&quot;robots- honoring- policy&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.datamode l.RobotsHonoring Policy&quot;&gt;&lt;br&gt;\n      &lt;string name=&quot;type&quot;&gt; ignore&lt;/string&gt;&lt;br&gt;\n      &lt;boolean name=&quot;masquerade&quot; &gt;false&lt;/boolean&gt;&lt;br&gt;\n      &lt;text name=&quot;custom- robots&quot;/&gt;&lt;br&gt;\n      &lt;stringList name=&quot;user-agents&quot; &gt;&lt;br&gt;\n      &lt;/stringList&gt;&lt;br&gt;\n    &lt;/newObject&gt;&lt;br&gt;\n    &lt;newObject name=&quot;frontier&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.frontier .BdbFrontier&quot; &gt;&lt;br&gt;\n      &lt;float name=&quot;delay- factor&quot;&gt;4. 0&lt;/float&gt;&lt;br&gt;\n      &lt;integer name=&quot;max-delay- ms&quot;&gt;20000&lt; /integer&gt;&lt;br&gt;\n      &lt;integer name=&quot;min-delay- ms&quot;&gt;2000&lt; /integer&gt;&lt;br&gt;\n      &lt;integer name=&quot;max-retries&quot; &gt;30&lt;/integer&gt;&lt;br&gt;\n      &lt;long name=&quot;retry- delay-seconds&quot; &gt;900&lt;/long&gt;&lt;br&gt;\n      &lt;integer name=&quot;preference- embed-hops&quot; &gt;1&lt;/integer&gt;&lt;br&gt;\n      &lt;integer name=&quot;total- bandwidth- usage-KB- sec&quot;&gt;0&lt;/integer&gt;&lt;br&gt;\n      &lt;integer name=&quot;max-per- host-bandwidth- usage-KB- sec&quot;&gt;0&lt;/integer&gt;&lt;br&gt;\n      &lt;string&lt;br&gt;\nname=&quot;queue- assignment- policy&quot;&gt;org. archive.crawler. frontier. HostnameQueueAss ignmentPolicy&lt; /string&gt;&lt;br&gt;\n      &lt;string name=&quot;force- queue-assignment &quot;/&gt;&lt;br&gt;\n      &lt;boolean name=&quot;pause- at-start&quot; &gt;false&lt;/boolean&gt;&lt;br&gt;\n      &lt;boolean name=&quot;pause- at-finish&quot; &gt;false&lt;/boolean&gt;&lt;br&gt;\n      &lt;boolean name=&quot;source- tag-seeds&quot; &gt;false&lt;/boolean&gt;&lt;br&gt;\n      &lt;boolean name=&quot;recovery- log-enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n      &lt;boolean name=&quot;hold-queues&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n      &lt;integer name=&quot;balance- replenish- amount&quot;&gt;3000&lt; /integer&gt;&lt;br&gt;\n      &lt;integer name=&quot;error- penalty-amount&quot; &gt;100&lt;/integer&gt;&lt;br&gt;\n      &lt;long name=&quot;queue- total-budget&quot; &gt;-1&lt;/long&gt;&lt;br&gt;\n      &lt;string&lt;br&gt;\nname=&quot;cost-policy&quot; &gt;org.archive. crawler.frontier .UnitCostAssignm entPolicy&lt; /string&gt;&lt;br&gt;\n      &lt;long name=&quot;snooze- deactivate- ms&quot;&gt;300000&lt; /long&gt;&lt;br&gt;\n      &lt;integer name=&quot;target- ready-backlog&quot; &gt;50&lt;/integer&gt;&lt;br&gt;\n      &lt;string&lt;br&gt;\nname=&quot;uri-included- structure&quot; &gt;org.archive. crawler.util. BdbUriUniqFilter &lt;/string&gt;&lt;br&gt;\n    &lt;/newObject&gt;&lt;br&gt;\n    &lt;map name=&quot;uri-canonical ization-rules&quot; &gt;&lt;br&gt;\n      &lt;newObject name=&quot;lc&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.url. canonicalize. LowercaseRule&quot; &gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n      &lt;newObject name=&quot;WWW&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.url. canonicalize. StripWWWRule&quot; &gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;map name=&quot;pre-fetch- processors&quot; &gt;&lt;br&gt;\n      &lt;newObject name=&quot;Preselector&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.prefetch .Preselector&quot; &gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;Preselector# decide-rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n        &lt;boolean name=&quot;override- logger&quot;&gt;false&lt; /boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;recheck- scope&quot;&gt;false&lt; /boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;block- all&quot;&gt;false&lt; /boolean&gt;&lt;br&gt;\n        &lt;string name=&quot;block- by-regexp&quot; /&gt;&lt;br&gt;\n        &lt;string name=&quot;allow- by-regexp&quot; &gt;.*xyz.*&lt; /string&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n      &lt;newObject name=&quot;Preprocessor&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.prefetch .PreconditionEnf orcer&quot;&gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;Preprocessor# decide-rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n        &lt;integer name=&quot;ip-validity- duration- seconds&quot;&gt; 21600&lt;/integer&gt;&lt;br&gt;\n        &lt;integer name=&quot;robot- validity- duration- seconds&quot;&gt; 86400&lt;/integer&gt;&lt;br&gt;\n        &lt;boolean name=&quot;calculate- robots-only&quot; &gt;false&lt;/boolean&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;map name=&quot;fetch- processors&quot; &gt;&lt;br&gt;\n      &lt;newObject name=&quot;HTTP&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.fetcher. FetchHTTP&quot; &gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;HTTP#decide- rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n            &lt;newObject name=&quot;regex&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.MatchesRegEx pDecideRule&quot; &gt;&lt;br&gt;\n              &lt;string name=&quot;decision&quot; &gt;ACCEPT&lt;/ string&gt;&lt;br&gt;\n              &lt;string name=&quot;regexp&quot; &gt;.*xyz.*&lt; /string&gt;&lt;br&gt;\n            &lt;/newObject&gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n        &lt;newObject name=&quot;midfetch- decide-rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n        &lt;integer name=&quot;timeout- seconds&quot;&gt; 1200&lt;/integer&gt;&lt;br&gt;\n        &lt;integer name=&quot;sotimeout- ms&quot;&gt;20000&lt; /integer&gt;&lt;br&gt;\n        &lt;integer name=&quot;fetch- bandwidth&quot; &gt;0&lt;/integer&gt;&lt;br&gt;\n        &lt;long name=&quot;max-length- bytes&quot;&gt;0&lt; /long&gt;&lt;br&gt;\n        &lt;boolean name=&quot;ignore- cookies&quot;&gt; false&lt;/boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;use-bdb- for-cookies&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;string name=&quot;load-cookies- from-file&quot; /&gt;&lt;br&gt;\n        &lt;string name=&quot;save-cookies- to-file&quot;/ &gt;&lt;br&gt;\n        &lt;string name=&quot;trust- level&quot;&gt;open&lt; /string&gt;&lt;br&gt;\n        &lt;stringList name=&quot;accept- headers&quot;&gt;&lt;br&gt;\n        &lt;/stringList&gt;&lt;br&gt;\n        &lt;string name=&quot;http-proxy- host&quot;/&gt;&lt;br&gt;\n        &lt;string name=&quot;http-proxy- port&quot;/&gt;&lt;br&gt;\n        &lt;string name=&quot;default- encoding&quot; &gt;ISO-8859- 1&lt;/string&gt;&lt;br&gt;\n        &lt;boolean name=&quot;digest- content&quot;&gt; true&lt;/boolean&gt;&lt;br&gt;\n        &lt;string name=&quot;digest- algorithm&quot; &gt;sha1&lt;/string&gt;&lt;br&gt;\n        &lt;boolean name=&quot;send-if- modified- since&quot;&gt;true&lt; /boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;send-if- none-match&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;send-connecti on-close&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;send-referer&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;send-range&quot; &gt;false&lt;/boolean&gt;&lt;br&gt;\n        &lt;string name=&quot;bind-address&quot; /&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n      &lt;newObject name=&quot;DNS&quot; class=&quot;org.archive. crawler.fetcher. FetchDNS&quot; &gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;DNS#decide- rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n        &lt;boolean name=&quot;accept- non-dns-resolves &quot;&gt;false&lt;/ boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;digest- content&quot;&gt; true&lt;/boolean&gt;&lt;br&gt;\n        &lt;string name=&quot;digest- algorithm&quot; &gt;sha1&lt;/string&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;map name=&quot;extract- processors&quot; &gt;&lt;br&gt;\n      &lt;newObject name=&quot;ExtractorHTML &quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.extracto r.ExtractorHTML&quot; &gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;ExtractorHTML #decide-rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n        &lt;boolean name=&quot;extract- javascript&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;treat- frames-as- embed-links&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;ignore- form-action- urls&quot;&gt;false&lt; /boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;overly- eager-link- detection&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;ignore- unexpected- html&quot;&gt;true&lt; /boolean&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;map name=&quot;write- processors&quot; &gt;&lt;br&gt;\n\t&lt;!- this is an ad-hoc writer for this crawl-&gt;&lt;br&gt;\n      &lt;newObject name=&quot;Sql&quot; class=&quot;org.archive. crawler.writer. sqlWriter&quot; &gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;Sql#decide- rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;map name=&quot;post-processo rs&quot;&gt;&lt;br&gt;\n      &lt;newObject name=&quot;Updater&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.postproc essor.CrawlState Updater&quot;&gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;Updater# decide-rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n      &lt;newObject name=&quot;LinksScoper&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.postproc essor.LinksScope r&quot;&gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;LinksScoper# decide-rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n        &lt;boolean name=&quot;override- logger&quot;&gt;false&lt; /boolean&gt;&lt;br&gt;\n        &lt;boolean name=&quot;seed-redirect s-new-seed&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;integer name=&quot;preference- depth-hops&quot; &gt;-1&lt;/integer&gt;&lt;br&gt;\n        &lt;newObject name=&quot;scope- rejected- url-rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n      &lt;newObject name=&quot;FrontierSched uler&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.postproc essor.FrontierSc heduler&quot;&gt;&lt;br&gt;\n        &lt;boolean name=&quot;enabled&quot; &gt;true&lt;/boolean&gt;&lt;br&gt;\n        &lt;newObject name=&quot;FrontierSched uler#decide- rules&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.decideru les.DecideRuleSe quence&quot;&gt;&lt;br&gt;\n          &lt;map name=&quot;rules&quot; &gt;&lt;br&gt;\n          &lt;/map&gt;&lt;br&gt;\n        &lt;/newObject&gt;&lt;br&gt;\n      &lt;/newObject&gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;map name=&quot;loggers&quot; &gt;&lt;br&gt;\n    &lt;/map&gt;&lt;br&gt;\n    &lt;string name=&quot;recover- path&quot;&gt;... &#92;logs&#92;recover. gz&lt;/string&gt;&lt;br&gt;\n    &lt;boolean name=&quot;checkpoint- copy-bdbje- logs&quot;&gt;true&lt; /boolean&gt;&lt;br&gt;\n    &lt;boolean name=&quot;recover- retain-failures&quot; &gt;false&lt;/boolean&gt;&lt;br&gt;\n    &lt;newObject name=&quot;credential- store&quot;&lt;br&gt;\nclass=&quot;org.archive. crawler.datamode l.CredentialStor e&quot;&gt;&lt;br&gt;\n      &lt;map name=&quot;credentials&quot; &gt;&lt;br&gt;\n      &lt;/map&gt;&lt;br&gt;\n    &lt;/newObject&gt;&lt;br&gt;\n  &lt;/controller&gt;&lt;br&gt;\n&lt;/crawl-order&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n    &lt;/div&gt;  \n\n\n\n&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;\n\n      &lt;hr size=1&gt;Never miss a thing.  &lt;a href=&quot;http://us.rd.yahoo.com/evt=51438/*http://www.yahoo.com/r/hs&quot;&gt; Make Yahoo your homepage.&lt;/a&gt;\n\n&lt;/body&gt;&lt;/html&gt;\r\n--0-1627611880-1204210950=:88660--\r\n\n"}}