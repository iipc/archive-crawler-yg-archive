{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":4480673,"authorName":"Dave Skinner","from":"Dave Skinner &lt;dave@...&gt;","profile":"frodobay","replyTo":"LIST","senderId":"bmEo6l-2QgREsiEyVeyBQlxFjH2cjKeNFhjywf9Q9FreZJhIlnIKWYO2_-eUp8aDMGqPboW0ZiMozRQ7RLCpt39wAvw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] How to crawl just country X (or language  X specific","postDate":"1106601293","msgId":1420,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMS4yLjAuMC4yMDA1MDEyNDEyNTkxNS4wM2I5OGFiMEAyMDYuMTYzLjEwMy40Pg==","inReplyToHeader":"PGN0M2xvOStoN2c1QGVHcm91cHMuY29tPg==","referencesHeader":"PGN0M2xvOStoN2c1QGVHcm91cHMuY29tPg=="},"prevInTopic":1415,"nextInTopic":1429,"prevInTime":1419,"nextInTime":1421,"topicId":1415,"numMessagesInTopic":3,"msgSnippet":"I ve got the tools to do a lot of that..... I d need more special filters to do all of it.  Right now I m pretty tired but after I wake back up I ll think","rawEmail":"Return-Path: &lt;dave@...&gt;\r\nX-Sender: dave@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 90478 invoked from network); 24 Jan 2005 21:15:12 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m13.grp.scd.yahoo.com with QMQP; 24 Jan 2005 21:15:12 -0000\r\nReceived: from unknown (HELO solid.net) (206.163.103.1)\n  by mta5.grp.scd.yahoo.com with SMTP; 24 Jan 2005 21:15:11 -0000\r\nReceived: from t_laptop.solid.net (or-65-40-223-71.dyn.sprint-hsd.net [65.40.223.71])\n\tby solid.net (8.12.8/8.12.8) with ESMTP id j0OLF4Zs018869\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 24 Jan 2005 13:15:07 -0800\r\nMessage-Id: &lt;6.1.2.0.0.20050124125915.03b98ab0@206.163.103.4&gt;\r\nX-Sender: dave@206.163.103.4 (Unverified)\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.1.2.0\r\nDate: Mon, 24 Jan 2005 13:14:53 -0800\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;ct3lo9+h7g5@...&gt;\r\nReferences: &lt;ct3lo9+h7g5@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\nX-eGroups-Remote-IP: 206.163.103.1\r\nFrom: Dave Skinner &lt;dave@...&gt;\r\nSubject: Re: [archive-crawler] How to crawl just country X (or language\n  X specific\r\nX-Yahoo-Group-Post: member; u=4480673\r\nX-Yahoo-Profile: frodobay\r\n\r\nI&#39;ve got the tools to do a lot of that.....\n\nI&#39;d need more special filters to do all of it.  Right now I&#39;m pretty tired \nbut after I wake back up I&#39;ll think about it\n\nI can do a lot of step 2 and 3 on the fly and eliminate the handwork.  What \nI basically do is broad crawls with a lot of special filters to keep the \ncrawl from expanding too much outside the criteria I&#39;m looking at.\n\n\n\nAt 12:27 PM 1/24/2005, you wrote:\n\n\n&gt;Hello,\n&gt;\n&gt;We have a need to crawl and index only those sites (or documents)\n&gt;where:\n&gt;a) or the content is published in language X (spoken in country X)\n&gt;b) the content is published in (any) other language, but the\n&gt;publisher/owner of the URL is from the country X.\n&gt;\n&gt;In other words: we want to collect data specific to a country and the\n&gt;language spoken in that country.\n&gt;\n&gt;- some ideas on how to achieve this:\n&gt;1) Collect and use a list of known addresses as the initial seed (that\n&gt;meet the country/language criteria)\n&gt;2) Crawl the list defined in 1) and extract links.\n&gt;3) For each unique link extracted in 2) - check whether\n&gt;the site or URL meets the country/language criteria, by:\n&gt;  - language detection\n&gt;  - whois database check (language is different, but publisher belongs\n&gt;to wanted country)\n&gt;  - comparison to list of known urls/organization\n&gt;  - what else?\n&gt;\n&gt;Possibly repeat 1,2,3) to gather (the as much as possible) complete\n&gt;URL space that belongs to the country/language area.\n&gt;\n&gt;Naturally, digging for the URL space should be done in a way that\n&gt;wastes as little bandwidth as possible (ie. the crawler should stop\n&gt;digging a path as soon as it finds a page not matching the criteria).\n&gt;\n&gt;How, in practise - should such functionality be\n&gt;embedded to heritirix.  Has someone done this before? Where,\n&gt;architecture-wise would such mechanism best fit? Can it even be\n&gt;done or would there be some more suitable tool for doing this.\n&gt;\n&gt;-- Jussi\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n"}}