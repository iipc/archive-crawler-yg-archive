{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264138474,"authorName":"Kenji Nagahashi","from":"Kenji Nagahashi &lt;knagahashi@...&gt;","profile":"kenznag","replyTo":"LIST","senderId":"asREaeb9UwNDlaIc83fTc3iBYPRRV3wMJxKf4O_0r-LH2f3uUDlVT4r34AkHyUWThZ0SVTJLnkbsL1IkYwxVL9rpsruQ5ihW7H1WyOc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: questions before we restart the crawl","postDate":"1327708118","msgId":7582,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMjMzN0Q2LjMwMzA1QGdtYWlsLmNvbT4=","inReplyToHeader":"PGpmdWcxbCs4ZzF2QGVHcm91cHMuY29tPg==","referencesHeader":"PGpmdWcxbCs4ZzF2QGVHcm91cHMuY29tPg=="},"prevInTopic":7578,"nextInTopic":7585,"prevInTime":7581,"nextInTime":7583,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"David, It is fairly common H3 goes 2x~3x faster at the beginning, where URI/s figure includes lots of DNS queries, many queues are ready, state database is","rawEmail":"Return-Path: &lt;knagahashi@...&gt;\r\nX-Sender: knagahashi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 7601 invoked from network); 27 Jan 2012 23:48:42 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m13.grp.sp2.yahoo.com with QMQP; 27 Jan 2012 23:48:42 -0000\r\nX-Received: from unknown (HELO mail-pz0-f48.google.com) (209.85.210.48)\n  by mta3.grp.sp2.yahoo.com with SMTP; 27 Jan 2012 23:48:42 -0000\r\nX-Received: by dakj40 with SMTP id j40so2071751dak.21\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 27 Jan 2012 15:48:41 -0800 (PST)\r\nX-Received: by 10.68.74.170 with SMTP id u10mr17998389pbv.99.1327708121920;\n        Fri, 27 Jan 2012 15:48:41 -0800 (PST)\r\nReturn-Path: &lt;knagahashi@...&gt;\r\nX-Received: from kenji-mbp.local (router300.sf.archive.org. [208.70.27.190])\n        by mx.google.com with ESMTPS id a5sm23717495pbh.15.2012.01.27.15.48.39\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Fri, 27 Jan 2012 15:48:40 -0800 (PST)\r\nMessage-ID: &lt;4F2337D6.30305@...&gt;\r\nDate: Fri, 27 Jan 2012 15:48:38 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;jfug1l+8g1v@...&gt;\r\nIn-Reply-To: &lt;jfug1l+8g1v@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nSubject: Re: [archive-crawler] Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=264138474; y=yzgCeJMV-ee14d-C1Rv4PcureSLTr67voUm1ajkJkM-vZA\r\nX-Yahoo-Profile: kenznag\r\n\r\nDavid,\n\nIt is fairly common H3 goes 2x~3x faster at the beginning, where URI/s \nfigure includes lots of DNS queries, many queues are ready, state \ndatabase is smaller, etc. I often see my crawlers running at &gt;200URI/s, \ntoo. (Also URI/s shown on the web UI is not really reliable.)\n\nIt is difficult to tell how many queues are enough to keep all threads \nbusy, as there are many factors affecting crawl speed. Assuming 0.5 sec \nprocessing time per URI and 100% concurrency, 1200 thread could do 2400 \nURI/s. On the other hand, assuming constant crawl delay of 3sec, 4000 \nactive queues can only emit 1333 URI/s. In this case, active queue size \nbecomes the limiting factor. If processing time per URI becomes 1 sec, \nthen processing time becomes a bottleneck. Some queues get snoozed much \nlonger than 3 sec and it makes URI emit rate lower. I tried drawing a \nactive-queue to crawl-speed graph, but what I can say is that there is a \nstrong correlation between them (roughly, 5000 queues -&gt; 40URI/s, 8000 \nqueue -&gt; 70URI/s ; variance is really big).\n\nLooking at your thread and frontier report, I noticed:\n\n- 994 queues are &quot;ready&quot;\n- 911 threads are in warcWriter\n\nthat is, you have enough active queues to keep threads busy, but threads \nare spending too much time on writing WARCs to process URIs on time. \nWarcWriter is the bottleneck in this case, probably because of small \nnumber of concurrent writers? What is your warcWriter.poolMaxActive?\n\n--Kenji\n\n(1/27/12 7:32 AM), david_pane1 wrote:\n&gt;\n&gt;\n&gt; Kenji,\n&gt;\n&gt; You understood correctly 25M pages/day for 5 machines. Right now, with\n&gt; three instances running, we are seeing around 17M pages per day. This\n&gt; makes me think that we may be saturating our network throughput.\n&gt;\n&gt; But, we have seen, at the beginning of the crawl, 250-300 URIs/second\n&gt; (an average of 52M pages/day over the first 3 days of the crawl. Once we\n&gt; get past the first few days, the crawl slows. During mid crawl, we found\n&gt; that stopping the crawl and restarting it improves the throughput but\n&gt; never back to 50M pages/day.\n&gt;\n&gt; How many queues do you need active per thread? Wouldn&#39;t Heritrix\n&gt; activate more queues if there are enough threads to handle them? We\n&gt; certainly have a lot of queues.\n&gt;\n&gt; As an example, on one instance we are seeing a rate of 93.25 URIs/sec\n&gt; average.\n&gt;\n&gt; Load:\n&gt; 1176 active of 1176 threads; 3,456.5 congestion ration 13688365 deepest\n&gt; queue; 62 average depth\n&gt;\n&gt; Threads:\n&gt; 1176 threads: 1176 ABOUT_TO_BEGIN_PROCESSOR; 911 warcWriter, 262\n&gt; fetchHttp, 2 candidates, 1 extractorHtml\n&gt;\n&gt; Frontier:\n&gt;\n&gt; RUN 12955728 URI queues: 4302 active (1200 in-process; 994 ready; 2108\n&gt; snoozed); 11429799 inactive; 0 ineligible; 0 retired; 1521627 exhausted.\n&gt;\n&gt; --David\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt; &lt;knagahashi@...&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; David,\n&gt;  &gt;\n&gt;  &gt; I understood &quot;25M + page/day&quot; was total for 5 machines, as you wrote\n&gt;  &gt; &quot;(these numbers are totals of all 5 instances combined)&quot;. Did you mean\n&gt;  &gt; 25M+ page/day/instance? If so, my &quot;100 threads&quot; comment is pointless.\n&gt;  &gt; Please disregard it.\n&gt;  &gt;\n&gt;  &gt; I&#39;m running broad crawl that captures everything linked: images, script,\n&gt;  &gt; CSS, PDF, Excel, ... even mpeg4 videos. Our Heritrix 3 runs on 8GB\n&gt;  &gt; memory + 4 core virtual machine (KVM), with 100 threads. it goes\n&gt;  &gt; ~60URI/s on average (per instance). Probably we could go as high as 150\n&gt;  &gt; threads to get higher crawl speed, but it comes with higher risk of\n&gt;  &gt; dying of OutOfMemoryError, empirically. Crawl speed is also limited by\n&gt;  &gt; lower disk I/O performance of VMs. 100 seems to be a good number for us.\n&gt;  &gt;\n&gt;  &gt; Yes, increasing threads brings significant increase in crawl speed, to\n&gt;  &gt; certain extent. If you don&#39;t have enough &quot;active queues,&quot; threads are\n&gt;  &gt; just wasted. There are other bottleneck, too, and it can change over\n&gt; time.\n&gt;  &gt;\n&gt;  &gt; At least we&#39;re getting sustained 60URI/s level of speed with 100\n&gt;  &gt; threads. With 1,200 threads and enough active queues, you should be\n&gt;  &gt; getting crawl speed much much higher than that (I&#39;ve never been able to\n&gt;  &gt; run my crawler with 1200 threads, though!)\n&gt;  &gt;\n&gt;  &gt; --Kenji\n&gt;  &gt;\n&gt;  &gt; (1/26/12 10:31 AM), david_pane1 wrote:\n&gt;  &gt; &gt; Kenji,\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Are you saying that you can get 25M pages per day on 100 threads and 1\n&gt;  &gt; &gt; instance or 25M URIs/day? Do you capture all images, pdfs, and\n&gt;  &gt; &gt; supporting page documents or are you just capturing html pages?\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; My test crawls before running our large crawl showed significant\n&gt;  &gt; &gt; increase in the number of pages captured when we increased the\n&gt; number of\n&gt;  &gt; &gt; threads.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; --David\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt;  &gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt;  &gt; &gt; &lt;knagahashi@&gt; wrote:\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Hi,\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; May be a bit off-topic, but 25M/day with 5 machine is average\n&gt; 58/s per\n&gt;  &gt; &gt; &gt; machine. Since I know Heritrix-3 can crawl at this speed with\n&gt; just 100\n&gt;  &gt; &gt; &gt; ToeThreads, I wonder if most of your 1200 ToeThreads are idle.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; While why you don&#39;t get much higher speed with 1200 threads is a big\n&gt;  &gt; &gt; &gt; question, it may make sense to cut down the number of ToeThreads if\n&gt;  &gt; &gt; &gt; you&#39;re okay with current crawl speed. Less threads will make H3 less\n&gt;  &gt; &gt; &gt; susceptible to memory problems... Just a thought.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; --Kenji\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; (1/20/12 9:54 PM), David Pane wrote:\n&gt;  &gt; &gt; &gt; &gt; Gordon,\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; Thank you for your response. And I am sorry for the\n&gt; overwhelming amount\n&gt;  &gt; &gt; &gt; &gt; of information...I think I am a little overwhelmed.... and\n&gt; feeling the\n&gt;  &gt; &gt; &gt; &gt; pressure.\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; 1) Our Bloom filter configuration:\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;bean id=&quot;uriUniqFilter&quot;\n&gt;  &gt; &gt; &gt; &gt; class=&quot;org.archive.crawler.util.BloomUriUniqFilter&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;bloomFilter&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;bean class=&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;constructor-arg value=&quot;400000000&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;constructor-arg value=&quot;30&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/property&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; 2) We are writing the crawl data to a NAS configured with RAID 6.\n&gt;  &gt; &gt; We did\n&gt;  &gt; &gt; &gt; &gt; see some problems with disk errors on the NAS earlier in the crawl\n&gt;  &gt; &gt; (late\n&gt;  &gt; &gt; &gt; &gt; Dec ). I recently found this out. We were/are running in a degraded\n&gt;  &gt; &gt; &gt; &gt; raid state - a few of the disks have been replaced and the RAID is\n&gt;  &gt; &gt; being\n&gt;  &gt; &gt; &gt; &gt; rebuilt. We didn&#39;t see any block device errors in the logs on\n&gt; the NAS\n&gt;  &gt; &gt; &gt; &gt; so the write failures we saw are probably not related to the\n&gt;  &gt; &gt; rebuild. We\n&gt;  &gt; &gt; &gt; &gt; did see some network hiccups (no outright failures) in the logs.\n&gt;  &gt; &gt; &gt; &gt; So, this may be the culprit for some of the\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; 3) Yes, we have been cross-feeding URIs.\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; --David\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; On 1/21/12 12:22 AM, Gordon Mohr wrote:\n&gt;  &gt; &gt; &gt; &gt; &gt; You&#39;ve provided an overwhelming amount of information and we\n&gt; may be\n&gt;  &gt; &gt; &gt; &gt; &gt; dealing with multiple issues, some of which have roots going back\n&gt;  &gt; &gt; &gt; &gt; &gt; earlier than the diagnostic data we now have available.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; A few key points of emphasis:\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; - we&#39;ve not run crawls with 1200 threads before, or on hardware\n&gt;  &gt; &gt; &gt; &gt; &gt; similar to yours, so our experience is only vaguely suggestive\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; - it&#39;s not the lower thread counts that are the real source of\n&gt;  &gt; &gt; &gt; &gt; &gt; concern; you can even adjust the number of threads mid-crawl.\n&gt; It&#39;s\n&gt;  &gt; &gt; &gt; &gt; &gt; that the error that killed the threads almost certainly left\n&gt; a queue\n&gt;  &gt; &gt; &gt; &gt; &gt; in a &#39;phantom&#39; state where no progress would be made crawling its\n&gt;  &gt; &gt; &gt; &gt; &gt; URIs, each time it happened, on each resume leading to the\n&gt;  &gt; &gt; current state.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; - without having understood and fixed whatever software or system\n&gt;  &gt; &gt; &gt; &gt; &gt; problems caused the earliest/most-foundational errors in your\n&gt; crawl,\n&gt;  &gt; &gt; &gt; &gt; &gt; it&#39;s impossible to say how likely they are to recur.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; With that in mind, I&#39;ll try to provide quick answers to your\n&gt; other\n&gt;  &gt; &gt; &gt; &gt; &gt; questions...\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; On 1/20/12 4:20 PM, David Pane wrote:\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; We have collected about 550 million pages along with the\n&gt; images and\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; supporting documents on our 5 instance crawl that was started\n&gt;  &gt; &gt; Dec. 23rd.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; Although we are please with the amount of data we captured to\n&gt;  &gt; &gt; date, we\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; are very concerned about the state of the Heritrix instances. If\n&gt;  &gt; &gt; fact,\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; we aren&#39;t very confident that the instances will last until the\n&gt;  &gt; &gt; end of\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; February. We are now running on a total of over 500 less threads\n&gt;  &gt; &gt; than\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; the configured 1200 threads/instance.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 0 - not running right now.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 1 - running on 1198 ( 2 less)\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 2 - running on 931 (269 less)\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 3 - running on 987 (213 less)\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 4 - running on 1170 (30 less)\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; Since we are seriously considering throwing away this past\n&gt;  &gt; &gt; month&#39;s work\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; and starting over, we would like to pick your brain on some\n&gt;  &gt; &gt; strategies\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; that will help us avoid getting into this situation again.\n&gt; We were\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; hoping to be done crawling by the end of February so this\n&gt;  &gt; &gt; restart will\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; put us behind schedule.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 1) Can we continue from here but with &quot;clean&quot; Heritrix\n&gt; instances?\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; Is there a way that we can continue from the this point\n&gt; forward, but\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; start with Heritrix instances that will not be corrupt due\n&gt; to sever\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; error? (e.g. using the\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;&gt; ) If\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; so, would you recommend doing this? You mentioned that this\n&gt; could be\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; time consuming. Each of our instances has downloaded around 170M\n&gt;  &gt; &gt; URIs,\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; they have over 700M queued URIs, what is your time estimate for\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; something this large?\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; We are willing to sacrifice a few days to get our crawler to\n&gt; a clean\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; state again so we can crawl for another 30 days at the pace we\n&gt;  &gt; &gt; have been\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; crawling.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; You can do a big &#39;frontier-recover&#39; log replay to avoid\n&gt;  &gt; &gt; recrawling the\n&gt;  &gt; &gt; &gt; &gt; &gt; same URIs, and approximate the earlier queue state.\n&gt; Splitting/filters\n&gt;  &gt; &gt; &gt; &gt; &gt; the logs manually beforehand as alluded to in the wiki page\n&gt; can speed\n&gt;  &gt; &gt; &gt; &gt; &gt; this process somewhat... but given the size of all your\n&gt; log-segments\n&gt;  &gt; &gt; &gt; &gt; &gt; that log grooming beforehand is itself likely to be a lengthy\n&gt;  &gt; &gt; process.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; I don&#39;t think we&#39;ve ever done it with logs of 170M crawled / 870M\n&gt;  &gt; &gt; &gt; &gt; &gt; discovered before, nor on any hardware comparable to yours.\n&gt; So it&#39;s\n&gt;  &gt; &gt; &gt; &gt; &gt; impossible to project its duration in your environment. It&#39;s\n&gt;  &gt; &gt; taken 2-3\n&gt;  &gt; &gt; &gt; &gt; &gt; days for us on smaller crawls, slower hardware.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; An added complication is that this older frontier-recover-log\n&gt; replay\n&gt;  &gt; &gt; &gt; &gt; &gt; technique happens in its own thread separate from the\n&gt; checkpointing\n&gt;  &gt; &gt; &gt; &gt; &gt; process, so it is not, itself, accurately checkpointed during the\n&gt;  &gt; &gt; long\n&gt;  &gt; &gt; &gt; &gt; &gt; reload process.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; At nearly 1B discovered URIs per node, even if you are using the\n&gt;  &gt; &gt; &gt; &gt; &gt; alternate BloomUriUniqFilter, if you are using it at its\n&gt; default size\n&gt;  &gt; &gt; &gt; &gt; &gt; (~500MB) it will now be heavily saturated and thus returning many\n&gt;  &gt; &gt; &gt; &gt; &gt; false-positives causing truly unique URIs to be rejected as\n&gt;  &gt; &gt; &gt; &gt; &gt; duplicates. (If you&#39;re using a significantly larger filter,\n&gt; you may\n&gt;  &gt; &gt; &gt; &gt; &gt; not yet be at a high false-positive rate: you&#39;d have to do\n&gt; the bloom\n&gt;  &gt; &gt; &gt; &gt; &gt; filter math. If you&#39;re still using BdbUriUniqFilter, you&#39;re\n&gt; way way\n&gt;  &gt; &gt; &gt; &gt; &gt; past the point where its disk seeks have usually made it too\n&gt; slow for\n&gt;  &gt; &gt; &gt; &gt; &gt; our purposes.)\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 2) What can be done to avoid corrupting the Heritrix instances?\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - What kind of strategies might we take to keep the crawl error\n&gt;  &gt; &gt; free?\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - Do you think the SEVER errors that we have seen are\n&gt;  &gt; &gt; deterministic or\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; random (e.g., triggered by occasional flaky network conditions,\n&gt;  &gt; &gt; disks,\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; race conditions, or whatever)?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Hard to say. The main thing I could suggest is watch very\n&gt; closely and\n&gt;  &gt; &gt; &gt; &gt; &gt; when a SEVERE error occurs, prioritize diagnosing and\n&gt; resolving the\n&gt;  &gt; &gt; &gt; &gt; &gt; cause while the info is fresh.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - Do you believe that we can reliably backup to the previous\n&gt;  &gt; &gt; checkpoint\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; if we watch the logs and stop as soon as we see the first SEVER\n&gt;  &gt; &gt; error?\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; If we do this, do you speculate that the same SEVER will occur\n&gt;  &gt; &gt; again?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Resuming from the latest checkpoint before an error believed to\n&gt;  &gt; &gt; &gt; &gt; &gt; corrupt the on-disk state will be the best strategy.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; If we never figure out the real cause, but run the same\n&gt; software on\n&gt;  &gt; &gt; &gt; &gt; &gt; the same machine, yes, I expect the same problem will recur!\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - Is there any reason why a Heritrix instance that is run while\n&gt;  &gt; &gt; binded\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; to one ip address can&#39;t be resumed binded to a different ip\n&gt; address?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Only the web UI to my knowledge binds to a chosen address,\n&gt; and it is\n&gt;  &gt; &gt; &gt; &gt; &gt; common to have it bind to all. I don&#39;t expect the outbound\n&gt; requests\n&gt;  &gt; &gt; &gt; &gt; &gt; would be hurt by a machine changing its IP address while the\n&gt;  &gt; &gt; crawl was\n&gt;  &gt; &gt; &gt; &gt; &gt; running, but I would run a test to be sure if that was an\n&gt; important,\n&gt;  &gt; &gt; &gt; &gt; &gt; expected transition.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 3) Should we configure the crawler with more instances and\n&gt; switch\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; between them?\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; We have seen that we can run a single instance to 100M pages +\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; supporting images and documents. Perhaps this means that we need\n&gt;  &gt; &gt; 10 or\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; more instances instead of 5. That raises the possibility of\n&gt;  &gt; &gt; running 2\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; instances per machine. If we could run 2, or even 4,\n&gt; instances on a\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; single machine, they would each run half as long.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; I don&#39;t think the problems as reported are specifically due\n&gt; to one\n&gt;  &gt; &gt; &gt; &gt; &gt; node&#39;s progress growing beyond a certain size, but it might\n&gt; be the\n&gt;  &gt; &gt; &gt; &gt; &gt; case that giant instances are more likely to suffer from, and\n&gt; harder\n&gt;  &gt; &gt; &gt; &gt; &gt; to recover from, single glitches (eg a single disk error). On the\n&gt;  &gt; &gt; &gt; &gt; &gt; other hand, many instances introduce more redundant overhead\n&gt; costs\n&gt;  &gt; &gt; &gt; &gt; &gt; (certain data structures, cross-feeding URIs if you&#39;re doing\n&gt;  &gt; &gt; that, etc.).\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - Can you suggest a way to start/stop instances from a script so\n&gt;  &gt; &gt; we can\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; change between instances automatically?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Not a mode I&#39;ve thought much about.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - Have you seen frequent starting / stopping of instances\n&gt; introduce\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; instability?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; No... but it might make you notice latent issues sooner.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 4) Crawl slows but restarting seems to improve the speed again.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; We noticed that the all of our instances would initially run at\n&gt;  &gt; &gt; a fast\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; pace. We would collect an average of 25M + pages/day for 2-3\n&gt;  &gt; &gt; days and\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; then the crawl would slow down to 10M pages/day over the next\n&gt;  &gt; &gt; few days.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; (these numbers are totals of all 5 instances combined). When we\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; restarted the instances, the average pages would improve back to\n&gt;  &gt; &gt; 25M +\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; pages/day. The total crawled numbers (TiB) also reflected the\n&gt;  &gt; &gt; slow down.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - Is this something that others have experienced as well?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; I don&#39;t recall hearing other reports of speed boosts after\n&gt;  &gt; &gt; &gt; &gt; &gt; checkpoint-resumes but others may have more experience.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 5) We are capturing tweets from twitter, harvesting the urls and\n&gt;  &gt; &gt; want to\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; crawl those urls within 1 day of receiving the tweet. Can you\n&gt;  &gt; &gt; recommend\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; a strategy for doing this with the 5 instances we are running?\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; - Do we need to run a separate crawler dedicated to this? If so,\n&gt;  &gt; &gt; can you\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; suggest a way to crawl out from the tweeted urls but when we get\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; additional urls from the tweets, quickly change focus to\n&gt; these urls\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; instead of the ones branching out. When adding urls as seeds,\n&gt;  &gt; &gt; can you\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; set a high priority to crawl those before the discovered urls?\n&gt;  &gt; &gt; Do you\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; recommend maybe setting up a specific crawl for these urls and\n&gt;  &gt; &gt; then only\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; crawl a few hopes from the seeds - injecting the urls from the\n&gt;  &gt; &gt; tweets as\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; seeds?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Dedicating a special script or crawler to URIs that come from\n&gt; such a\n&gt;  &gt; &gt; &gt; &gt; &gt; constrained source (Twitter feeds), or that need to be\n&gt; crawled in a\n&gt;  &gt; &gt; &gt; &gt; &gt; special timeframe, or according to other special limits\n&gt; (fewer hops),\n&gt;  &gt; &gt; &gt; &gt; &gt; could make sense.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; It would take some customization of the queueing-policy or\n&gt;  &gt; &gt; &gt; &gt; &gt; &#39;precedence&#39; features of Heritrix to allow URIs added\n&gt; mid-crawl to be\n&gt;  &gt; &gt; &gt; &gt; &gt; prioritized above those already discovered and queued. The most\n&gt;  &gt; &gt; simple\n&gt;  &gt; &gt; &gt; &gt; &gt; possible customization might be a UriPrecedencePolicy that\n&gt; takes all\n&gt;  &gt; &gt; &gt; &gt; &gt; zero-hop URIs (which all seeds and most direct-fed URIs would\n&gt; be) and\n&gt;  &gt; &gt; &gt; &gt; &gt; gives them a higher precedence (lower precedence number) than all\n&gt;  &gt; &gt; &gt; &gt; &gt; other URIs.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; 6) I think the answer is no for this question, but I will ask it\n&gt;  &gt; &gt; anyway.\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; If you have a Heritrix instance that is configured for 1200\n&gt;  &gt; &gt; threads on\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; one machine, can you recover from a checkpoint from that\n&gt; 1200 thread\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; configuration on a different machine with an Heritrix instance\n&gt;  &gt; &gt; that is\n&gt;  &gt; &gt; &gt; &gt; &gt;&gt; configured for less threads (e.g. the default 25 threads)?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Yes - there&#39;s no need to keep the thread count the same after a\n&gt;  &gt; &gt; &gt; &gt; &gt; resume. None of the checkpoint structures (or usual disk\n&gt; structures)\n&gt;  &gt; &gt; &gt; &gt; &gt; are based on the number of worker threads (&#39;ToeThreads&#39;)... as\n&gt;  &gt; &gt; &gt; &gt; &gt; mentioned above you can even vary the number of threads in a\n&gt; running\n&gt;  &gt; &gt; &gt; &gt; &gt; crawl.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; - Gordon\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt;\n&gt;\n&gt; \n\n\n"}}