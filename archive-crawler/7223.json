{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":289645082,"authorName":"helloitsmaxine","from":"&quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;","profile":"helloitsmaxine","replyTo":"LIST","senderId":"bgKDsP1L6KOBUlKNGmqwbs2mUbiUoSPzsW2YynKqQZEBaY5G6BZCZy2DMd1A79Y4tsBYkB3oci43GUXjMze_5teaXiabaIGlUb9QT14","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Crawl rate decreasing with time?","postDate":"1311613211","msgId":7223,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGowazdlcitqMDRnQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRFMjhCRjdGLjEwMTAyMDZAYXJjaGl2ZS5vcmc+"},"prevInTopic":7219,"nextInTopic":7234,"prevInTime":7222,"nextInTime":7224,"topicId":7213,"numMessagesInTopic":9,"msgSnippet":"Hey Gordon, I ve looked into your suggestions though nothing in the reports jumped out really readily but will continue to investigate. However I wanted to ask","rawEmail":"Return-Path: &lt;itsmaxine@...&gt;\r\nX-Sender: itsmaxine@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 46186 invoked from network); 25 Jul 2011 17:00:12 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m17.grp.re1.yahoo.com with QMQP; 25 Jul 2011 17:00:12 -0000\r\nX-Received: from unknown (HELO ng3-ip1.bullet.mail.ne1.yahoo.com) (98.138.215.94)\n  by mta1.grp.re1.yahoo.com with SMTP; 25 Jul 2011 17:00:12 -0000\r\nX-Received: from [98.138.217.179] by ng3.bullet.mail.ne1.yahoo.com with NNFMP; 25 Jul 2011 17:00:12 -0000\r\nX-Received: from [69.147.65.173] by tg4.bullet.mail.ne1.yahoo.com with NNFMP; 25 Jul 2011 17:00:12 -0000\r\nX-Received: from [98.137.34.35] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 25 Jul 2011 17:00:12 -0000\r\nDate: Mon, 25 Jul 2011 17:00:11 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;j0k7er+j04g@...&gt;\r\nIn-Reply-To: &lt;4E28BF7F.1010206@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;\r\nSubject: Re: Crawl rate decreasing with time?\r\nX-Yahoo-Group-Post: member; u=289645082; y=mu-NKijQETX7io5oTgeFHTkk71wUFGY8Te6BfHKVBsTFlpe3QENbCM9ZSBiix3nhcOSToP4UJDTKHs8\r\nX-Yahoo-Profile: helloitsmaxine\r\n\r\nHey Gordon,\nI&#39;ve looked into your suggestions though nothing in the reports=\r\n jumped out really readily but will continue to investigate.\n\nHowever I wan=\r\nted to ask you about a possibility that others have been mentioning--could =\r\nit be possible that this slow-down is due to a memory leak? Is there a way =\r\nto test for that? Someone suggested that I stop the crawl, restart it on a =\r\nfresh jvm, and see what happens. I paused the crawl, started it from the re=\r\ncovery log in a new jvm, and the crawl rates shot right back up to where th=\r\ney started. Would this be indicative of a memory leak? If so, it seems like=\r\n this problem is pretty inevitable since it has happened pretty much every =\r\ntime without fail. Just wanted to know what you (or anyone else) thinks abo=\r\nut this?\n\nThanks!\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;goj=\r\nomo@...&gt; wrote:\n&gt;\n&gt; Looks like you&#39;re on a 4GB Mac with a 2GB heap, so ther=\r\ne shouldn&#39;t be a\n&gt; swapping problem.\n&gt; \n&gt; 250 threads may be a lot for a 2G=\r\nB heap. There are no firm rules, but a \n&gt; very rough rule I&#39;ve used is to t=\r\nake the heap size, deduct the 60% used \n&gt; by default by the BerkeleyDB-JE-b=\r\nased structures, then divide the \n&gt; remaining value (800MB in your case) by=\r\n ~5MB/thread to get a plausible \n&gt; thread count.\n&gt; \n&gt; *If* the problem is t=\r\nhreads waiting for unresponsive hosts, then \n&gt; reducing soTimeout may help =\r\na little. The one-line reports on the \n&gt; reports page, or the longer &#39;threa=\r\nds report&#39;, may give a hint if this is \n&gt; the block. But you can&#39;t make soT=\r\nimeout too small, there are real delays \n&gt; for busy networks/server for whi=\r\nch you may not want to miss that URL \n&gt; entirely. It&#39;s a tradeoff you&#39;ll ha=\r\nve to decide.\n&gt; \n&gt; Carefully watching the crawl and aggressively removing U=\r\nRLs from \n&gt; unwanted/unresponsive sites (eg by adding new scope limitations=\r\n during \n&gt; the crawl) may offer a better response to slowdowns due to unres=\r\nponsive \n&gt; sites.\n&gt; \n&gt; If threads spending their time on slow/big resources=\r\n are an issue, \n&gt; reducing the timeout-seconds or maximum size to download =\r\nsettings could \n&gt; help a little. The threads report and crawl log might ind=\r\nicate if this \n&gt; is a contributing factor.\n&gt; \n&gt; As a Mac I&#39;m guessing there=\r\n&#39;s just a single hard drive. That&#39;s going to \n&gt; cap performance, with all q=\r\nueueing/lookups/scratch-files/content-writing \n&gt; competing for the single d=\r\nisk.\n&gt; \n&gt; When the tradeoffs associated with the Bloom option may be right =\r\nfor \n&gt; your project is something you&#39;ll have to weigh.\n&gt; \n&gt; - Gordon\n&gt; \n&gt; O=\r\nn 7/21/11 4:41 PM, helloitsmaxine wrote:\n&gt; &gt; I&#39;m referring to what it says =\r\non the Activity Monitor, though\n&gt; &gt; admittedly I&#39;m not sure what the releva=\r\nnce of it is. Here are\n&gt; &gt; screencaps of the console/Activity Monitor (2 pa=\r\nnes):\n&gt; &gt; http://img39.imageshack.us/img39/2212/ss25h36m.jpg\n&gt; &gt; http://img=\r\n29.imageshack.us/img29/9120/ss25h36mcpu.jpg\n&gt; &gt;\n&gt; &gt; The swap reported is 4.=\r\n4gb at this point-about 25h into the\n&gt; &gt; crawl...is that a large enough amo=\r\nunt to warrant scaling back on\n&gt; &gt; heap?\n&gt; &gt;\n&gt; &gt; The HTTP timeout-seconds v=\r\nalue I have now is 1200 (that&#39;s 20\n&gt; &gt; mins...seems long?) and the sotimeou=\r\nt-ms is 20,000 (20s, I guess that\n&gt; &gt; makes sense). Would it help to reduce=\r\n both or just the sotimeout? How\n&gt; &gt; much of a reduction would you suggest?=\r\n Could I make it as low as 1\n&gt; &gt; second? Or is maybe 5-10 better?\n&gt; &gt;\n&gt; &gt; C=\r\nurrently it looks like URI&#39;s crawled is still under a million,\n&gt; &gt; though I=\r\n would eventually like to grow it to the tens of\n&gt; &gt; millions--would lookin=\r\ng into the BloomUriUniqFilter be worth it at\n&gt; &gt; this point?\n&gt; &gt;\n&gt; &gt; Thanks=\r\n for your suggestions; I really appreciate it!\n&gt; &gt;\n&gt; &gt; --- In archive-crawl=\r\ner@yahoogroups.com, Gordon Mohr&lt;gojomo@&gt;\n&gt; &gt; wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt; What do you m=\r\nean by &quot;VM size&quot;? (What tool is reporting that\n&gt; &gt;&gt; number?)\n&gt; &gt;&gt;\n&gt; &gt;&gt; It w=\r\nould be very atypical for the heap size or JavaVM process\n&gt; &gt;&gt; address spac=\r\ne to be 150 gigabytes.\n&gt; &gt;&gt;\n&gt; &gt;&gt; What is the hardware like? (RAM, CPU, disk=\r\n count/speed)\n&gt; &gt;&gt;\n&gt; &gt;&gt; If all the threads are working on something, and th=\r\ne &#39;congestion\n&gt; &gt;&gt; ratio&#39; is high, then the problem is not that there&#39;s too=\r\n little\n&gt; &gt;&gt; that&#39;s eligible to crawl politely.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Some top possibil=\r\nities:\n&gt; &gt;&gt;\n&gt; &gt;&gt; =95 Java process size has grown larger than RAM and excess=\r\nive\n&gt; &gt;&gt; swapping is occurring. Due to Java&#39;s pattern of memory access, you=\r\n\n&gt; &gt;&gt; essentially never want to be using swap. If &#39;top&#39; or &#39;vmstat&#39; show\n&gt; =\r\n&gt;&gt; any swap being used, add RAM or scale back the heap so that it\n&gt; &gt;&gt; isn&#39;=\r\nt.\n&gt; &gt;&gt;\n&gt; &gt;&gt; =95 most threads are making fetches against unresponsive sites=\r\n, which\n&gt; &gt;&gt; can take a long time to timeout. Large crawls that hit giant\n&gt;=\r\n &gt;&gt; link-lists to defunct/fake sites can experience this. More threads,\n&gt; &gt;=\r\n&gt; shortening the FetchHTTP soTimeout, and manually pruning the bad\n&gt; &gt;&gt; URI=\r\ns can help.\n&gt; &gt;&gt;\n&gt; &gt;&gt; =95 the crawl has grown so large that accesses to the=\r\n\n&gt; &gt;&gt; data-structures which overflow to disk (notably the default\n&gt; &gt;&gt; &#39;alr=\r\neady-seen&#39; BdbUriUniqFilter) are now dominating its time usage.\n&gt; &gt;&gt; On bro=\r\nader and larger crawls -- expected to grow to more than a few\n&gt; &gt;&gt; tens of =\r\nmillions of URIs -- you may want to consider the\n&gt; &gt;&gt; BloomUriUniqFilter, t=\r\nhough it has other RAM costs and imprecision\n&gt; &gt;&gt; caveats.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Hope t=\r\nhis helps,\n&gt; &gt;&gt;\n&gt; &gt;&gt; - Gordon\n&gt; &gt;&gt;\n&gt; &gt;&gt; On 7/21/11 3:59 PM, helloitsmaxine =\r\nwrote:\n&gt; &gt;&gt;&gt; Some other details I&#39;ve noticed are that according to the\n&gt; &gt;&gt;=\r\n&gt; activity monitor, the VM size is about 150gb, and CPU usage\n&gt; &gt;&gt;&gt; during =\r\nthe crawl is typically very low, with over 95% idle.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Is that we=\r\nird? If there is so much CPU and VM available, would\n&gt; &gt;&gt;&gt; it just make sen=\r\nse to keep increasing memory allocation and #\n&gt; &gt;&gt;&gt; threads to keep crawl r=\r\nates up? Anyone have experience with this\n&gt; &gt;&gt;&gt; sort of thing?\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; =\r\n--- In archive-crawler@yahoogroups.com,\n&gt; &gt;&gt;&gt; &quot;helloitsmaxine&quot;&lt;itsmaxine@&gt; =\r\n  wrote:\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; On my crawls I&#39;ve noticed that they generally have b=\r\neen\n&gt; &gt;&gt;&gt;&gt; starting out at pretty good rates, ie. 1500kb/s, but then after\n=\r\n&gt; &gt;&gt;&gt;&gt; a few hours, this goes down to 0-50kb/s and pretty much stays\n&gt; &gt;&gt;&gt;&gt;=\r\n there.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; After reading about some other people&#39;s same problems=\r\n I tried a\n&gt; &gt;&gt;&gt;&gt; few tweaks: - increasing JVM memory to 1024 - increasing =\r\n#\n&gt; &gt;&gt;&gt;&gt; threads from 50 to 100 - increasing in/out recording buffers\n&gt; &gt;&gt;&gt;=\r\n&gt; However the problem still persists.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; Some information about =\r\nmy crawl is: - using Heritrix 1.4 - the\n&gt; &gt;&gt;&gt;&gt; max heap is being used (ie. =\r\ncurrent and max are the same) - the\n&gt; &gt;&gt;&gt;&gt; threads are all pretty much acti=\r\nve (99-100 out of 100 active at\n&gt; &gt;&gt;&gt;&gt; a time) - very high congestion ratio=\r\n, (ie. 7000) - so far it\n&gt; &gt;&gt;&gt;&gt; has crawled 674003 URI&#39;s in about 20h.\n&gt; &gt;&gt;=\r\n&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; Does anyone have insight into how I can prevent this problem?\n&gt; &gt;=\r\n&gt;&gt;&gt; I&#39;ve read this:\n&gt; &gt;&gt;&gt;&gt; https://webarchive.jira.com/wiki/display/Heritri=\r\nx/making+a+busy+crawl+go+faster\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; and a few other=\r\n posts but am not sure what applies to my situation and\n&gt; &gt;&gt; what would be =\r\nthe best next steps to take. Or if there&#39;s any other\n&gt; &gt;&gt; information that =\r\nmight be helpful let me know!\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; Thanks!\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;=\r\n&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; ------------------------------------\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Yahoo! Group=\r\ns Links\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ------------------------=\r\n------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}