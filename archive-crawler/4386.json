{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"9XFpXH73o4FMaoc0Aes-WiJafPAIlDCIYKj5cov-afsmggCs3xuKJVXnBekX8Posnvb42e59R5Ju37ep9F3gguW0C9Kp1DIG","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] How to setup a crawl that gathers the hostnames without downloading files","postDate":"1183041047","msgId":4386,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODNDNjE3LjEwNzA0MDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDg5MmY2YTM0MDcwNjI3MTE1OXA2ODZhMGY4NXE2NTk1MGUyMGFjYzk1MzAwQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGY1dTI5ZCtwaWxwQGVHcm91cHMuY29tPiA8NDY4MkE5QTUuNTAyMDJAYXJjaGl2ZS5vcmc+IDw4OTJmNmEzNDA3MDYyNzExNTlwNjg2YTBmODVxNjU5NTBlMjBhY2M5NTMwMEBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":4378,"nextInTopic":4387,"prevInTime":4385,"nextInTime":4387,"topicId":4370,"numMessagesInTopic":7,"msgSnippet":"Hi Mike, I think that Gordon s suggestion of writing a new decide rule or BeanshellDecideRule is the right way to go. Test crawls will probably get you want","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 83436 invoked from network); 28 Jun 2007 14:31:58 -0000\r\nReceived: from unknown (66.218.66.70)\n  by m47.grp.scd.yahoo.com with QMQP; 28 Jun 2007 14:31:58 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta12.grp.scd.yahoo.com with SMTP; 28 Jun 2007 14:31:57 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id E46AC1415FD1A\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 28 Jun 2007 07:30:53 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 17114-07-31 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 28 Jun 2007 07:30:53 -0700 (PDT)\r\nReceived: from [127.0.0.1] (nor75-24-88-170-99-175.fbx.proxad.net [88.170.99.175])\n\tby mail.archive.org (Postfix) with ESMTP id 4925714156F78\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 28 Jun 2007 07:30:52 -0700 (PDT)\r\nMessage-ID: &lt;4683C617.1070400@...&gt;\r\nDate: Thu, 28 Jun 2007 07:30:47 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (Windows/20070509)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f5u29d+pilp@...&gt; &lt;4682A9A5.50202@...&gt; &lt;892f6a340706271159p686a0f85q65950e20acc95300@...&gt;\r\nIn-Reply-To: &lt;892f6a340706271159p686a0f85q65950e20acc95300@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] How to setup a crawl that gathers the hostnames\n without downloading files\r\nX-Yahoo-Group-Post: member; u=137477665; y=2kuzIxIpMPjTlDpMWIMev8zuinTjNSbabJIsBqi9nDK4RsfAwg\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Mike,\n\nI think that Gordon&#39;s suggestion of writing a new decide rule or \nBeanshellDecideRule is the right way to go. Test crawls will probably \nget you want you need but you have to expect Web to change. If this is a \nrepeating crawl you will probably have to conduct test crawls for each \n&quot;real&quot; crawl.\n\nSome other tips:\n- You can set max-length-bytes in the HTTP fetcher to something small. \nProbably not 1 byte -- 1KB might work. However, conducting a test crawl \nin this manner will probably fetch only seeds and maybe a few other links.\n- You can setup a simple decide rule filter on the ARCWriter to accept \n(write) only dns records. Simple accept regex decide rule as ^dns:.*$ \nwould do.\n- As Gordon pointed out, you will probably want to fetch all documents \nin order to discover all hosts as in a real crawl. You can probably \noptimize this with a scope that does not fetch images, pdf, doc and similar.\n\nI hope this helps.\ni.\n\n\n&gt; Gordon,\n&gt; \n&gt; Thanks for the info. I was hoping there was a way to quickly gather a \n&gt; list of hostnames for a large crawl without pulling down all the data. I \n&gt; was hoping there was a way to download 1 byte and then move on. Our \n&gt; problem is the individuals we are working with do not seem to be able to \n&gt; come up with an accurate exclusion list and so I am doing a run-around \n&gt; approach to gather a hostname list and then determine if they should \n&gt; have excluded or included the host.\n&gt; \n&gt; Thanks,\n&gt; Mike\n&gt; \n&gt; On 6/27/07, *Gordon Mohr* &lt;gojomo@... \n&gt; &lt;mailto:gojomo@...&gt;&gt; wrote:\n&gt; \n&gt;     Doing a &#39;test&#39; crawl that discovers all the hostnames a &#39;real&#39; crawl\n&gt;     would discover would necessarily visit -- and download -- all the same\n&gt;     documents. So the load on the target servers is the same, and the\n&gt;     process you&#39;ve outlined won&#39;t save any load on the unwanted-local\n&gt;     hosts,\n&gt;     and will double the load on the wanted hosts.\n&gt; \n&gt;     If you don&#39;t mind the load on the local hosts -- you just don&#39;t want\n&gt;     their content -- you could crawl everything, then post-process the\n&gt;     material collected (ARC files) to discard the unwanted material, once\n&gt;     the IP addresses are known.\n&gt; \n&gt;     Really, though, it sounds like you want a scope DecideRule that acts on\n&gt;     IPs rather than hostnames. We don&#39;t have one but it would be relatively\n&gt;     straightforward to write. The MatchesRegExpDecideRule would be the\n&gt;     model, but instead of comparing the full URI&#39;s String representation,\n&gt;     just the IP would be compared.\n&gt; \n&gt;     One gotcha: the first time URIs are considered for scoping, the IP may\n&gt;     not be known -- the DNS lookup is a specific step triggered by a URI&#39;s\n&gt;     first attempted fetch. So you&#39;d want to rule-in all URIs with\n&gt;     not-yet-known IPs, then one recheckin (Prescoper processor) some would\n&gt;     be ruled-out (resulting in -5000 lines in your crawl.log).\n&gt; \n&gt;     Short of writing a new Java DecideRule, this could also be a job for\n&gt;     the\n&gt;     BeanshellDecideRule, which lets you specify a Beanshell-language\n&gt;     (Java-inspired scripting-language) script file to be run against\n&gt;     URIs to\n&gt;     make the ACCEPT/REJECT decision.\n&gt; \n&gt;     Hope this helps,\n&gt; \n&gt;     - Gordon @ IA\n&gt; \n&gt;     mjjjhjemj wrote:\n&gt;     &gt;  Is this possible?\n&gt;     &gt;\n&gt;     &gt;  I have a very large crawl that has numerous seeds and may potentially\n&gt;     &gt;  take weeks to complete. I do not want to crawl local sites accessible\n&gt;     &gt;  under a certain IP mask. Where Heritrix will exclude sites based on IP\n&gt;     &gt;  entered into a surts-source-file with decision=&#39;REJECT&#39; these sites\n&gt;     &gt;  are accessed using the typical hostname and not IP and are therefore\n&gt;     &gt;  not excluded.\n&gt;     &gt;\n&gt;     &gt;  I have been given an exclude list in typical hostname &#39;www.cnn.com\n&gt;     &lt;http://www.cnn.com&gt;&#39;\n&gt;     &gt;  form, but have confirmed that it is not complete and therefore my\n&gt;     &gt;  crawl scope is greater than it should be. This brings me to why I wish\n&gt;     &gt;  to complete a test crawl where the goal is to gather only the lists of\n&gt;     &gt;  URIs traversed, but not gather all the files. I will then parse the\n&gt;     &gt;  crawl.log and do a lookup on the hostname to determine if the IP and\n&gt;     &gt;  canonical name and see if they should be included in the crawl scope.\n&gt;     &gt;\n&gt;     &gt;  Any help would be greatly appreciated.\n&gt;     &gt;\n&gt;     &gt;  Thanks,\n&gt;     &gt;  Mike\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;\n&gt;      &gt; Yahoo! Groups Links\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt; \n&gt; \n&gt; \n\n\n"}}