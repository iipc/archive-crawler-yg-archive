{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":6903103,"authorName":"Tom Emerson","from":"Tom Emerson &lt;Tree@...&gt;","profile":"tree02139","replyTo":"LIST","senderId":"E1bPXCh5IVJgI_IFMRat3uZyqplMYJ8lTzGJkzh2FreiWDPDbufz-q4YFzAzErM0Hnp7GQ9cJM2ihlRzc1NyTAP-1oFmDXQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Half-brained idea on GZIP ARC file creation","postDate":"1090187622","msgId":671,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDE2NjM0LjYxNzk4LjY0MDgxNi45MjAwMzJAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","inReplyToHeader":"PDQwRkFEMjBELjkwNzA1MDZAYXJjaGl2ZS5vcmc+","referencesHeader":"PDE2NjMzLjUyMjk2LjMyMjQyNy4zNDA4OTBAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4JPDQwRkFEMjBELjkwNzA1MDZAYXJjaGl2ZS5vcmc+"},"prevInTopic":670,"nextInTopic":672,"prevInTime":670,"nextInTime":672,"topicId":669,"numMessagesInTopic":5,"msgSnippet":"... Yes, libarc parses the GZIP header itself too, which lead me to think about this. ... Good idea: I was thinking that the header itself was compressed, but ","rawEmail":"Return-Path: &lt;Tree@...&gt;\r\nX-Sender: Tree@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 52838 invoked from network); 18 Jul 2004 21:53:43 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m4.grp.scd.yahoo.com with QMQP; 18 Jul 2004 21:53:43 -0000\r\nReceived: from unknown (HELO mailserver.basistech.com) (199.88.205.4)\n  by mta4.grp.scd.yahoo.com with SMTP; 18 Jul 2004 21:53:43 -0000\r\nReceived: from postfix.basistech.com ([10.1.3.65] RDNS failed) by mailserver.basistech.com with Microsoft SMTPSVC(6.0.3790.0);\n\t Sun, 18 Jul 2004 17:53:42 -0400\r\nReceived: by postfix.basistech.com (Postfix, from userid 5007)\n\tid B68FD18841D; Sun, 18 Jul 2004 17:53:42 -0400 (EDT)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\nMessage-ID: &lt;16634.61798.640816.920032@...&gt;\r\nDate: Sun, 18 Jul 2004 17:53:42 -0400\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;40FAD20D.9070506@...&gt;\r\nReferences: &lt;16633.52296.322427.340890@...&gt;\n\t&lt;40FAD20D.9070506@...&gt;\r\nX-Mailer: VM 7.18 under Emacs 21.2.1\r\nReturn-Path: tree@...\r\nX-OriginalArrivalTime: 18 Jul 2004 21:53:42.0790 (UTC) FILETIME=[B1318260:01C46D11]\r\nX-eGroups-Remote-IP: 199.88.205.4\r\nFrom: Tom Emerson &lt;Tree@...&gt;\r\nReply-To: tree@...\r\nSubject: Re: [archive-crawler] Half-brained idea on GZIP ARC file creation\r\nX-Yahoo-Group-Post: member; u=6903103\r\nX-Yahoo-Profile: tree02139\r\n\r\nstack writes:\n&gt; I like this idea of adding a new optional gzip &#39;extra&#39; header field that \n&gt; has in it the length of the compressed member (Yes, ARCReader could \n&gt; exploit such a field if present; it does its own parse of gzip header \n&gt; now).  \n\nYes, libarc parses the GZIP header itself too, which lead me to think\nabout this.\n\n&gt; To avoid an extra copy during the writing of the gzipped ARC, the writer \n&gt; might instead backup and fill in the compressed length after gzipping \n&gt; had finished.  Would need to profile to make sure this extra I/O didn&#39;t \n&gt; come at some outlandish cost (General tendency should be toward \n&gt; minimizing crawl-time I/O).\n\nGood idea: I was thinking that the header itself was compressed, but\nof course it isn&#39;t. The header could reserve 10-digit offset (padded\nwith leading zeros) which is filled in after the stream is written,\nthat way it would be a fixed size.\n\nThe IO overhead would be two seeks and a write: seek back to the\nfield, write the data, seek back to the end of the stream.\n\n&gt; I added your suggestion for consideration as part of ARC file revision \n&gt; proposal: http://crawler.archive.org/cgi-bin/wiki.pl?ArcRevisionProposal.\n\nThe downside to this is the extra space taken up by the information:\n14 bytes per record (assuming a 10-byte offset). This could be reduced\nif the offset were instead stored as an 8-byte long long stored in\nlittle-endian byte order (to reflect the ordering used in the rest of\nthe header). That would give us 12 bytes per record. If we restrict\noffsets to unsigned 32-bit numbers than we are down to 8 bytes per\nrecord... probably a safe bet.\n\n&gt; FYI: The IA header is written in the first record only, not on \n&gt; subsequent members.\n\nAh, I hadn&#39;t noticed that. IMHO I&#39;d rather see it written on all\nrecords, for consistency, but it doesn&#39;t really matter in the scheme\nof things.\n\n-- \nTom Emerson                                          Basis Technology Corp.\nSoftware Architect                                 http://www.basistech.com\n  &quot;Beware the lollipop of mediocrity: lick it once and you suck forever&quot;\n\n"}}