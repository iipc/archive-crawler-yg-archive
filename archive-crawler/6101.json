{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":67599886,"authorName":"farbgeist","from":"&quot;farbgeist&quot; &lt;Farbgeist@...&gt;","profile":"farbgeist","replyTo":"LIST","senderId":"r2HyQtNrK3swsGuM9I0anhdVRiJNcsL5JH7lvoDaCyQ54DoUEonoIqGyjDp5MI-UDuWARd8McWrmcLIfxvJRA3Zwhix1","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Web-Scale Frontier","postDate":"1255534025","msgId":6101,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhiNHFrOSttanZkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRBRDUxRUMwLjkwNTA2MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":6100,"nextInTopic":6102,"prevInTime":6100,"nextInTime":6102,"topicId":6099,"numMessagesInTopic":6,"msgSnippet":"Thanks for the quick answer Gordon! ... Do you have some examples of typical URIs/s on different configurations (memory/# cpus) for crawls that already use the","rawEmail":"Return-Path: &lt;Farbgeist@...&gt;\r\nX-Sender: Farbgeist@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 55505 invoked from network); 14 Oct 2009 15:27:18 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m2.grp.sp2.yahoo.com with QMQP; 14 Oct 2009 15:27:18 -0000\r\nX-Received: from unknown (HELO n38b.bullet.mail.sp1.yahoo.com) (66.163.168.152)\n  by mta2.grp.sp2.yahoo.com with SMTP; 14 Oct 2009 15:27:18 -0000\r\nX-Received: from [69.147.65.149] by n38.bullet.mail.sp1.yahoo.com with NNFMP; 14 Oct 2009 15:27:06 -0000\r\nX-Received: from [98.137.34.73] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 14 Oct 2009 15:27:06 -0000\r\nDate: Wed, 14 Oct 2009 15:27:05 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;hb4qk9+mjvd@...&gt;\r\nIn-Reply-To: &lt;4AD51EC0.9050607@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;farbgeist&quot; &lt;Farbgeist@...&gt;\r\nSubject: Re: Web-Scale Frontier\r\nX-Yahoo-Group-Post: member; u=67599886; y=Fx3qfpxqgaHTr3B_x7shG2lnrbbEeKJlCwyU57fsgxvrB8xz\r\nX-Yahoo-Profile: farbgeist\r\n\r\nThanks for the quick answer Gordon!\n\n&gt; Not true; the crawl should continue =\r\nwithout problems as long as you&#39;ve \n&gt; still got free disk space on whatever=\r\n volume holds your crawl&#39;s &#39;state&#39; \n&gt; directory. Frontier operations -- tes=\r\nting URIs for prior inclusion, and \n&gt; enqueuing new URIs -- will become slo=\r\nwer as they require seeks/reads \n&gt; over ever-larger disk structures.\n&gt;\n\nDo =\r\nyou have some examples of typical URIs/s on different configurations (memor=\r\ny/# cpus) for crawls that already use the disk?\nA first test of a broad cra=\r\nwl (4 hops, 400 toe-threads on a dual core Athlon 3000+ with 2Gb ram  and 1=\r\n00Mb bandwith resulted in  ~50 URIs/s using Heritrix 2.0.2 after ~ 2 hours =\r\nwhich did not change significantly after 10 hours..\n \n\n&gt; \n&gt; For crawls wher=\r\ne a single machine is expected to visit &gt; 100 million \n&gt; URIs, to avoid the=\r\n slowdown as the crawl grows, we usually swap the \n&gt; disk-based already-inc=\r\nluded class (BdbUriUniqFilter) for an in-memory\n&gt; implementation (BloomUriU=\r\nniqFilter) that doesn&#39;t slow over time, but \n&gt; instead has a small false-po=\r\nsitive rate (that then grows if the filter \n&gt; becomes oversaturated). (The =\r\ndefaults, which are adjustable if you have \n&gt; more RAM, use 512MB to acheiv=\r\ne a 1-in-4-million false-positive rate up \n&gt; through 125 million discovered=\r\n URIs.)\n&gt; \n\nWhere is the upper limit? Do you need 512MB every 125 million U=\r\nRIs or\nis the false positive rate increasing drastically?\n\n&gt; IA has done cr=\r\nawls up to 2 billion URIs with Heritrix using multiple \n&gt; machines, and we =\r\nknow of outside teams who have done crawls of over 8 \n&gt; billion URIs using =\r\nthe same general techniques, which can be scaled \n&gt; further with more machi=\r\nnes.\n&gt; \nCan you tell numbers of how many machines (RAM, cores, bandwith) yo=\r\nu used in which timeframe for the 2 billion URIs? In that case you used the=\r\n BdbUriUniqFilter, right?\n\n&gt; Separate but related: we&#39;ve long been interest=\r\ned in having a \n&gt; already-included structure matching that described in the=\r\n Mercator \n&gt; papers (or as updated in the recent IRLbot paper), which would=\r\n offer a \n&gt; disk-based structure that wouldn&#39;t slow as much with growth as =\r\nour \n&gt; current implementation. \n(The manner in which candidate \n&gt; URIs are =\r\npassed through a duplicate filter, allowing for batching and \n&gt; without the=\r\n assumption of instant enqueuing, was designed to allow these \n&gt; techniques=\r\n to be dropped-in when needed.)\n&gt; \n\nDo you believe HBase on a cluster could=\r\n possibly act as a substitute to DRUM?\n\nMy test-crawl resulted in less than=\r\n 1/5 of bandwith usage (about 2000KB/s), while 1,600,000 pages where downlo=\r\naded and ~ 5,000,000 queued. Why?\n\nbest regards\nfarbgeist\n\n\n\n\n"}}