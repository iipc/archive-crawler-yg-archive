{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"2zx1gP82LPz304z3QqDm_XqytBlfm3K_2ylkEqg1xBtXZPNXE_WdwGzhJuUbxjZoYSgUfeoOWo3g-fJkpKXrR4jUH1OtzMCW","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Fetch process","postDate":"1169242671","msgId":3729,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1QjEzQTJGLjEwMDA3MDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDUxOTc1Mi44NTI2LnFtQHdlYjU4OTEzLm1haWwucmUxLnlhaG9vLmNvbT4=","referencesHeader":"PDUxOTc1Mi44NTI2LnFtQHdlYjU4OTEzLm1haWwucmUxLnlhaG9vLmNvbT4="},"prevInTopic":3724,"nextInTopic":3735,"prevInTime":3728,"nextInTime":3730,"topicId":3637,"numMessagesInTopic":18,"msgSnippet":"Hi Artem, If your seeds don t end with html?|doc|ppt|swf then you will get -5000. But I believe that the rule sequence will do what you want -- everything from","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 76925 invoked from network); 19 Jan 2007 21:38:52 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m23.grp.scd.yahoo.com with QMQP; 19 Jan 2007 21:38:52 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta6.grp.scd.yahoo.com with SMTP; 19 Jan 2007 21:38:52 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id D24011418E0BE\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 19 Jan 2007 13:38:43 -0800 (PST)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 31799-01-86 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tFri, 19 Jan 2007 13:38:37 -0800 (PST)\r\nReceived: from [127.0.0.1] (c-67-180-203-212.hsd1.ca.comcast.net [67.180.203.212])\n\tby mail.archive.org (Postfix) with ESMTP id 8287214176594\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 19 Jan 2007 13:38:37 -0800 (PST)\r\nMessage-ID: &lt;45B13A2F.1000703@...&gt;\r\nDate: Fri, 19 Jan 2007 13:37:51 -0800\r\nUser-Agent: Thunderbird 1.5.0.9 (Windows/20061207)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;519752.8526.qm@...&gt;\r\nIn-Reply-To: &lt;519752.8526.qm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Re: Fetch process\r\nX-Yahoo-Group-Post: member; u=137477665; y=9nxgA7NNxhJu3-PBOdm-He6gPyZsQFjvX-xjpvwIofMm0IXniA\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Artem,\n\nIf your seeds don&#39;t end with html?|doc|ppt|swf then you will get -5000.\nBut I believe that the rule sequence will do what you want --\neverything from certain domain(s) ending with specified file extensions.\n\nTry adding index.html to your seeds. For example: \nhttp://yahoo.com/index.html instead of http://yahoo.com/\n\nAlso, it seems that you are missing PrerequisiteAcceptDecideRule. You \nshould probably add this rule as the last rule in the sequence.\nThis will allow fetching of mandatory prerequisites (dns and robots.txt).\n\nTake care,\ni.\n\n&gt; Hi Igor,\n&gt; \n&gt; I&#39;ve tried the following rules sequence:\n&gt; \n&gt; &lt;newObject name=&quot;scope&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.DecidingScope&quot;&gt;\n&gt;       &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;       &lt;string name=&quot;seedsfile&quot;&gt;seeds.txt&lt;/string&gt;\n&gt;       &lt;boolean name=&quot;reread-seeds-on-config&quot;&gt;true&lt;/boolean&gt;\n&gt;       &lt;newObject name=&quot;decide-rules&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.DecideRuleSequence&quot;&gt;\n&gt;         &lt;map name=&quot;rules&quot;&gt;\n&gt;           &lt;newObject name=&quot;rejectByDefault&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.RejectDecideRule&quot;&gt;\n&gt;           &lt;/newObject&gt;\n&gt;           &lt;newObject name=&quot;onDomainsDecideRule&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.OnDomainsDecideRule&quot;&gt;\n&gt;             &lt;string name=&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n&gt;             &lt;string name=&quot;surts-source-file&quot;&gt;&lt;/string&gt;\n&gt;             &lt;boolean name=&quot;seeds-as-surt-prefixes&quot;&gt;true&lt;/boolean&gt;\n&gt;             &lt;string name=&quot;surts-dump-file&quot;&gt;&lt;/string&gt;\n&gt;             &lt;boolean name=&quot;also-check-via&quot;&gt;false&lt;/boolean&gt;\n&gt;             &lt;boolean name=&quot;rebuild-on-reconfig&quot;&gt;true&lt;/boolean&gt;\n&gt;           &lt;/newObject&gt;\n&gt;           &lt;newObject name=&quot;rejectIfTooManyHops&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.TooManyHopsDecideRule&quot;&gt;\n&gt;             &lt;integer name=&quot;max-hops&quot;&gt;3&lt;/integer&gt;\n&gt;           &lt;/newObject&gt;\n&gt;           &lt;newObject name=&quot;acceptIfTranscluded&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.TransclusionDecideRule&quot;&gt;\n&gt;             &lt;integer name=&quot;max-trans-hops&quot;&gt;2&lt;/integer&gt;\n&gt;           &lt;/newObject&gt;\n&gt;           &lt;newObject name=&quot;rejectIfPathological&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.PathologicalPathDecideRule&quot;&gt;\n&gt;             &lt;integer name=&quot;max-repetitions&quot;&gt;2&lt;/integer&gt;\n&gt;           &lt;/newObject&gt;\n&gt;           &lt;newObject name=&quot;rejectIfTooManyPathSegs&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.TooManyPathSegmentsDecideRule&quot;&gt;\n&gt;             &lt;integer name=&quot;max-path-depth&quot;&gt;5&lt;/integer&gt;\n&gt;           &lt;/newObject&gt;\n&gt;           &lt;newObject name=&quot;filePattern&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.NotMatchesFilePatternDecideRule&quot;&gt;\n&gt;             &lt;string name=&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n&gt;             &lt;string name=&quot;use-preset-pattern&quot;&gt;Custom&lt;/string&gt;\n&gt;             &lt;string name=&quot;regexp&quot;&gt;^(?i)^.*&#92;.(?:html?|doc|ppt)$&lt;/string&gt;\n&gt;           &lt;/newObject&gt;\n&gt;         &lt;/map&gt;\n&gt;       &lt;/newObject&gt;\n&gt;     &lt;/newObject&gt;\n&gt; \n&gt; But I got an -5000 error after start the Heritrix.\n&gt; \n&gt; If I change the last rule to the:\n&gt; \n&gt; &lt;newObject name=&quot;filePattern&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.MatchesFilePatternDecideRule&quot;&gt;\n&gt;             &lt;string name=&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n&gt;             &lt;string name=&quot;use-preset-pattern&quot;&gt;Custom&lt;/string&gt;\n&gt;             &lt;string name=&quot;regexp&quot;&gt;.*(?i)(&#92;.(doc|pdf|ppt|swf))$&lt;/string&gt;\n&gt; &lt;/newObject&gt;\n&gt; \n&gt; I get again all files from the domain I&#39;m interested in. What is wrong?\n&gt; \n&gt; Thanks.\n&gt; \n&gt; Regards,\n&gt; Artem.\n&gt; \n&gt; */Igor Ranitovic &lt;igor@...&gt;/* wrote:\n&gt; \n&gt;     Hi Artem,\n&gt; \n&gt;      &gt; 1) OnHostDecideRule - to accept anything on same hosts.\n&gt;      &gt; 2) NotOnHostsDecideRule - to reject anything on not same hosts.\n&gt;      &gt; 3) OnDomainsDecideRule - to accept anything on same domains.\n&gt;      &gt; 4) NotOnDomainsDecideRule - to reject anything not on same domains.\n&gt; \n&gt;     If all 4 rules are using seeds.txt file to create prefixes then you\n&gt;     don&#39;t need rules 1,2 and 4:\n&gt;     - rule 1 is not needed since rule 3 is broader in scope.\n&gt;     - rule 2 has no effect since very first rule you have rejects all URIs\n&gt;     by default (see the order file and RejectDecideRule).\n&gt;     - rule 4 is not need for same reason as the rule 2.\n&gt; \n&gt;      &gt; Yes, now I save to ARC only html/text files.\n&gt;      &gt; I&#39;m trying to do: extract only links (from html) to some\n&gt;      &gt; files (in this case, *.doc and *.ppt) and save only these links to\n&gt;      &gt; the crawl.log file.\n&gt;      &gt;\n&gt;      &gt; In other words, I want to restrict fetching and processing of all\n&gt;      &gt; URIs that not end with the specified pattern.\n&gt; \n&gt;     Would this work?\n&gt; \n&gt;     1) reject all URIs by default (RejectDecideRule)\n&gt;     2) accept all URIs within domain (OnDomainsDecideRule)\n&gt;     3) reject all URIs that don&#39;t match specified files extension\n&gt;     (NoMatchesFilePatternDecideRule). If you want only htm(l), docs and\n&gt;     ppts\n&gt;     use &#39;^(?i)^.*&#92;.(?:html?|doc|ppt)$&#39;\n&gt; \n&gt;     Take care,\n&gt;     i.\n&gt; \n&gt;      &gt; Thanks.\n&gt;      &gt;\n&gt;      &gt; Regards,\n&gt;      &gt; Artem.\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt; --- In archive-crawler@yahoogroups.com\n&gt;     &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Igor Ranitovic &lt;igor@...&gt;\n&gt;      &gt; wrote:\n&gt;      &gt;&gt;\n&gt;      &gt;&gt; Hi Artem,\n&gt;      &gt;&gt;\n&gt;      &gt;&gt; I am not sure why you have the following decide rule sequence:\n&gt;      &gt;&gt;\n&gt;      &gt;&gt; 1) Accept OnHostsDecideRule\n&gt;      &gt;&gt; 2) Then you reject OnHostsDecideRule. I am not sure what you are\n&gt;      &gt; trying\n&gt;      &gt;&gt; to do here.\n&gt;      &gt;&gt; 3) Accept OnDomainsDecideRule - what is you scope? Do you want to\n&gt;      &gt; crawl\n&gt;      &gt;&gt; in domain or host scope?\n&gt;      &gt;&gt; 3) Again you reject tOnDomainsDecideRule.\n&gt;      &gt;&gt;\n&gt;      &gt;&gt;&gt; &gt; Thanks a lot for help.\n&gt;      &gt;&gt;&gt; &gt; I&#39;ve added ContentTypeRegExFilter to the\n&gt;      &gt; &#39;midfetch-filter&#39; and\n&gt;      &gt;&gt;&gt; &gt; &#39;write-processors&#39; sections, and now only html/text\n&gt;      &gt; contents are\n&gt;      &gt;&gt;&gt; &gt; saved into the ARC file.\n&gt;      &gt;&gt;&gt; &gt;\n&gt;      &gt;&gt;&gt; &gt; The one problem I couldn&#39;t solve is to accept only URIs\n&gt;      &gt; that end\n&gt;      &gt;&gt;&gt; &gt; with the specified pattern (for example, &quot;.doc, *.ppt&quot;).\n&gt;      &gt;&gt;\n&gt;      &gt;&gt; What are you trying to do? You want to allow (fetch and process)\n&gt;      &gt; only\n&gt;      &gt;&gt; .doc and .ppt, but save only html/text to ARC files.\n&gt;      &gt;&gt;\n&gt;      &gt;&gt; Take care,\n&gt;      &gt;&gt; i.\n&gt;      &gt;&gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt; Yahoo! Groups Links\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt; \n&gt; \n&gt; ------------------------------------------------------------------------\n&gt; Sucker-punch spam &lt; \n&gt; http://us.rd.yahoo.com/evt=49981/*http://advision.webevents.yahoo.com/mailbeta/features_spam.html&gt; \n&gt; with award-winning protection.\n&gt; Try the free Yahoo! Mail Beta. &lt; \n&gt; http://us.rd.yahoo.com/evt=49981/*http://advision.webevents.yahoo.com/mailbeta/features_spam.html&gt; \n&gt; \n\n\n"}}