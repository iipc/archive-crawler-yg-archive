{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (archive.org)","from":"&quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"h976CYCtOcMrFNlu-5m_mQJzgrGqjzHv0VnliF5eB6bsnC8FZw7KkqxUqUl2K-klD6KoC5Ik1BMJPbzfJBQndIJKU2P5767OHRKQQUZCDAdv3s3cDkyb","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Politeness proposal, and CrawlHost class","postDate":"1138051678","msgId":2581,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzRDU0QTVFLjQwNDA5MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQzRDUzNjg4LjQwNTAwMDVAbWV0YWNhcnRhLmNvbT4=","referencesHeader":"PDQzRDEzRDU5LjgwOTA3MDBAbWV0YWNhcnRhLmNvbT4gPDQzRDUyRTdCLjIwOTA1MDVAYXJjaGl2ZS5vcmc+IDw0M0Q1MzY4OC40MDUwMDA1QG1ldGFjYXJ0YS5jb20+"},"prevInTopic":2580,"nextInTopic":2582,"prevInTime":2580,"nextInTime":2582,"topicId":2574,"numMessagesInTopic":5,"msgSnippet":"... Interesting. If the sites don t want to be crawled at all, and you re trying to look like a human browser, there are lots of other issues you d have to","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 39303 invoked from network); 23 Jan 2006 21:31:11 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m8.grp.scd.yahoo.com with QMQP; 23 Jan 2006 21:31:11 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta5.grp.scd.yahoo.com with SMTP; 23 Jan 2006 21:31:11 -0000\r\nReceived: (qmail 23033 invoked by uid 100); 23 Jan 2006 21:22:11 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.10?) (gojomo@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 23 Jan 2006 21:22:11 -0000\r\nMessage-ID: &lt;43D54A5E.4040907@...&gt;\r\nDate: Mon, 23 Jan 2006 13:27:58 -0800\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc3 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;43D13D59.8090700@...&gt; &lt;43D52E7B.2090505@...&gt; &lt;43D53688.4050005@...&gt;\r\nIn-Reply-To: &lt;43D53688.4050005@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-84.0 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 2:12:4:0\r\nFrom: &quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Politeness proposal, and CrawlHost class\r\nX-Yahoo-Group-Post: member; u=137285340; y=LWvAPiktPGlatz-mBP-1HRFWA3akKjgOrvg4agL8389l\r\nX-Yahoo-Profile: gojomo\r\n\r\nKarl Wright wrote:\n&gt;&gt;(I presume that it&#39;s generally better to space requests out evenly, rather\n&gt;&gt;than pulse on and off. That is, if you&#39;re going to hit a site 4K times\n&gt;&gt;over a four-day period, it&#39;s be better to do 1K a day, roughly 40 every\n&gt;&gt;hour, than to do 2K every other day, and 0 on the off days.)\n&gt;&gt;\n&gt; \n&gt; \n&gt; Actually, that&#39;s the assumption that seems to be false.\n&gt; We&#39;ve had complaints from webmasters who were being crawled slowly and \n&gt; steadily over a 4 day period.  Although the rate of crawl did not exceed \n&gt; one document per 38 seconds, and the maximum bandwidth used did not \n&gt; exceed 133 bytes per second, we still got complained at, because we did \n&gt; not LOOK like a user - we looked like a crawler.  We concluded that we \n&gt; needed to look more &quot;bursty&quot;, with a random amount between bursts, to \n&gt; seem like a real user.\n\nInteresting. If the sites don&#39;t want to be crawled at all, and you&#39;re\ntrying to look like a human browser, there are lots of other issues you&#39;d\nhave to address, too -- like User-Agent and total traffic coming from a\nsingle IP address (not to mention issues of legal authority to collect\nwithout implied target site permission). We at IA Strongly Discourage\ncrawling against a site owner&#39;s wishes.\n\nIf random pauses are preferred by target sites, it seems that the\npoliteness delays could be drawn from a random distribution around the\ndesired average delays for the intended effect.\n\n&gt;&gt;The key method for including alternate information about politeness\n&gt;&gt;delays is AbstractFrontier.politenessDelayFor(). Considering your\n&gt;&gt;need, this method could be changed to look inside the CrawlURI to\n&gt;&gt;see if an earlier step/Processor had already calculated a delay, and\n&gt;&gt;use that rather than the default calculation. That would make it\n&gt;&gt;easier for custom delays to be effected without editting/overriding\n&gt;&gt;the core Frontier code. If you need this change, let us know and I&#39;ll\n&gt;&gt;look into the possibility further.\n&gt; \n&gt; \n&gt; I&#39;d need to figure out the delay based on some time window into the past \n&gt; (e.g. 2 days, settable), and the number of fetches over that time \n&gt; period.  I don&#39;t know of any current data structure in Heritrix where \n&gt; this kind of information can be tracked or attached - any ideas?\n\nHaving thought about this in similar contexts in the past, I believe\nkeeping an accurate sliding window of past traffic can be complicated\nand require a largish memory footprint.\n\nSome sort of decaying average would be one alternative.\n\nBut, I think you could effectively get the desired result with a\nprobabilistic approach as mentioned above. The average delay would\nensure the overall rate is correct over the long, but the variance in\ndelays would occasionally give pauses of many hours or even days.\n\nSomething like a 1/(2^n) chance of multiplying the standard delay by\nn might do the trick. If the math is right, including any clipping of\nextreme values you apply, keeping any record of the overall visit rate\ncould be rendered superfluous -- and only interesting as a fallback\ndouble-check.\n\n- Gordon @ IA\n\n\n"}}