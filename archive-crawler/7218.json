{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":289645082,"authorName":"helloitsmaxine","from":"&quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;","profile":"helloitsmaxine","replyTo":"LIST","senderId":"TMcCECPUQi9b_IRI6bU8dFReCxEk5vevlwQWjoEnz3KCgtK6LuSBcHrcS4peAWZI_Rrwkzm10idALOddE0QEvvPJp83nB28MAIqsx5w","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Help from those who already made their mistakes","postDate":"1311291937","msgId":7218,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGowYWRuMStqcnFoQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGl1Y3J0Yithc2x1QGVHcm91cHMuY29tPg=="},"prevInTopic":7189,"nextInTopic":0,"prevInTime":7217,"nextInTime":7219,"topicId":7189,"numMessagesInTopic":2,"msgSnippet":"Although I m the first to admit I haven t had the most experience with this, I ve recently been using Heritrix to do web crawls and it seems like it would have","rawEmail":"Return-Path: &lt;itsmaxine@...&gt;\r\nX-Sender: itsmaxine@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 40771 invoked from network); 21 Jul 2011 23:45:39 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m16.grp.re1.yahoo.com with QMQP; 21 Jul 2011 23:45:39 -0000\r\nX-Received: from unknown (HELO n41b.bullet.mail.sp1.yahoo.com) (66.163.168.155)\n  by mta3.grp.re1.yahoo.com with SMTP; 21 Jul 2011 23:45:39 -0000\r\nX-Received: from [69.147.65.147] by n41.bullet.mail.sp1.yahoo.com with NNFMP; 21 Jul 2011 23:45:39 -0000\r\nX-Received: from [98.137.34.51] by t10.bullet.mail.sp1.yahoo.com with NNFMP; 21 Jul 2011 23:45:39 -0000\r\nDate: Thu, 21 Jul 2011 23:45:37 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;j0adn1+jrqh@...&gt;\r\nIn-Reply-To: &lt;iucrtb+aslu@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;\r\nSubject: Re: Help from those who already made their mistakes\r\nX-Yahoo-Group-Post: member; u=289645082; y=lOFn5Tciz6rtiQDwFOUUoiRCA7FQF4HvDTckbDu1feFuTSkDhG6wMuXQ3FA_BTRDNJh_hfqpeFxTFtk\r\nX-Yahoo-Profile: helloitsmaxine\r\n\r\nAlthough I&#39;m the first to admit I haven&#39;t had the most experience with this=\r\n, I&#39;ve recently been using Heritrix to do web crawls and it seems like it w=\r\nould have everything you need to accomplish your goal. It&#39;s pretty easy/fas=\r\nt to install and if you don&#39;t want to deal with processing special file typ=\r\nes, you can download everything to static pages in a file/directory structu=\r\nre for pretty easy page re-rendering. As I understand it also has filters t=\r\nhat let you restrict the crawls to certain domains, etc. that might be usef=\r\nul for getting all the pages in one forum.\n\n--- In archive-crawler@yahoogro=\r\nups.com, &quot;Craigiri&quot; &lt;fire@...&gt; wrote:\n&gt;\n&gt; Greeting to the group!\n&gt; \n&gt; I hav=\r\ne a project which I would like to tackle and need opinions on the best way(=\r\ns) to go about it. \n&gt; \n&gt; The scope of the project would be relatively simpl=\r\ne - something like this:\n&gt; Archive well less than 1,000 total sites which f=\r\nocus on a particular type of content - mostly user to user sites (forums, e=\r\ntc.) about, for instance, boats. \n&gt; The archived sites would not be used un=\r\ntil and unless the existing sites ceased to exist. The archive would exist =\r\nonly for that reason - to make certain some content did not go to the great=\r\n digital trash can in the sky. I would only need to crawl these sites once =\r\na year or so as they contain relatively timeless content (how to wire a Bos=\r\nton Whaler boat, for instance)...\n&gt; \n&gt; The archiving program or a post-proc=\r\nessor must have the ability to convert dynamic sites (forums, etc.) to stat=\r\nic pages which would run without dependencies. \n&gt; \n&gt; So, my initial questio=\r\nns:\n&gt; \n&gt; 1. What would be the best crawler or SW to use to do this? I&#39;ve se=\r\nen stuff like httrack and heretrix as well as some small programs (sitesuck=\r\ner, etc.) which run on a mac, etc.\n&gt; \n&gt; 2. Will advanced configuration be n=\r\needed for each and every site I desire to download? Further to this questio=\r\nn, have folks written extensions which, for instance, might be designed to =\r\nbest download a site which runs on VBB (a popular forum package)....or do w=\r\ne each have to reinvent the wheel?\n&gt; \n&gt; 3. Further to #2, do y&#39;all work wit=\r\nh other programs to massage the data afterwards....as an example, strip out=\r\n the ads, etc.?\n&gt; \n&gt; I run a dedicated centos server which could run the cr=\r\nawler...or, I could run it on my home machine or another barebones server w=\r\nhich I hook up at home. Given the relatively small number of sites I intend=\r\n to archive (probably only 200-300 a year at first), this might work out.\n&gt;=\r\n \n&gt; Any and all advice appreciated. I would like to avoid as many errors an=\r\nd missteps as possible.\n&gt;\n\n\n\n"}}