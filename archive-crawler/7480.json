{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"OvuArW4fh6Kchzb87OBh9jWnVxU3He0g0AS93j-Z4-cnDmb4ualJGBMk8tfmHwPtWRAxNlF6_6CJsh0uYFNkBwM3AsQTDxc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] robots.txt problem","postDate":"1324609825","msgId":7480,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFRjNGMTIxLjEwMDA0MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRFRjNCN0JELjYwODA1MDZAY3MuY211LmVkdT4=","referencesHeader":"PGpjNThodCt1Z3VwQGVHcm91cHMuY29tPiA8NEVFODA5RDcuNzAxMDEwMEBhcmNoaXZlLm9yZz4gPDRFRjM4RDk5LjEwODA0MDZAY3MuY211LmVkdT4gPDRFRjNBRTJCLjUwMTA5MDdAYXJjaGl2ZS5vcmc+IDw0RUYzQjdCRC42MDgwNTA2QGNzLmNtdS5lZHU+"},"prevInTopic":7479,"nextInTopic":7487,"prevInTime":7479,"nextInTime":7481,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"Looks like this is a bug, a regression from H1. See: https://webarchive.jira.com/browse/HER-1976 Master at github has the 5-character fix (~line 48 of","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 56607 invoked from network); 23 Dec 2011 03:10:28 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m9.grp.sp2.yahoo.com with QMQP; 23 Dec 2011 03:10:28 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.sp2.yahoo.com with SMTP; 23 Dec 2011 03:10:27 -0000\r\nX-Received: (qmail 13783 invoked by uid 0); 23 Dec 2011 03:10:26 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 23 Dec 2011 03:10:26 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4EF3F121.1000405@...&gt;\r\nDate: Thu, 22 Dec 2011 19:10:25 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: David Pane &lt;dpane@...&gt;, archive-crawler@yahoogroups.com\r\nReferences: &lt;jc58ht+ugup@...&gt; &lt;4EE809D7.7010100@...&gt; &lt;4EF38D99.1080406@...&gt; &lt;4EF3AE2B.5010907@...&gt; &lt;4EF3B7BD.6080506@...&gt;\r\nIn-Reply-To: &lt;4EF3B7BD.6080506@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] robots.txt problem\r\nX-Yahoo-Group-Post: member; u=137285340; y=TZmR5tneHCaNBeaCdWBYymC-hDSTU5DTiWQjBzc2LmQ2\r\nX-Yahoo-Profile: gojomo\r\n\r\nLooks like this is a bug, a regression from H1. See:\n\nhttps://webarchive.jira.com/browse/HER-1976\n\nMaster at github has the 5-character fix (~line 48 of RobotsPolicy.java).\n\nOther comments/answers interspersed below...\n\nOn 12/22/11 3:05 PM, David Pane wrote:\n&gt; Thank you Gordon.\n&gt;\n&gt; On 12/22/11 5:24 PM, Gordon Mohr wrote:\n&gt;&gt; Robots-enforcement is completely separate from scoping/DecideRules, so\n&gt;&gt; changes to SurtPrefixedDecideRule couldn&#39;t be responsible for robots\n&gt;&gt; issues.\n&gt;&gt;\n&gt;&gt; (Though, if your changes are a significant new capability, such as\n&gt;&gt; triggering DNS lookups or even consulting the existing cached\n&gt;&gt; CrawlHost lookups during scoping, you might want to package them as\n&gt;&gt; another rule that could be applied in serial, rather than an extension\n&gt;&gt; of SurtPrefixedDecideRule.)\n&gt;\n&gt; I chose to add the additional functionality so that if the SURT is found\n&gt; through the initial check, it does not check for the host. I may indeed\n&gt; add this functionality as a separate deciderule in the futre.\n\nA number of the DecideRules are &quot;one-way&quot;, in that they will only \nACCEPT, or only REJECT. Such rules already have an optimization that if \nthe &#39;decision so far&#39; is already in the same direction as the current \nrule, the current rule doesn&#39;t bother running. (So, if the URI is \nalready REJECTed, and the rule sees its only possible decision is REJECT \nor PASS, it doesn&#39;t bother with its usual logic.) So I think the \ntwo-rule approach could be nearly as efficient.\n\n&gt;&gt; Can you reproduce the same behavior when trying to crawl a single one\n&gt;&gt; of the problem URIs, such as for example....\n&gt;&gt;\n&gt;&gt; http:/tmbw.net/wiki/index.php?title=1996&action=edit\n&gt;&gt;\n&gt;&gt; ...with either the default configuration or your usual configuration\n&gt;&gt; with just this one seed?\n&gt; I would rather not do this as I am a little concerne. The webmaster was\n&gt; not too happy that this was happening.. He has also blocked access to us.\n\nI ran this test, and reproduced the problem.\n\nStepping through, it appears the issue is H3 is only checking the URI&#39;s \n&#39;path&#39; component, not the &#39;path&#39; and &#39;query&#39;. It&#39;s a regression from H1, \nwhich does the right thing.\n\nSee: https://webarchive.jira.com/browse/HER-1976\n\nThe minimal fix is committed at github:master, but there will be a \nfurther renaming and test coverage before the issue is closed.\n\n&gt;&gt; FYI, whether a mid-crawl change has taken effect the way you&#39;d like\n&gt;&gt; depends on how you added it. The rules that take their starting\n&gt;&gt; configuration from lists of URIs/patterns (in either CXML or from\n&gt;&gt; source files on disk) are *not* constantly monitoring those files for\n&gt;&gt; changes... so simply editing those files on disk *won&#39;t* take\n&gt;&gt; immediate effect.\n&gt;&gt;\n&gt;&gt; However, if you drop a &quot;updates.seeds&quot; file into the &#39;action&#39;\n&gt;&gt; directory, seeds and directive lines (starting with &#39;+&#39; or &#39;-&#39;) do get\n&gt;&gt; announced to the crawl in a way that (in the usual configuration) will\n&gt;&gt; be noticed by accept/reject SurtPrefixedDecideRules, in which case\n&gt;&gt; they do live-update their in-memory rule-in/rule-out lists.\n&gt;&gt;\n&gt; To be clear, I can create a &quot;updates.seeds&quot; file and drop it in the\n&gt; action directory in the format as this so that they are included in the\n&gt; black list.\n&gt;\n&gt; +http://(net,tmbw,\n\nBy default, rules that decide ACCEPT pay attention to the &#39;+&#39; lines. \nRules that decide REJECT pay attention to the &#39;-&#39; lines. So using this \nmethod for live-adding new REJECT prefixes, you&#39;ll want to use &#39;-&#39;. (See \nSurtPrefixedDecideRule.nonseedLine().)\n\n&gt; Will an edited blacklist surts file used that is configured in the\n&gt; SurtPrefixedDecideRules eventually get applied in a live crawl?\n\nNo - the declared source file (or inline list of strings in CXML) is \nnever rescanned during a single crawl launch: only at the start. Changes \nthere will take effect only on the next launch.\n\n- Gordon\n\n&gt; --David\n&gt;\n&gt;&gt; - Gordon\n&gt;&gt;\n&gt;&gt; On 12/22/11 12:05 PM, David Pane wrote:\n&gt;&gt;&gt; I am running a crawl using heritrix.3.1.1 from the git repository with\n&gt;&gt;&gt; some minor changes that I do not believe is the cause of this problem.\n&gt;&gt;&gt; (I made changes in SurtPrefixedDecideRule.java to accommodate the\n&gt;&gt;&gt; checking of ip addresses as well as surts in my black list surt file. )\n&gt;&gt;&gt;\n&gt;&gt;&gt; I had a webmaster complain that the crawler was not obeying his\n&gt;&gt;&gt; robots.txt file. He reported that the crawler was requesting pages that\n&gt;&gt;&gt; should be dissallowed by directives for &quot;*&quot; user agent.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Here are some sample pages that it is requesting:\n&gt;&gt;&gt; /wiki/index.php?title=They_Might_Be_Giants&action=edit\n&gt;&gt;&gt; Http Code: 200 Date: Dec 22 13:04:15 Http Version: HTTP/1.0 Size in\n&gt;&gt;&gt; Bytes: 54449\n&gt;&gt;&gt; Referer: http://tmbw.net/wiki/They_Might_Be_Giants\n&gt;&gt;&gt;\n&gt;&gt;&gt; /wiki/index.php?title=Category:Sung_By_John_Flansburgh&action=edit\n&gt;&gt;&gt; Http Code: 200 Date: Dec 22 13:04:54 Http Version: HTTP/1.0 Size in\n&gt;&gt;&gt; Bytes: 21164\n&gt;&gt;&gt; Referer: http://tmbw.net/wiki/Category:Sung_By_John_Flansburgh\n&gt;&gt;&gt;\n&gt;&gt;&gt; /wiki/index.php?title=Help_talk:Contents&action=edit\n&gt;&gt;&gt; Http Code: 200 Date: Dec 22 13:05:23 Http Version: HTTP/1.0 Size in\n&gt;&gt;&gt; Bytes: 22155\n&gt;&gt;&gt; Referer: http://tmbw.net/wiki/Help_talk:Contents\n&gt;&gt;&gt;\n&gt;&gt;&gt; /wiki/index.php?title=1996&action=edit\n&gt;&gt;&gt; Http Code: 200 Date: Dec 22 13:05:48 Http Version: HTTP/1.0 Size in\n&gt;&gt;&gt; Bytes: 20813\n&gt;&gt;&gt; Referer: http://tmbw.net/wiki/1996\n&gt;&gt;&gt;\n&gt;&gt;&gt; /wiki/index.php?title=Here_Comes_Science/Charts&action=render&chartOnly=Billboard%20Kids%20Albums\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; Http Code: 200 Date: Dec 22 13:06:18 Http Version: HTTP/1.0 Size in\n&gt;&gt;&gt; Bytes: 1069\n&gt;&gt;&gt; Referer: http://tmbw.net/wiki/Here_Comes_Science\n&gt;&gt;&gt;\n&gt;&gt;&gt; /wiki/index.php?title=Dial-A-Song&action=edit\n&gt;&gt;&gt; Http Code: 200 Date: Dec 22 13:06:46 Http Version: HTTP/1.0 Size in\n&gt;&gt;&gt; Bytes: 29906\n&gt;&gt;&gt; Referer: http://tmbw.net/wiki/Dial-A-Song\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; His robots.txt file is here: http://tmbw.net/robots.txt\n&gt;&gt;&gt;\n&gt;&gt;&gt; I added his domain to my surt black list file while the crawler is\n&gt;&gt;&gt; running. I am assuming that this update will be applied. Please let me\n&gt;&gt;&gt; know if I am correct in this assumption.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks,\n&gt;&gt;&gt;\n&gt;&gt;&gt; David\n&gt;&gt;\n\n"}}