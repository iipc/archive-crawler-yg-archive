{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":473207283,"authorName":"Kristinn Sigur√∞sson","from":"=?iso-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kristinn@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"HvC7wOvPUh5l14O8_3HZf2RBgsfgRDPSaPc0yK8CiimL5Zt2exczgATQwhdM6_UdODo9CwYHUpu4pi_qcb-LBbtyqX1izF6aUkxAUNYVBqwg2JbxzmDwqMHsYpzKuDJSQUmLkaTh9avZ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: Ignore robots.txt for specific URLs","postDate":"1364390297","msgId":8000,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEU0ODZEOTNFMzcyRDhDNDI4MkU4REYwNzgzOTJGMTJEOTVBNjhCQGJsaWtpLmJva2hsYWRhLmxvY2FsPg==","inReplyToHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MTBFNDhBMTFAdzJrMy1ic3BleDE+","referencesHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MTBFNDhBMTFAdzJrMy1ic3BleDE+"},"prevInTopic":7999,"nextInTopic":8001,"prevInTime":7999,"nextInTime":8001,"topicId":7999,"numMessagesInTopic":4,"msgSnippet":"Hi Roger, One option would be to create a sheet overlay that uses DecideRuledSheetAssociation rather SurtPrefixesSheetAssociation to determine when it should","rawEmail":"Return-Path: &lt;kristinn@...&gt;\r\nX-Sender: kristinn@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 94611 invoked from network); 27 Mar 2013 13:18:36 -0000\r\nX-Received: from unknown (10.193.84.163)\n  by m12.grp.bf1.yahoo.com with QMQP; 27 Mar 2013 13:18:36 -0000\r\nX-Received: from unknown (HELO vala.bok.hi.is) (130.208.186.10)\n  by mta3.grp.bf1.yahoo.com with SMTP; 27 Mar 2013 13:18:36 -0000\r\nX-Received: from bliki.bokhlada.local (bliki.bok.hi.is [130.208.186.6])\n\tby vala.bok.hi.is (8.13.8/8.13.8) with ESMTP id r2RDIXRP003007\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 27 Mar 2013 13:18:33 GMT\r\nX-Received: from BLIKI.bokhlada.local ([2002:82d0:ba06::82d0:ba06]) by\n bliki.bokhlada.local ([2002:82d0:ba06::82d0:ba06]) with mapi id\n 14.01.0438.000; Wed, 27 Mar 2013 13:18:18 +0000\r\nTo: &quot;archive-crawler@yahoogroups.com&quot; &lt;archive-crawler@yahoogroups.com&gt;\r\nThread-Topic: Ignore robots.txt for specific URLs\r\nThread-Index: Ac4q2ayT4QK+7sC/RtWpe5y/8CY9PgAEX5SQ\r\nDate: Wed, 27 Mar 2013 13:18:17 +0000\r\nMessage-ID: &lt;E486D93E372D8C4282E8DF078392F12D95A68B@...&gt;\r\nReferences: &lt;74C97E7DF5A7784D997217FF75D1216610E48A11@w2k3-bspex1&gt;\r\nIn-Reply-To: &lt;74C97E7DF5A7784D997217FF75D1216610E48A11@w2k3-bspex1&gt;\r\nAccept-Language: en-US, is-IS\r\nContent-Language: en-US\r\nX-MS-Has-Attach: \r\nX-MS-TNEF-Correlator: \r\nContent-Type: text/plain; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nMIME-Version: 1.0\r\nX-Spam-Status: No, score=0.0 required=4.6 tests=none autolearn=disabled\n\tversion=3.3.1\r\nX-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on gleipnir.rhi.hi.is\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: =?iso-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kristinn@...&gt;\r\nSubject: RE: Ignore robots.txt for specific URLs\r\nX-Yahoo-Group-Post: member; u=473207283; y=ounLjyLVKGZnFrry5XUyZVeBF7IV_aZauc0rrGsmcSY3u7Tq\r\nX-Yahoo-Profile: kristsi25\r\n\r\nHi Roger,\n\nOne option would be to create a sheet overlay that uses DecideRu=\r\nledSheetAssociation rather SurtPrefixesSheetAssociation to determine when i=\r\nt should be applied.\n\nYou could then use HopsPathMatchesRegexDecideRule to =\r\nmatch only seeds and their embeds.\n\nIt all might look something like:\n\n  &lt;!=\r\n-- Let seeds ignore robots --&gt;\n  &lt;bean class=3D&quot;org.archive.crawler.spring.=\r\nDecideRuledSheetAssociation&quot;&gt;\n    &lt;property name=3D&quot;rules&quot;&gt;\n      &lt;bean cla=\r\nss=3D&quot;org.archive.modules.deciderules.DecideRuleSequence&quot;&gt;\n        &lt;propert=\r\ny name=3D&quot;rules&quot;&gt;\n          &lt;list&gt;\n            &lt;!-- Begin by REJECTing all.=\r\n.. --&gt;\n            &lt;bean class=3D&quot;org.archive.modules.deciderules.RejectDec=\r\nideRule&quot;&gt; &lt;/bean&gt;\n            &lt;!-- ...then ACCEPT those whose hoppath impli=\r\nes seed or embed of seed --&gt;\n            &lt;bean class=3D&quot;org.archive.modules=\r\n.deciderules.surt.HopsPathMatchesRegexDecideRule &quot;&gt;\n              &lt;property=\r\n name=3D&quot;decision&quot; value=3D&quot;ACCEPT&quot; /&gt; &lt;!-- Default behavior --&gt;\n          =\r\n    &lt;property name=3D&quot;regex&quot; value=3D&quot;^.E?$&quot; /&gt;\n            &lt;/bean&gt;\n       =\r\n   &lt;/list&gt;\n        &lt;/property&gt;\n      &lt;/bean&gt;\n    &lt;/property&gt;\n    &lt;property =\r\nname=3D&quot;targetSheetNames&quot;&gt;\n      &lt;list&gt;\n        &lt;value&gt;ignoreRobots&lt;/value&gt;=\r\n\n      &lt;/list&gt;\n    &lt;/property&gt;\n  &lt;/bean&gt;\n\n  &lt;bean id=3D&quot;ignoreRobots&quot; class=\r\n=3D&quot;org.archive.spring.Sheet&quot;&gt;\n    &lt;property name=3D&quot;map&quot;&gt;\n      &lt;map&gt;\n    =\r\n    &lt;entry key=3D&quot;preconditions.calculateRobotsOnly&quot; value=3D&quot;true&quot; /&gt;\n    =\r\n  &lt;/map&gt;\n    &lt;/property&gt;\n  &lt;/bean&gt;\n\nThe above is untested but should be rou=\r\nghly on the right track.\n\nYou can, of course, override the robots using oth=\r\ner settings (such as the &#39;ignore&#39; policy) but I find this useful as it leav=\r\nes trace in the crawl.log that the robots.txt were willfully ignored.\n\nBest=\r\n,\nKris\n\n\n------------------------------------------------------------------=\r\n-------\nLandsb=F3kasafn =CDslands - H=E1sk=F3lab=F3kasafn | Arngr=EDmsg=F6t=\r\nu 3 - 107 Reykjav=EDk\nS=EDmi/Tel: +354 5255600 | www.landsbokasafn.is\n-----=\r\n--------------------------------------------------------------------\nfyrirv=\r\nari/disclaimer - http://fyrirvari.landsbokasafn.is\n&gt; -----Original Message-=\r\n----\n&gt; From: archive-crawler@yahoogroups.com [mailto:archive-\n&gt; crawler@yah=\r\noogroups.com] On Behalf Of Coram, Roger\n&gt; Sent: 27. mars 2013 10:58\n&gt; To: a=\r\nrchive-crawler@yahoogroups.com\n&gt; Subject: [archive-crawler] Ignore robots.t=\r\nxt for specific URLs\n&gt;\n&gt;\n&gt;\n&gt; Hi,\n&gt; We&#39;re considering ignoring robots.txt fo=\r\nr some URLs, specifically\n&gt; seeds and anything embedded on a seed. We&#39;ve tr=\r\nied using a .force\n&gt; file in the Action Directory but robots.txt seems to o=\r\nverrule this.\n&gt;\n&gt;\n&gt;\n&gt; Any suggestions as to how to go about this?\n&gt;\n&gt;\n&gt;\n&gt; T=\r\nhanks,\n&gt;\n&gt; Roger\n&gt;\n&gt; \n\n"}}