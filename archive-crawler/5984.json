{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"L4Pw5vKYn3-5YkeO0izFAb7fLsYpo-rkIWeO705RyhKBRM3YyqCHvbHUkClyPIbqnfO3NdLa76mHUIUBfoe9w5PYmRpAsYU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Re: domain scope with millions of seeds","postDate":"1250203303","msgId":5984,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBODQ5NkE3LjYwMDA2MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGg1dmxlbCtwNWdtQGVHcm91cHMuY29tPg==","referencesHeader":"PGg1dmxlbCtwNWdtQGVHcm91cHMuY29tPg=="},"prevInTopic":5977,"nextInTopic":5986,"prevInTime":5983,"nextInTime":5985,"topicId":5969,"numMessagesInTopic":5,"msgSnippet":"... We absolutely understand the value of this model -- increasingly it s also what we at the Archive and our partners want. (We have been thinking more of","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 77225 invoked from network); 13 Aug 2009 22:42:24 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m6.grp.re1.yahoo.com with QMQP; 13 Aug 2009 22:42:24 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta2.grp.sp2.yahoo.com with SMTP; 13 Aug 2009 22:42:24 -0000\r\nX-Received: (qmail 52344 invoked from network); 13 Aug 2009 22:41:45 -0000\r\nX-Received: from 67.188.14.54 (HELO ?192.168.1.114?) (67.188.14.54)\n  by relay01.pair.com with SMTP; 13 Aug 2009 22:41:45 -0000\r\nX-pair-Authenticated: 67.188.14.54\r\nMessage-ID: &lt;4A8496A7.6000607@...&gt;\r\nDate: Thu, 13 Aug 2009 15:41:43 -0700\r\nUser-Agent: Thunderbird 2.0.0.22 (Windows/20090605)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;h5vlel+p5gm@...&gt;\r\nIn-Reply-To: &lt;h5vlel+p5gm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: domain scope with millions of seeds\r\nX-Yahoo-Group-Post: member; u=137285340; y=9pF9uTngB0A5ZVTRUocTQUrDHUX11eLtFknTaccweoWO\r\nX-Yahoo-Profile: gojomo\r\n\r\njoehung302 wrote:\n&gt;&gt; It seems your general goal is &quot;get a lot (or everything) from \n&gt;&gt; seeds/distinguished-domains, a little from everything else&quot;.\n&gt;&gt;\n&gt; \n&gt; Gordon,\n&gt; \n&gt; Yes. This is exactly the kind of crawl we&#39;d like to do after years of using Heritrix.\n&gt; \n&gt; We knew that we can crawl as many pages as we want using Heritrix. Today we use 12 crawlers to crawl 6B pages and I can comfortably say that we can linearly scale that model by adding more cralwer instances, up to 20B pages.\n&gt; \n&gt; But the reality is every business has its own value-added and there really need to be a way to justify the Internet data (6B pages &gt;= 60TB compressed archives, text only) becuase it costs a lot to maintain and process.\n&gt; \n&gt; That&#39;s why the strategy you mentioned,\n&gt; &quot;get a lot (or everything) from seeds/distinguished-domains, a little from everything else&quot;.\n&gt; is important for a real business. The business would pick and maintain the seeds (many MMs) and only reach out *a little* for the *relevant* information. For a vertical search application (like us), it is important to communicate which part of the Internet you provide because it doesn&#39;t really make sense to cover all Internet after all. A seed list with many MMs USLs is a good start.\n&gt; \n&gt; Ideally the crawl strategy I&#39;d like to implement is:\n&gt; 1) Be able to crawl *almost* all pages from the domain derived from seeds.\n&gt; 2) Add the one-level out *domains* (not just links) and put them into the scope (not seeds, otherwise it becomes broad crawl)\n&gt; \n&gt; With this we can start to maintain a meaningful seedlist for our business and will be able to explain the value-added with our search application.\n\nWe absolutely understand the value of this model -- increasingly it&#39;s \nalso what we at the Archive and our partners want. (We have been \nthinking more of &#39;deeper&#39; sets in the thousands of sites, with \n&#39;sampling&#39; from millions, but that&#39;s likely to change over time as well.)\n\nThere are a bunch of changes either already in the H3 codebase or \nplanned for it that should help:\n\n- most of the bottlenecks making it hard to use giant seedlists have \nbeen removed (and the rest should be gone before official release)\n\n- adding seeds and mutating the SURT-based scopes after a crawl has \nbegun is easier and more efficient (though, a SURT-based scope still \nhasn&#39;t been optimized for millions of distinct SURT prefixes)\n\n- the H2/H3 sheets-and-associations model more easily allows target \nhosts/domains/site-areas to be moved between categories of alternate \nsettings like &#39;sample&#39;, &#39;deeper&#39;, &#39;deepest&#39;\n\n- the H2/H3 queue-precedence mechanism offers the possibility of \ndeferring any limited crawler attention towards &#39;sample&#39; sites until \nafter &#39;deeper&#39; sites reach some target level of completion (though of \ncourse, if there&#39;s not enough &#39;deeper&#39; queues ready to crawl politely, \nthreads will dip into the &#39;sample&#39; queues to keep busy)\n\nH3 is still months from being recommended for production use but as an \nexpert operator/customizer/coder, you might want to start considering it.\n\n- Gordon @ IA\n\n\n"}}