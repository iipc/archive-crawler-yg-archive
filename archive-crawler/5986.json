{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"5RkalGNqTx5GBuUvFSjdSzQ0DBkSbxyxdjP044TCPi7OqgdCVQgkaghF8Z7m3VbL6yHkI-7Liq0nafH2Lee3Fcf8RosAstoG_ppQCKld","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: domain scope with millions of seeds","postDate":"1250270996","msgId":5986,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg2NDZ1aytsZ2k5QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRBODQ5NkE3LjYwMDA2MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":5984,"nextInTopic":0,"prevInTime":5985,"nextInTime":5987,"topicId":5969,"numMessagesInTopic":5,"msgSnippet":"Gordon, This is great to hear. Is there any documentation related to this new scoping feature? I d like to take a look and we might have a chance to exercise","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 13350 invoked from network); 14 Aug 2009 17:30:16 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m3.grp.re1.yahoo.com with QMQP; 14 Aug 2009 17:30:16 -0000\r\nX-Received: from unknown (HELO n37b.bullet.mail.sp1.yahoo.com) (66.163.168.151)\n  by mta1.grp.sp2.yahoo.com with SMTP; 14 Aug 2009 17:30:16 -0000\r\nX-Received: from [69.147.65.147] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 14 Aug 2009 17:29:58 -0000\r\nX-Received: from [98.137.34.184] by t10.bullet.mail.sp1.yahoo.com with NNFMP; 14 Aug 2009 17:29:58 -0000\r\nDate: Fri, 14 Aug 2009 17:29:56 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h646uk+lgi9@...&gt;\r\nIn-Reply-To: &lt;4A8496A7.6000607@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: domain scope with millions of seeds\r\nX-Yahoo-Group-Post: member; u=132996324; y=bi-NqFKV2zc-p0n_7EharXLU_sXYAcbmVY_veHAG_qDhjsPWqA\r\nX-Yahoo-Profile: joehung302\r\n\r\nGordon,\n\nThis is great to hear. Is there any documentation related to this =\r\nnew scoping feature? I&#39;d like to take a look and we might have a chance to =\r\nexercise the new version by end of this year.\n\nCheers,\n-Joe\n\n--- In archive=\r\n-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; joehung302 wr=\r\note:\n&gt; &gt;&gt; It seems your general goal is &quot;get a lot (or everything) from \n&gt; =\r\n&gt;&gt; seeds/distinguished-domains, a little from everything else&quot;.\n&gt; &gt;&gt;\n&gt; &gt; \n&gt;=\r\n &gt; Gordon,\n&gt; &gt; \n&gt; &gt; Yes. This is exactly the kind of crawl we&#39;d like to do =\r\nafter years of using Heritrix.\n&gt; &gt; \n&gt; &gt; We knew that we can crawl as many p=\r\nages as we want using Heritrix. Today we use 12 crawlers to crawl 6B pages =\r\nand I can comfortably say that we can linearly scale that model by adding m=\r\nore cralwer instances, up to 20B pages.\n&gt; &gt; \n&gt; &gt; But the reality is every b=\r\nusiness has its own value-added and there really need to be a way to justif=\r\ny the Internet data (6B pages &gt;=3D 60TB compressed archives, text only) bec=\r\nuase it costs a lot to maintain and process.\n&gt; &gt; \n&gt; &gt; That&#39;s why the strate=\r\ngy you mentioned,\n&gt; &gt; &quot;get a lot (or everything) from seeds/distinguished-d=\r\nomains, a little from everything else&quot;.\n&gt; &gt; is important for a real busines=\r\ns. The business would pick and maintain the seeds (many MMs) and only reach=\r\n out *a little* for the *relevant* information. For a vertical search appli=\r\ncation (like us), it is important to communicate which part of the Internet=\r\n you provide because it doesn&#39;t really make sense to cover all Internet aft=\r\ner all. A seed list with many MMs USLs is a good start.\n&gt; &gt; \n&gt; &gt; Ideally th=\r\ne crawl strategy I&#39;d like to implement is:\n&gt; &gt; 1) Be able to crawl *almost*=\r\n all pages from the domain derived from seeds.\n&gt; &gt; 2) Add the one-level out=\r\n *domains* (not just links) and put them into the scope (not seeds, otherwi=\r\nse it becomes broad crawl)\n&gt; &gt; \n&gt; &gt; With this we can start to maintain a me=\r\naningful seedlist for our business and will be able to explain the value-ad=\r\nded with our search application.\n&gt; \n&gt; We absolutely understand the value of=\r\n this model -- increasingly it&#39;s \n&gt; also what we at the Archive and our par=\r\ntners want. (We have been \n&gt; thinking more of &#39;deeper&#39; sets in the thousand=\r\ns of sites, with \n&gt; &#39;sampling&#39; from millions, but that&#39;s likely to change o=\r\nver time as well.)\n&gt; \n&gt; There are a bunch of changes either already in the =\r\nH3 codebase or \n&gt; planned for it that should help:\n&gt; \n&gt; - most of the bottl=\r\nenecks making it hard to use giant seedlists have \n&gt; been removed (and the =\r\nrest should be gone before official release)\n&gt; \n&gt; - adding seeds and mutati=\r\nng the SURT-based scopes after a crawl has \n&gt; begun is easier and more effi=\r\ncient (though, a SURT-based scope still \n&gt; hasn&#39;t been optimized for millio=\r\nns of distinct SURT prefixes)\n&gt; \n&gt; - the H2/H3 sheets-and-associations mode=\r\nl more easily allows target \n&gt; hosts/domains/site-areas to be moved between=\r\n categories of alternate \n&gt; settings like &#39;sample&#39;, &#39;deeper&#39;, &#39;deepest&#39;\n&gt; \n=\r\n&gt; - the H2/H3 queue-precedence mechanism offers the possibility of \n&gt; defer=\r\nring any limited crawler attention towards &#39;sample&#39; sites until \n&gt; after &#39;d=\r\neeper&#39; sites reach some target level of completion (though of \n&gt; course, if=\r\n there&#39;s not enough &#39;deeper&#39; queues ready to crawl politely, \n&gt; threads wil=\r\nl dip into the &#39;sample&#39; queues to keep busy)\n&gt; \n&gt; H3 is still months from b=\r\neing recommended for production use but as an \n&gt; expert operator/customizer=\r\n/coder, you might want to start considering it.\n&gt; \n&gt; - Gordon @ IA\n&gt;\n\n\n\n"}}