{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":214587980,"authorName":"Christian Kohlschuetter","from":"Christian Kohlschuetter &lt;ck-heritrix@...&gt;","replyTo":"LIST","senderId":"d66x6y2RCIZR48Rs-F6-wPNjRNnJfjRmcZGR92QH1J1isOc9iwA5sSIzWJZGIcipcd25e9YFpNqazqSXD54N1ixcWDyd2NIOoYZmdRpwExPD1SmnZ4e6Bg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Optimize URL scheduling?","postDate":"1111066491","msgId":1666,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDUwMzE3MTQzNC41MTgzMC5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4="},"prevInTopic":0,"nextInTopic":1667,"prevInTime":1665,"nextInTime":1667,"topicId":1666,"numMessagesInTopic":4,"msgSnippet":"Dear all, during my ODP-centric broad crawls (using Heritrix CVS HEAD with my BucketQueueAssignmentPolicy), I just noticed that - very often - all 50 ","rawEmail":"Return-Path: &lt;ck-heritrix@...&gt;\r\nX-Sender: ck-heritrix@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 6508 invoked from network); 17 Mar 2005 13:36:21 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m26.grp.scd.yahoo.com with QMQP; 17 Mar 2005 13:36:21 -0000\r\nReceived: from unknown (HELO mail.newsclub.de) (130.75.2.42)\n  by mta6.grp.scd.yahoo.com with SMTP; 17 Mar 2005 13:36:21 -0000\r\nReceived: (qmail 2158 invoked by uid 2002); 17 Mar 2005 13:36:19 -0000\r\nReceived: from ck-heritrix@... by nhf3.rrzn.uni-hannover.de by uid 207 with qmail-scanner-1.21 \n (clamscan: 0.67. spamassassin: 2.63.  Clear:RC:0(130.75.87.112):SA:0(-2.6/5.0):. \n Processed in 4.793676 secs); 17 Mar 2005 13:36:19 -0000\r\nX-Spam-Status: No, hits=-2.6 required=5.0\r\nReceived: from pc112.l3s.uni-hannover.de (HELO mail.newsclub.de) (webmail@...@130.75.87.112)\n  by nhf3.rrzn.uni-hannover.de with RC4-MD5 encrypted SMTP; 17 Mar 2005 13:36:14 -0000\r\nOrganization: NewsClub\r\nTo: archive-crawler@yahoogroups.com\r\nDate: Thu, 17 Mar 2005 14:34:51 +0100\r\nUser-Agent: KMail/1.7.2\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n  charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nMessage-Id: &lt;200503171434.51830.ck-heritrix@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Christian Kohlschuetter &lt;ck-heritrix@...&gt;\r\nSubject: Optimize URL scheduling?\r\nX-Yahoo-Group-Post: member; u=214587980\r\n\r\nDear all,\n\nduring my ODP-centric broad crawls (using Heritrix CVS HEAD with=\r\n my \nBucketQueueAssignmentPolicy), I just noticed that - very often - all 5=\r\n0 \nToeThreads seem to stay in the Postselector processor for a long time (4=\r\n0s in \nsome cases), resulting in a very slow throughput of avg. 14 docs/sec=\r\n \n(currently 1214032 of 3378892 discovered pages were crawled).\n\nI guess th=\r\nat it is simply caused by the already-seen test and the URI enqueue \noperat=\r\nion, which are both called in the Postselector class (requiring \nsynchroniz=\r\ned access to the underlying Bdb database).\n\nAs far as I understand, the cra=\r\nwling cycle simply processes all chains \n(pre-processors, fetchers, extract=\r\nors etc.) in line. This means that any \ncongestion in the post-processing s=\r\ntage will unavoidably delay the processing \nof succeeding entries in the qu=\r\neue of scheduled URLs (&quot;ready-queue&quot;). \nIncreasing the number of ToeThreads=\r\n would make things even worse because many \nmore already-seen/enqueue reque=\r\nsts would then have to be synchronized with \nthe underlying bdb database.\n\n=\r\nI think, decoupling the crawling and link-enqueueing stages (=3D making the=\r\nm run \nin parallel), could reduce the overall delay and improve crawling \np=\r\nerformance: Couldn&#39;t the Postselector simply enqueue all extracted Links \n(=\r\nincluding duplicates -- ie. no deduping overhead) to a Thread-Local, \ndisk-=\r\nbacked queue (&quot;postprocess-queue&quot;)? Another Thread/pool of Threads would \nt=\r\nhen dequeue items from that queue and try to schedule them (doing the \nalre=\r\nady-seen test and enqueue them into the ready-queue).\n\nThe Postselector wou=\r\nld almost instantly finish and let the &quot;crawling&quot; \nToeThread pass on to the=\r\n next scheduled URL. On the other side, the queue of \nextracted links will =\r\ngrow very fast, but that should not render a problem (it \nis disk-based). A=\r\nnd at some point, if that queue becomes very long, we can \nstill pause craw=\r\nling new pages until the postprocess-queue&#39;s size reaches a \nlow-water thre=\r\nshold again.\n\nDoes that make sense?\n\nBest regards,\n-- \nChristian Kohlsch=FC=\r\ntter\nmailto: ck -at- NewsClub.de\n\n"}}