{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"1QgZfrgAvOx3-vfkbudivlPVwpEJQQS8sx-KZclE_3_9_U8_3mPmkFZv_YSonEMlnmUM5DKJIbmf91cy9SkYIIGLnnOuMvQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix 1.14.4 - Hard disk breaking","postDate":"1334640901","msgId":7654,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGOEQwMTA1LjMwMjAwMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGptNGc4cCs4OWF1QGVHcm91cHMuY29tPg==","referencesHeader":"PGptNGc4cCs4OWF1QGVHcm91cHMuY29tPg=="},"prevInTopic":7648,"nextInTopic":7678,"prevInTime":7653,"nextInTime":7655,"topicId":7646,"numMessagesInTopic":6,"msgSnippet":"BdbFrontier with the default BdbUriUniqFilter isn t the most efficient with random disk seeks, and our rough rule-of-thumb, developed from experience on","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 75702 invoked from network); 17 Apr 2012 05:35:04 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m11.grp.sp2.yahoo.com with QMQP; 17 Apr 2012 05:35:04 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta5.grp.sp2.yahoo.com with SMTP; 17 Apr 2012 05:35:04 -0000\r\nX-Received: (qmail 58469 invoked by uid 0); 17 Apr 2012 05:35:02 -0000\r\nX-Received: from 70.36.143.78 (HELO silverbook.local) (70.36.143.78)\n  by relay03.pair.com with SMTP; 17 Apr 2012 05:35:02 -0000\r\nX-pair-Authenticated: 70.36.143.78\r\nMessage-ID: &lt;4F8D0105.3020003@...&gt;\r\nDate: Mon, 16 Apr 2012 22:35:01 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:11.0) Gecko/20120327 Thunderbird/11.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;matteo.ceccarello@...&quot; &lt;matteo.ceccarello@...&gt;\r\nReferences: &lt;jm4g8p+89au@...&gt;\r\nIn-Reply-To: &lt;jm4g8p+89au@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix 1.14.4 - Hard disk breaking\r\nX-Yahoo-Group-Post: member; u=137285340; y=zhsh6Q3RzooXuGn8uiGavmhAq13jTPQRnH0WfQLsCcUa\r\nX-Yahoo-Profile: gojomo\r\n\r\nBdbFrontier with the default BdbUriUniqFilter isn&#39;t the most efficient \nwith random disk seeks, and our rough rule-of-thumb, developed from \nexperience on machines that have 4GB-8GB of RAM, is to consider swapping \nout the BdbUriUniqFilter for the BloomUriUniqFilter on crawls expected \nto grow past 50 million discovered URIs, and into the hundreds of \nmillions of URIs.\n\nThe Bloom implementation keeps track of all &#39;seen&#39; URIs in memory, \ntrading a small false-positive error rate for compactness. The default \nparameters, which can be changed, maintains a 1-in-4-million false \npositive rate through about 125 million seen URIs, using about 500MB of \nRAM. It&#39;s a little bit slower than the alternative in the beginning, but \nnever gets any slower over time. (And even as it grows past its planned \nsize, its false error rate just creeps up.)\n\nOther than that, making sure all the crawler&#39;s key disk paths are spread \nover available independent volumes, as Bjarne and Travis mention, can \nhelp somewhat. Depending on exactly what your crawl is like and what \nbottlenecks you&#39;re seeing, and your system characteristics, there might \nbe other crawl settings that could be tweaked for some improvement.\n\nSome other possibilities for future improvement, especially for the \nUriUniqFilter component which is subject to constant growth and random \naccess, would be to blend several of the techniques and use a larger \nin-memory cache for commonly-seen URIs (in front of the \nBdbUriUniqFilter), which depending on implementation choice might \noutperform the automatic caching provided by the BDB-JE library.\n\nThe best technique I&#39;ve read about uses a lagged sort and merge to batch \ntogether the most possible already-seen tests within the fewest \nseeks/reads/writes. It&#39;s described in the 2001 &quot;High-Performance Web \nCrawling&quot; paper by Najork and Heydon, and perhaps improvable by the \napproach described 2008 &quot;IRLbot: Scaling to 6 Billion Pages and Beyond&quot; \npaper by Lee, Leonard, Wang, and Loguinov.\n\nWe tried to maintain the possibility of such lagged (non-instant, \nreordered) unique-testing in the Heritrix frontier design, to allow \nthese techniques to be swapped in. There&#39;s one long-ago implementation \nof a similar approach in the codebase as the &#39;FPMergeUriUniqFilter&#39;, but \nthat might need work to function with the current codebase, or reveal \nother ordering-sensitive bugs, and it was never rigorously benchmarked \nagainst the other options inside the Heritrix context.\n\n- Gordon\n\n\nOn 4/11/12 10:50 AM, matteo.ceccarello@... wrote:\n&gt; Hi all,\n&gt;\n&gt; We are two students of the University of Padova, Italy. We are attempting a large crawl using the Heritrix crawler (version 1.14.4). The frontier we are using is the bdbfrontier. We noticed that using this frontier, the crawler is constantly accessing the disk, performing small reads/writes that\n&gt;   a) are inefficient in terms of aggregated disk I/O\n&gt;   b) move the disk head a lot, which is not very healthy for the disk itself. In fact, we have already lost multiple disks in the process.\n&gt;\n&gt; Is there some way to avoid this problem?\n&gt;\n&gt; Thanks for the help\n&gt;\n&gt; Matteo Ceccarello,\n&gt; Alessandro Secco\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}