{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":318366001,"authorName":"_threeputt","from":"&quot;_threeputt&quot; &lt;gbullock@...&gt;","profile":"_threeputt","replyTo":"LIST","senderId":"9Hnv7zWUBPvCMawnBy3VGDaY-GiHVfdbgGJE07X9KOLVyVxpVIE_rVLOmAeMU8nK6mx26A21f9lJI_kmb59r6eKlj6vnMXcnzeD7aA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Advice from the experts (please)","postDate":"1185411426","msgId":4464,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PGY4OHJoMitibTNjQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":4477,"prevInTime":4463,"nextInTime":4465,"topicId":4464,"numMessagesInTopic":2,"msgSnippet":"First of all, let me congratulate you on a robust, extendable project. My compliments... I m starting a project that will crawl millions of pre-configured ","rawEmail":"Return-Path: &lt;gbullock@...&gt;\r\nReceived: (qmail 15202 invoked from network); 26 Jul 2007 17:23:30 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m55.grp.scd.yahoo.com with QMQP; 26 Jul 2007 17:23:30 -0000\r\nReceived: from unknown (HELO n12a.bullet.sp1.yahoo.com) (69.147.64.109)\n  by mta11.grp.scd.yahoo.com with SMTP; 26 Jul 2007 17:23:29 -0000\r\nReceived: from [216.252.122.217] by n12.bullet.sp1.yahoo.com with NNFMP; 26 Jul 2007 17:22:41 -0000\r\nReceived: from [66.218.69.3] by t2.bullet.sp1.yahoo.com with NNFMP; 26 Jul 2007 17:22:41 -0000\r\nReceived: from [66.218.66.91] by t3.bullet.scd.yahoo.com with NNFMP; 26 Jul 2007 17:22:41 -0000\r\nX-Sender: gbullock@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 72804 invoked from network); 26 Jul 2007 00:57:09 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m54.grp.scd.yahoo.com with QMQP; 26 Jul 2007 00:57:09 -0000\r\nReceived: from unknown (HELO n8b.bullet.sp1.yahoo.com) (69.147.64.169)\n  by mta11.grp.scd.yahoo.com with SMTP; 26 Jul 2007 00:57:09 -0000\r\nReceived: from [216.252.122.218] by n8.bullet.sp1.yahoo.com with NNFMP; 26 Jul 2007 00:57:06 -0000\r\nReceived: from [66.218.69.6] by t3.bullet.sp1.yahoo.com with NNFMP; 26 Jul 2007 00:57:06 -0000\r\nReceived: from [66.218.66.85] by t6.bullet.scd.yahoo.com with NNFMP; 26 Jul 2007 00:57:06 -0000\r\nDate: Thu, 26 Jul 2007 00:57:06 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;f88rh2+bm3c@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;_threeputt&quot; &lt;gbullock@...&gt;\r\nSubject: Advice from the experts (please)\r\nX-Yahoo-Group-Post: member; u=318366001; y=ZwFJj71RTknMjoHzu65JaAuUWRFd2JerAmOTucv-LKbOZwE2KA\r\nX-Yahoo-Profile: _threeputt\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 26 Jul 2007 17:22:41 -0000\r\n\r\nFirst of all, let me congratulate you on a robust, extendable project. \nMy =\r\ncompliments...\n\n\nI&#39;m starting a project that will crawl millions of pre-con=\r\nfigured\nurls.  We want to have a category assigned to each url so we only w=\r\nant\nto crawl text and images from each site without following any external\n=\r\nlinks.  This will therefore require using DomainScoped crawling only\n(Right=\r\n?).\n\nI am therefore under the impression that each url to be crawled should=\r\n\nbe treated as a seed.  Is this true?\n\nIf so, how do I best supply those (m=\r\nillions of) urls to the crawler? \nI could use the &quot;Start the crawl with a r=\r\necovery log&quot; option that I\nread about here:\nhttp://webteam.archive.org/conf=\r\nluence/display/Heritrix/Feed+URLs+in+bulk+to+a+crawler,\nbut that would requ=\r\nire that an extremely large flat file be managed,\nand I feel like having ro=\r\nws in a table for the urls and their\n(potentially) multiple categories woul=\r\nd be a more workable solution.\nI could, I suppose, look up each each url in=\r\n the database as I process\nthem in order to get their categories, but that =\r\nwould require they\nexist in the database and then be exported to the &quot;recov=\r\nery file&quot;.\n\nI&#39;ve only been perusing the code for a day or so, but it seems =\r\nlike a\nbetter solution might be to sub-class BdbFrontier, create a thread\nt=\r\nhat watches the WorkQueue, and then batches up urls from the database\nas th=\r\ne number of items on the queue diminishes.\n\nDoes that sound like a good sol=\r\nution?  Do you have any better suggestions.\n\nThank you so much for your tim=\r\ne.\n\nGlenn Bullock\n\nSoftware Engineer\nSurf Recon, Inc\nwww.surfrecon.com\n\n\n"}}