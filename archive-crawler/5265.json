{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":347605000,"authorName":"lpeterus","from":"&quot;lpeterus&quot; &lt;lpeterus@...&gt;","profile":"lpeterus","replyTo":"LIST","senderId":"J01WGkRCOHnkqPQmqzR3qzKCc7RZRnSbSK5XZHwlLkjjtzvRCvkA3gmWjLjfVmO6Z0zP01zk7sCEaaFGdJZPW7O2KAt3BA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Content type specific crawling?","postDate":"1212428436","msgId":5265,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcyMWJhaytxaGNkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGcxdWI3NCt0aWRyQGVHcm91cHMuY29tPg=="},"prevInTopic":5253,"nextInTopic":5273,"prevInTime":5264,"nextInTime":5266,"topicId":5248,"numMessagesInTopic":9,"msgSnippet":"Hi! Thanks for the reply. I did have a ContentTypeMatchesRegExpDecideRule under the writer processor section with the following regex (?i)application/xml.* But","rawEmail":"Return-Path: &lt;lpeterus@...&gt;\r\nX-Sender: lpeterus@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 93963 invoked from network); 2 Jun 2008 17:40:37 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m48.grp.scd.yahoo.com with QMQP; 2 Jun 2008 17:40:37 -0000\r\nX-Received: from unknown (HELO n46c.bullet.mail.sp1.yahoo.com) (66.163.168.180)\n  by mta18.grp.scd.yahoo.com with SMTP; 2 Jun 2008 17:40:37 -0000\r\nX-Received: from [216.252.122.219] by n46.bullet.mail.sp1.yahoo.com with NNFMP; 02 Jun 2008 17:40:37 -0000\r\nX-Received: from [66.218.69.2] by t4.bullet.sp1.yahoo.com with NNFMP; 02 Jun 2008 17:40:37 -0000\r\nX-Received: from [66.218.66.74] by t2.bullet.scd.yahoo.com with NNFMP; 02 Jun 2008 17:40:37 -0000\r\nDate: Mon, 02 Jun 2008 17:40:36 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g21bak+qhcd@...&gt;\r\nIn-Reply-To: &lt;g1ub74+tidr@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;lpeterus&quot; &lt;lpeterus@...&gt;\r\nSubject: Re: Content type specific crawling?\r\nX-Yahoo-Group-Post: member; u=347605000; y=aiLooLeroppagXO7k9Gg6jEqrGVC_dXLcVJMN_PvIV3VY1k\r\nX-Yahoo-Profile: lpeterus\r\n\r\nHi!\n\nThanks for the reply. I did have a ContentTypeMatchesRegExpDecideRule\n=\r\nunder the writer processor section with the following regex\n(?i)application=\r\n/xml.*\nBut it still seems to be writing the text/html pages from dynamic\nUR=\r\nLs. Did I use the wrong type of expression? Thanks.\n\n--- In archive-crawler=\r\n@yahoogroups.com, &quot;ermhes82&quot; &lt;ermhes@...&gt; wrote:\n&gt;\n&gt; You can prevent this i=\r\nf you include in decide-rules a \n&gt; ContentTypeMatchesRegExpDecideRule o \n&gt; =\r\nContentTypeNotMatchesRegExpDecideRule.\n&gt; \n&gt; Mario.\n&gt; \n&gt; --- In archive-craw=\r\nler@yahoogroups.com, &quot;lpeterus&quot; &lt;lpeterus@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi all!\n&gt; &gt; \n&gt; &gt;=\r\n I have a question about how to exclude dynamic URLs. I&#39;m using regular\n&gt; &gt;=\r\n expression rules based on suggestions from previous posting. I would\n&gt; &gt; l=\r\nike to filter out everything except for text/xml and application/xml\n&gt; &gt; fi=\r\nles. The filters are applied on the scope, midfetch, and the arc\n&gt; &gt; writer=\r\n processor. \n&gt; &gt; So far everything works ok, except it still dowloads some =\r\ntext/html\n&gt; &gt; files even though html is part of the rejection regexp in the=\r\n scope.\n&gt; &gt; Turns out all of the text/html downloads are dynamic URLs like =\r\nthis, \n&gt; &gt; http://somewebpage/subdir/?C=3DD;O=3DA \n&gt; &gt; My question then is =\r\nhow do I prevent these text/html from being\n&gt; &gt; written into the arc file. =\r\nThanks!\n&gt; &gt; \n&gt; &gt; Shawn\n&gt; &gt;\n&gt;\n\n\n\n"}}