{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":204413223,"authorName":"lists@graemes.com","from":"&quot;lists@...&quot; &lt;lists@...&gt;","profile":"GraemeSeaton","replyTo":"LIST","senderId":"OCBCG_H9N_H3TGuWGSuDJPTykOzoWHJOcvO8epFVD-fTJtbKERcDw9l2ls84SKM82A02LSsxhs1G7SieH0Oc_xODMkBJjKC0oiYzeyrh","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling","postDate":"1182990866","msgId":4379,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODMwMjEyLjMwNjAzMDlAZ3JhZW1lcy5jb20+","inReplyToHeader":"PDQ2ODJBNDdDLjgwNTA4MDVAYXJjaGl2ZS5vcmc+","referencesHeader":"PDkyOTI4NS41MDQ4Ny5xbUB3ZWI1MDMwNi5tYWlsLnJlMi55YWhvby5jb20+IDw0NjgyQTQ3Qy44MDUwODA1QGFyY2hpdmUub3JnPg=="},"prevInTopic":4373,"nextInTopic":4383,"prevInTime":4378,"nextInTime":4380,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"HCC seems to offer a number of answers to this kind of configuration but getting hold of documentation regarding it (try http://crawler.sourceforge.net/hcc )","rawEmail":"Return-Path: &lt;lists@...&gt;\r\nX-Sender: lists@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 9385 invoked from network); 28 Jun 2007 00:54:35 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m46.grp.scd.yahoo.com with QMQP; 28 Jun 2007 00:54:35 -0000\r\nReceived: from unknown (HELO graemes.com) (83.105.18.153)\n  by mta10.grp.scd.yahoo.com with SMTP; 28 Jun 2007 00:54:33 -0000\r\nReceived: (qmail 28578 invoked by uid 453); 28 Jun 2007 00:34:27 -0000\r\nReceived: from eddie.graemes.com (HELO eddie.graemes.com) (192.168.7.12)\n    by graemes.com (qpsmtpd/0.32) with ESMTP; Thu, 28 Jun 2007 01:34:27 +0100\r\nMessage-ID: &lt;46830212.3060309@...&gt;\r\nDate: Thu, 28 Jun 2007 01:34:26 +0100\r\nUser-Agent: Thunderbird 2.0.0.4 (X11/20070615)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;929285.50487.qm@...&gt; &lt;4682A47C.8050805@...&gt;\r\nIn-Reply-To: &lt;4682A47C.8050805@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------070202070603050706060009&quot;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;lists@...&quot; &lt;lists@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=204413223; y=aNvlo3xgo-4XSxZgU4lpKTrzDlIi1ic1HbyRok6J9Abq35R1p3JO\r\nX-Yahoo-Profile: GraemeSeaton\r\n\r\n\r\n--------------070202070603050706060009\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nHCC seems to offer a number of answers to this kind of configuration but \ngetting hold of documentation regarding it (try \nhttp://crawler.sourceforge.net/hcc ) is a slightly challenging.  Is HCC \nstill in active use? If so, can someone point me to a tad more doco.?\n\nAs an aside - How many toes should a Heritrix on a quad core/cpu system \nbe able to mange if it is assumed that memory/io/bandwidth are removed \nfrom the equation?\n\nRegards,\nGraeme\n\nGordon Mohr wrote:\n&gt;\n&gt; Jigar Patel wrote:\n&gt; &gt; Thanks a lot Gordon,\n&gt; &gt;\n&gt; &gt; You solved my problem.\n&gt; &gt;\n&gt; &gt; One more thing I want to know that,\n&gt; &gt; Can I use same settings for two different independent machines for \n&gt; distributed crawling ?\n&gt;\n&gt; Yes; that is the usual case for distributed crawling: each of the\n&gt; crawlers has the exact same initial configuration, *except* for the\n&gt; HashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to\n&gt; decline to process URIs mapped to the others.\n&gt;\n&gt; There is no automatic facility for sharing the settings; you must\n&gt; manually copy the order.xml (and possibly other files) between crawlers.\n&gt; Also, there is no automatic facility for cross-feeding the URIs each\n&gt; crawler rejects. You must watch the crawl.log (or diversions logs from\n&gt; the mapper) and decide if the URIs should be fed to the sibling crawlers.\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt; &gt; Please tell me how does it work and coordinate with different machine ?\n&gt; &gt;\n&gt; &gt; Thanks\n&gt; &gt;\n&gt; &gt; Jigar\n&gt; &gt;\n&gt; &gt; Gordon Mohr &lt;gojomo@... &lt;mailto:gojomo%40archive.org&gt;&gt; wrote:\n&gt; &gt; Jigar Patel wrote:\n&gt; &gt;&gt; Presently I am running two heritrix instances on the same machine on\n&gt; &gt;&gt; different port...\n&gt; &gt;\n&gt; &gt; In general, you only want to use distributed crawling, with URIs\n&gt; &gt; partitioned across separate cooperating crawlers, to spread a crawl \n&gt; over\n&gt; &gt; multiple independent machines. If using a single machine, a single\n&gt; &gt; crawler instance will be more efficient.\n&gt; &gt;\n&gt; &gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule\n&gt; &gt;&gt; I added HashCrawlMapper at two places as you suggested\n&gt; &gt;&gt; I made same configuration setting and seed file at each place.\n&gt; &gt;&gt;\n&gt; &gt;&gt; But as I run my job it gives me following error in seed file and\n&gt; &gt;&gt; nothing was crawled.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Heritrix(-5002)-Blocked by custom prefetch processor\n&gt; &gt;&gt;\n&gt; &gt;&gt; Please let me know why I am getting such error...\n&gt; &gt;&gt;\n&gt; &gt;&gt; Is anything missing ?\n&gt; &gt;\n&gt; &gt; This is the expected crawl.log result for URIs that are considered by a\n&gt; &gt; crawler, but mapped to be handled by one of the others in the group of\n&gt; &gt; crawlers. With a proper configuration, some but not all lines in your\n&gt; &gt; crawl.log will have this code.\n&gt; &gt;\n&gt; &gt; For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and\n&gt; &gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; \n&gt; of &#39;2&#39;.\n&gt; &gt;\n&gt; &gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, \n&gt; but\n&gt; &gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the\n&gt; &gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the\n&gt; &gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.\n&gt; &gt;\n&gt; &gt; - Gordon @ IA\n&gt; &gt;\n&gt; &gt;&gt; Regards,\n&gt; &gt;&gt;\n&gt; &gt;&gt; Jigar Patel\n&gt; &gt;&gt;\n&gt; &gt;&gt; --- In archive-crawler@yahoogroups.com \n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Gordon Mohr &lt;gojomo@...&gt;\n&gt; &gt;&gt; wrote:\n&gt; &gt;&gt;&gt; nt_bdr wrote:\n&gt; &gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n&gt; &gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we\n&gt; &gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600\n&gt; &gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8\n&gt; &gt;&gt; machines\n&gt; &gt;&gt;&gt; (&gt;1 billion URLs fetched).\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; For background see some previous threads including:\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2909 \n&gt; &lt;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&gt;\n&gt; &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/3060 \n&gt; &lt;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Roughly how we do it:\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; - Use BloomFilterUriUniqFilter with its defaults -- which devotes\n&gt; &gt;&gt;&gt; about 500MB to this structure and keeps the false-positive\n&gt; &gt;&gt; (mistakenly\n&gt; &gt;&gt;&gt; believed to have been previously-scheduled) rate under 1-in-4-\n&gt; &gt;&gt; million up\n&gt; &gt;&gt;&gt; through 125 million URIs discovered.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+\n&gt; &gt;&gt; heap\n&gt; &gt;&gt;&gt; - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in\n&gt; &gt;&gt; queues\n&gt; &gt;&gt;&gt; named by the reversed-host (com,example,) rather than usual host\n&gt; &gt;&gt;&gt; (example.com)\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor\n&gt; &gt;&gt; chain:\n&gt; &gt;&gt;&gt; * Once, immediately before the PreconditionEnforcer. This one\n&gt; &gt;&gt; has\n&gt; &gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any\n&gt; &gt;&gt; scheduled\n&gt; &gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)\n&gt; &gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has\n&gt; &gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any\n&gt; &gt;&gt; discovered\n&gt; &gt;&gt;&gt; outlinks before they are scheduled.)\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a\n&gt; &gt;&gt; number 0\n&gt; &gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and\n&gt; &gt;&gt; all\n&gt; &gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers,\n&gt; &gt;&gt; n).\n&gt; &gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the\n&gt; &gt;&gt; SURT\n&gt; &gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI\n&gt; &gt;&gt; is\n&gt; &gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to\n&gt; &gt;&gt; a\n&gt; &gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally.\n&gt; &gt;&gt;&gt; Depending on the character of your crawl, you may want to feed\n&gt; &gt;&gt; these\n&gt; &gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore\n&gt; &gt;&gt; them.\n&gt; &gt;&gt;&gt; The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key\n&gt; &gt;&gt; before\n&gt; &gt;&gt;&gt; mapping -- used to ensure that all subdomains of example.com are\n&gt; &gt;&gt; treated\n&gt; &gt;&gt;&gt; the same as example.com for mapping purposes. The first match of\n&gt; &gt;&gt; this\n&gt; &gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small\n&gt; &gt;&gt;&gt; example would be:\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything\n&gt; &gt;&gt; through\n&gt; &gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it\n&gt; &gt;&gt; uses\n&gt; &gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns\n&gt; &gt;&gt; that\n&gt; &gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs,\n&gt; &gt;&gt; like\n&gt; &gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level\n&gt; &gt;&gt;&gt; domains.)\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; - All crawlers are launched with the same configuration,\n&gt; &gt;&gt; including\n&gt; &gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate.\n&gt; &gt;&gt; Seeds\n&gt; &gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early\n&gt; &gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed\n&gt; &gt;&gt; are\n&gt; &gt;&gt;&gt; done so by an external process/scripts.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; - Gordon @ IA\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; Yahoo! Groups Links\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ---------------------------------\n&gt; &gt; Food fight? Enjoy some healthy debate\n&gt; &gt; in the Yahoo! Answers Food & Drink Q&A.\n&gt;\n&gt;  \n\r\n--------------070202070603050706060009\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta content=&quot;text/html;charset=ISO-8859-1&quot; http-equiv=&quot;Content-Type&quot;&gt;\n&lt;/head&gt;\n&lt;body bgcolor=&quot;#ffffff&quot; text=&quot;#000000&quot;&gt;\nHCC seems to offer a number of answers to this kind of configuration\nbut getting hold of documentation regarding it (try\n&lt;a class=&quot;moz-txt-link-freetext&quot; href=&quot;http://crawler.sourceforge.net/hcc&quot;&gt;http://crawler.sourceforge.net/hcc&lt;/a&gt; ) is a slightly challenging.&nbsp; Is HCC\nstill in active use? If so, can someone point me to a tad more doco.?&lt;br&gt;\n&lt;br&gt;\nAs an aside - How many toes should a Heritrix on a quad core/cpu system\nbe able to mange if it is assumed that memory/io/bandwidth are removed\nfrom the equation?&lt;br&gt;\n&lt;br&gt;\nRegards,&lt;br&gt;\nGraeme&lt;br&gt;\n&lt;br&gt;\nGordon Mohr wrote:\n&lt;blockquote cite=&quot;mid:4682A47C.8050805@...&quot; type=&quot;cite&quot;&gt;&lt;!-- Network content --&gt;\n\n  &lt;div id=&quot;ygrp-text&quot;&gt;\n  &lt;p&gt;Jigar Patel wrote:&lt;br&gt;\n&gt; Thanks a lot Gordon,&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; You solved my problem.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; One more thing I want to know that, &lt;br&gt;\n&gt; Can I use same settings for two different independent machines for\ndistributed crawling ?&lt;br&gt;\n  &lt;br&gt;\nYes; that is the usual case for distributed crawling: each of the &lt;br&gt;\ncrawlers has the exact same initial configuration, *except* for the &lt;br&gt;\nHashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to &lt;br&gt;\ndecline to process URIs mapped to the others.&lt;br&gt;\n  &lt;br&gt;\nThere is no automatic facility for sharing the settings; you must &lt;br&gt;\nmanually copy the order.xml (and possibly other files) between\ncrawlers. &lt;br&gt;\nAlso, there is no automatic facility for cross-feeding the URIs each &lt;br&gt;\ncrawler rejects. You must watch the crawl.log (or diversions logs from &lt;br&gt;\nthe mapper) and decide if the URIs should be fed to the sibling\ncrawlers.&lt;br&gt;\n  &lt;br&gt;\n- Gordon @ IA&lt;br&gt;\n  &lt;br&gt;\n&gt; Please tell me how does it work and coordinate with different\nmachine ?&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Thanks&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Jigar&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Gordon Mohr &lt;&lt;a moz-do-not-send=&quot;true&quot;\n href=&quot;mailto:gojomo%40archive.org&quot;&gt;gojomo@archive.&lt;wbr&gt;org&lt;/a&gt;&gt;\nwrote:&lt;br&gt;\n&gt; Jigar Patel wrote:&lt;br&gt;\n&gt;&gt; Presently I am running two heritrix instances on the same\nmachine on &lt;br&gt;\n&gt;&gt; different port...&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; In general, you only want to use distributed crawling, with URIs &lt;br&gt;\n&gt; partitioned across separate cooperating crawlers, to spread a\ncrawl over &lt;br&gt;\n&gt; multiple independent machines. If using a single machine, a single\n  &lt;br&gt;\n&gt; crawler instance will be more efficient.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule&lt;br&gt;\n&gt;&gt; I added HashCrawlMapper at two places as you suggested&lt;br&gt;\n&gt;&gt; I made same configuration setting and seed file at each place.&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt; But as I run my job it gives me following error in seed file\nand &lt;br&gt;\n&gt;&gt; nothing was crawled.&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt; Heritrix(-5002)&lt;wbr&gt;-Blocked by custom prefetch processor &lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt; Please let me know why I am getting such error...&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt; Is anything missing ?&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; This is the expected crawl.log result for URIs that are considered\nby a &lt;br&gt;\n&gt; crawler, but mapped to be handled by one of the others in the\ngroup of &lt;br&gt;\n&gt; crawlers. With a proper configuration, some but not all lines in\nyour &lt;br&gt;\n&gt; crawl.log will have this code.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; For example, for two crawlers, one should have the &#39;local-name&#39;\n&#39;0&#39; and &lt;br&gt;\n&gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39;\nof &#39;2&#39;.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to\n&#39;1&#39;, but &lt;br&gt;\n&gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in\nthe &lt;br&gt;\n&gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the &lt;br&gt;\n&gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; - Gordon @ IA&lt;br&gt;\n&gt; &lt;br&gt;\n&gt;&gt; Regards,&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt; Jigar Patel&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt; --- In &lt;a moz-do-not-send=&quot;true&quot;\n href=&quot;mailto:archive-crawler%40yahoogroups.com&quot;&gt;archive-crawler@&lt;wbr&gt;yahoogroups.&lt;wbr&gt;com&lt;/a&gt;,\nGordon Mohr &lt;a class=&quot;moz-txt-link-rfc2396E&quot; href=&quot;mailto:gojomo@...&quot;&gt;&lt;gojomo@...&gt;&lt;/a&gt; &lt;br&gt;\n&gt;&gt; wrote:&lt;br&gt;\n&gt;&gt;&gt; nt_bdr wrote:&lt;br&gt;\n&gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?&lt;br&gt;\n&gt;&gt;&gt; In a crude fashion, yes. It is more manual and less\ndynamic than we &lt;br&gt;\n&gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6\nmachines (&gt;600 &lt;br&gt;\n&gt;&gt;&gt; million URLs visited), and know of work elsewhere over up\nto 8 &lt;br&gt;\n&gt;&gt; machines &lt;br&gt;\n&gt;&gt;&gt; (&gt;1 billion URLs fetched).&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; For background see some previous threads including:&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; &lt;a moz-do-not-send=&quot;true&quot;\n href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/2909&lt;/a&gt;&lt;br&gt;\n&gt;&gt;&gt; &lt;a moz-do-not-send=&quot;true&quot;\n href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/3060&lt;/a&gt;&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; Roughly how we do it:&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; - Use BloomFilterUriUniqF&lt;wbr&gt;ilter with its defaults --\nwhich devotes &lt;br&gt;\n&gt;&gt;&gt; about 500MB to this structure and keeps the false-positive\n  &lt;br&gt;\n&gt;&gt; (mistakenly &lt;br&gt;\n&gt;&gt;&gt; believed to have been previously-schedule&lt;wbr&gt;d) rate\nunder 1-in-4-&lt;br&gt;\n&gt;&gt; million up &lt;br&gt;\n&gt;&gt;&gt; through 125 million URIs discovered.&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with\n~1.8GB+ &lt;br&gt;\n&gt;&gt; heap&lt;br&gt;\n&gt;&gt;&gt; - Use SurtAuthorityAssign&lt;wbr&gt;mentPolicy, so URIs are\ngrouped in &lt;br&gt;\n&gt;&gt; queues &lt;br&gt;\n&gt;&gt;&gt; named by the reversed-host (com,example,&lt;wbr&gt;) rather than\nusual host &lt;br&gt;\n&gt;&gt;&gt; (example.com)&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the\nprocessor &lt;br&gt;\n&gt;&gt; chain:&lt;br&gt;\n&gt;&gt;&gt; * Once, immediately before the PreconditionEnforce&lt;wbr&gt;r.\nThis one &lt;br&gt;\n&gt;&gt; has &lt;br&gt;\n&gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts\nany &lt;br&gt;\n&gt;&gt; scheduled &lt;br&gt;\n&gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly\nseeds.)&lt;br&gt;\n&gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This\none has &lt;br&gt;\n&gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts\nany &lt;br&gt;\n&gt;&gt; discovered &lt;br&gt;\n&gt;&gt;&gt; outlinks before they are scheduled.)&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a\n  &lt;br&gt;\n&gt;&gt; number 0 &lt;br&gt;\n&gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per\nmachine, and &lt;br&gt;\n&gt;&gt; all &lt;br&gt;\n&gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of\ncrawlers, &lt;br&gt;\n&gt;&gt; n).&lt;br&gt;\n&gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here,\nthe &lt;br&gt;\n&gt;&gt; SURT &lt;br&gt;\n&gt;&gt;&gt; authority part, because of the above choice -- and decides\nif a URI &lt;br&gt;\n&gt;&gt; is &lt;br&gt;\n&gt;&gt;&gt; handled by the current crawler or one of its siblings. If\nmapped to &lt;br&gt;\n&gt;&gt; a &lt;br&gt;\n&gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled\nlocally. &lt;br&gt;\n&gt;&gt;&gt; Depending on the character of your crawl, you may want to\nfeed &lt;br&gt;\n&gt;&gt; these &lt;br&gt;\n&gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to\nignore &lt;br&gt;\n&gt;&gt; them.&lt;br&gt;\n&gt;&gt;&gt; The &#39;reduce-prefix-&lt;wbr&gt;pattern&#39; may be used to trim the\nqueue key &lt;br&gt;\n&gt;&gt; before &lt;br&gt;\n&gt;&gt;&gt; mapping -- used to ensure that all subdomains of\nexample.com are &lt;br&gt;\n&gt;&gt; treated &lt;br&gt;\n&gt;&gt;&gt; the same as example.com for mapping purposes. The first\nmatch of &lt;br&gt;\n&gt;&gt; this &lt;br&gt;\n&gt;&gt;&gt; pattern, if present, is what is used for mapping purposes.\nA small &lt;br&gt;\n&gt;&gt;&gt; example would be:&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*&lt;wbr&gt;)|[&#92;w,]{9}&lt;wbr&gt;)&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything\n  &lt;br&gt;\n&gt;&gt; through &lt;br&gt;\n&gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything\nelse, it &lt;br&gt;\n&gt;&gt; uses &lt;br&gt;\n&gt;&gt;&gt; the first 9 characters. You could imagine more complicated\npatterns &lt;br&gt;\n&gt;&gt; that &lt;br&gt;\n&gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter\nTLDs, &lt;br&gt;\n&gt;&gt; like &lt;br&gt;\n&gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign\n3rd-level &lt;br&gt;\n&gt;&gt;&gt; domains.)&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; - All crawlers are launched with the same configuration, &lt;br&gt;\n&gt;&gt; including &lt;br&gt;\n&gt;&gt;&gt; the same seeds, but otherwise do not (themselves)\ncommunicate. &lt;br&gt;\n&gt;&gt; Seeds &lt;br&gt;\n&gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by\nthe early &lt;br&gt;\n&gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be\ncross-fed &lt;br&gt;\n&gt;&gt; are &lt;br&gt;\n&gt;&gt;&gt; done so by an external process/scripts.&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&gt; - Gordon @ IA&lt;br&gt;\n&gt;&gt;&gt;&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt; Yahoo! Groups Links&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt;&gt;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; ------------&lt;wbr&gt;---------&lt;wbr&gt;---------&lt;wbr&gt;---&lt;br&gt;\n&gt; Food fight? Enjoy some healthy debate&lt;br&gt;\n&gt; in the Yahoo! Answers Food &amp; Drink Q&amp;A.&lt;br&gt;\n  &lt;br&gt;\n  &lt;/p&gt;\n  &lt;/div&gt;\n\n&lt;!--End group email --&gt;&lt;/blockquote&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------070202070603050706060009--\r\n\n"}}