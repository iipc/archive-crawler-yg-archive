{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"3FneOwKuGcGu9WZJI9rAhffJWV3cAANsMEJqzhTJ6_rFSfl-X0fCoj1NTV8Dof2D3E5sPt2SlRfHKRKljBKR37CsQupLbXNcwsBQqbM7","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Experience on 1B documents crawl","postDate":"1149319076","msgId":2909,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU1cmQzNCtwMzNrQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":2910,"prevInTime":2908,"nextInTime":2910,"topicId":2909,"numMessagesInTopic":2,"msgSnippet":"We just finished a large scale crawl using Heritrix. We ve crawled 1 billion URLs within 3 months. We d like to share with the list on the crawling experience.","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 40640 invoked from network); 3 Jun 2006 07:18:22 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m28.grp.scd.yahoo.com with QMQP; 3 Jun 2006 07:18:22 -0000\r\nReceived: from unknown (HELO n15a.bullet.sc5.yahoo.com) (66.163.187.158)\n  by mta8.grp.scd.yahoo.com with SMTP; 3 Jun 2006 07:18:22 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.163.187.123] by n15.bullet.sc5.yahoo.com with NNFMP; 03 Jun 2006 07:17:58 -0000\r\nReceived: from [66.218.69.5] by t4.bullet.sc5.yahoo.com with NNFMP; 03 Jun 2006 07:17:58 -0000\r\nReceived: from [66.218.66.91] by t5.bullet.scd.yahoo.com with NNFMP; 03 Jun 2006 07:17:58 -0000\r\nDate: Sat, 03 Jun 2006 07:17:56 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;e5rd34+p33k@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Experience on 1B documents crawl\r\nX-Yahoo-Group-Post: member; u=132996324; y=4AQfAxB_dRzn3ldv16mz2QGl9pmavRYuwQgMP5xfFinw8stJYg\r\nX-Yahoo-Profile: joehung302\r\n\r\nWe just finished a large scale crawl using Heritrix. We&#39;ve crawled 1\nbillio=\r\nn URLs within 3 months. We&#39;d like to share with the list on the\ncrawling ex=\r\nperience.\n\n1. Hardware & Bandwidth\n- 8 crawler machines. Each machine is a =\r\n2 dual-core AMD with 4GB\nmemory, CentOS 4.2 x86_64. Every crawler is equipp=\r\ned with 4TB storage\nat RAID 5.\n- 100Mbps bandwidth for 3 months.\n- Extra 40=\r\nTB storage array to hold crawl data. \n\n2. Heritrix configuration\n- We use 3=\r\n2-bit JVM, despite that OS is running at 64-bit, client\nmode. (server mode =\r\nconsistent gave us HotSpot errors).\n- Heritrix 1.6.0, some key parameters:\n=\r\n  . total-bandwidth-usage-KB-sec: 1100KB\n  . BloomUriUniqFilter: default se=\r\ntting\n  . SurtAuthorityQueueAssignmentPolicy\n  . bdb-cache-percent: 20 (Thi=\r\ns turns out to be key in memory usage)\n  . checkpoint-copy-bdbje-logs: fals=\r\ne (ie, &quot;fast&quot; checkpoint. The\nusual checkpoint is not practical for large c=\r\nrawls).\n\n- We use CrawlMap to do split crawl, with either SurtScope or\nBroa=\r\ndScope (more on when to use SurtSope/BroadScope below).\n- We wrote several =\r\nshell scripts to &quot;control&quot; Heritrix through JMX. We\nrarely use the web cons=\r\nole. ;)\n\n3. Process\n- Start with 0.25 million seeds from dmoz.org\n- The fir=\r\nst is &quot;.com&quot; crawl. We configure all 8 crawlers to crawl\n&quot;.com&quot; with SurtPr=\r\nefixScope + CrawlMap. \n- The 2nd is &quot;.edu&quot; crawl. Again the configuration i=\r\ns SurtPrefixScope\n+ CrawlMap.\n- The 3rd is &quot;dot everything else&quot;. This is d=\r\none using BroadScope +\nCrawlMap with a couple exclude filters to filter out=\r\n .com and .edu.\n\n4. The results\n- We were able to collect 500M documents fr=\r\nom .com, and 250M from\n.edu, and 250M from everything else. \n- A few times =\r\nthe crawlers were not able to checkpoint due to some\nmalformed documents. W=\r\ne were able to recover the crawl from previous\ncheckpoints.\n- Phone calls f=\r\nrom web masters...\n\nBig thanks to the Heritrix team, keep up the great work=\r\n!\n\n\n\n\n\n"}}