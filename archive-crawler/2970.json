{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"lo8GqJNlujGy_6ct6ZVBOBCgrnwTogmVUQneDcc4d2sqKbPQBIvgwuBoJLfDJ4CFGaPLOKyprRbT8OvmghnuCBR1yERYtVE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Limiting crawl to single host/domain","postDate":"1151012096","msgId":2970,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0OUIwRDAwLjgwNjA5MDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ0OUFFNDU5LjQwMDA2QGNzLm9kdS5lZHU+","referencesHeader":"PGU3OXFrbStpZnZtQGVHcm91cHMuY29tPiA8ZmVhZGI1NWMwNjA2MjAyMTUxeTJhMDdhZGQ4b2U4MWIyZmMxYWNiYTdiNDZAbWFpbC5nbWFpbC5jb20+IDw0NDk5OUQ3Qy4yMDQwNjA3QGNzLm9kdS5lZHU+IDw0NDlBRTQ1OS40MDAwNkBjcy5vZHUuZWR1Pg=="},"prevInTopic":2969,"nextInTopic":2971,"prevInTime":2969,"nextInTime":2971,"topicId":2953,"numMessagesInTopic":11,"msgSnippet":"... With the legacy/ classic scopes (PathScope/HostScope/DomainScope), the transitive include (as governed by trans-hops ) is how necessary prerequisites","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 56745 invoked from network); 22 Jun 2006 21:33:57 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m37.grp.scd.yahoo.com with QMQP; 22 Jun 2006 21:33:57 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta5.grp.scd.yahoo.com with SMTP; 22 Jun 2006 21:33:57 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 47E0B1415661C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 22 Jun 2006 14:33:59 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 26894-01-21 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 22 Jun 2006 14:33:58 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id AEDF3141564E1\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 22 Jun 2006 14:33:58 -0700 (PDT)\r\nMessage-ID: &lt;449B0D00.8060903@...&gt;\r\nDate: Thu, 22 Jun 2006 14:34:56 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e79qkm+ifvm@...&gt; &lt;feadb55c0606202151y2a07add8oe81b2fc1acba7b46@...&gt; &lt;44999D7C.2040607@...&gt; &lt;449AE459.40006@...&gt;\r\nIn-Reply-To: &lt;449AE459.40006@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Limiting crawl to single host/domain\r\nX-Yahoo-Group-Post: member; u=137285340; y=2gL3t8D9kOVrD30158sMx4q7rLCFPQHRc4FRNJcu2qXF\r\nX-Yahoo-Profile: gojomo\r\n\r\nFrank McCown wrote:\n&gt; I would like to use a single seed URL of\n&gt; \n&gt; http://www.cs.odu.edu/~fmccown/webdev/\n&gt; \n&gt; and have only resources with URLs that match\n&gt; \n&gt; http://www.cs.odu.edu/~fmccown/webdev/*\n&gt; \n&gt; be crawled.  I&#39;m using PathScope but getting &quot;transitive includes&quot; \n&gt; (images, etc.) from other hosts in my crawls.\n&gt; \n&gt; I saw this thread:\n&gt; \n&gt; http://groups.yahoo.com/group/archive-crawler/message/2552?threaded=1&var=1&p=3\n&gt; \n&gt; and what I can gather from it, I should be able to set max-trans-hops=0 \n&gt; to solve the problem.  This does not work though.  I get the single \n&gt; crawl log entry:\n&gt; \n&gt; 2006-06-22T18:33:00.461Z   -63          - \n&gt; http://www.cs.odu.edu/~fmccown/webdev/ - - no-type #003 - - - -\n&gt; \n&gt; Any ideas?\n\nWith the legacy/&#39;classic&#39; scopes (PathScope/HostScope/DomainScope), the \n&#39;transitive include&#39; (as governed by &#39;trans-hops&#39;) is how necessary \nprerequisites get allowed: neither &#39;dns:www.cs.odu.edu&#39; nor \n&#39;http://www.cs.odu.edu/robots.txt&#39; are trivially extensions of your seed \nURL, but they both must be fetched before your seed URL (and deeper \nURLs) are fetched. So, when you try to get the effect you want with a \n&#39;max-trans-hops&#39; of &#39;0&#39;, you get the -63 (&quot;prerequisite failed: \nunschedulable&quot;) error.\n\nMy recommendation for specifying what you want to crawl is to use the \nDecidingScope with the following rules:\n\n  RejectAllDecideRule (by default)\n  SurtPrefixedDecideRule (ACCEPT, use SURTs implied by seeds)\n  PrerequisiteDecideRule\n\nAll URLs will be by default REJECTed, but then those that are either \nextensions of the SURT prefix implied by your seed or prerequisites will \nbe ACCEPTed. (If you later want transitively-accept discovered URIs \noutside the main focus or reject bad paths, etc., you could by adding \nmore rules to the chain.)\n\n(Your seed turns into the SURT-form URI...\n\n   http://(edu,odu,cs,www,)/~fmccown/webdev/\n\n...which if used as a prefix for testing all other SURT-form URIs is \nequivalent to your shell-glob form...\n\n   http://www.cs.odu.edu/~fmccown/webdev/*\n\nThe point of SURT forms is to turn all URI patterns matching the natural \nsubdomain/subdirectory heirarchy into strict prefix matches, rather than \nglob patterns or regexes with internal expansions. This is especially \nimportant if you have hundreds or thousands of prefixes rather than \npatterns: you can then keep them sorted and test for inclusion of a \nmatching prefix with a single lookup-nearest-then-prefix-compare instead \nof a series of pattern-matches. )\n\nHope this helps,\n\n- Gordon @ IA\n\nPS: One other FYI: I know you wrote a tool to crawl the Wayback Machine \na while back; we&#39;re finally about to change the various public WM error \npages to return non-200 HTTP error codes, in the next few days. So keep \nthat in mind if you were using some other way to detect WM errors.\n\n\n"}}