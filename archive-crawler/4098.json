{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":305432243,"authorName":"louisleiyu","from":"&quot;louisleiyu&quot; &lt;yul@...&gt;","profile":"louisleiyu","replyTo":"LIST","senderId":"IdqaiXqtdkvWuguh3tYZuw03U_si-RF2smLD47U9nZY7D57a0UJni4tSiC_s2KzQ34PLPyr2h62tnoKyoqS1-ZU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Constructing a web graph","postDate":"1176329969","msgId":4098,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGV2am10aCtxMnBwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ2MUQ1QUQzLjEwMTA2MDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":4097,"nextInTopic":4099,"prevInTime":4097,"nextInTime":4099,"topicId":4059,"numMessagesInTopic":8,"msgSnippet":"After I read the documentation, from my understand, heretrix achieves filter of duplicated pages by basically keeping a hash of already encountered urls; thus","rawEmail":"Return-Path: &lt;yul@...&gt;\r\nX-Sender: yul@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 73891 invoked from network); 11 Apr 2007 22:20:47 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m37a.grp.scd.yahoo.com with QMQP; 11 Apr 2007 22:20:47 -0000\r\nReceived: from unknown (HELO n9b.bullet.sp1.yahoo.com) (69.147.64.101)\n  by mta13.grp.scd.yahoo.com with SMTP; 11 Apr 2007 22:20:47 -0000\r\nReceived: from [216.252.122.216] by n9.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 22:19:30 -0000\r\nReceived: from [66.218.69.3] by t1.bullet.sp1.yahoo.com with NNFMP; 11 Apr 2007 22:19:30 -0000\r\nReceived: from [66.218.66.82] by t3.bullet.scd.yahoo.com with NNFMP; 11 Apr 2007 22:19:30 -0000\r\nDate: Wed, 11 Apr 2007 22:19:29 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;evjmth+q2pp@...&gt;\r\nIn-Reply-To: &lt;461D5AD3.1010602@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;louisleiyu&quot; &lt;yul@...&gt;\r\nSubject: Re: Constructing a web graph\r\nX-Yahoo-Group-Post: member; u=305432243; y=zbwFR2Uy2HH3a8K0OY-jj93hSRr8TOC5QqNVfewXnCot56ybSQ\r\nX-Yahoo-Profile: louisleiyu\r\n\r\nAfter I read the documentation, from my understand, heretrix \nachieves filt=\r\ner of duplicated pages by basically keeping a hash of \nalready encountered =\r\nurls; thus any newly encountered url is compared \nagainst the hash and it m=\r\natches, it doesn&#39;t get recorded in the log, \nand it doesn&#39;t give birth to c=\r\nhildren.\n\ni wonder if I can change the writer processor so it DOES get \nrec=\r\norded in the log; other than that the rest are the same (repeated \nusl are =\r\nstill not allowed to give birth to children)\n\nLou\n\n--- In archive-crawler@y=\r\nahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; \nwrote:\n&gt;\n&gt; If I understand correc=\r\ntly, this analysis requires even the \nredundant \n&gt; discovered links to be s=\r\naved somewhere, since they are not \nscheduled for \n&gt; redundant visitation w=\r\nithin the same crawl.\n&gt; \n&gt; One option would be to perform a post-crawl anal=\r\nysis on your ARC \nfiles \n&gt; to re-extract the links.\n&gt; \n&gt; Another would be t=\r\no insert a processor that logs the outlinks \nbefore \n&gt; scoping and/or sched=\r\nuling have whittled them down.\n&gt; \n&gt; The current experimental WARC-writing p=\r\nrocessor uses the \ndiscovered \n&gt; outlinks as example metadata for the new &#39;=\r\nmetadata&#39; record, so a \nside \n&gt; effect of its operation is to save aside th=\r\ne data you want. Both \nthe \n&gt; format of WARCs and the content of specific r=\r\necords is going to \nchange, \n&gt; so I can&#39;t recommend depending on the curren=\r\nt processor for \n&gt; functionality, but it may provide a model for other code=\r\n to save \naside \n&gt; this info.\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; Andrea Goethals wrote=\r\n:\n&gt; &gt; If I understand the original post correctly - this is something \nwe a=\r\nlso \n&gt; &gt; want\n&gt; &gt; to implement (logging of &quot;intra&quot;-harvest duplicates not \n=\r\ndownloaded).\n&gt; &gt; \n&gt; &gt; The heritrix 1.12 supports deduping *between* crawls =\r\nbut older \nheritrixs\n&gt; &gt; dedupe *within* crawls already. Ideally we would l=\r\nike to see \nlogging \n&gt; &gt; options\n&gt; &gt; for both kinds of deduplication. We wa=\r\nnt this intra-harvest \ndedupe logging\n&gt; &gt; so that we can know all the paren=\r\nts seen for downloaded \nresources - not \n&gt; &gt; just\n&gt; &gt; the one first parent =\r\nthat currently gets logged in crawl.log.\n&gt; &gt; \n&gt; &gt; I haven&#39;t yet looked into=\r\n where the extra logging should go - \njust \n&gt; &gt; wanted to\n&gt; &gt; add to this t=\r\nhread that this is something we want too & are \nwilling to\n&gt; &gt; implement (i=\r\nf there&#39;s not already a way to log this) because it \nwill effect\n&gt; &gt; how we=\r\n implement our harvest q/a and takedown request handling.\n&gt; &gt; \n&gt; &gt; Andrea\n&gt;=\r\n &gt; \n&gt; &gt; On 11 Apr 2007 06:12:44 -0700, mbarlotta &lt;barlotta_michael@...&gt; \nwr=\r\note:\n&gt; &gt;&gt;\n&gt; &gt;&gt;   &gt; I&#39;ve tried playing around with the setting for herdrix \n=\r\n1.1.2 but I&#39;m\n&gt; &gt;&gt; &gt; getting nowhere; I&#39;ve read that older version of hered=\r\nrix \ndoes not\n&gt; &gt;&gt; &gt; have the ability to filter out duplicate pages, so per=\r\nhaps I \nshould\n&gt; &gt;&gt; &gt; try older versions of heredrix?\n&gt; &gt;&gt;\n&gt; &gt;&gt; The current=\r\n version of Heritrix does not dedupe pages by \ndefault you\n&gt; &gt;&gt; would have =\r\nto configure the job with additional processors to \nget it to\n&gt; &gt;&gt; do that.=\r\n If you crawl sites without dedupe you get what ever \nyour\n&gt; &gt;&gt; decide rule=\r\ns allow.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Read more about it here:\n&gt; &gt;&gt; \nhttp://webteam.archive.or=\r\ng/confluence/display/Heritrix/Feature+Notes+\n-\n&gt; &gt;&gt; +1.12.0\n&gt; &gt;&gt;\n&gt; &gt;&gt; What =\r\nare you using to visualize your Web Graph?\n&gt; &gt;&gt;\n&gt; &gt;&gt; HTH,\n&gt; &gt;&gt; Mike\n&gt; &gt;&gt;\n&gt; =\r\n&gt;&gt;  \n&gt; &gt;&gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}