{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":328711374,"authorName":"Robert Svoboda","from":"Robert Svoboda &lt;svobodar@...&gt;","profile":"r080tic","replyTo":"LIST","senderId":"n0UVuVN_edXGJYhrLTTuu0z5xy-QsrGcyCIVLyJk7Ey8uDshsmtF9C9_1j6iZ38UtquWeo93KsiCwPr0n1kbcxQ6IdURGm4","spamInfo":{"isSpam":false,"reason":"3"},"subject":"crawl scope questions","postDate":"1195826122","msgId":4743,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PDIwMDcxMTIzMTM1NTIyLkdGNTIxOEBzbXRwLmNhc2FibGFuY2EuY3o+"},"prevInTopic":0,"nextInTopic":0,"prevInTime":4742,"nextInTime":4744,"topicId":4743,"numMessagesInTopic":1,"msgSnippet":"Hi list, I m trying to crawl RSS feeds which are then saved to ARC files. Typical larger crawls (domain or more domains) are going well. Anyway I have some","rawEmail":"Return-Path: &lt;svobodar@...&gt;\r\nReceived: (qmail 61866 invoked from network); 23 Nov 2007 23:25:01 -0000\r\nReceived: from unknown (66.218.67.95)\n  by m36.grp.scd.yahoo.com with QMQP; 23 Nov 2007 23:25:01 -0000\r\nReceived: from unknown (HELO n41b.bullet.mail.sp1.yahoo.com) (66.163.168.155)\n  by mta16.grp.scd.yahoo.com with SMTP; 23 Nov 2007 23:25:01 -0000\r\nReceived: from [216.252.122.219] by n41.bullet.mail.sp1.yahoo.com with NNFMP; 23 Nov 2007 23:25:01 -0000\r\nReceived: from [66.218.69.6] by t4.bullet.sp1.yahoo.com with NNFMP; 23 Nov 2007 23:25:01 -0000\r\nReceived: from [66.218.66.75] by t6.bullet.scd.yahoo.com with NNFMP; 23 Nov 2007 23:25:01 -0000\r\nX-Sender: svobodar@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 64189 invoked from network); 23 Nov 2007 13:55:54 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m57.grp.scd.yahoo.com with QMQP; 23 Nov 2007 13:55:54 -0000\r\nX-Received: from unknown (HELO smtp.casablanca.cz) (217.11.225.65)\n  by mta18.grp.scd.yahoo.com with SMTP; 23 Nov 2007 13:55:54 -0000\r\nX-Received: from [193.179.157.94] (helo=rsv)\n\tby smtp.casablanca.cz with esmtpa (Exim 4.63)\n\t(envelope-from &lt;svobodar@...&gt;)\n\tid 1IvZ0C-0002ef-Pc\n\tfor archive-crawler@yahoogroups.com; Fri, 23 Nov 2007 14:55:28 +0100\r\nDate: Fri, 23 Nov 2007 14:55:22 +0100\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;20071123135522.GF5218@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Disposition: inline\r\nUser-Agent: Mutt/1.5.16 (2007-06-11)\r\nX-smtp-casablanca-cz-MailScanner: Found to be clean\r\nX-smtp-casablanca-cz-MailScanner-SpamCheck: not spam,\n\tSpamAssassin (not cached, score=-1.44, required 7,\n\tautolearn=disabled, ALL_TRUSTED -1.44)\r\nX-smtp-casablanca-cz-MailScanner-From: svobodar@...\r\nX-Spam-Status: No\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Robert Svoboda &lt;svobodar@...&gt;\r\nSubject: crawl scope questions\r\nX-Yahoo-Group-Post: member; u=328711374; y=nHNVQ-Y1uwSnvdabgwJoKqkR6jsv1uKmizmysG-x7zeeVw\r\nX-Yahoo-Profile: r080tic\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 23 Nov 2007 23:25:00 -0000\r\n\r\nHi list,\n\nI&#39;m trying to crawl RSS feeds which are then saved to ARC\nfiles. Typical larger crawls (domain or more domains) are\ngoing well. Anyway I have some trouble configuring following\ntwo scenarios:\n\n1. Direct feed crawl\n\nSay we have 1 or more direct URIs pointing to feeds. I want\nheritrix to take these addresses, download the feeds, save\nthem to ARC. End of the job.\n\n2. Page containing links to feeds\n\nThere are sites which provide all their feeds on one page. I&#39;d\nlike to enter such page as seed and let heritrix follow these\nlinks used on this page, no further. These feeds are again\nsaved to ARC.\n\nI have ARC writer configured properly as it already works in\nother scenarios.\n\nThanks,\nRobert\n\n"}}