{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"1yaODM1HSlA2olS9QlSXrpAcDCX5c5m7qEhFqDb1t6TJ-rK4HHoo5R2YOr4CxCAKmaTTMEP5NY0vvAefrRB2W-AHEt1Dy88","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Downloading PDF files from a site using heritrix 3.0 [2 Attachments]","postDate":"1292178650","msgId":6901,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREMDUxNERBLjMwMjA2MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEFBTkxrVGlrOHRtVjE5PUgxT3RLcUpaS0dGd18rd3NiU0NfdV9CaENjT21OMEBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGlrcENNWlZpUXlUZVQ1bVZRcl9jbWcxSjdKcjJ1OWNCdkJIUjRuZEBtYWlsLmdtYWlsLmNvbT4JPDRDRkVBMDc1LjQwNTAxMDJAYXJjaGl2ZS5vcmc+CTxBQU5Ma1RpbjlTN04ycz1DVkZSdXRucFhaWnNRUjRoWVlzR2hWNDhTRC01Q3JAbWFpbC5nbWFpbC5jb20+CTw0RDAxNzZCNy43MDcwMTA3QGFyY2hpdmUub3JnPiA8QUFOTGtUaWs4dG1WMTk9SDFPdEtxSlpLR0Z3Xyt3c2JTQ191X0JoQ2NPbU4wQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":6898,"nextInTopic":6902,"prevInTime":6900,"nextInTime":6902,"topicId":6866,"numMessagesInTopic":7,"msgSnippet":"Thanks for sending the configuration file; that helps focus in on the problem right away. You are using this URI as your seed: http://bip.kprm.gov.pl/kprm/ As","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 78794 invoked from network); 12 Dec 2010 18:30:53 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m11.grp.re1.yahoo.com with QMQP; 12 Dec 2010 18:30:53 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta2.grp.re1.yahoo.com with SMTP; 12 Dec 2010 18:30:53 -0000\r\nX-Received: (qmail 19443 invoked by uid 0); 12 Dec 2010 18:30:51 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay02.pair.com with SMTP; 12 Dec 2010 18:30:51 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4D0514DA.3020607@...&gt;\r\nDate: Sun, 12 Dec 2010 10:30:50 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.12) Gecko/20101027 Thunderbird/3.1.6\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Maciej Grela &lt;maciej.grela@...&gt;\r\nReferences: &lt;AANLkTikpCMZViQyTeT5mVQr_cmg1J7Jr2u9cBvBHR4nd@...&gt;\t&lt;4CFEA075.4050102@...&gt;\t&lt;AANLkTin9S7N2s=CVFRutnpXZZsQR4hYYsGhV48SD-5Cr@...&gt;\t&lt;4D0176B7.7070107@...&gt; &lt;AANLkTik8tmV19=H1OtKqJZKGFw_+wsbSC_u_BhCcOmN0@...&gt;\r\nIn-Reply-To: &lt;AANLkTik8tmV19=H1OtKqJZKGFw_+wsbSC_u_BhCcOmN0@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Downloading PDF files from a site using heritrix\n 3.0 [2 Attachments]\r\nX-Yahoo-Group-Post: member; u=137285340; y=WbhIcwES0N74SQbUDf-eSdCzq5a3R4JUBifsNKwJr1WB\r\nX-Yahoo-Profile: gojomo\r\n\r\nThanks for sending the configuration file; that helps focus in on the \nproblem right away.\n\nYou are using this URI as your seed:\n\nhttp://bip.kprm.gov.pl/kprm/\n\nAs a result, the automatic crawler deduction of where you want the crawl \nto go assumes you only want URIs that are extensions of that URI. An \nexample PDF that wasn&#39;t retrieved actually starts with a different URI \npath-component:\n\nhttp://bip.kprm.gov.pl/kprm/      ...versus...\n|||||||||||||||||||||||XXXXX\nhttp://bip.kprm.gov.pl/g2/2010_11/3678_fileot.pdf\n\nSome ways to remedy would include:\n\n(1) Adding the site&#39;s root page as an actual starting seed:\n\nhttp://bip.kprm.gov.pl/\n\n(2) Adding a special &#39;prefix directive&#39; to tell the crawl you want to \naccept other URIs. These can be mixed alongside your seed URIs, but \nbegin with a &#39;+&#39;. For example:\n\n+http://bip.kprm.gov.pl/g2/\n\n...would allow all URIs on that host beginning with the &#39;/g2/&#39; path. Or, \na directive of...\n\n+http://bip.kprm.gov.pl/\n\n...would rule-in all URIs discovered on that host, without necessarily \nvisiting that URI directly. (In all liklihood, another page on the site \n*does* link to that root URI, so in practice, this directive is nearly \nequivalent to supplying that root URI as a seed.)\n\nIt&#39;s not required that Heritrix deduce scope rules from seeds -- it&#39;s a \nconfigurable option of the SurtPrefixedDecideRule whether it scans the \nseeds for implied scoping, or uses an explicit list provided by the \noperator, or both.\n\nBut, using the seeds usually matches the intuitive understanding of \ncrawl operators (especially when supplying mostly root-page-URI seeds), \nand spares them having to supply extra configuration, so it&#39;s the default.\n\nHope this helps,\n\n- Gordon @ IA\n\n\nOn 12/11/10 4:27 PM, Maciej Grela wrote:\n&gt; [Attachment(s) &lt;#TopText&gt; from Maciej Grela included below]\n&gt;\n&gt; 2010/12/10 Gordon Mohr &lt;gojomo@... &lt;mailto:gojomo@...&gt;&gt;\n&gt;\n&gt;     Did your crawl of the site finish cleanly? (I just tried a test\n&gt;     crawl of\n&gt;     the same site, and many of the fetch attempts are timing out -- which\n&gt;     could be network/server issues, or the site&#39;s own attempts to\n&gt;     slow/block\n&gt;     crawling.)\n&gt;\n&gt; Yes.\n&gt;\n&gt;     Are there any errors in the crawl.log or nonfatal-errors.log that refer\n&gt;     to the URIs of interest?\n&gt;\n&gt;   No. To be sure I&#39;ve ran the crawl again today (attached\n&gt; crawler-beans.xml and crawl.log).\n&gt;\n&gt;\n&gt;     http://bip.kprm.gov.pl/g2/2010_11/3678_fileot.pdf\n&gt;\n&gt;     However, currently many timeouts on page fetches are preventing my test\n&gt;     crawl from reaching that PDF. Perhaps the same thing is happening for\n&gt;     your crawls, and the proper fix is to both make the crawler more\n&gt;     patient, and wait longer for the crawl to finish.\n&gt;\n&gt;\n&gt; Where to check for timeouts ? Haven&#39;t found any hints of them happening\n&gt; in the logfiles generated for this crawl.\n&gt;\n&gt;\n&gt;     So, you may want to adjust the FetchHTTP &#39;soTimeoutMs&#39; setting, to wait\n&gt;     longer than 20000ms for a socket reply. (I&#39;ve increased it to 60000 and\n&gt;     I&#39;m still seeing timeouts form the site.) Setting &#39;timeoutSeconds&#39; to\n&gt;     more than 1200 (20 minutes) may also be necessary.\n&gt;\n&gt;     Hope this helps,\n&gt;\n&gt;     - Gordon @ IA\n&gt;\n&gt;\n&gt;\n&gt; Tried to run with timeoutSeconds to 2400 and soTimeoutMs to 60000 but\n&gt; the results are the same.\n&gt;\n&gt; Best regards,\n&gt; Maciej Grela\n&gt;\n&gt;\n&gt; Attachment(s) from Maciej Grela\n&gt;\n&gt; 2 of 2 File(s)\n&gt;\n&gt; crawl.log &lt;http://xa.yimg.com/kq/groups/8759867/1221242207/name/crawl%2Elog&gt;\n&gt; crawler-beans.cxml\n&gt; &lt;http://xa.yimg.com/kq/groups/8759867/1268202320/name/crawler-beans%2Ecxml&gt;\n&gt;\n&gt; \n\n"}}