{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":194872127,"authorName":"hinoglu","from":"&quot;hinoglu&quot; &lt;hinoglu@...&gt;","profile":"hinoglu","replyTo":"LIST","senderId":"Cp4Wjq_UPMxGks6jx2n3vPYG943rfol7AnvXjeOIleTvtogmDc9rCxmdNadI3O7QTmGfm3aSYKuGMSeAATaZi-H3bqA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: java.lang.OutOfMemoryError: GC overhead limit exceeded","postDate":"1193838322","msgId":4644,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZnYTB0aStlczNxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ3Mjc5MTNBLjIwMDAzMDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":4642,"nextInTopic":4664,"prevInTime":4643,"nextInTime":4645,"topicId":4629,"numMessagesInTopic":9,"msgSnippet":"... which ... current setup is running on a single machine for demontration purposes, in the future there will be around hundreds of sites to be crawled. none","rawEmail":"Return-Path: &lt;hinoglu@...&gt;\r\nX-Sender: hinoglu@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 35068 invoked from network); 31 Oct 2007 13:45:27 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m43.grp.scd.yahoo.com with QMQP; 31 Oct 2007 13:45:27 -0000\r\nX-Received: from unknown (HELO n11b.bullet.sp1.yahoo.com) (69.147.64.107)\n  by mta17.grp.scd.yahoo.com with SMTP; 31 Oct 2007 13:45:26 -0000\r\nX-Received: from [216.252.122.218] by n11.bullet.sp1.yahoo.com with NNFMP; 31 Oct 2007 13:45:23 -0000\r\nX-Received: from [66.218.69.6] by t3.bullet.sp1.yahoo.com with NNFMP; 31 Oct 2007 13:45:23 -0000\r\nX-Received: from [66.218.66.75] by t6.bullet.scd.yahoo.com with NNFMP; 31 Oct 2007 13:45:23 -0000\r\nDate: Wed, 31 Oct 2007 13:45:22 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;fga0ti+es3q@...&gt;\r\nIn-Reply-To: &lt;4727913A.2000307@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;hinoglu&quot; &lt;hinoglu@...&gt;\r\nSubject: Re: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\nX-Yahoo-Group-Post: member; u=194872127; y=zMS1nukPxIWp5DNSkP06PNEK6K_6SlnIvy_KumVnY-TMFQ\r\nX-Yahoo-Profile: hinoglu\r\n\r\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt; \n=\r\n&gt; We typically do not run multiple instances per JVM in our crawling, so \n&gt;=\r\n we usually leave the cache percentage at its default (setting &#39;0&#39;,\nwhich \n=\r\n&gt; means use the BDB default, which would be 60% unless reset with a BDB \n&gt; =\r\nproperty). (In big crawls using an alternate non-BDB UriUniqFilter, we \n&gt; m=\r\nay lower this to 20%.)\n&gt; \n&gt; So, we have no recommendation for an optimal va=\r\nlue based on experience. \n&gt; If 60% works well for a single crawl, I would a=\r\nvoid having the \n&gt; percentages of all active crawls total greater than 60%.=\r\n\n\ncurrent setup is running on a single machine for demontration purposes, \n=\r\nin the future there will be around hundreds of sites to be crawled. \nnone o=\r\nf my crawl jobs are that big (~50Mbs) but sites are consisting of \ntiny uni=\r\nque contents like small images, icons and text data in amounts of \ntens of =\r\nthousands.   \n\nso, it seems that i will need many more machines set up and =\r\nrunning, and \na good job distributor script which will not assign more than=\r\n 2-3 jobs\nto  each \nmachine at the same time. \n \n\n&gt; \n&gt; If you&#39;re only crawl=\r\ning ~20 main sites, do you have many more than 20 \n&gt; domain overrides? (I w=\r\nouldn&#39;t be concerned unless it was in the \n&gt; thousands, and even then it mi=\r\nght not be a problem.)\n\nsorry for misguidance in the previous mail, no i&#39;m =\r\nnot using any\ndomain overrides.\ni have 5 different profiles, only differing=\r\n in user-agent settings.\nand all of them \nhave the same regex url canonical=\r\nization rule which lets heritrix to\nrip off a session id \nin the middle of =\r\nthe uri of some sites. not all the sites have this\nuri schema, but \nthis ru=\r\nle doesn&#39;t seem to get in the way in other sites.\n\n&gt; Do your crawls usually=\r\n finish by themselves or are they stopped by the \n&gt; operator/JMX?\n\nyes, the=\r\ny finish themselves, operator script controls job&#39;s status\nfrom the jmx int=\r\nerface\nquerying the relevant instance with getjobstatus command in every 10=\r\n\nseconds:\n\njava/org/archive/crawler/Heritrix.java:\n...\npublic String getJob=\r\nStatus(String UID) {\n        CrawlJob job =3D getJobHandler().getJob(UID);\n=\r\n\n        if (job =3D=3D null) {\n            return &quot;null&quot;;\n        }\n\n     =\r\n   return job.getStatus();\n    }\n...\n\ni&#39;ve added some functions like this o=\r\nne to ease the controlling of\ncrawler.\nscript passes destroy() call to rele=\r\nvant instance, when any\ngetjobstatus call returns \nany kind of &#39;Finished&#39;, =\r\n&#39;Finished -blah&#39; messages. \n\ni&#39;m not that into java and java&#39;s gc, but yet =\r\nthere happens no\ndecrease in amount of memory allocated \n(shown in heritrix=\r\n webui&#39;s console) when and even much after an\ninstance is destroyed.\n\n\n"}}