{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"lekash","from":"lekash &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"PTZR04iQtovOI1uz5RxL2_qLvzgpizyuysY0YRjD6UUKQlTGpYt0fnch9HcMNm-RuKmXClG2MghvoxhwW4gWegOx","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Crawling 100 million pages","postDate":"1212790676","msgId":5301,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NDlCNzk0LjIwNTA2MDJAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PDIwMDgwNjA2MjEzMC5tNTZMVXR2UDAwMjYyM0BteDEuYmF5YXJlYS5uZXQ+","referencesHeader":"PDIwMDgwNjA2MjEzMC5tNTZMVXR2UDAwMjYyM0BteDEuYmF5YXJlYS5uZXQ+"},"prevInTopic":5300,"nextInTopic":5302,"prevInTime":5300,"nextInTime":5302,"topicId":5292,"numMessagesInTopic":11,"msgSnippet":"Hi, Some quad core AMD frob from Tyan or Supermicro with 16G and 4T of disk. (I use many, of similar ilk) No particular upper limit on B/W.  But it doesn t","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 30990 invoked from network); 6 Jun 2008 22:18:31 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m54.grp.scd.yahoo.com with QMQP; 6 Jun 2008 22:18:31 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta15.grp.scd.yahoo.com with SMTP; 6 Jun 2008 22:18:31 -0000\r\nX-Received: from [192.168.0.12] (72.20.109.026.bayarea.net [72.20.109.26])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id m56MIVHB097301\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 6 Jun 2008 15:18:31 -0700 (PDT)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;4849B794.2050602@...&gt;\r\nDate: Fri, 06 Jun 2008 15:17:56 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.11) Gecko/20050727\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;200806062130.m56LUtvP002623@...&gt;\r\nIn-Reply-To: &lt;200806062130.m56LUtvP002623@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: lekash &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Crawling 100 million pages\r\nX-Yahoo-Group-Post: member; u=90724651; y=UR7wiGbTgboA-fxK97iynFYetYcuoiJ5uFMhrqxKNc4q\r\nX-Yahoo-Profile: lekash\r\n\r\nHi,\nSome quad core AMD frob from Tyan or Supermicro with 16G and 4T of disk.\n(I use many, of similar ilk)\n\nNo particular upper limit on B/W.  But it doesn&#39;t take a lot of B/W to \ndo that.\n350M * 25K /average page = 10T.\n\nAt 100 Mb/sec = 1 T / 1 day. \n\nThis went over about a 3 month period, so means it was probably runnning \n10 Mb/sec.\n\nIt was the highest count on one.  Most of them fall down around 200M - 250M.\n\nJohn\n\n\nLeo Dagum wrote:\n\n&gt; Hi John,\n&gt;\n&gt;  \n&gt;\n&gt; What hardware does a crawler instance run on and what is your Internet \n&gt; b/w?  385M/3m from a single crawler instance is very impressive.\n&gt;\n&gt;  \n&gt;\n&gt; - leo\n&gt;\n&gt;  \n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n&gt; *From:* archive-crawler@yahoogroups.com \n&gt; [mailto:archive-crawler@yahoogroups.com] *On Behalf Of *lekash\n&gt; *Sent:* Friday, June 06, 2008 1:21 PM\n&gt; *To:* archive-crawler@yahoogroups.com\n&gt; *Subject:* Re: [archive-crawler] Crawling 100 million pages\n&gt;\n&gt;  \n&gt;\n&gt; Hi there,\n&gt;\n&gt; I&#39;m like to very much encourage being careful about &#39;speeding up \n&gt; heritrix&#39;.\n&gt; Politeness is really important on the net. I keep doubling my inter\n&gt; crawl delay every\n&gt; year, and still people have problems.\n&gt;\n&gt; Its not a heretrix limit you will be looking at. Its a hardware\n&gt; capacity limit.\n&gt; (Though the most I&#39;ve gotten a single crawler to do is\n&gt; 385M over a three month period.)\n&gt; A crawler group, I&#39;ve gotten to 6B /year.\n&gt; So, 100 M /month is well within the operating range.\n&gt;\n&gt; The 1.x crawler seems to have a limit around 680M for a single crawler,\n&gt; like the queued numbers stop going up then. Never got there, so don&#39;t\n&gt; know what would happen. I haven&#39;t run a multi-billion one on the 2.x line,\n&gt; maybe next year.\n&gt;\n&gt; The things that run out of space are memory for java heap space, and\n&gt; something funny with the database. Re-writing scratch files in that\n&gt; seems to be what slows it down later, despite a still wide frontier.\n&gt;\n&gt; And of course, urls are not a consistent measure, as size varies widely.\n&gt; e.g. .gov sites are 10 times as dense in text/pdf as .com sites,\n&gt; and .com sites have all the visuals and movies. Depends on what\n&gt; you are crawling.\n&gt;\n&gt; John\n&gt;\n&gt; hijbul_bd wrote:\n&gt;\n&gt;&gt; Dear All\n&gt;&gt;\n&gt;&gt; I would like to crawl 100 million pages with in a month for crawling\n&gt;&gt; research. As far i know some research crawler(IRLbot(6 billion pages in\n&gt;&gt; 41 days), polybot(120 millions pages in 19 days)) can download huge\n&gt;&gt; pages in short amount of time wich is not open source. In 2005\n&gt;&gt; according to some blog site Heritrix can download about 20 miilion\n&gt;&gt; pages in a month. What is the speed of current Heritrix version and How\n&gt;&gt; can I speed up heritrix to download 100 million or at least 50 million\n&gt;&gt; pages with in a month. Are there any ohter open source crawler which\n&gt;&gt; can do this?\n&gt;&gt;\n&gt;&gt; Thanks in Advance\n&gt;&gt; Hijbul Alam\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;  \n\n\n"}}