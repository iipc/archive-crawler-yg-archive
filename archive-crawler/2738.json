{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225011788,"authorName":"Karl Wright","from":"Karl Wright &lt;kwright@...&gt;","profile":"daddywri","replyTo":"LIST","senderId":"cgUmPkALjtih1pct73-IHfqUtkuOXRlg1kXQKMQId0JFDhHrs-8t6fMeY3yJYDsEtLg6oYlOOtf3ojOzrQFPqngdUVeTj7qnrEA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Robots.txt parsing problem?","postDate":"1141842634","msgId":2738,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0MEYyMkNBLjUwMjAzMDBAbWV0YWNhcnRhLmNvbT4=","inReplyToHeader":"PDQ0MEYxRTQ2LjgwMzAwMDJAYXJjaGl2ZS5vcmc+","referencesHeader":"PDQ0MEYxQkE3LjIwNTAyMDNAbWV0YWNhcnRhLmNvbT4gPDQ0MEYxRTQ2LjgwMzAwMDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":2737,"nextInTopic":2739,"prevInTime":2737,"nextInTime":2739,"topicId":2736,"numMessagesInTopic":7,"msgSnippet":"... It seems clear from the logs that 24 hours elapsed since whatever change occurred.  The date on the robots.txt is 3/6 and the date of the crawl is 3/8. I","rawEmail":"Return-Path: &lt;kwright@...&gt;\r\nX-Sender: kwright@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 34735 invoked from network); 8 Mar 2006 18:29:11 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m32.grp.scd.yahoo.com with QMQP; 8 Mar 2006 18:29:11 -0000\r\nReceived: from unknown (HELO metacarta.com) (65.77.47.18)\n  by mta5.grp.scd.yahoo.com with SMTP; 8 Mar 2006 18:29:11 -0000\r\nReceived: from localhost (silene.metacarta.com [65.77.47.24])\n\tby metacarta.com (Postfix) with ESMTP id 00E0D518035\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  8 Mar 2006 13:29:09 -0500 (EST)\r\nReceived: from metacarta.com ([65.77.47.18])\n\tby localhost (silene.metacarta.com [65.77.47.24]) (amavisd-new, port 10024)\n\twith ESMTP id 06976-08 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 8 Mar 2006 13:29:09 -0500 (EST)\r\nReceived: from [65.77.47.197] (dhcp-65-77-47-197.metacarta.com [65.77.47.197])\n\tby metacarta.com (Postfix) with ESMTP id 318A551800D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  8 Mar 2006 13:29:09 -0500 (EST)\r\nMessage-ID: &lt;440F22CA.5020300@...&gt;\r\nDate: Wed, 08 Mar 2006 13:30:34 -0500\r\nUser-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;440F1BA7.2050203@...&gt; &lt;440F1E46.8030002@...&gt;\r\nIn-Reply-To: &lt;440F1E46.8030002@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: by amavisd-new-20030616-p10 (Debian) at metacarta.com\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Karl Wright &lt;kwright@...&gt;\r\nSubject: Re: [archive-crawler] Robots.txt parsing problem?\r\nX-Yahoo-Group-Post: member; u=225011788; y=EhLnAth_kfgKRMQQeEWrVxG_7YXTDaJvox-oxW5UO8nkMaY\r\nX-Yahoo-Profile: daddywri\r\n\r\nIgor Ranitovic wrote:\n&gt; Hi Karl,\n&gt; \n&gt; Heritrix rechecks robots.txt files every 24 hours by default. Did you change that value?\n&gt; \n&gt; It seems that this robots file has been recently modified and it is possible that 24 hours has not \n&gt; elapsed since the modification.\n&gt; If that is the case, you can do several things: force fetch the http://uaelp.pennnet.com/robots.txt \n&gt; or change the value of robot-validity-duration-seconds.\n&gt; \n&gt; Take care,\n&gt; i.\n&gt; \n&gt; \n\nIt seems clear from the logs that 24 hours elapsed since whatever change \noccurred.  The date on the robots.txt is 3/6 and the date of the crawl \nis 3/8.\n\nI don&#39;t know what the robots.txt looked like prior to that, of course, \nsince this is not my website.\n\nWhat do you think about the fact that the final line is EOF-terminated, \nnot newline terminated?  Would that upset Heritrix&#39; parser?\n\nKarl\n\n&gt;&gt;Hi,\n&gt;&gt;\n&gt;&gt;We got dinged again by using Heritrix in that a crawlee complained that \n&gt;&gt;we were ignoring their robots.txt file.  On the face of it, they look \n&gt;&gt;like they are correct in complaining:\n&gt;&gt;\n&gt;&gt;http://uaelp.pennnet.com/robots.txt\n&gt;&gt;===================================\n&gt;&gt;\n&gt;&gt;# pennwell robots.txt\n&gt;&gt;# updated 3/6/06 by bwn\n&gt;&gt;\n&gt;&gt;User-agent: Googlebot\n&gt;&gt;Disallow: /Search/\n&gt;&gt;Disallow: /search/\n&gt;&gt;Disallow: /Userreg/\n&gt;&gt;Disallow: /userreg/\n&gt;&gt;Disallow: /Nav/\n&gt;&gt;Disallow: /nav/\n&gt;&gt;Disallow: /js\n&gt;&gt;Disallow: /JS\n&gt;&gt;Disallow: /whitepapers/wp_redirect.cfm\n&gt;&gt;Disallow: /*.js$\n&gt;&gt;\n&gt;&gt;User-agent: *\n&gt;&gt;Disallow: /Search/\n&gt;&gt;Disallow: /search/\n&gt;&gt;Disallow: /Userreg/\n&gt;&gt;Disallow: /userreg/\n&gt;&gt;Disallow: /Nav/\n&gt;&gt;Disallow: /nav/\n&gt;&gt;Disallow: /js\n&gt;&gt;Disallow: /JS\n&gt;&gt;Disallow: /whitepapers/wp_redirect.cfm\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;heritrix crawl log portion\n&gt;&gt;==========================\n&gt;&gt;\n&gt;&gt;2006-03-08T16:11:41.295Z   200      17266 \n&gt;&gt;http://uaelp.pennnet.com/whitepapers/wp_redirect.cfm?id=305 LLL \n&gt;&gt;http://uaelp.pennnet.com/whitepapers/wp.cfm?id=305 text/html #075 \n&gt;&gt;20060308161137362+1062 R5C73R3EPNNO4UYMWRPBTUJD3TWW32X7 2t\n&gt; \n&gt; \n&gt;&gt;According to our reading of the robots.txt spec, this URL should not \n&gt;&gt;have been crawled.  The only reason I can find for the failure may be \n&gt;&gt;that the last line is not terminated with a newline, but rather just an EOF.\n&gt;&gt;\n&gt;&gt;Any thoughts?\n&gt;&gt;\n&gt;&gt;Karl\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}