{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256346715,"authorName":"Adam Fisk","from":"&quot;Adam Fisk&quot; &lt;adamfisk@...&gt;","profile":"afisk3","replyTo":"LIST","senderId":"3p7yWq54fu7uoPep6F5TCjrAeQzCKVtPhREVbCss2JhmyEjeXUFvNl327f0F34I5QeRF2aXwQKjl1OeJjIY58alcp_T3H0Y","spamInfo":{"isSpam":false,"reason":"12"},"subject":"OutOfMemoryError on small crawl","postDate":"1140731588","msgId":2709,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR0bGFzNStnczhuQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":2712,"prevInTime":2708,"nextInTime":2710,"topicId":2709,"numMessagesInTopic":7,"msgSnippet":"Hi Everyone- I m consistently running into OutOfMemoryErrors running crawls with only about 20 sites.  I m only getting output to my console, so please excuse","rawEmail":"Return-Path: &lt;adamfisk@...&gt;\r\nX-Sender: adamfisk@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 8754 invoked from network); 23 Feb 2006 21:53:38 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m24.grp.scd.yahoo.com with QMQP; 23 Feb 2006 21:53:38 -0000\r\nReceived: from unknown (HELO n7a.bullet.scd.yahoo.com) (66.94.237.41)\n  by mta1.grp.scd.yahoo.com with SMTP; 23 Feb 2006 21:53:38 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.66.58] by n7.bullet.scd.yahoo.com with NNFMP; 23 Feb 2006 21:53:09 -0000\r\nReceived: from [66.218.66.76] by t7.bullet.scd.yahoo.com with NNFMP; 23 Feb 2006 21:53:09 -0000\r\nDate: Thu, 23 Feb 2006 21:53:08 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;dtlas5+gs8n@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Adam Fisk&quot; &lt;adamfisk@...&gt;\r\nSubject: OutOfMemoryError on small crawl\r\nX-Yahoo-Group-Post: member; u=256346715; y=NJeOHkgzKYHm8QTWqPswxqlEKxgkg0rwaFiaZTPMkQwJ\r\nX-Yahoo-Profile: afisk3\r\n\r\nHi Everyone-\n\nI&#39;m consistently running into OutOfMemoryErrors running crawl=\r\ns with\nonly about 20 sites.  I&#39;m only getting output to my console, so plea=\r\nse\nexcuse the formatting.  I&#39;m confused about the &quot;max-depth&quot; and\n&quot;average-=\r\ndepth&quot; readings here.  Those can&#39;t possibly be path depths,\nright?  I&#39;m usi=\r\nng DecidingScopeRule, and I&#39;m wondering if I might be\ndoing something to in=\r\nadvertently bypass &quot;TooManyPathSegs&quot;,\n&quot;TooManyHops&quot;, or &quot;Pathological&quot; deci=\r\nde rules, although it certainly\nlooks right to me.\n\nOtherwise, I probably s=\r\nhould not be getting this with so few sites, right?\n\nI&#39;m not doing anything=\r\n with the -Xmx rules or anything like that, as\nI&#39;d prefer to diagnose the p=\r\nroblem before taking such measures.  \n\nCould there be an issue with Decidin=\r\ngScope?   I&#39;m currently not\nprocessing the crawled data at all -- just lett=\r\ning it download the ARC\nfiles as usual.  The problem does seem to crop up o=\r\nn the site listed\nbelow, but there&#39;s nothing odd about those pages, and a c=\r\nrawl of only\nthat site does fine.\n\nThis is running on a machine with 2GB of=\r\n RAM and two Xeon processors\nand a couple of RAIDs.\n\nThanks for any assista=\r\nnce.  \n\n-Adam\n \n\n02/23/2006 18:38:02 +0000 SEVERE\norg.archive.crawler.frame=\r\nwork.ToeThread seriousError Serious error\noccured trying to process &#39;CrawlU=\r\nRI http://www.int\nelihealth.com/IH/ihtIH/WSIHW000/29721/32087.html LL\nhttp:=\r\n//www.intelihealth.com/IH/ihtIH/WSIHW000/29721/29721.html?k=3Dnavx408x29721=\r\n\nin ExtractorHTML&#39;\n[ToeThread #19:\nhttp://www.intelihealth.com/IH/ihtIH/WSI=\r\nHW000/29721/32087.html\n CrawlURI\nhttp://www.intelihealth.com/IH/ihtIH/WSIHW=\r\n000/29721/32087.html LL\nhttp://www.intelihealth.com/IH/ihtIH/WSIHW000/29721=\r\n/29721.html?k=3Dnavx408x2\n9721    0 attempts\n    in processor: ExtractorHTM=\r\nL\n    ACTIVE for 430ms\n    step: ABOUT_TO_BEGIN_PROCESSOR for 155ms\n    jav=\r\na.lang.Thread.dumpThreads(Native Method)\n    java.lang.Thread.getStackTrace=\r\n(Thread.java:1383)\n   \nde.kohlschuetter.j5compat.Stacktraces5.getStackTrace=\r\n(Stacktraces5.java:29)\n    org.archive.crawler.framework.ToeThread.reportTo=\r\n(ToeThread.java:517)\n    org.archive.crawler.framework.ToeThread.reportTo(T=\r\noeThread.java:596)\n    org.archive.util.DevUtils.extraInfo(DevUtils.java:65=\r\n)\n   \norg.archive.crawler.framework.ToeThread.seriousError(ToeThread.java:2=\r\n34)\n   \norg.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.j=\r\nava:329)\n    org.archive.crawler.framework.ToeThread.run(ToeThread.java:153=\r\n)\n]\ntimestamp            discovered      queued   downloaded      \n2006-02-=\r\n23T18:38:02Z      171957      134756        36648      \n\ndoc/s(avg)   KB/s(=\r\navg)   dl-failures   busy-thread   mem-use-KB     \n4.55(4.22)    139(105)  =\r\n           0             6        53734         \n\nheap-size-KB   congestion=\r\n   max-depth   avg-depth\n65088               1.0       28372        6416\n\nj=\r\nava.lang.OutOfMemoryError: Java heap space\n\n\n\n\n"}}