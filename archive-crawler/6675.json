{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Z9nK2lZ81Y1lUxR-KZkL9tQphD5ywAbkoH0bW11l1DDISYPkxKuc_e6Jyc-bJG4aerTN2fVGPFgEEVbBQPdPsGM_gud-z-0","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Problem with robots.txt IGNORE policy","postDate":"1281639897","msgId":6675,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDNjQ0NUQ5LjYwNzAxQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGk0MTZiMSt2ZWQzQGVHcm91cHMuY29tPg==","referencesHeader":"PGk0MTZiMSt2ZWQzQGVHcm91cHMuY29tPg=="},"prevInTopic":6672,"nextInTopic":6677,"prevInTime":6674,"nextInTime":6676,"topicId":6671,"numMessagesInTopic":9,"msgSnippet":"... Because there is a separate dial for controlling whether (and to what extend) Crawl-Delay is respected, my preference would be for the normal IGNORE","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 78969 invoked from network); 12 Aug 2010 19:05:01 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.sp2.yahoo.com with QMQP; 12 Aug 2010 19:05:01 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta3.grp.sp2.yahoo.com with SMTP; 12 Aug 2010 19:05:01 -0000\r\nX-Received: (qmail 16811 invoked from network); 12 Aug 2010 19:05:00 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay00.pair.com with SMTP; 12 Aug 2010 19:05:00 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4C6445D9.60701@...&gt;\r\nDate: Thu, 12 Aug 2010 12:04:57 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.11) Gecko/20100711 Thunderbird/3.0.6\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: kristsi25 &lt;kris@...&gt;\r\nReferences: &lt;i416b1+ved3@...&gt;\r\nIn-Reply-To: &lt;i416b1+ved3@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Problem with robots.txt IGNORE policy\r\nX-Yahoo-Group-Post: member; u=137285340; y=WZXW4ErThIHtL7xyroW2UkL3wi4b4WP8Si5fro-kJUvm\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 8/12/10 9:09 AM, kristsi25 wrote:\n&gt; We wish to ignore robots.txt as far as they exclude us from content BUT we would like to respect the crawl-delay (at least up to some number of seconds). However, if you currently select IGNORE as your honoring policy the crawl-delay is never even read, much less enforced.\n&gt;\n&gt; Is there a way to configure the robots honoring to accomplish this or is this a deficiency in Heritrix?\n&gt;\n&gt; If this is not possible, does it make sense to amend the IGNORE policy or should a new IGNORE_EXCEPT_CRAWLDELAY policy be added?\n\nBecause there is a separate &#39;dial&#39; for controlling whether (and to what \nextend) &#39;Crawl-Delay&#39; is respected, my preference would be for the \nnormal IGNORE policy to be fixed to mean &#39;ignore disallows&#39; -- while \nstill using other info (like Crawl-Delay, Sitemap, whatever) that might \nbe in a &#39;robots.txt&#39;.\n\nAn option for simulating what you want without changing the current \nIGNORE or adding a new policy would be to run your crawl with a CLASSIC \nrobots-respecting policy, but set the &#39;calculateRobotsOnly&#39; flag on \nPreconditionEnforcer. Rather than canceling the fetching of URIs that \nare robots-precluded, this setting merely marks them up with an annotation.\n\n(The original motivation for this feature was to allow a later scripted \nProcessor to apply more sophisticated logic as to whether to a \nprecluded-URI deserves fetching anyway -- such as it is required to \nrender a robots-allowed page. It&#39;s also been used to run a crawl \nignoring robots but then report, by scanning the crawl.log for that \nannotation, exactly which URIs wouldn&#39;t have been fetched under strict \nrobots rules.)\n\n- Gordon @ IA\n\n\n"}}