{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"u2V1z3SKAulj_82gazAdxq-Tl-P9_PvTlo0WA89nl2Gjz24BAqhFyGVWegyN2icqhmu55EtyCb43__wsRjcZnguPJN9MR1I","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] OutOfMemoryError on small crawl","postDate":"1140735133","msgId":2712,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzRkUzQzlELjYwNjA0MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGR0bGFzNStnczhuQGVHcm91cHMuY29tPg==","referencesHeader":"PGR0bGFzNStnczhuQGVHcm91cHMuY29tPg=="},"prevInTopic":2709,"nextInTopic":2713,"prevInTime":2711,"nextInTime":2713,"topicId":2709,"numMessagesInTopic":7,"msgSnippet":"It looks like your heap is capped at a maximum size of 64MB -- that s the Java default in the absence of any -Xmx setting in Java 1.4 and previous -- so your","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 36843 invoked from network); 23 Feb 2006 22:52:15 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m27.grp.scd.yahoo.com with QMQP; 23 Feb 2006 22:52:15 -0000\r\nReceived: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta4.grp.scd.yahoo.com with SMTP; 23 Feb 2006 22:52:15 -0000\r\nReceived: (qmail 19836 invoked from network); 23 Feb 2006 22:52:14 -0000\r\nReceived: from unknown (HELO ?10.0.10.13?) (unknown)\n  by unknown with SMTP; 23 Feb 2006 22:52:14 -0000\r\nX-pair-Authenticated: 71.141.167.101\r\nMessage-ID: &lt;43FE3C9D.6060402@...&gt;\r\nDate: Thu, 23 Feb 2006 14:52:13 -0800\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc4 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;dtlas5+gs8n@...&gt;\r\nIn-Reply-To: &lt;dtlas5+gs8n@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] OutOfMemoryError on small crawl\r\nX-Yahoo-Group-Post: member; u=137285340; y=rD4ouzdhCzPlOKodpfNHXoE6kaILBhwyYI-KGTiMTRZL\r\nX-Yahoo-Profile: gojomo\r\n\r\nIt looks like your heap is capped at a maximum size of 64MB -- \nthat&#39;s the Java default in the absence of any -Xmx setting in Java \n1.4 and previous -- so your 2GB of RAM isn&#39;t doing the crawler any \ngood.\n\nUse of -Xmx is definitely indicated; if the machine is dedicated \nto crawling, and you want the crawler to be able to use all the \nRAM, -Xmx1500m would be justified.\n\n(It&#39;s probably possible to crawl 20 hosts in 64MB, if there isn&#39;t \nan explosion of subdomains, and you use only a small number of \nthreads -- but I doubt that&#39;s a real constraint you want to try to \nlive within.)\n\nThe &#39;max-depth&#39; and &#39;average-depth&#39; readings on that status line \n(and in the crawler console) refer to the size of queues: \n&#39;max-depth&#39; is the longest queue in the frontier, &#39;average-depth&#39; \nis the average of all queue sizes.\n\n- Gordon @ IA\n\nAdam Fisk wrote:\n&gt; Hi Everyone-\n&gt; \n&gt; I&#39;m consistently running into OutOfMemoryErrors running crawls with\n&gt; only about 20 sites.  I&#39;m only getting output to my console, so please\n&gt; excuse the formatting.  I&#39;m confused about the &quot;max-depth&quot; and\n&gt; &quot;average-depth&quot; readings here.  Those can&#39;t possibly be path depths,\n&gt; right?  I&#39;m using DecidingScopeRule, and I&#39;m wondering if I might be\n&gt; doing something to inadvertently bypass &quot;TooManyPathSegs&quot;,\n&gt; &quot;TooManyHops&quot;, or &quot;Pathological&quot; decide rules, although it certainly\n&gt; looks right to me.\n&gt; \n&gt; Otherwise, I probably should not be getting this with so few sites, right?\n&gt; \n&gt; I&#39;m not doing anything with the -Xmx rules or anything like that, as\n&gt; I&#39;d prefer to diagnose the problem before taking such measures.  \n&gt; \n&gt; Could there be an issue with DecidingScope?   I&#39;m currently not\n&gt; processing the crawled data at all -- just letting it download the ARC\n&gt; files as usual.  The problem does seem to crop up on the site listed\n&gt; below, but there&#39;s nothing odd about those pages, and a crawl of only\n&gt; that site does fine.\n&gt; \n&gt; This is running on a machine with 2GB of RAM and two Xeon processors\n&gt; and a couple of RAIDs.\n&gt; \n&gt; Thanks for any assistance.  \n&gt; \n&gt; -Adam\n&gt;  \n&gt; \n&gt; 02/23/2006 18:38:02 +0000 SEVERE\n&gt; org.archive.crawler.framework.ToeThread seriousError Serious error\n&gt; occured trying to process &#39;CrawlURI http://www.int\n&gt; elihealth.com/IH/ihtIH/WSIHW000/29721/32087.html LL\n&gt; http://www.intelihealth.com/IH/ihtIH/WSIHW000/29721/29721.html?k=navx408x29721\n&gt; in ExtractorHTML&#39;\n&gt; [ToeThread #19:\n&gt; http://www.intelihealth.com/IH/ihtIH/WSIHW000/29721/32087.html\n&gt;  CrawlURI\n&gt; http://www.intelihealth.com/IH/ihtIH/WSIHW000/29721/32087.html LL\n&gt; http://www.intelihealth.com/IH/ihtIH/WSIHW000/29721/29721.html?k=navx408x2\n&gt; 9721    0 attempts\n&gt;     in processor: ExtractorHTML\n&gt;     ACTIVE for 430ms\n&gt;     step: ABOUT_TO_BEGIN_PROCESSOR for 155ms\n&gt;     java.lang.Thread.dumpThreads(Native Method)\n&gt;     java.lang.Thread.getStackTrace(Thread.java:1383)\n&gt;    \n&gt; de.kohlschuetter.j5compat.Stacktraces5.getStackTrace(Stacktraces5.java:29)\n&gt;     org.archive.crawler.framework.ToeThread.reportTo(ToeThread.java:517)\n&gt;     org.archive.crawler.framework.ToeThread.reportTo(ToeThread.java:596)\n&gt;     org.archive.util.DevUtils.extraInfo(DevUtils.java:65)\n&gt;    \n&gt; org.archive.crawler.framework.ToeThread.seriousError(ToeThread.java:234)\n&gt;    \n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:329)\n&gt;     org.archive.crawler.framework.ToeThread.run(ToeThread.java:153)\n&gt; ]\n&gt; timestamp            discovered      queued   downloaded      \n&gt; 2006-02-23T18:38:02Z      171957      134756        36648      \n&gt; \n&gt; doc/s(avg)   KB/s(avg)   dl-failures   busy-thread   mem-use-KB     \n&gt; 4.55(4.22)    139(105)             0             6        53734         \n&gt; \n&gt; heap-size-KB   congestion   max-depth   avg-depth\n&gt; 65088               1.0       28372        6416\n&gt; \n&gt; java.lang.OutOfMemoryError: Java heap space\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}