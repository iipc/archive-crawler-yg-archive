{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":10305499,"authorName":"kolnaser","from":"&quot;kolnaser&quot; &lt;ALHOORI@...&gt;","profile":"kolnaser","replyTo":"LIST","senderId":"As9sPGrh_1mvZ72av0q7ZRgLb3HXDQBxGqe4BiyldM9Vmc4s-o-n6WM5DhiuuGHhFsfdbE8ML7aY77Uo3JWQlLm8BG5EqlY","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to download HTML files from a relative path","postDate":"1240767975","msgId":5797,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGd0MjZsNytpcXFkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ5RjQzRUNFLjQwNTA1QGdtYWlsLmNvbT4="},"prevInTopic":5796,"nextInTopic":5802,"prevInTime":5796,"nextInTime":5799,"topicId":5795,"numMessagesInTopic":11,"msgSnippet":"First , thanks a lot for the reply ! I am using 1.1.4.3 ( is there any advantage of moving to the new 2.0 ?) I want to crawl this site :www.citeulike.org , but","rawEmail":"Return-Path: &lt;ALHOORI@...&gt;\r\nX-Sender: ALHOORI@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 59299 invoked from network); 26 Apr 2009 17:47:00 -0000\r\nX-Received: from unknown (69.147.108.202)\n  by m4.grp.re1.yahoo.com with QMQP; 26 Apr 2009 17:47:00 -0000\r\nX-Received: from unknown (HELO n42b.bullet.mail.sp1.yahoo.com) (66.163.168.156)\n  by mta3.grp.re1.yahoo.com with SMTP; 26 Apr 2009 17:47:00 -0000\r\nX-Received: from [69.147.65.171] by n42.bullet.mail.sp1.yahoo.com with NNFMP; 26 Apr 2009 17:46:18 -0000\r\nX-Received: from [98.137.34.73] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 26 Apr 2009 17:46:18 -0000\r\nDate: Sun, 26 Apr 2009 17:46:15 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gt26l7+iqqd@...&gt;\r\nIn-Reply-To: &lt;49F43ECE.40505@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;kolnaser&quot; &lt;ALHOORI@...&gt;\r\nSubject: Re: How to download HTML files from a relative path\r\nX-Yahoo-Group-Post: member; u=10305499; y=6sFnzdCuzgd3kp66EmpWclrh7K75Eds4tr_dsSGeDGb9F64\r\nX-Yahoo-Profile: kolnaser\r\n\r\n\nFirst , thanks a lot for the reply !\n\nI am using 1.1.4.3 ( is there any ad=\r\nvantage of moving to the new 2.0 ?)\n\nI want to crawl this site :www.citeuli=\r\nke.org , but the default will download all html , css, images etc \n\nI just =\r\nneed the HTML pages that are generated from this path for example\n\nhttp://w=\r\nww.citeulike.org/user/reyez/article/4406466\n\nAdding HTML or HTM would gener=\r\nate the page also ?!\nhttp://www.citeulike.org/user/reyez/article/4406466.ht=\r\nml\nhttp://www.citeulike.org/user/reyez/article/4406466.htm\n\nSo I used/tried=\r\n \n\nhttp://www.citeulike.org/user/.*/article/.*\n\nand\n\nhttp://www.citeulike.o=\r\nrg/user/.*/article/.*&#92;.html$\n\nwith DecidingScope and MatchesRegExpDecideRul=\r\ne , and many other options but I didn&#39;t succeed. I got some images and some=\r\n hosts that are other than citeulike.org, and some HTML pages that I don&#39;t =\r\nneed.\n\nSo I add this to get only the HTML \n\n1. Add the ContentTypeRegExFilt=\r\ner as a midfetch-filter and write-processor Archiver filter.\n\n2. In the set=\r\ntings for both filters set the if-match-return to true, and  use the regexp=\r\n &#39;(?i)text/html.*&#39;\n\n3) MatchesRegExpDecideRule\nREJECT\n\n.*(?i)&#92;.(a|ai|aif|ai=\r\nfc|aiff|asc|avi|bcpio|bin|bmp|bz2|c|cdf|cgi|cg&#92;\nm|class|cpio|cpp?|cpt|csh|c=\r\nss|cxx|dcr|dif|dir|djv|djvu|dll|dmg|dms|doc|dtd|dv|dv&#92;\ni|dxr|eps|etx|exe|ez=\r\n|gif|gram|grxml|gtar|h|hdf|hqx|ice|ico|ics|ief|ifb|iges|igs|&#92;\niso|jnlp|jp2|=\r\njpe|jpeg|jpg|js|kar|latex|lha|lzh|m3u|mac|man|mathml|me|mesh|mid|mi&#92;\ndi|mif=\r\n|mov|movie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|nc|o|oda|ogg|pbm|pct|p&#92;=\r\n\ndb|pdf|pgm|pgn|pic|pict|pl|png|pnm|pnt|pntg|ppm|ppt|ps|py|qt|qti|qtif|ra|r=\r\nam|ras&#92;\n|rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|sgml|sh|shar|silo|sit|skd|skm|sk=\r\np|skt|smi|smi&#92;\nl|snd|so|spl|src|srpm|sv4cpio|sv4crc|svg|swf|t|tar|tcl|tex|t=\r\nexi|texinfo|tgz|tif|&#92;\ntiff|tr|tsv|ustar|vcd|vrml|vxml|wav|wbmp|wbxml|wml|wm=\r\nlc|wmls|wmlsc|wrl|xbm|xht|x&#92;\nhtml|xls|xml|xpm|xsl|xslt|xwd|xyz|z|zip)$\n\nThi=\r\ns delayed the crawling process. (any other helpful idea ?)\n\nWhat I need now=\r\n is how to download only that HTM/HTML pages from this path \nhttp://www.cit=\r\neulike.org/user/.*/article/.*&#92;.html$\n\n?\n\nBecause I am getting some HTML pag=\r\nes from other links , example\nhttp://www.citeulike.org/user/birolilu/author=\r\n/Siu:H\n\nI added a list of rejected pages , in the scope \n\nhttp://www.citeul=\r\nike.org/user/.*/author/.*\nhttp://www.citeulike.org/news\nhttp://www.citeulik=\r\ne.org/faq\nhttp://www.citeulike.org/tag/internet\nhttp://www.citeulike.org/pd=\r\nf_options\nhttp://www.citeulike.org/groups\nhttp://www.citeulike.org/user/.*/=\r\ntag/.*\nhttp://www.citeulike.org/howto\nhttp://www.citeulike.org/journals\n\nBu=\r\nt that stop the crawling !\n\nI added the NotMatchesRegExpDecideRule in the A=\r\nRCWriterProcessor , but that didn&#39;t help aslo .\n\n\n\n--- In archive-crawler@y=\r\nahoogroups.com, Sergey Khenkin &lt;skhenkin@...&gt; wrote:\n&gt;\n&gt; Hi,\n&gt; \n&gt; I&#39;m not s=\r\nure which heritrix version you are using but in 1.14.3 you can \n&gt; do this u=\r\nsing DecidingScope and MatchesRegExpDecideRule with regexp like\n&gt; \n&gt; http:/=\r\n/www.SITENAME/.*/TAG/.*\n&gt; \n&gt; or maybe\n&gt; \n&gt; http://www.SITENAME/.*/TAG/.*&#92;.h=\r\ntml?$\n&gt; \n&gt; depending on what you want to achieve.\n&gt; \n&gt; But as far as I unde=\r\nrstand it is also important how links to these HTML \n&gt; are gathered during =\r\nthe crawl. Most probably they are found inside other \n&gt;   HTML documents wh=\r\nich won&#39;t be matched by the regexps above. This \n&gt; should be addressed some=\r\nhow.\n&gt; \n&gt; Regards,\n&gt; Sergey\n&gt; \n&gt; \n&gt; &gt; How can I download HTML files only fr=\r\nom this path for example\n&gt; &gt; \n&gt; &gt; http://www.SITENAME/MEMBERNAME &lt;http://ww=\r\nw.SITENAME/MEMBERNAME&gt;(s)/TAG(S)/\n&gt; &gt; \n&gt; &gt; I used regexp\n&gt; &gt; \n&gt; &gt; http://ww=\r\nw.SITENAME/.*/TAG &lt;http://www.SITENAME/.*/TAG&gt;(S)/\n&gt; &gt; \n&gt; &gt; With URIregExpF=\r\nilter in FilterDecideRule , but it didn&#39;t work , any \n&gt; &gt; suggestion ?\n&gt;\n\n\n=\r\n\n"}}