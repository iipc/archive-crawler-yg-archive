{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":289645082,"authorName":"helloitsmaxine","from":"&quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;","profile":"helloitsmaxine","replyTo":"LIST","senderId":"BhmH-50LZ1eq9_ULX_H1YJdjtcyy5L7h2_zZ3FBlSDyDVFguLO3AkPeMEipK4F7hvwgKm57Zi07cPuSkD0m9h72EkBG9jHstKUzPDno","spamInfo":{"isSpam":false,"reason":"6"},"subject":"(Memory leak experience?) Re: Crawl rate decreasing with time?","postDate":"1311801577","msgId":7235,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGowcHZkOSsxcm5jQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGowcHU5aythN3Z2QGVHcm91cHMuY29tPg=="},"prevInTopic":7234,"nextInTopic":7237,"prevInTime":7234,"nextInTime":7236,"topicId":7213,"numMessagesInTopic":9,"msgSnippet":"Sorry I just realized my last question was really vague. The null pointer exception alerts look like this: Time: \t Jul. 27, 2011 21:17:07 GMT Level: \t WARNING ","rawEmail":"Return-Path: &lt;itsmaxine@...&gt;\r\nX-Sender: itsmaxine@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 60625 invoked from network); 27 Jul 2011 21:19:38 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m8.grp.sp2.yahoo.com with QMQP; 27 Jul 2011 21:19:38 -0000\r\nX-Received: from unknown (HELO n43d.bullet.mail.sp1.yahoo.com) (66.163.169.157)\n  by mta1.grp.sp2.yahoo.com with SMTP; 27 Jul 2011 21:19:38 -0000\r\nX-Received: from [69.147.65.172] by n43.bullet.mail.sp1.yahoo.com with NNFMP; 27 Jul 2011 21:19:38 -0000\r\nX-Received: from [98.137.35.12] by t14.bullet.mail.sp1.yahoo.com with NNFMP; 27 Jul 2011 21:19:38 -0000\r\nDate: Wed, 27 Jul 2011 21:19:37 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;j0pvd9+1rnc@...&gt;\r\nIn-Reply-To: &lt;j0pu9k+a7vv@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;\r\nSubject: (Memory leak experience?) Re: Crawl rate decreasing with time?\r\nX-Yahoo-Group-Post: member; u=289645082; y=nLQOOxbQIjTRUUaLmOzvh5crAHlJ9pxsQwUHW4TW9LjB0gMCp24VsMnEPzt38uhhFZyDIbNugu1umFI\r\nX-Yahoo-Profile: helloitsmaxine\r\n\r\nSorry I just realized my last question was really vague. The null pointer e=\r\nxception alerts look like this:\n\nTime: \t Jul. 27, 2011 21:17:07 GMT\nLevel: =\r\n\t WARNING\nMessage: \t\nExtractorHTML: NullPointerException (in thread &#39;ToeThr=\r\nead #20: http://freemusicarchive.org/&#39;; in processor &#39;ExtractorHTML&#39;)\nExcep=\r\ntion: \t\njava.lang.NullPointerException\nStacktrace: java.lang.NullPointerExc=\r\neption\n\tat org.archive.crawler.extractor.ExtractorHTML.processGeneralTag(Ex=\r\ntractorHTML.java:439)\n\tat org.archive.crawler.extractor.ExtractorHTML.extra=\r\nct(ExtractorHTML.java:667)\n\tat org.archive.crawler.extractor.ExtractorHTML.=\r\nextract(ExtractorHTML.java:613)\n\tat org.archive.crawler.extractor.Extractor=\r\n.innerProcess(Extractor.java:67)\n\tat org.archive.crawler.framework.Processo=\r\nr.process(Processor.java:109)\n\tat org.archive.crawler.framework.ToeThread.p=\r\nrocessCrawlUri(ToeThread.java:306)\n\tat org.archive.crawler.framework.ToeThr=\r\nead.run(ToeThread.java:154)\n\n--- In archive-crawler@yahoogroups.com, &quot;hello=\r\nitsmaxine&quot; &lt;itsmaxine@...&gt; wrote:\n&gt;\n&gt; I&#39;m still pretty stuck on this proble=\r\nm and am wondering if anyone in general has found memory leaks. Also what i=\r\ns a null pointer message in the alerts section usually indicative of? For r=\r\neference I&#39;m running Heritrix 1.14.4.\n&gt; \n&gt; --- In archive-crawler@yahoogrou=\r\nps.com, &quot;helloitsmaxine&quot; &lt;itsmaxine@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hey Gordon,\n&gt; &gt; I&#39;ve l=\r\nooked into your suggestions though nothing in the reports jumped out really=\r\n readily but will continue to investigate.\n&gt; &gt; \n&gt; &gt; However I wanted to ask=\r\n you about a possibility that others have been mentioning--could it be poss=\r\nible that this slow-down is due to a memory leak? Is there a way to test fo=\r\nr that? Someone suggested that I stop the crawl, restart it on a fresh jvm,=\r\n and see what happens. I paused the crawl, started it from the recovery log=\r\n in a new jvm, and the crawl rates shot right back up to where they started=\r\n. Would this be indicative of a memory leak? If so, it seems like this prob=\r\nlem is pretty inevitable since it has happened pretty much every time witho=\r\nut fail. Just wanted to know what you (or anyone else) thinks about this?\n&gt;=\r\n &gt; \n&gt; &gt; Thanks!\n&gt; &gt; \n&gt; &gt; --- In archive-crawler@yahoogroups.com, Gordon Moh=\r\nr &lt;gojomo@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Looks like you&#39;re on a 4GB Mac with a 2GB he=\r\nap, so there shouldn&#39;t be a\n&gt; &gt; &gt; swapping problem.\n&gt; &gt; &gt; \n&gt; &gt; &gt; 250 thread=\r\ns may be a lot for a 2GB heap. There are no firm rules, but a \n&gt; &gt; &gt; very r=\r\nough rule I&#39;ve used is to take the heap size, deduct the 60% used \n&gt; &gt; &gt; by=\r\n default by the BerkeleyDB-JE-based structures, then divide the \n&gt; &gt; &gt; rema=\r\nining value (800MB in your case) by ~5MB/thread to get a plausible \n&gt; &gt; &gt; t=\r\nhread count.\n&gt; &gt; &gt; \n&gt; &gt; &gt; *If* the problem is threads waiting for unrespons=\r\nive hosts, then \n&gt; &gt; &gt; reducing soTimeout may help a little. The one-line r=\r\neports on the \n&gt; &gt; &gt; reports page, or the longer &#39;threads report&#39;, may give=\r\n a hint if this is \n&gt; &gt; &gt; the block. But you can&#39;t make soTimeout too small=\r\n, there are real delays \n&gt; &gt; &gt; for busy networks/server for which you may n=\r\not want to miss that URL \n&gt; &gt; &gt; entirely. It&#39;s a tradeoff you&#39;ll have to de=\r\ncide.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Carefully watching the crawl and aggressively removing U=\r\nRLs from \n&gt; &gt; &gt; unwanted/unresponsive sites (eg by adding new scope limitat=\r\nions during \n&gt; &gt; &gt; the crawl) may offer a better response to slowdowns due =\r\nto unresponsive \n&gt; &gt; &gt; sites.\n&gt; &gt; &gt; \n&gt; &gt; &gt; If threads spending their time o=\r\nn slow/big resources are an issue, \n&gt; &gt; &gt; reducing the timeout-seconds or m=\r\naximum size to download settings could \n&gt; &gt; &gt; help a little. The threads re=\r\nport and crawl log might indicate if this \n&gt; &gt; &gt; is a contributing factor.\n=\r\n&gt; &gt; &gt; \n&gt; &gt; &gt; As a Mac I&#39;m guessing there&#39;s just a single hard drive. That&#39;s=\r\n going to \n&gt; &gt; &gt; cap performance, with all queueing/lookups/scratch-files/c=\r\nontent-writing \n&gt; &gt; &gt; competing for the single disk.\n&gt; &gt; &gt; \n&gt; &gt; &gt; When the =\r\ntradeoffs associated with the Bloom option may be right for \n&gt; &gt; &gt; your pro=\r\nject is something you&#39;ll have to weigh.\n&gt; &gt; &gt; \n&gt; &gt; &gt; - Gordon\n&gt; &gt; &gt; \n&gt; &gt; &gt; =\r\nOn 7/21/11 4:41 PM, helloitsmaxine wrote:\n&gt; &gt; &gt; &gt; I&#39;m referring to what it =\r\nsays on the Activity Monitor, though\n&gt; &gt; &gt; &gt; admittedly I&#39;m not sure what t=\r\nhe relevance of it is. Here are\n&gt; &gt; &gt; &gt; screencaps of the console/Activity =\r\nMonitor (2 panes):\n&gt; &gt; &gt; &gt; http://img39.imageshack.us/img39/2212/ss25h36m.j=\r\npg\n&gt; &gt; &gt; &gt; http://img29.imageshack.us/img29/9120/ss25h36mcpu.jpg\n&gt; &gt; &gt; &gt;\n&gt; =\r\n&gt; &gt; &gt; The swap reported is 4.4gb at this point-about 25h into the\n&gt; &gt; &gt; &gt; c=\r\nrawl...is that a large enough amount to warrant scaling back on\n&gt; &gt; &gt; &gt; hea=\r\np?\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; The HTTP timeout-seconds value I have now is 1200 (that&#39;=\r\ns 20\n&gt; &gt; &gt; &gt; mins...seems long?) and the sotimeout-ms is 20,000 (20s, I gue=\r\nss that\n&gt; &gt; &gt; &gt; makes sense). Would it help to reduce both or just the soti=\r\nmeout? How\n&gt; &gt; &gt; &gt; much of a reduction would you suggest? Could I make it a=\r\ns low as 1\n&gt; &gt; &gt; &gt; second? Or is maybe 5-10 better?\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Current=\r\nly it looks like URI&#39;s crawled is still under a million,\n&gt; &gt; &gt; &gt; though I w=\r\nould eventually like to grow it to the tens of\n&gt; &gt; &gt; &gt; millions--would look=\r\ning into the BloomUriUniqFilter be worth it at\n&gt; &gt; &gt; &gt; this point?\n&gt; &gt; &gt; &gt;\n=\r\n&gt; &gt; &gt; &gt; Thanks for your suggestions; I really appreciate it!\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; =\r\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@&gt;\n&gt; &gt; &gt; &gt; wrot=\r\ne:\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; What do you mean by &quot;VM size&quot;? (What tool is reporting=\r\n that\n&gt; &gt; &gt; &gt;&gt; number?)\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; It would be very atypical for the=\r\n heap size or JavaVM process\n&gt; &gt; &gt; &gt;&gt; address space to be 150 gigabytes.\n&gt; =\r\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; What is the hardware like? (RAM, CPU, disk count/speed)\n&gt; &gt;=\r\n &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; If all the threads are working on something, and the &#39;conges=\r\ntion\n&gt; &gt; &gt; &gt;&gt; ratio&#39; is high, then the problem is not that there&#39;s too litt=\r\nle\n&gt; &gt; &gt; &gt;&gt; that&#39;s eligible to crawl politely.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Some top p=\r\nossibilities:\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; =95 Java process size has grown larger than=\r\n RAM and excessive\n&gt; &gt; &gt; &gt;&gt; swapping is occurring. Due to Java&#39;s pattern of=\r\n memory access, you\n&gt; &gt; &gt; &gt;&gt; essentially never want to be using swap. If &#39;t=\r\nop&#39; or &#39;vmstat&#39; show\n&gt; &gt; &gt; &gt;&gt; any swap being used, add RAM or scale back th=\r\ne heap so that it\n&gt; &gt; &gt; &gt;&gt; isn&#39;t.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; =95 most threads are ma=\r\nking fetches against unresponsive sites, which\n&gt; &gt; &gt; &gt;&gt; can take a long tim=\r\ne to timeout. Large crawls that hit giant\n&gt; &gt; &gt; &gt;&gt; link-lists to defunct/fa=\r\nke sites can experience this. More threads,\n&gt; &gt; &gt; &gt;&gt; shortening the FetchHT=\r\nTP soTimeout, and manually pruning the bad\n&gt; &gt; &gt; &gt;&gt; URIs can help.\n&gt; &gt; &gt; &gt;&gt;=\r\n\n&gt; &gt; &gt; &gt;&gt; =95 the crawl has grown so large that accesses to the\n&gt; &gt; &gt; &gt;&gt; da=\r\nta-structures which overflow to disk (notably the default\n&gt; &gt; &gt; &gt;&gt; &#39;already=\r\n-seen&#39; BdbUriUniqFilter) are now dominating its time usage.\n&gt; &gt; &gt; &gt;&gt; On bro=\r\nader and larger crawls -- expected to grow to more than a few\n&gt; &gt; &gt; &gt;&gt; tens=\r\n of millions of URIs -- you may want to consider the\n&gt; &gt; &gt; &gt;&gt; BloomUriUniqF=\r\nilter, though it has other RAM costs and imprecision\n&gt; &gt; &gt; &gt;&gt; caveats.\n&gt; &gt; =\r\n&gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Hope this helps,\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; - Gordon\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;=\r\n On 7/21/11 3:59 PM, helloitsmaxine wrote:\n&gt; &gt; &gt; &gt;&gt;&gt; Some other details I&#39;v=\r\ne noticed are that according to the\n&gt; &gt; &gt; &gt;&gt;&gt; activity monitor, the VM size=\r\n is about 150gb, and CPU usage\n&gt; &gt; &gt; &gt;&gt;&gt; during the crawl is typically very=\r\n low, with over 95% idle.\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt; Is that weird? If there is so=\r\n much CPU and VM available, would\n&gt; &gt; &gt; &gt;&gt;&gt; it just make sense to keep incr=\r\neasing memory allocation and #\n&gt; &gt; &gt; &gt;&gt;&gt; threads to keep crawl rates up? An=\r\nyone have experience with this\n&gt; &gt; &gt; &gt;&gt;&gt; sort of thing?\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;=\r\n --- In archive-crawler@yahoogroups.com,\n&gt; &gt; &gt; &gt;&gt;&gt; &quot;helloitsmaxine&quot;&lt;itsmaxi=\r\nne@&gt;   wrote:\n&gt; &gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; On my crawls I&#39;ve noticed that they gen=\r\nerally have been\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; starting out at pretty good rates, ie. 1500kb/s=\r\n, but then after\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; a few hours, this goes down to 0-50kb/s and pre=\r\ntty much stays\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; there.\n&gt; &gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; After reading about =\r\nsome other people&#39;s same problems I tried a\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; few tweaks: - increa=\r\nsing JVM memory to 1024 - increasing #\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; threads from 50 to 100 - =\r\nincreasing in/out recording buffers\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; However the problem still pe=\r\nrsists.\n&gt; &gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; Some information about my crawl is: - using H=\r\neritrix 1.4 - the\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; max heap is being used (ie. current and max ar=\r\ne the same) - the\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; threads are all pretty much active (99-100 out=\r\n of 100 active at\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; a time) - very high congestion ratio, (ie. 700=\r\n0) - so far it\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; has crawled 674003 URI&#39;s in about 20h.\n&gt; &gt; &gt; &gt;&gt;&gt;&gt;=\r\n\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; Does anyone have insight into how I can prevent this problem?\n&gt;=\r\n &gt; &gt; &gt;&gt;&gt;&gt; I&#39;ve read this:\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; https://webarchive.jira.com/wiki/displ=\r\nay/Heritrix/making+a+busy+crawl+go+faster\n&gt; &gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; =\r\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt; and a few other posts but am not sure what applies to my sit=\r\nuation and\n&gt; &gt; &gt; &gt;&gt; what would be the best next steps to take. Or if there&#39;=\r\ns any other\n&gt; &gt; &gt; &gt;&gt; information that might be helpful let me know!\n&gt; &gt; &gt; &gt;=\r\n&gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;&gt; Thanks!\n&gt; &gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt;=\r\n &gt; &gt; &gt;&gt;&gt; ------------------------------------\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt; Yahoo! Gr=\r\noups Links\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt;=\r\n &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; ------------------------------------\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Yahoo! =\r\nGroups Links\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}