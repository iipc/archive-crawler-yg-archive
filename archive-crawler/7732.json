{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"John Lekashman","from":"John Lekashman &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"f28u72qhhp8W0R10clyTatA76RN2inhjl1i5oh2ssZFv6w2i_21NiQ31cZjP8FQRI5m0f7D6ZuSqes2bW2IdNoBG4kyleI3qD8s","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Stucked in 690 million discovered.","postDate":"1343409016","msgId":7732,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwMTJDQjc4LjQwODAzMDVAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PGp1dTkwNyt1Zm1pQGVHcm91cHMuY29tPg==","referencesHeader":"PGp1dTkwNyt1Zm1pQGVHcm91cHMuY29tPg=="},"prevInTopic":7731,"nextInTopic":7733,"prevInTime":7731,"nextInTime":7733,"topicId":7731,"numMessagesInTopic":6,"msgSnippet":"Hi, You still on Heretrix 1? I know that H 1 had a problem of an upper limit of around 700M urls per crawler. Split the crawl with a hashmapper. Don t know if","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 26212 invoked from network); 27 Jul 2012 17:10:18 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m6.grp.sp2.yahoo.com with QMQP; 27 Jul 2012 17:10:18 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta4.grp.sp2.yahoo.com with SMTP; 27 Jul 2012 17:10:18 -0000\r\nX-Received: from john-lekashmans-macbook-pro.local (c-24-6-140-74.hsd1.ca.comcast.net [24.6.140.74])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id q6RHAGTT042587;\n\tFri, 27 Jul 2012 10:10:16 -0700 (PDT)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;5012CB78.4080305@...&gt;\r\nDate: Fri, 27 Jul 2012 10:10:16 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:14.0) Gecko/20120713 Thunderbird/14.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com, Elverton &lt;uelverton@...&gt;\r\nReferences: &lt;juu907+ufmi@...&gt;\r\nIn-Reply-To: &lt;juu907+ufmi@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------090206050409090704090209&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: John Lekashman &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Stucked in 690 million discovered.\r\nX-Yahoo-Group-Post: member; u=90724651; y=WLoS06cWedhFOEYfzWBGFeM5I7GUtLAISwVSQHisyO01\r\nX-Yahoo-Profile: lekash\r\n\r\n\r\n--------------090206050409090704090209\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nHi,\nYou still on Heretrix 1?\n\nI know that H 1 had a problem of an upper limit of around 700M urls per \ncrawler.\nSplit the crawl with a hashmapper.\n\nDon&#39;t know if H 3 has that problem, I always split those as well, to get \nthings done\nin finite time.\n\nJohn\n\nOn 7/27/12 7:35 AM, Elverton wrote:\n&gt;\n&gt; Hello everybody.\n&gt;\n&gt; Well, I&#39;m having a big trouble this time. Before I explain the \n&gt; problem, here is the system configuration:\n&gt;\n&gt; - 24 GB RAM\n&gt; - Intel(R) Xeon(R) CPU E5520 @ 2.27GHz\n&gt; - 1.8TB hard disk for Heritrix. (I don&#39;t use warc in this crawl. My \n&gt; only target is to know how many (approx.) URLs a domain has.) The \n&gt; usage of the disk is: used 500GB, free 1.3TB.\n&gt; - 16GB java heap size for heritrix.\n&gt; - Java 1.7.0_05\n&gt;\n&gt; Here is the Heritrix configuration that I consider helpful to the\n&gt; problem:\n&gt;\n&gt; - bdb-cache-percent = 25\n&gt; - frontier = BdbFrontier\n&gt; - max-delay-ms = 10000\n&gt; - min-delay-ms = 2000\n&gt; - respect-crawl-delay-up-to-secs = 300\n&gt; - max-retries = 10\n&gt; - retry-delay-seconds = 30\n&gt; - timeout-seconds = 1200\n&gt; - sotimeout-ms = 20000\n&gt;\n&gt; % ----------------------------------------------------------\n&gt;\n&gt; So, my problem is: the crawl stucked in 690 million discovered. \n&gt; (Queued it&#39;s around 520 million and downloaded is around 170 million).\n&gt;\n&gt; The strange thing is the download/uri rate.\n&gt;\n&gt; Docs/s(avg): 53.2(60.77)\n&gt; KB/s(avg): 2069(3205)\n&gt;\n&gt; It continues, in some way, good in theory (about 3 or 4 million uri \n&gt; crawled per day if you have 53.2 uri&#39;s during all day), but the real \n&gt; crawled per day is below 500.000 (discovered).\n&gt;\n&gt; Looking at some number in the last five days:\n&gt; Queued Downloaded\n&gt; 541054381 133121289\n&gt; 535322185 138522175\n&gt; 530280577 143176680\n&gt; 525907149 147086865\n&gt; 520568517 151604201\n&gt;\n&gt; Notice that the queued decreases at the &quot;same&quot; rate that downloaded \n&gt; increases. The problem could be getting URIs to the queue. A possible \n&gt; is the URIs discovered now had be crawled before, and doesn&#39;t go the \n&gt; the queue anymore. But the domain I&#39;m crawling has about 2 million \n&gt; domains and I got only 70.000, so there&#39;re many URI&#39;s to be crawled \n&gt; yet. :)\n&gt;\n&gt; Other possibility I thought could be a swap problem (too much I/O). \n&gt; For my surprise (using vmstat), the swpd is 0.\n&gt;\n&gt; Another problem could be know if a URI was crawled already.\n&gt; Before the URI goes to the frontier, heritrix verifies it in a queue, \n&gt; using the hash technique. If the crawling is big enough, the search \n&gt; get slower, even using hash, because there are many URI&#39;s for a key in \n&gt; hash table.\n&gt;\n&gt; But, I really don&#39;t know the exactly problem. Anyone had this problem \n&gt; or could point a direction?\n&gt;\n&gt; Thanks,\n&gt; Elverton.\n&gt;\n&gt; \n\n\r\n--------------090206050409090704090209\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta content=&quot;text/html; charset=ISO-8859-1&quot;\n      http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#FFFFFF&quot; text=&quot;#000000&quot;&gt;\n    &lt;div class=&quot;moz-cite-prefix&quot;&gt;Hi,&lt;br&gt;\n      You still on Heretrix 1?&nbsp; &lt;br&gt;\n      &lt;br&gt;\n      I know that H 1 had a problem of an upper limit of around 700M\n      urls per crawler.&lt;br&gt;\n      Split the crawl with a hashmapper.&lt;br&gt;\n      &lt;br&gt;\n      Don&#39;t know if H 3 has that problem, I always split those as well,\n      to get things done&lt;br&gt;\n      in finite time.&lt;br&gt;\n      &lt;br&gt;\n      John&lt;br&gt;\n      &lt;br&gt;\n      On 7/27/12 7:35 AM, Elverton wrote:&lt;br&gt;\n    &lt;/div&gt;\n    &lt;blockquote cite=&quot;mid:juu907+ufmi@...&quot; type=&quot;cite&quot;&gt;\n      &lt;span style=&quot;display:none&quot;&gt;&nbsp;&lt;/span&gt;\n      \n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;Hello everybody.&lt;br&gt;\n              &lt;br&gt;\n              Well, I&#39;m having a big trouble this time. Before I explain\n              the problem, here is the system configuration:&lt;br&gt;\n              &lt;br&gt;\n              - 24 GB RAM&lt;br&gt;\n              - Intel(R) Xeon(R) CPU E5520 @ 2.27GHz&lt;br&gt;\n              - 1.8TB hard disk for Heritrix. (I don&#39;t use warc in this\n              crawl. My only target is to know how many (approx.) URLs a\n              domain has.) The usage of the disk is: used 500GB, free\n              1.3TB.&lt;br&gt;\n              - 16GB java heap size for heritrix.&lt;br&gt;\n              - Java 1.7.0_05&lt;br&gt;\n              &lt;br&gt;\n              Here is the Heritrix configuration that I consider helpful\n              to the&lt;br&gt;\n              problem:&lt;br&gt;\n              &lt;br&gt;\n              - bdb-cache-percent = 25&lt;br&gt;\n              - frontier = BdbFrontier&lt;br&gt;\n              - max-delay-ms = 10000&lt;br&gt;\n              - min-delay-ms = 2000&lt;br&gt;\n              - respect-crawl-delay-up-to-secs = 300&lt;br&gt;\n              - max-retries = 10&lt;br&gt;\n              - retry-delay-seconds = 30&lt;br&gt;\n              - timeout-seconds = 1200&lt;br&gt;\n              - sotimeout-ms = 20000&lt;br&gt;\n              &lt;br&gt;\n              %\n              ----------------------------------------------------------&lt;br&gt;\n              &lt;br&gt;\n              So, my problem is: the crawl stucked in 690 million\n              discovered. (Queued it&#39;s around 520 million and downloaded\n              is around 170 million).&lt;br&gt;\n              &lt;br&gt;\n              The strange thing is the download/uri rate.&lt;br&gt;\n              &lt;br&gt;\n              Docs/s(avg): 53.2(60.77)&lt;br&gt;\n              KB/s(avg): 2069(3205)&lt;br&gt;\n              &lt;br&gt;\n              It continues, in some way, good in theory (about 3 or 4\n              million uri crawled per day if you have 53.2 uri&#39;s during\n              all day), but the real crawled per day is below 500.000\n              (discovered). &lt;br&gt;\n              &lt;br&gt;\n              Looking at some number in the last five days:&lt;br&gt;\n              Queued Downloaded&lt;br&gt;\n              541054381 133121289&lt;br&gt;\n              535322185 138522175&lt;br&gt;\n              530280577 143176680&lt;br&gt;\n              525907149 147086865&lt;br&gt;\n              520568517 151604201&lt;br&gt;\n              &lt;br&gt;\n              Notice that the queued decreases at the &quot;same&quot; rate that\n              downloaded increases. The problem could be getting URIs to\n              the queue. A possible is the URIs discovered now had be\n              crawled before, and doesn&#39;t go the the queue anymore. But\n              the domain I&#39;m crawling has about 2 million domains and I\n              got only 70.000, so there&#39;re many URI&#39;s to be crawled yet.\n              :)&lt;br&gt;\n              &lt;br&gt;\n              Other possibility I thought could be a swap problem (too\n              much I/O). For my surprise (using vmstat), the swpd is 0.\n              &lt;br&gt;\n              &lt;br&gt;\n              Another problem could be know if a URI was crawled\n              already. &lt;br&gt;\n              Before the URI goes to the frontier, heritrix verifies it\n              in a queue, using the hash technique. If the crawling is\n              big enough, the search get slower, even using hash,\n              because there are many URI&#39;s for a key in hash table.&lt;br&gt;\n              &lt;br&gt;\n              But, I really don&#39;t know the exactly problem. Anyone had\n              this problem or could point a direction?&lt;br&gt;\n              &lt;br&gt;\n              Thanks,&lt;br&gt;\n              Elverton.&lt;br&gt;\n              &lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- end group email --&gt;\n    &lt;/blockquote&gt;\n    &lt;br&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------090206050409090704090209--\r\n\n"}}