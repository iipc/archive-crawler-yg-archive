{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"TMP-g90LlOQuiogykgb_eMC3zl1N7798khVChEAzQSEKHLGeocMzv8MPxzzIVhlv98uBNKHJyoGPfT4cu95UbLuQUVE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: questions before we restart the crawl","postDate":"1327445693","msgId":7568,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMUYzNkJELjkwNTA0QGNzLmNtdS5lZHU+","inReplyToHeader":"PDRGMUE0QjlDLjUwOTAwMDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PDRGMTU5NEQwLjIwOTA4MDhAY3MuY211LmVkdT4gPDRGMTVCQjNBLjUwMzA2QGFyY2hpdmUub3JnPiA8NEYxOUIzOTEuMTA3MDQwMUBjcy5jbXUuZWR1PiA8NEYxQTA0QkMuNjAxMDQwMkBjcy5jbXUuZWR1PiA8NEYxQTRCOUMuNTA5MDAwN0BhcmNoaXZlLm9yZz4="},"prevInTopic":7567,"nextInTopic":7569,"prevInTime":7567,"nextInTime":7569,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"Gordon, We are going to attempt to do a split recovery on one instance.  I want to separate this process into another directory.  Does the recovery process","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 23226 invoked from network); 24 Jan 2012 22:54:59 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m8.grp.sp2.yahoo.com with QMQP; 24 Jan 2012 22:54:59 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.95)\n  by mta1.grp.sp2.yahoo.com with SMTP; 24 Jan 2012 22:54:59 -0000\r\nX-Received: from [128.2.209.200] (SAVOY.LTI.CS.CMU.EDU [128.2.209.200])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id q0OMssXo022828\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tTue, 24 Jan 2012 17:54:54 -0500\r\nMessage-ID: &lt;4F1F36BD.90504@...&gt;\r\nDate: Tue, 24 Jan 2012 17:54:53 -0500\r\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: Gordon Mohr &lt;gojomo@...&gt;\r\nCc: archive-crawler@yahoogroups.com, Noah Levitt &lt;nlevitt@...&gt;\r\nReferences: &lt;4F1594D0.2090808@...&gt; &lt;4F15BB3A.50306@...&gt; &lt;4F19B391.1070401@...&gt; &lt;4F1A04BC.6010402@...&gt; &lt;4F1A4B9C.5090007@...&gt;\r\nIn-Reply-To: &lt;4F1A4B9C.5090007@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2011.5.19.222118\r\nX-SMTP-Spam-Clean: 8% (\n BODYTEXTP_SIZE_3000_LESS 0, BODY_SIZE_2000_2999 0, BODY_SIZE_5000_LESS 0, BODY_SIZE_7000_LESS 0, DATE_TZ_NEG_0500 0, __ANY_URI 0, __BOUNCE_CHALLENGE_SUBJ 0, __BOUNCE_NDR_SUBJ_EXEMPT 0, __CANPHARM_UNSUB 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __SANE_MSGID 0, __TO_MALFORMED_2 0, __URI_NO_MAILTO 0, __URI_NO_PATH 0, __URI_NO_WWW 0, __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.95\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=500983475; y=ABgf6rSSXkDc_gyLKmffXdMEoBa96r74T_VxaezPPPcXUPx0dRhK3Q\r\nX-Yahoo-Profile: david_pane1\r\n\r\nGordon,\n\nWe are going to attempt to do a split recovery on one instance.  I want \nto separate this process into another directory.  Does the recovery \nprocess only need the frontier.recover files, the crawler-beans.cxml, \nopt-out.surt and seeds.txt? Can all checkpoints and state files be \ndeleted and the warc files and logs moved to another location?\n\n--David\n\n\nOn 1/21/2012 12:22 AM, Gordon Mohr wrote:\n&gt;&gt; 1) Can we continue from here but with &quot;clean&quot; Heritrix instances?\n&gt;&gt;\n&gt;&gt; Is there a way that we can continue from the this point forward, but\n&gt;&gt; start with Heritrix instances that will not be corrupt due to sever\n&gt;&gt; error? (e.g. using the\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery ) If\n&gt;&gt; so, would you recommend doing this? You mentioned that this could be\n&gt;&gt; time consuming. Each of our instances has downloaded around 170M URIs,\n&gt;&gt; they have over 700M queued URIs, what is your time estimate for\n&gt;&gt; something this large?\n&gt;&gt;\n&gt;&gt; We are willing to sacrifice a few days to get our crawler to a clean\n&gt;&gt; state again so we can crawl for another 30 days at the pace we have been\n&gt;&gt; crawling.\n&gt;\n&gt; You can do a big &#39;frontier-recover&#39; log replay to avoid recrawling the\n&gt; same URIs, and approximate the earlier queue state. Splitting/filters\n&gt; the logs manually beforehand as alluded to in the wiki page can speed\n&gt; this process somewhat... but given the size of all your log-segments\n&gt; that log grooming beforehand is itself likely to be a lengthy process.\n&gt;\n&gt; I don&#39;t think we&#39;ve ever done it with logs of 170M crawled / 870M\n&gt; discovered before, nor on any hardware comparable to yours. So it&#39;s\n&gt; impossible to project its duration in your environment. It&#39;s taken 2-3\n&gt; days for us on smaller crawls, slower hardware.\n&gt;\n&gt; An added complication is that this older frontier-recover-log replay\n&gt; technique happens in its own thread separate from the checkpointing\n&gt; process, so it is not, itself, accurately checkpointed during the long\n&gt; reload process.\n&gt;\n&gt; At nearly 1B discovered URIs per node, even if you are using the\n&gt; alternate BloomUriUniqFilter, if you are using it at its default size\n&gt; (~500MB) it will now be heavily saturated and thus returning many\n&gt; false-positives causing truly unique URIs to be rejected as duplicates.\n&gt; (If you&#39;re using a significantly larger filter, you may not yet be at a\n&gt; high false-positive rate: you&#39;d have to do the bloom filter math. If\n&gt; you&#39;re still using BdbUriUniqFilter, you&#39;re way way past the point where\n&gt; its disk seeks have usually made it too slow for our purposes.)\n\n"}}