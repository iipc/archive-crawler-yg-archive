{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":465980601,"authorName":"Zach Bailey","from":"Zach Bailey &lt;zach.bailey@...&gt;","replyTo":"LIST","senderId":"EaNeF-dQVCXRP63uFp7hRgcMN9olYvU1GTADB36pNDWTAXRCIQtNsCbtBxGA7rWrQ1RJRCtUumzd5Znm9bHlkq4k7jQ2PZTPGT2bCjE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: WriterPool Members getting interrupted at conclusion of crawl?","postDate":"1285285736","msgId":6745,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOUJFNzY4LjgwNDA0MDlAZGF0YWNsaXAuY29tPg==","inReplyToHeader":"PDRDOUI3OTE4LjQwMzAzMDFAZGF0YWNsaXAuY29tPg==","referencesHeader":"PDRDOUI3OTE4LjQwMzAzMDFAZGF0YWNsaXAuY29tPg=="},"prevInTopic":6744,"nextInTopic":0,"prevInTime":6744,"nextInTime":6746,"topicId":6744,"numMessagesInTopic":2,"msgSnippet":"It looks like Bertrand Dechoux was having this exact same problem back at the beginning of February and managed to work around it by adding some additional","rawEmail":"Return-Path: &lt;zach.bailey@...&gt;\r\nX-Sender: zach.bailey@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 99099 invoked from network); 23 Sep 2010 23:49:00 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m17.grp.re1.yahoo.com with QMQP; 23 Sep 2010 23:49:00 -0000\r\nX-Received: from unknown (HELO mail-gy0-f173.google.com) (209.85.160.173)\n  by mta1.grp.re1.yahoo.com with SMTP; 23 Sep 2010 23:49:00 -0000\r\nX-Received: by gya1 with SMTP id 1so1067019gya.4\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Sep 2010 16:48:59 -0700 (PDT)\r\nX-Received: by 10.151.157.10 with SMTP id j10mr3589136ybo.365.1285285739804;\n        Thu, 23 Sep 2010 16:48:59 -0700 (PDT)\r\nReturn-Path: &lt;zach.bailey@...&gt;\r\nX-Received: from znbailey-2.local ([173.160.76.185])\n        by mx.google.com with ESMTPS id t20sm294879ybm.5.2010.09.23.16.48.58\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Thu, 23 Sep 2010 16:48:59 -0700 (PDT)\r\nMessage-ID: &lt;4C9BE768.8040409@...&gt;\r\nDate: Thu, 23 Sep 2010 19:48:56 -0400\r\nUser-Agent: Postbox 2.0.0b4 (Macintosh/20100915)\r\nMIME-Version: 1.0\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nReferences: &lt;4C9B7918.4030301@...&gt;\r\nIn-Reply-To: &lt;4C9B7918.4030301@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------070605060700000600010908&quot;\r\nFrom: Zach Bailey &lt;zach.bailey@...&gt;\r\nSubject: Re: WriterPool Members getting interrupted at conclusion of crawl?\r\nX-Yahoo-Group-Post: member; u=465980601\r\n\r\n\r\n--------------070605060700000600010908\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nIt looks like Bertrand Dechoux was having this exact same problem back \nat the beginning of February and managed to work around it by adding \nsome additional lifecycle events to the heritrix code.\n\nInitial Problem: \nhttp://tech.groups.yahoo.com/group/archive-crawler/message/6345\nHis Resolution: \nhttp://tech.groups.yahoo.com/group/archive-crawler/message/6350\n\nBertrand, if you&#39;re out there, I would appreciate if you could share \nyour custom CrawlController code... :)\n\nHere is the problem as I see it, after digging through the code for a \ncouple of hours this afternoon:\n\nThe WriterPoolProcessor pool is closed when the Spring lifecycle calls \nstop() - the problem is, this happens when the application context is \nbeing destroyed which happens AFTER CrawlController.completeStop() is \ncalled - which is where the toe threads are interrupted (via \nToePool.cleanup()).\n\nIt looks like you&#39;re already using the spring ApplicationEvent \nfacilities which is great. I think the simplest way to solve this would \nbe to make WriterPoolProcessor implement ApplicationListener and then \nimplement the onApplicationEvent method like this:\n\npublic void onApplicationEvent(ApplicationEvent applicationEvent) {\n         if ( applicationEvent instanceof CrawlStateEvent ) {\n             CrawlStateEvent crawlEvent = (CrawlStateEvent) \napplicationEvent;\n             if ( crawlEvent.getState() == \nCrawlController.State.STOPPING ) {\n                 getPool().close();\n             }\n         }\n     }\n\nThis ensures the pool closes BEFORE the threads get forcibly interrupted \nwhich should keep everything happy. Thoughts on that solution?\n\nAlso, in general - it looks like the thread pool management you&#39;re doing \nis rather old school and re-invents the wheel a bit. Have you considered \ntaking a look at using the java.util.concurrent facilities instead, \nspecifically ThreadPoolExecutor?\n\n-Zach\n\nZach Bailey wrote:\n&gt; I wanted to run this by you guys and get your thoughts - I am running \n&gt; into a problem with the HDFS writer I&#39;m using to write crawl results \n&gt; from heritrix 3. If you&#39;re curious you can find the code here: \n&gt; http://github.com/openplaces/heritrix-hdfs-writer\n&gt;\n&gt; The problem I&#39;m seeing is this - at the end of the crawl, it appears \n&gt; the job-related threads are being interrupted before they&#39;re \n&gt; completely finished processing (though I have not verified this by \n&gt; tracing through the code). Here is a warning I see pop up in the logs \n&gt; at the end of the crawl:\n&gt;\n&gt; 2010-09-23 15:05:13.125 INFO thread-18 \n&gt; org.archive.crawler.framework.CrawlJob.onApplicationEvent() STOPPING\n&gt; 2010-09-23 15:05:13.126 INFO thread-18 \n&gt; org.archive.crawler.framework.CrawlJob.onApplicationEvent() FINISHED\n&gt; 10/09/23 15:05:13 WARN hdfs.DFSClient: DFSOutputStream \n&gt; ResponseProcessor exception  for block \n&gt; blk_5887928276646679438_1127java.io.InterruptedIOException: Interruped \n&gt; while waiting for IO on channel \n&gt; java.nio.channels.SocketChannel[connected local=/127.0.0.1:57686 \n&gt; remote=/127.0.0.1:50010]. 46903 millis timeout left.\n&gt;     at \n&gt; org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:349)\n&gt;     at \n&gt; org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n&gt;     at \n&gt; org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)\n&gt;     at \n&gt; org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\n&gt;     at java.io.DataInputStream.readFully(DataInputStream.java:178)\n&gt;     at java.io.DataInputStream.readLong(DataInputStream.java:399)\n&gt;     at \n&gt; org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2397)\n&gt;\n&gt; This is a problem because it causes an abnormal termination of \n&gt; communication between the DFSClient and the HDFS server, which results \n&gt; in the crawl file not being closed correctly. Examining the state of \n&gt; the HDFS file after this happens we see it has a file size of 0:\n&gt;\n&gt; zach@znbailey-2:bin$ ./hadoop fs -ls /crawl\n&gt; Found 1 items\n&gt; -rw-r--r--   3 zach supergroup          0 2010-09-23 10:55 \n&gt; /crawl/CrawlData-20100923145529-00000\n&gt;\n&gt; and running an HDFS &quot;fsck&quot; shows us that HDFS thinks the file is still \n&gt; &quot;open&quot;:\n&gt;\n&gt; zach@znbailey-2:bin$ ./hadoop fsck /\n&gt; Status: HEALTHY\n&gt;  Total size:    0 B\n&gt;  Total dirs:    3\n&gt;  Total files:    0 *(Files currently being written: 1)*\n&gt;  Total blocks (validated):    0 *(Total open file blocks (not \n&gt; validated): 1)*\n&gt;  Minimally replicated blocks:    0\n&gt;  Over-replicated blocks:    0\n&gt;  Under-replicated blocks:    0\n&gt;  Mis-replicated blocks:        0\n&gt;  Default replication factor:    1\n&gt;  Average block replication:    0.0\n&gt;  Corrupt blocks:        0\n&gt;  Missing replicas:        0\n&gt;  Number of data-nodes:        1\n&gt;  Number of racks:        1\n&gt;\n&gt; As I said I have not verified this behavior 100% and I&#39;m only going \n&gt; off the evidence in the stack traces, so I&#39;m not sure if my hypothesis \n&gt; about heritrix interrupting these threads is right or not. So, I was \n&gt; hoping you could shed some light on that.\n&gt;\n&gt; If it is indeed the case that this is how heritrix is behaving, then \n&gt; do I need to see about modifying the HDFS writer code to handle these \n&gt; thread interrupts better?\n&gt;\n&gt; Thanks,\n&gt; -Zach \n\r\n--------------070605060700000600010908\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta content=&quot;text/html; charset=ISO-8859-1&quot;\n      http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#ffffff&quot; text=&quot;#000000&quot;&gt;\n    It looks like Bertrand Dechoux was having this exact same problem\n    back at the beginning of February and managed to work around it by\n    adding some additional lifecycle events to the heritrix code.&lt;br&gt;\n    &lt;br&gt;\n    Initial Problem:\n    &lt;a class=&quot;moz-txt-link-freetext&quot; href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/6345&quot;&gt;http://tech.groups.yahoo.com/group/archive-crawler/message/6345&lt;/a&gt;&lt;br&gt;\n    His Resolution:\n    &lt;a class=&quot;moz-txt-link-freetext&quot; href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/6350&quot;&gt;http://tech.groups.yahoo.com/group/archive-crawler/message/6350&lt;/a&gt;&lt;br&gt;\n    &lt;br&gt;\n    Bertrand, if you&#39;re out there, I would appreciate if you could share\n    your custom CrawlController code... :)&lt;br&gt;\n    &lt;br&gt;\n    Here is the problem as I see it, after digging through the code for\n    a couple of hours this afternoon:&lt;br&gt;\n    &lt;br&gt;\n    The WriterPoolProcessor pool is closed when the Spring lifecycle\n    calls stop() - the problem is, this happens when the application\n    context is being destroyed which happens AFTER\n    CrawlController.completeStop() is called - which is where the toe\n    threads are interrupted (via ToePool.cleanup()). &lt;br&gt;\n    &lt;br&gt;\n    It looks like you&#39;re already using the spring ApplicationEvent\n    facilities which is great. I think the simplest way to solve this\n    would be to make WriterPoolProcessor implement ApplicationListener\n    and then implement the onApplicationEvent method like this:&lt;br&gt;\n    &lt;br&gt;\n    &lt;tt&gt;public void onApplicationEvent(ApplicationEvent\n      applicationEvent) {&lt;br&gt;\n      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ( applicationEvent instanceof CrawlStateEvent ) {&lt;br&gt;\n      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CrawlStateEvent crawlEvent = (CrawlStateEvent)\n      applicationEvent;&lt;br&gt;\n      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ( crawlEvent.getState() ==\n      CrawlController.State.STOPPING ) {&lt;br&gt;\n      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; getPool().close();&nbsp;&nbsp; &lt;br&gt;\n      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;br&gt;\n      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; }&lt;/tt&gt;&lt;br&gt;\n    &lt;br&gt;\n    This ensures the pool closes BEFORE the threads get forcibly\n    interrupted which should keep everything happy. Thoughts on that\n    solution?&lt;br&gt;\n    &lt;br&gt;\n    Also, in general - it looks like the thread pool management you&#39;re\n    doing is rather old school and re-invents the wheel a bit. Have you\n    considered taking a look at using the java.util.concurrent\n    facilities instead, specifically ThreadPoolExecutor?&lt;br&gt;\n    &lt;br&gt;\n    -Zach&lt;br&gt;\n    &lt;br&gt;\n    Zach Bailey wrote:\n    &lt;blockquote cite=&quot;mid:4C9B7918.4030301@...&quot; type=&quot;cite&quot;&gt;\n      &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;\n        charset=ISO-8859-1&quot;&gt;\n      I wanted to run this by you guys and get your thoughts - I am\n      running into a problem with the HDFS writer I&#39;m using to write\n      crawl results from heritrix 3. If you&#39;re curious you can find the\n      code here: &lt;a moz-do-not-send=&quot;true&quot;\n        class=&quot;moz-txt-link-freetext&quot;\n        href=&quot;http://github.com/openplaces/heritrix-hdfs-writer&quot;&gt;http://github.com/openplaces/heritrix-hdfs-writer&lt;/a&gt;&lt;br&gt;\n      &lt;br&gt;\n      The problem I&#39;m seeing is this - at the end of the crawl, it\n      appears the job-related threads are being interrupted before\n      they&#39;re completely finished processing (though I have not verified\n      this by tracing through the code). Here is a warning I see pop up\n      in the logs at the end of the crawl:&lt;br&gt;\n      &lt;br&gt;\n      &lt;tt&gt;2010-09-23 15:05:13.125 INFO thread-18\n        org.archive.crawler.framework.CrawlJob.onApplicationEvent()\n        STOPPING&lt;br&gt;\n        2010-09-23 15:05:13.126 INFO thread-18\n        org.archive.crawler.framework.CrawlJob.onApplicationEvent()\n        FINISHED&lt;br&gt;\n        10/09/23 15:05:13 WARN hdfs.DFSClient: DFSOutputStream\n        ResponseProcessor exception&nbsp; for block\n        blk_5887928276646679438_1127java.io.InterruptedIOException:\n        Interruped while waiting for IO on channel\n        java.nio.channels.SocketChannel[connected local=/127.0.0.1:57686\n        remote=/127.0.0.1:50010]. 46903 millis timeout left.&lt;br&gt;\n        &nbsp;&nbsp;&nbsp; at\norg.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:349)&lt;br&gt;\n        &nbsp;&nbsp;&nbsp; at\norg.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)&lt;br&gt;\n        &nbsp;&nbsp;&nbsp; at\n        org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)&lt;br&gt;\n        &nbsp;&nbsp;&nbsp; at\n        org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)&lt;br&gt;\n        &nbsp;&nbsp;&nbsp; at\n        java.io.DataInputStream.readFully(DataInputStream.java:178)&lt;br&gt;\n        &nbsp;&nbsp;&nbsp; at\n        java.io.DataInputStream.readLong(DataInputStream.java:399)&lt;br&gt;\n        &nbsp;&nbsp;&nbsp; at\norg.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2397)&lt;/tt&gt;&lt;br&gt;\n      &lt;br&gt;\n      This is a problem because it causes an abnormal termination of\n      communication between the DFSClient and the HDFS server, which\n      results in the crawl file not being closed correctly. Examining\n      the state of the HDFS file after this happens we see it has a file\n      size of 0:&lt;br&gt;\n      &lt;br&gt;\n      &lt;tt&gt;zach@znbailey-2:bin$ ./hadoop fs -ls /crawl&lt;br&gt;\n        Found 1 items&lt;br&gt;\n        -rw-r--r--&nbsp;&nbsp; 3 zach supergroup&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 2010-09-23 10:55\n        /crawl/CrawlData-20100923&lt;span\n          __postbox-detected-content=&quot;__postbox-detected-phone&quot;\n          class=&quot;__postbox-detected-content __postbox-detected-phone&quot;\n          style=&quot;display: inline; font-size: inherit; padding: 0pt;&quot;&gt;145529-0000&lt;/span&gt;0&lt;br&gt;\n      &lt;/tt&gt;&lt;br&gt;\n      and running an HDFS &quot;fsck&quot; shows us that HDFS thinks the file is\n      still &quot;open&quot;:&lt;br&gt;\n      &lt;br&gt;\n      &lt;tt&gt;zach@znbailey-2:bin$ ./hadoop fsck /&lt;br&gt;\n        Status: HEALTHY&lt;br&gt;\n        &nbsp;Total size:&nbsp;&nbsp;&nbsp; 0 B&lt;br&gt;\n        &nbsp;Total dirs:&nbsp;&nbsp;&nbsp; 3&lt;br&gt;\n        &nbsp;Total files:&nbsp;&nbsp;&nbsp; 0 &lt;b&gt;(Files currently being written: 1)&lt;/b&gt;&lt;br&gt;\n        &nbsp;Total blocks (validated):&nbsp;&nbsp;&nbsp; 0 &lt;b&gt;(Total open file blocks (not\n          validated): 1)&lt;/b&gt;&lt;br&gt;\n        &nbsp;Minimally replicated blocks:&nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n        &nbsp;Over-replicated blocks:&nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n        &nbsp;Under-replicated blocks:&nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n        &nbsp;Mis-replicated blocks:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n        &nbsp;Default replication factor:&nbsp;&nbsp;&nbsp; 1&lt;br&gt;\n        &nbsp;Average block replication:&nbsp;&nbsp;&nbsp; 0.0&lt;br&gt;\n        &nbsp;Corrupt blocks:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n        &nbsp;Missing replicas:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n        &nbsp;Number of data-nodes:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 1&lt;br&gt;\n        &nbsp;Number of racks:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 1&lt;/tt&gt;&lt;br&gt;\n      &lt;br&gt;\n      As I said I have not verified this behavior 100% and I&#39;m only\n      going off the evidence in the stack traces, so I&#39;m not sure if my\n      hypothesis about heritrix interrupting these threads is right or\n      not. So, I was hoping you could shed some light on that.&lt;br&gt;\n      &lt;br&gt;\n      If it is indeed the case that this is how heritrix is behaving,\n      then do I need to see about modifying the HDFS writer code to\n      handle these thread interrupts better?&lt;br&gt;\n      &lt;br&gt;\n      Thanks,&lt;br&gt;\n      -Zach &lt;/blockquote&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------070605060700000600010908--\r\n\n"}}