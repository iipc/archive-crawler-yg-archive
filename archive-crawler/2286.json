{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":83282930,"authorName":"Jay","from":"&quot;Jay&quot; &lt;bighead007us@...&gt;","profile":"bighead007us","replyTo":"LIST","senderId":"5BtB6qXCcvArO9kBnLaM7mPJYLCnHo31gu1kHDX28duJOmNxt9bLVAQiLUf_YvCilVCruu9CldzCHH1R3Kk5Cen5B1u9","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Different Robots Honoring in One Job","postDate":"1130394936","msgId":2286,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRqcHNmbys5aDE5QGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":2287,"prevInTime":2285,"nextInTime":2287,"topicId":2286,"numMessagesInTopic":3,"msgSnippet":"Hello Everyone, I am wondering whether is there any way that I can have different robots.txt honoring policy in one job.  Since I have several urls that I","rawEmail":"Return-Path: &lt;bighead007us@...&gt;\r\nX-Sender: bighead007us@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 76207 invoked from network); 27 Oct 2005 06:35:40 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m35.grp.scd.yahoo.com with QMQP; 27 Oct 2005 06:35:40 -0000\r\nReceived: from unknown (HELO n16a.bulk.scd.yahoo.com) (66.94.237.45)\n  by mta3.grp.scd.yahoo.com with SMTP; 27 Oct 2005 06:35:40 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n16.bulk.scd.yahoo.com with NNFMP; 27 Oct 2005 06:35:37 -0000\r\nReceived: from [66.218.66.86] by mailer5.bulk.scd.yahoo.com with NNFMP; 27 Oct 2005 06:35:37 -0000\r\nDate: Thu, 27 Oct 2005 06:35:36 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;djpsfo+9h19@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Jay&quot; &lt;bighead007us@...&gt;\r\nSubject: Different Robots Honoring in One Job\r\nX-Yahoo-Group-Post: member; u=83282930; y=hmeTDSF0cDHt0-ZnzLGz-7kDLX-1y0R4ce0YD8PER9oF_iHqS7dV\r\nX-Yahoo-Profile: bighead007us\r\n\r\nHello Everyone,\n\nI am wondering whether is there any way that I can have di=\r\nfferent robots.txt honoring policy \nin one job.  Since I have several urls =\r\nthat I wanted to crawl but one of the url is disallowing /\nimages/ director=\r\ny with robots.txt, but I wanted to crawl the images.  At the same time, I d=\r\non&#39;t \nwant to ignore other urls&#39; robots.txt.  Also I don&#39;t want to separate=\r\n the url in different job.  \n\nAny thoughts?\n\nThx,\nJay\n\n\n\n\n"}}