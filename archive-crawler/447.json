{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":182123250,"authorName":"Kaisa Kaunonen","from":"&quot;Kaisa Kaunonen&quot; &lt;kaisa.kaunonen@...&gt;","profile":"kaisa_kaunonen","replyTo":"LIST","senderId":"gdlRhErX83JRhEJIX_LAY1JUvKJxA36zsKtoTs3x7q69q72bmS0H0hUUhhLwm8n3NeOjTtJbPOfTCOzyY6umq9Ozse11vsprST83Ge6Y_JakOaGS","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Large scale crawling with Heritrix","postDate":"1085485633","msgId":447,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM4dmJvMStkbTdrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQwQjIzQTBFLjIwMTA2MDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":445,"nextInTopic":448,"prevInTime":446,"nextInTime":448,"topicId":432,"numMessagesInTopic":9,"msgSnippet":"Hi, ... much ... Just a thought :) Urls with parameters look like search results from databases. More parameters = (often) a larger database. If a url has six","rawEmail":"Return-Path: &lt;kaisa.kaunonen@...&gt;\r\nX-Sender: kaisa.kaunonen@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 55849 invoked from network); 25 May 2004 11:47:17 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m21.grp.scd.yahoo.com with QMQP; 25 May 2004 11:47:17 -0000\r\nReceived: from unknown (HELO n37.grp.scd.yahoo.com) (66.218.66.105)\n  by mta4.grp.scd.yahoo.com with SMTP; 25 May 2004 11:47:17 -0000\r\nReceived: from [66.218.66.124] by n37.grp.scd.yahoo.com with NNFMP; 25 May 2004 11:47:14 -0000\r\nDate: Tue, 25 May 2004 11:47:13 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;c8vbo1+dm7k@...&gt;\r\nIn-Reply-To: &lt;40B23A0E.2010608@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Length: 7744\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.105\r\nFrom: &quot;Kaisa Kaunonen&quot; &lt;kaisa.kaunonen@...&gt;\r\nSubject: Re: Large scale crawling with Heritrix\r\nX-Yahoo-Group-Post: member; u=182123250\r\nX-Yahoo-Profile: kaisa_kaunonen\r\n\r\n\nHi,\n\n--- In archive-crawler@yahoogroups.com, Igor Ranitovic &lt;igor@a...&gt; \nw=\r\nrote:\n\n&gt; &gt; *)  Reject urls with too many parameters. Set maximum number of\n=\r\n&gt; &gt; parameters to some value: are 4 parameters acceptable, but 5 too \nmuch\n=\r\n&gt; &gt; (or some other limit)?\n&gt; &gt; \n&gt; &gt; http:/www=85...net/?\n&gt; &gt; project_id=3D2=\r\n&ykieli=3Dfi&startfrom=3D0&stopto=3D6&commands=3D329&pic=3D\n&gt; \n&gt; I am not s=\r\nure why would you want to do this?\n&gt; \n&gt; Take care.\n&gt; i.\n\n\nJust a thought :)=\r\n\n\nUrls with parameters look like search results from databases. More \nparam=\r\neters =3D&gt; (often) a larger database. If a url has six params  and \neach of=\r\n them has 10 possible values =3D&gt; a million combinations and \nresult pages.=\r\n Even a lesser amount is much. \n\nA pcsuperstore may have every bolt and nut=\r\n in an online catalog via \nsearch interface into their database. I don&#39;t kn=\r\now why but harvesters \nI&#39;ve known this far collect mass database entries if=\r\n I don&#39;t try to \nstop them. \n\nBut true, it&#39;s difficult to see what&#39;s some s=\r\nort of catalog by just \nlooking at the number of parameters.\n\nkaisa\n\n&gt; &gt; --=\r\n- In archive-crawler@yahoogroups.com, &quot;Kristinn Sigurdsson&quot;\n&gt; &gt; &lt;kris@a...&gt;=\r\n wrote:\n&gt; &gt;&gt;  Hi all,\n&gt; &gt;&gt;\n&gt; &gt;&gt;  This week I&#39;ve been experimenting with run=\r\nning a crawl over the\n&gt; &gt; entire .is\n&gt; &gt;&gt;  TLD. As expected I&#39;ve encountere=\r\nd several problems and \nlimitations\n&gt; &gt; that I\n&gt; &gt;&gt;  thought might of inter=\r\nest to this community as this is (as far \nas I\n&gt; &gt; know)\n&gt; &gt;&gt;  the first at=\r\ntempt to run Heritrix on such a scale. Crawling has\n&gt; &gt; been done\n&gt; &gt;&gt;  wit=\r\nh very recent development builds.\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Many of the limits I&#39;ve encoun=\r\ntered are already known but I&#39;m\n&gt; &gt; reiterating\n&gt; &gt;&gt;  them here for the sak=\r\ne of completeness.\n&gt; &gt;&gt;\n&gt; &gt;&gt;  The .is TLD has a little over 10000 registere=\r\nd second level\n&gt; &gt; domains. A list\n&gt; &gt;&gt;  of them with a www prefix was used=\r\n as a seed list. Instead of \none\n&gt; &gt; of the\n&gt; &gt;&gt;  scopes shipped with Herit=\r\nrix I wrote a custom scope that limited\n&gt; &gt; the crawl\n&gt; &gt;&gt;  to hosts with t=\r\nhe .is suffix (plus the usual transitive \nincludes).\n&gt; &gt;&gt;\n&gt; &gt;&gt;  The first p=\r\nroblem I encountered was Heritrix&#39;s excessive use of \nfile\n&gt; &gt;&gt;  handlers. =\r\nIn addition to numerous other files used by Heritrix \neach\n&gt; &gt; one of\n&gt; &gt;&gt; =\r\n the 10000+ domains immediately needed its own host queue with \ntwo\n&gt; &gt; fil=\r\ne\n&gt; &gt;&gt;  handlers each. This led me to (with the help of Gordon and \nMichael=\r\n)\n&gt; &gt; to\n&gt; &gt;&gt;  redesign the so called DiskBackedQueues so that they only ha=\r\nve \nopen\n&gt; &gt; files\n&gt; &gt;&gt;  when they are large enough to warrant it. I.e. whe=\r\nn items in \nthem\n&gt; &gt; are too\n&gt; &gt;&gt;  many to fit into the memory &#39;head&#39; that =\r\nthey have (200 files for\n&gt; &gt; the host\n&gt; &gt;&gt;  queues).\n&gt; &gt;&gt;\n&gt; &gt;&gt;  This fix dr=\r\nastically reduced the number of open files and for \nthe\n&gt; &gt; most part\n&gt; &gt;&gt; =\r\n eliminated this problem. With it out of the way it was possible \nto\n&gt; &gt; la=\r\nunch a\n&gt; &gt;&gt;  crawl with this many seeds.\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Initial progress was qu=\r\nite impressive, running at over 50 \ndocuments\n&gt; &gt; per\n&gt; &gt;&gt;  second initiall=\r\ny. Eventually it started to gradually drop and \nnow\n&gt; &gt; several\n&gt; &gt;&gt;  days =\r\nlater it stands at around 17 documents per second. I&#39;m \nunsure\n&gt; &gt; of the\n&gt;=\r\n &gt;&gt;  reason for this gradual decline. It may be related to increasing\n&gt; &gt; s=\r\nizes of\n&gt; &gt;&gt;  various data structures.\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Over the course of the pa=\r\nst 3 days the crawl has downloaded \nnearly\n&gt; &gt; 4 million\n&gt; &gt;&gt;  documents to=\r\ntaling over 150 GB. In addition some 11 million plus\n&gt; &gt; documents\n&gt; &gt;&gt;  ha=\r\nve been discovered and are waiting processing. While \nimpressive\n&gt; &gt; this i=\r\ns\n&gt; &gt;&gt;  still only scratching the top of the .is domain. I estimate that\n&gt; =\r\n&gt; currently\n&gt; &gt;&gt;  documents 3 link hops from the seeds are being processed.=\r\n\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Memory use was initially my main concern. The crawl is running =\r\n\non a\n&gt; &gt; machine\n&gt; &gt;&gt;  with 1.5 GB RAM and the JVM max heap size was set t=\r\no 1.25 GB. To\n&gt; &gt; date the\n&gt; &gt;&gt;  JVM has only allocated itself 1 GB and gar=\r\nbage collection still\n&gt; &gt; drops the\n&gt; &gt;&gt;  memory being used to almost half =\r\nthat.\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Disk use by the disk bound queues however has been much gr=\r\neater\n&gt; &gt; then I\n&gt; &gt;&gt;  anticipated. With said 11 million URLs waiting in th=\r\ne queues \nthey\n&gt; &gt; now take\n&gt; &gt;&gt;  up about 16 GB. This comes out at about 1=\r\n.6 KB per URI. This \nwill\n&gt; &gt; turn out\n&gt; &gt;&gt;  to be the limiting factor for =\r\nmy current crawl since the disk in\n&gt; &gt; question\n&gt; &gt;&gt;  only has another 3 GB=\r\n free so it will be exhausted soon.\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Even with much larger disks,=\r\n say 200 GB, crawls will be limited \nto\n&gt; &gt; having\n&gt; &gt;&gt;  120-130 million UR=\r\nIs waiting. This seems like a huge number \nuntil\n&gt; &gt; you\n&gt; &gt;&gt;  consider tha=\r\nt the .is domain would seem to have at least that \nmany\n&gt; &gt; documents\n&gt; &gt;&gt; =\r\n based on this crawl. Doing a crawl like this on an even larger\n&gt; &gt; scale w=\r\nould\n&gt; &gt;&gt;  seem to merit trying to reduce the size (on disk) of these URIs.=\r\n\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Of course a crawl of that scope is not possible until the list =\r\n\nof\n&gt; &gt; already\n&gt; &gt;&gt;  seen URIs can be disk backed.  With the current metho=\r\nd of using \n4\n&gt; &gt; byte\n&gt; &gt;&gt;  fingerprints for each encountered URI 1 GB of =\r\nmemory can hold\n&gt; &gt; around 28\n&gt; &gt;&gt;  million URIs. Even with a machine with =\r\n4 GB RAM would not be \nable\n&gt; &gt; to scale\n&gt; &gt;&gt;  up to even a full .is TLD cr=\r\nawl.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Some of my thoughts on dealing with the limits fo=\r\nllow:\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Moving the list of encountered URIs to disk seems to be \ni=\r\nmperative.\n&gt; &gt; But that\n&gt; &gt;&gt;  will pose even greater demands on disk space =\r\nso I would suggest\n&gt; &gt; that we\n&gt; &gt;&gt;  remain aware of that issue and try (wh=\r\nenever possible) to limit \nthe\n&gt; &gt; size of\n&gt; &gt;&gt;  the data being written to =\r\ndisk to that which is actually needed.\n&gt; &gt; That may\n&gt; &gt;&gt;  have the addition=\r\nal benefit of speeding I/O operations (even if \nonly\n&gt; &gt;&gt;  marginally). Per=\r\nhaps it should also be possible to split the \ndata\n&gt; &gt; among many\n&gt; &gt;&gt;  dis=\r\nk (of course this can be done at the OS level to some \nextent).\n&gt; &gt;&gt;\n&gt; &gt;&gt;  =\r\nMulti machine setup might alleviate some of this but for any \nsort\n&gt; &gt; of l=\r\narge\n&gt; &gt;&gt;  scale crawl (targeting 100 million +) each machine would \nprobab=\r\nly be\n&gt; &gt;&gt;  handling tens of millions URIs at least. For truly large scale\n=\r\n&gt; &gt; crawls (like\n&gt; &gt;&gt;  the ones I know IA wants to do) the demands grow eve=\r\nn more.\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Another approach which might limit the problem is the\n&gt; =\r\n&gt; implementation of the\n&gt; &gt;&gt;  site first crawling strategy. In my crawl all=\r\n .is domains are\n&gt; &gt; tackled in\n&gt; &gt;&gt;  parallel. This makes the crawl a true=\r\n breadth first crawl. A \nsite\n&gt; &gt; first\n&gt; &gt;&gt;  approach would let Heritrix f=\r\nocus it&#39;s efforts on a limited \nnumber\n&gt; &gt; of sites\n&gt; &gt;&gt;  at a time. Since =\r\nthe usual pattern for crawling a site is an \ninitial\n&gt; &gt;&gt;  explosion of ava=\r\nilable URIs followed by a steady decline and\n&gt; &gt; eventual\n&gt; &gt;&gt;  exhaustion =\r\nwe would likely not wind up having as huge a number \nof\n&gt; &gt; URIs\n&gt; &gt;&gt;  wait=\r\ning (at least not as quickly). This does nothing for the \nlimits\n&gt; &gt; impose=\r\nd\n&gt; &gt;&gt;  by the demands on RAM memory though.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;  Kristinn Sigur=\r\n=F0sson\n&gt; &gt;&gt;  Software engineer\n&gt; &gt;&gt;  National and University Library of Ic=\r\neland\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; *Yahoo! Groups Sponsor*\n&gt; &gt; ADVERTISEMENT\n&gt; &gt;=\r\n \n&lt;http://rd.yahoo.com/SIG=3D129nfh7uo/M=3D295196.4901138.6071305.3001176/D=\r\n=3D\ngroups/S=3D1705004924:HM/EXP=3D1085481210/A=3D2128215/R=3D0/SIG=3D10se9=\r\n6mf6/*htt\np://companion.yahoo.com&gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --------------------------=\r\n----------------------------------------\n------\n&gt; &gt; *Yahoo! Groups Links*\n&gt;=\r\n &gt; \n&gt; &gt;     * To visit your group on the web, go to:\n&gt; &gt;       http://group=\r\ns.yahoo.com/group/archive-crawler/\n&gt; &gt;        \n&gt; &gt;     * To unsubscribe fro=\r\nm this group, send an email to:\n&gt; &gt;       archive-crawler-unsubscribe@yahoo=\r\ngroups.com\n&gt; &gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?\ns=\r\nubject=3DUnsubscribe&gt;\n&gt; &gt;        \n&gt; &gt;     * Your use of Yahoo! Groups is su=\r\nbject to the Yahoo! Terms of\n&gt; &gt;       Service &lt;http://docs.yahoo.com/info/=\r\nterms/&gt;. \n&gt; &gt; \n&gt; &gt;\n\n\n"}}