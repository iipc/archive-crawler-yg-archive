{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"tswxToaIJaBQpyTxyfV-1qraS0_jw_TK3RdQKpcQZy4Z-frFXzpdF8mOzanVPCwY7qIVqnDPE5LsWe7uXm-9ecJvJOtCG3M","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix v1 frontier and reconstruction files from ARC","postDate":"1274910580","msgId":6543,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRCRkQ5Nzc0LjYwNDA0MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRCRjQ4NDdFLjUwNzAzMDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PGhzaDQ4Mis0ZDI3QGVHcm91cHMuY29tPiA8NEJGNDg0N0UuNTA3MDMwN0BhcmNoaXZlLm9yZz4="},"prevInTopic":6528,"nextInTopic":0,"prevInTime":6542,"nextInTime":6544,"topicId":6524,"numMessagesInTopic":3,"msgSnippet":"Some other ideas why your final report might show queues with pending URIs: - certain fetch errors on the queues incur the fetch-penalty-amount budget cost;","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 67609 invoked from network); 26 May 2010 21:49:42 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.sp2.yahoo.com with QMQP; 26 May 2010 21:49:42 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.sp2.yahoo.com with SMTP; 26 May 2010 21:49:42 -0000\r\nX-Received: (qmail 65673 invoked from network); 26 May 2010 21:49:41 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 26 May 2010 21:49:41 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4BFD9774.6040408@...&gt;\r\nDate: Wed, 26 May 2010 14:49:40 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.9) Gecko/20100317 Thunderbird/3.0.4\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;steve@...&quot; &lt;steve@...&gt;, \n goblin_cz &lt;adam.brokes@...&gt;\r\nReferences: &lt;hsh482+4d27@...&gt; &lt;4BF4847E.5070307@...&gt;\r\nIn-Reply-To: &lt;4BF4847E.5070307@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix v1 frontier and reconstruction files\n from ARC\r\nX-Yahoo-Group-Post: member; u=137285340; y=7K9mOoGD25hHZ6aH_b9kZhOCz7IoRK2Q9a659bv9Vw-t\r\nX-Yahoo-Profile: gojomo\r\n\r\nSome other ideas why your final report might show queues with pending URIs:\n\n- certain fetch errors on the queues incur the &#39;fetch-penalty-amount&#39; \nbudget cost; this could lead to using the total-budget even with far \nfewer than 20K successful fetches\n\n- if the queue had URIs remaining and *wasn&#39;t* over-budget, the crawler \nwould have kept trying barring a manual pause/terminate -- but perhaps \nin a slow-retry loop, if the server had become unresponsive. Perhaps the \ncrawl was stopped (or reports prepared) while it was still trying \nunresponsive sites?\n\nSince archived responses are not necessarily the same as the files/data \nbacking a site, it&#39;s not necessarily possible to &#39;exactly&#39; restore the \noriginal site, even if the archival crawls are very good snapshots of \nits apparent state.\n\nStill, a recrawl of the archive with a crawler that mirrors HTTP \nresponses into a directory-tree similar to the URL-patterns may \napproximate the desired result. Heritrix with MirrorWriter might do the \ntrick, but note that MirrorWriter is a contributed component we don&#39;t \nhave any experience using at the Internet Archive. Another crawler that \nmaps responses into directory-tree files by default (like HTTrack) might \nalso be a good choice. With either, it might help to perform the \n&#39;mirror&#39; crawl against a Wayback in &#39;proxy&#39; mode, to hide date skew and \nclosest-date-redirects.\n\n- Gordon @ IA\n\nOn 5/19/10 5:38 PM, steve@... wrote:\n&gt; hi Adam,\n&gt;\n&gt; some speculation on your frontier budget question; i see\n&gt; that you have hold-queues=true, perhaps your crawl has\n&gt; finished by reaching some global limit (e.g. max-time,\n&gt; max-docs, etc.) leaving newly created but inactive queues\n&gt; under budget, but having some downloaded by transclusion?\n&gt; 5998 is suspiciously close to 2x balance-replenish-amount\n&gt; though - as if the queue may have been active a couple of\n&gt; times before the crawl &quot;finished&quot;. i would try setting\n&gt; hold-queues=false, and see if those queues reach their\n&gt; budget.\n&gt;\n&gt; i&#39;m not sure how best to recreate a site from your archive,\n&gt; but have you considered crawling your own archive using\n&gt; MirrorWriterProcessor, or another crawler designed for\n&gt; mirror writing, like HTTrack? otherwise, a class using\n&gt; W/ARCReader to extract records and write them to the right\n&gt; paths directly from your W/ARCs might be the best start.\n&gt;\n&gt; hope that helps.\n&gt;\n&gt; /steve@...\n&gt;\n&gt;\n&gt; On 5/13/10 8:01 AM, goblin_cz wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Hi,\n&gt;&gt;\n&gt;&gt; I am using heritrix 1.14.3 and the frontier budget functionality\n&gt;&gt; (described here:\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Frontier+queue+budgets\n&gt;&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Frontier+queue+budgets&gt;)\n&gt;&gt;\n&gt;&gt; I set budget limit to 20,000. UnitCost and SurtPrefixAssigment policy.\n&gt;&gt;\n&gt;&gt; Well, maybe this will be more clear:\n&gt;&gt;\n&gt;&gt; &lt;string name=&quot;queue-assignment-policy&quot;&gt;\n&gt;&gt; org.archive.crawler.frontier.SurtAuthorityQueueAssignmentPolicy\n&gt;&gt; &lt;/string&gt;\n&gt;&gt; &lt;string name=&quot;force-queue-assignment&quot;/&gt;\n&gt;&gt; &lt;boolean name=&quot;pause-at-start&quot;&gt;true&lt;/boolean&gt;\n&gt;&gt; &lt;boolean name=&quot;pause-at-finish&quot;&gt;true&lt;/boolean&gt;\n&gt;&gt; &lt;boolean name=&quot;source-tag-seeds&quot;&gt;false&lt;/boolean&gt;\n&gt;&gt; &lt;boolean name=&quot;recovery-log-enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;&gt; &lt;boolean name=&quot;hold-queues&quot;&gt;true&lt;/boolean&gt;\n&gt;&gt; &lt;integer name=&quot;balance-replenish-amount&quot;&gt;3000&lt;/integer&gt;\n&gt;&gt; &lt;integer name=&quot;error-penalty-amount&quot;&gt;100&lt;/integer&gt;\n&gt;&gt; &lt;long name=&quot;queue-total-budget&quot;&gt;20000&lt;/long&gt;\n&gt;&gt; &#8722;\n&gt;&gt; &lt;string name=&quot;cost-policy&quot;&gt;\n&gt;&gt; org.archive.crawler.frontier.UnitCostAssignmentPolicy\n&gt;&gt; &lt;/string&gt;\n&gt;&gt; &lt;long name=&quot;snooze-deactivate-ms&quot;&gt;300&lt;/long&gt;\n&gt;&gt;\n&gt;&gt; When the crawl has been paused I forced generation of final reports to\n&gt;&gt; disk. In such way I can import this data to excel sheet and analyze it.\n&gt;&gt; The most interesting is host-report.txt. And there I found that some of\n&gt;&gt; queues has less than the limit (e.g. 8500) downloaded and despite that\n&gt;&gt; has some urls in remaining column.\n&gt;&gt;\n&gt;&gt; for example:\n&gt;&gt; url: kvetena.cz\n&gt;&gt; downloaded: 5998\n&gt;&gt; remaining: 29084\n&gt;&gt;\n&gt;&gt; I am not sure how this could happen and how to reach limit of these sites.\n&gt;&gt;\n&gt;&gt; Second question.\n&gt;&gt;\n&gt;&gt; There have been risen requirement to extract data from one domain.\n&gt;&gt;\n&gt;&gt; Our case:\n&gt;&gt; - the provider that hosted domain lost their data\n&gt;&gt; - we have have two years crawls in our archive\n&gt;&gt; - we need to extract the data just before the crash and restore it\n&gt;&gt; exactly as it was on the server\n&gt;&gt;\n&gt;&gt; What is the best and fastest way to do that? I am asking because I am\n&gt;&gt; sure that this is not unusual case.\n&gt;&gt;\n&gt;&gt; Thanks a lot,\n&gt;&gt;\n&gt;&gt; best regards.\n&gt;&gt;\n&gt;&gt; Adam\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}