{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack@archive.org","from":"&quot;stack@...&quot; &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"xv50o2ZqQewynw8nz9LTrY-8PkhTyxDptd9xKJlG2RLeCPm34qpLr2_jstsmZRUlGfzmj0hzhY52pXIFK2Ji_mbBxdQ2Q-yZ8pcwquPf","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] duplicated crawling resource by following &quot;Content-Location&quot; referal","postDate":"1116864499","msgId":1869,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyOTFGRkYzLjIwMkBhcmNoaXZlLm9yZz4=","inReplyToHeader":"PDQyOTE5MkRBLjgwMDAxMDhAamx1LmVkdS5jbj4=","referencesHeader":"PDQyOTE5MkRBLjgwMDAxMDhAamx1LmVkdS5jbj4="},"prevInTopic":1864,"nextInTopic":0,"prevInTime":1868,"nextInTime":1870,"topicId":1864,"numMessagesInTopic":2,"msgSnippet":"... Seems like we shouldn t extract the content of Content-Location headers?  We might go further and add the content of the Content-Location header to the","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 44254 invoked from network); 23 May 2005 16:08:45 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m23.grp.scd.yahoo.com with QMQP; 23 May 2005 16:08:45 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta6.grp.scd.yahoo.com with SMTP; 23 May 2005 16:08:44 -0000\r\nReceived: from [192.168.1.125] ([192.168.1.125])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id j4NFE8C18077\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 23 May 2005 08:14:08 -0700\r\nMessage-ID: &lt;4291FFF3.202@...&gt;\r\nDate: Mon, 23 May 2005 09:08:19 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.5) Gecko/20050105 Debian/1.7.5-1\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;429192DA.8000108@...&gt;\r\nIn-Reply-To: &lt;429192DA.8000108@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: &quot;stack@...&quot; &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] duplicated crawling resource by following &quot;Content-Location&quot;\n referal\r\nX-Yahoo-Group-Post: member; u=168599281\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nxuqy wrote:\n\n&gt;       my seed list contains a URL : &quot;http://www.debian.org/&quot;. When I\n&gt; fetch the URL then its http response header field contains\n&gt; &quot;Content-Location: http://www.debian.org/index.en.html&quot; field. Then\n&gt; Heritrix&#39;s &#39;extractHTTP&#39; module extract the latter URL and insert it\n&gt; into crawl frontier, with the same priority as 301(2) referal, though in\n&gt; this case its fetch status is 200. Because &#39;Content-Location&#39; is meant\n&gt; to supply the original resource presentation with respect to the request\n&gt; URL, following such newly discovered URL doesn&#39;t seem make sense,\n&gt; because both URLs refer to the same resource exactly. The ideal approach\n&gt; to this problem seem to make the two URLs as if they are the same, maybe\n&gt; exploiting additional database to store such information.\n\n\nSeems like we shouldn&#39;t extract the content of &#39;Content-Location&#39; \nheaders?  We might go further and add the content of the \n&#39;Content-Location&#39; header to the already-seen database so we positively \ndon&#39;t crawl the &#39;varient&#39; of this &#39;response entity&#39; (See 14.14 of RFC2616).\n\n&gt;       Can you think of a means to avoid downloading the same resource\n&gt; twice  and  at the same time to avoid further duplicated crawling in the\n&gt; future?\n\nWe plan adding means of detecting duplicate pages.  The mechanism will \nlikely be a database of hashes/fingerprints/shingles which we&#39;ll \noptionally reference after each page download. Pages w/o entries in the \nduplicates db we&#39;ll process (link-extract and save).  Others will be \ndiscarded.\n\nThe topic has been discussed in the past up on this list.  See in \nparticular this informative note on awkward issues detecting duplicate \npages: http://groups.yahoo.com/group/archive-crawler/message/1498 (See \nalso the AR frontier.  It already does a form of &#39;duplicate detection&#39;).\n\nYours,\nSt.Ack\n\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}