{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"waq1tgA38P_XRQaBREdd4Z7OA7WljsrSzXFNlhjrLdNm3Ya3K1OhjJ5y46B6qAopR6Qrt47Nbu0WmZ79vcWiSMHKcmDUOnk","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Lowering stack size (-Xss)?","postDate":"1221183189","msgId":5470,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4QzlDNkQ1LjMwODA4MDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PFAtSVJDLUVYQkUwMXE2RHFhSlkwMDAwMTdjMkBFWC5VQ09QLkVEVT4=","referencesHeader":"PFAtSVJDLUVYQkUwMUF3MTlEQnEwMDAwMTY4OUBFWC5VQ09QLkVEVT4JPDQ4Qzk3NUFELjcwNDA1MDhAYXJjaGl2ZS5vcmc+IDxQLUlSQy1FWEJFMDFxNkRxYUpZMDAwMDE3YzJARVguVUNPUC5FRFU+"},"prevInTopic":5469,"nextInTopic":5471,"prevInTime":5469,"nextInTime":5471,"topicId":5467,"numMessagesInTopic":5,"msgSnippet":"... Yes, I would presume that too: it s just the unlucky Thread-creation attempt that was happened to be one too many. ... Since each crawler will use some","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 39262 invoked from network); 12 Sep 2008 01:33:09 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m46.grp.scd.yahoo.com with QMQP; 12 Sep 2008 01:33:09 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta15.grp.scd.yahoo.com with SMTP; 12 Sep 2008 01:33:09 -0000\r\nX-Received: (qmail 38258 invoked from network); 12 Sep 2008 01:33:08 -0000\r\nX-Received: from unknown (HELO ?192.168.1.88?) (unknown)\n  by unknown with SMTP; 12 Sep 2008 01:33:08 -0000\r\nX-pair-Authenticated: 67.170.220.186\r\nMessage-ID: &lt;48C9C6D5.3080801@...&gt;\r\nDate: Thu, 11 Sep 2008 18:33:09 -0700\r\nUser-Agent: Thunderbird 2.0.0.16 (Windows/20080708)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;P-IRC-EXBE01Aw19DBq00001689@...&gt;\t&lt;48C975AD.7040508@...&gt; &lt;P-IRC-EXBE01q6DqaJY000017c2@...&gt;\r\nIn-Reply-To: &lt;P-IRC-EXBE01q6DqaJY000017c2@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Lowering stack size (-Xss)?\r\nX-Yahoo-Group-Post: member; u=137285340; y=F4KzwNhZ7UV4tHqixh6czP7fdQVy7X6PFaMpm4KUtww6\r\nX-Yahoo-Profile: gojomo\r\n\r\n\n\nErik Hetzner wrote:\n&gt; At Thu, 11 Sep 2008 12:46:53 -0700,\n&gt; Gordon Mohr wrote:\n&gt;&gt; Thoughts:\n&gt;&gt;\n&gt;&gt; What out-of-memory error do you get, exactly? (Can you trigger the same \n&gt;&gt; error under Java 1.6, which sometimes gives more informative OOME/error \n&gt;&gt; messages?)\n&gt; \n&gt; Sure. This is with:\n&gt; \n&gt; java version &quot;1.6.0_06&quot;\n&gt; Java(TM) SE Runtime Environment (build 1.6.0_06-b02)\n&gt; Java HotSpot(TM) Server VM (build 10.0-b22, mixed mode)\n&gt; \n&gt; Exception in thread &quot;StartNextJob&quot; java.lang.OutOfMemoryError: unable to create new native thread\n&gt;         at java.lang.Thread.start0(Native Method)\n&gt;         at java.lang.Thread.start(Thread.java:597)\n&gt;         at com.sleepycat.je.utilint.DaemonThread.runOrPause(DaemonThread.java:73)\n&gt;         at com.sleepycat.je.cleaner.Cleaner.runOrPause(Cleaner.java:307)\n&gt;         at com.sleepycat.je.dbi.EnvironmentImpl.runOrPauseDaemons(EnvironmentImpl.java:475)\n&gt;         at com.sleepycat.je.dbi.EnvironmentImpl.envConfigUpdate(EnvironmentImpl.java:420)\n&gt;         at com.sleepycat.je.dbi.EnvironmentImpl.&lt;init&gt;(EnvironmentImpl.java:387)\n&gt;         at com.sleepycat.je.dbi.DbEnvPool.getEnvironment(DbEnvPool.java:102)\n&gt;         at com.sleepycat.je.dbi.DbEnvPool.getEnvironment(DbEnvPool.java:54)\n&gt;         at com.sleepycat.je.Environment.&lt;init&gt;(Environment.java:103)\n&gt;         at org.archive.util.bdbje.EnhancedEnvironment.&lt;init&gt;(EnhancedEnvironment.java:53)\n&gt;         at org.archive.crawler.framework.CrawlController.setupBdb(CrawlController.java:485)\n&gt;         at org.archive.crawler.framework.CrawlController.initialize(CrawlController.java:372)\n&gt;         at org.archive.crawler.admin.CrawlJob.setupForCrawlStart(CrawlJob.java:848)\n&gt;         at org.archive.crawler.admin.CrawlJobHandler.startNextJobInternal(CrawlJobHandler.java:1142)\n&gt;         at org.archive.crawler.admin.CrawlJobHandler$3.run(CrawlJobHandler.java:1125)\n&gt;         at java.lang.Thread.run(Thread.java:619)\n&gt; \n&gt; I presume that it is just coincidence that it happens to be bdbje\n&gt; maxing out the threads here and not the toethreads.\n\nYes, I would presume that too: it&#39;s just the unlucky Thread-creation \nattempt that was happened to be one too many.\n\n&gt; Iâ€™d forgotten that I could configure ToeThreads. I have been using\n&gt; max-toe-threads=50 which is probably overkill since we are crawling\n&gt; single sites here. Having set to 5, I have now got at least 102\n&gt; crawlers in one JVM. This instance has only ~1000 threads in it, as\n&gt; compared with another instance with max-toe-threads=50 running 20\n&gt; crawls which has ~1100 threads.\n\nSince each crawler will use some overhead from non-heap memory other \nthan thread stacks -- such as file handles, memory-mapped-IO resources, \nzlib compression buffers, etc. -- I&#39;m not surprised that with more \ncrawlers, somewhat fewer threads can be spawned.\n\nIf each crawler is truly aimed at only one domain-name (not including \nsubdomains, but including occasional fetches of offsite inline \nresources), then 2 threads should be enough: one that is always busy on \nthe main site, and one that rotates among the occasional \nother-referenced sites.\n\n&gt;&gt; Because stack space comes from non-heap memory, this may be a case where \n&gt;&gt; making your heap space smaller paradoxically allows you to launch more \n&gt;&gt; crawlers/threads, given a specific -Xss setting, without hitting an error.\n&gt; \n&gt; Thanks for this. I think that I have noticed a difference between\n&gt; running with -Xmx3000m vs. -Xmx2048m.\n&gt; \n&gt;&gt; The risk of using smaller stack size settings would be hitting\n&gt;&gt; StackOveflowErrors in crawler code. In my experience, the deepest\n&gt;&gt; recursion that happens in Heritrix is in the regular-expression\n&gt;&gt; link-extraction, against certain worst-case input. We&#39;ve added a\n&gt;&gt; number of guards against this risk -- capping match sizes and adding\n&gt;&gt; explicit catch-and-recover code against StackOverflowError in\n&gt;&gt; Extractors -- but I still suspect that&#39;s the most likely first place\n&gt;&gt; to hit problems with smaller -Xss values.\n&gt; \n&gt; Thanks! This is good to look out for. The test site that I use the\n&gt; crawlers on is not going to exercise this, unfortunately.\n&gt; \n&gt; But if I am not mistaken, even a stack overflow that kills a thread\n&gt; will not endanger the crawl or the JVM, right?\n\nI don&#39;t believe it will, but I&#39;m not absolutely certain.\n\nStackOverflowError is a subclass of Error, which the Javadoc says \n&quot;indicates serious problems that a reasonable application should not try \nto catch. Most such errors are abnormal conditions.&quot; (This language \nhasn&#39;t changed since Java 1.3.)\n\nTheoretically, a StackOverflowError could happen at any method-call. \nPerhaps even, inside a JVM native routine, due to a overflow on the C \ncall-stack. So whatever operation was in progress could be in an \narbitrarily half-done and thus corrupt state, especially if it was \noperating on objects shared with other threads.\n\nHowever, I don&#39;t find StackOverflowErrors so abnormal given the \nrecursive implementation of the Java regex support, and the kind of \narbitrarily odd input that can be encountered by broad web crawls. So, \nif we know a specific deeply-recursing regex match is a threat for a \nStackOverflowError, is seems reasonable to me to catch and recover from \nthat situation.\n\nI don&#39;t expect anything in the Java-land regex implementation would \nleave dangerous cruft around no matter where it was aborted, and our \nregex matching code essentially works on data local to the current \nthread.  *Maybe* a half-done-but-not-fully-completed native-code \noperation, like a new object creation, could leave trouble that blows up \nat a later point... but I&#39;ve never noticed anything that looked like \nthat, nor heard other crashes blamed on native data corruption due to \nstack overflows. So I think it&#39;s safe.\n\n  fact this is a 32-bit JVM; I should have specified that. I will\n&gt; have install the 64-bit JVM & run some experiments with that as well,\n&gt; since I think that we will need a heap greater than 10G to squeeze out\n&gt; the crawlers that we want.\n\nIs there a reason you need all the crawlers in the same JVM, rather than \nseveral JVMs on the same system?\n\nOf course there&#39;s redundant overhead with multiple JVMs, but I would \nalso think that since there&#39;s a risk that one crawl uniquely steps on \nsome novel crashing or resource-depleting bug, ruining all other crawls \nin the same JVM, some isolation would also be beneficial. Plus, the \n64bit-addressing expansion might be even larger than the multiple JVM \noverheads.\n\n&gt;&gt; Hope this helps,\n&gt; \n&gt; Very much! Thanks for everything, Gordon. I will let the list know\n&gt; what we can squeeze out of Heritrix.\n\nYou&#39;re welcome!\n\n- Gordon @ IA\n\n"}}