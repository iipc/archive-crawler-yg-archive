{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":226767221,"authorName":"Matt Ittigson","from":"Matt Ittigson &lt;cydatamatt@...&gt;","replyTo":"LIST","senderId":"8vRrEVvsry0PKzZHMuHGYLLuLuPyaZuqK1UlvmmKbnrGiyHmYiLPn6RWEy3wUpNqtGcBUivRpSl3gTJFUq8Yar4YZrogPRf21sBz","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] JMX importUri not causing more crawling","postDate":"1118334458","msgId":1937,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDlkMWU0NTI1MDUwNjA5MDkyNzE3MmExYjcxQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDQyQTdCQjBDLjUwODA3MDRAYXJjaGl2ZS5vcmc+","referencesHeader":"PDlkMWU0NTI1MDUwNjA4MTgwMTE5NDBmNTY2QG1haWwuZ21haWwuY29tPgkgPDQyQTdCQjBDLjUwODA3MDRAYXJjaGl2ZS5vcmc+"},"prevInTopic":1935,"nextInTopic":0,"prevInTime":1936,"nextInTime":1938,"topicId":1930,"numMessagesInTopic":7,"msgSnippet":"... Shame on me.  Thanks to your suggestions about the various logs to look in I discovered that it was actually my application with the bug, not Heritrix.  I","rawEmail":"Return-Path: &lt;cydatamatt@...&gt;\r\nX-Sender: cydatamatt@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 59196 invoked from network); 9 Jun 2005 16:27:38 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m27.grp.scd.yahoo.com with QMQP; 9 Jun 2005 16:27:38 -0000\r\nReceived: from unknown (HELO zproxy.gmail.com) (64.233.162.205)\n  by mta6.grp.scd.yahoo.com with SMTP; 9 Jun 2005 16:27:38 -0000\r\nReceived: by zproxy.gmail.com with SMTP id 12so13967nzp\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 09 Jun 2005 09:27:38 -0700 (PDT)\r\nReceived: by 10.36.2.18 with SMTP id 18mr520417nzb;\n        Thu, 09 Jun 2005 09:27:38 -0700 (PDT)\r\nReceived: by 10.36.56.15 with HTTP; Thu, 9 Jun 2005 09:27:38 -0700 (PDT)\r\nMessage-ID: &lt;9d1e45250506090927172a1b71@...&gt;\r\nDate: Thu, 9 Jun 2005 11:27:38 -0500\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;42A7BB0C.5080704@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nReferences: &lt;9d1e452505060818011940f566@...&gt;\n\t &lt;42A7BB0C.5080704@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Matt Ittigson &lt;cydatamatt@...&gt;\r\nReply-To: Matt Ittigson &lt;cydatamatt@...&gt;\r\nSubject: Re: [archive-crawler] JMX importUri not causing more crawling\r\nX-Yahoo-Group-Post: member; u=226767221\r\n\r\nOn 6/8/05, stack &lt;stack@...&gt; wrote:\n&gt; Matt Ittigson wrote:\n&gt; \n&gt; &gt; I=\r\n&#39;m using the importURI JMX call to add sites to a running job.  I&#39;ve\n&gt; &gt; wr=\r\napped the cmdline-jmxclient in a perl script which depending on the\n&gt; &gt; lev=\r\nel of activity, adds more URL&#39;s to the job.  The problem is, the\n&gt; &gt; thread=\r\ns don&#39;t seem to actually crawl the newly added URL&#39;s.  I&#39;m using\n&gt; &gt; the Bd=\r\nbFrontier and PathScrope.  I&#39;m limiting the host-based queue&#39;s\n&gt; &gt; to 200 U=\r\nRL&#39;s, and the Frontier queue&#39;s are all exhausted.  But I&#39;m\n&gt; &gt; setting the =\r\nforceFetch to true and I&#39;d be very surprised if every\n&gt; &gt; single newly adde=\r\nd URL would sort into an existing exhausted queue.\n&gt; \n&gt; And for sure the ad=\r\nded URLs are in scope?  (You can enable logging of\n&gt; all rejected on the Po=\r\nstselector -- or if you&#39;re using a recent HEAD, on\n&gt; LinksScoper).\n&gt;\n&gt; What=\r\n if you add URLs that have already been crawled (See the\n&gt; crawl.log).   If=\r\n  you add these w/ forceFetch, are they not being crawled?\n&gt; \n&gt; Try adding =\r\nURLs as for-sure in-scope seeds.  Do these get crawled?\n&gt; \n&gt; Let me know.  =\r\nThis needs to be working for us in short order so if\n&gt; you&#39;re having proble=\r\nms, I&#39;m interested.\n\nShame on me.  Thanks to your suggestions about the var=\r\nious logs to\nlook in I discovered that it was actually my application with =\r\nthe bug,\nnot Heritrix.  I was trying to be a little too smart about escapin=\r\ng\nthe url&#39;s that went into Heritrix.\n\nThanks for your help.  Sorry to have =\r\ntaken your time with a non-Heritrix issue.\n\n-matt\n\n"}}