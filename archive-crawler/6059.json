{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":415459286,"authorName":"shichuanwuhan@yahoo.cn","from":"&quot;shichuanwuhan@...&quot; &lt;shichuanwuhan@...&gt;","profile":"shichuanwuhan@yahoo.cn","replyTo":"LIST","senderId":"wPSndFspwraxxasrjcSVu6xGUqr1sprZN1GJoCg7T4eV4fQLxNJnJ16EQcvtI0mzgokEzOn4Ch4VTK5kkgpjwkP06GT-DExy5mO9t0L042PRo78Jk_uHFM8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Basic question: How to limit the crawling scope within a host?","postDate":"1254122162","msgId":6059,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg5cG5yaStzdHJAZUdyb3Vwcy5jb20+","inReplyToHeader":"PDRBQzA1MzQ1LjUwMDAzMDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":6055,"nextInTopic":6062,"prevInTime":6058,"nextInTime":6060,"topicId":6051,"numMessagesInTopic":6,"msgSnippet":"Thank you Gordon. I follow your instruction and some homepages can be crawled now. But why we put http://www.cs.cmu.edu in seeds and","rawEmail":"Return-Path: &lt;shichuanwuhan@...&gt;\r\nX-Sender: shichuanwuhan@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 32138 invoked from network); 28 Sep 2009 07:16:20 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.sp2.yahoo.com with QMQP; 28 Sep 2009 07:16:20 -0000\r\nX-Received: from unknown (HELO n5-vm6.bullet.mail.sp2.yahoo.com) (67.195.135.101)\n  by mta2.grp.sp2.yahoo.com with SMTP; 28 Sep 2009 07:16:20 -0000\r\nX-Received: from [67.195.134.48] by n5.bullet.mail.sp2.yahoo.com with NNFMP; 28 Sep 2009 07:16:04 -0000\r\nX-Received: from [69.147.65.171] by t1.bullet.mail.sp2.yahoo.com with NNFMP; 28 Sep 2009 07:16:04 -0000\r\nX-Received: from [98.137.35.12] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 28 Sep 2009 07:16:04 -0000\r\nDate: Mon, 28 Sep 2009 07:16:02 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h9pnri+str@...&gt;\r\nIn-Reply-To: &lt;4AC05345.5000305@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;shichuanwuhan@...&quot; &lt;shichuanwuhan@...&gt;\r\nSubject: Re: Basic question: How to limit the crawling scope within a host?\r\nX-Yahoo-Group-Post: member; u=415459286; y=brozShctOoMoE8ZDw0PqF4jxWlcrcqOPVShUuP46wI3KT92eol7p9CsSLEFf_Xnh0A\r\nX-Yahoo-Profile: shichuanwuhan@...\r\n\r\nThank you Gordon. I follow your instruction and some homepages can be crawl=\r\ned now. But why we put &#39;http://www.cs.cmu.edu&#39; in seeds and &#39;http://people.=\r\ncs.cmu.edu/faculty/index.html&#39; in surts-source-file? What if I change their=\r\n position?\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...=\r\n&gt; wrote:\n&gt;\n&gt; \n&gt; \n&gt; shichuanwuhan@... wrote:\n&gt; &gt; Dear Gordon,\n&gt; &gt; \n&gt; &gt;     T=\r\nhanks a lot. Your answer gives me a deeper understanding of Heritrix. \n&gt; &gt; =\r\n      \n&gt; &gt;     But I still can&#39;t configure it correctly following your inst=\r\nruction so I turn to you for help. Sorry about that :)\n&gt; &gt; \n&gt; &gt;     1.Is it=\r\n optional to use &#39;surts-source-file&#39;? I add the URL:&#39;+http://people.cs.cmu.=\r\nedu/faculty/index.html&#39; in the &#39;seeds&#39; table. Is it enough?\n&gt; \n&gt; It is opti=\r\nonal, if you want to specify a lot of SURT prefixes.\n&gt; \n&gt; The one &#39;+&#39; direc=\r\ntive, added to the seeds, is enough for a crawl that \n&gt; starts on &#39;www.cs.c=\r\nmu.edu&#39; to follow discovered links to \n&gt; &#39;people.cs.cmu.edu&#39;.\n&gt; \n&gt; &gt;     2.=\r\n When I want to use &#39;surts-source-file&#39;, I don&#39;t know where to put it on my=\r\n disk. I am using Windows XP. I try to put it in the same directory with &#39;s=\r\needs.txt&#39;, but it doesn&#39;t work.\n&gt; \n&gt; The path entered here is interpreted r=\r\nelative to the job directory, \n&gt; where seeds.txt is, so that should work. H=\r\now do you know it&#39;s not \n&gt; finding the file? Is there an error?\n&gt; \n&gt; &gt;     =\r\n3.Would you please give me an example of writing a correct SURT to\n&gt; &gt; craw=\r\nl all html files under &#39;http://people.cs.cmu.edu/faculty/&#39;?\n&gt; \n&gt; I tried a =\r\ncrawl based on the bundled &#39;deciding-default&#39;, with the seed \n&gt; &#39;http://www=\r\n.cs.cmu.ed&#39; and the added directive \n&gt; &#39;+http://people.cs.cmu.edu/faculty/i=\r\nndex.html&#39;. It found the \n&gt; &#39;http://people.cs.cmu.edu/faculty/index.html&#39; U=\r\nRI from the main site.\n&gt; \n&gt; The very first faculty URI listed on that page =\r\non &#39;www.cs.cmu.edu&#39; (as \n&gt; opposed to some other host like &#39;www-2&#39;) is \n&gt; &#39;=\r\nhttp://www.cs.cmu.edu/~dga/&#39;. After a short while, my test crawl \n&gt; fetched=\r\n this URI.\n&gt; \n&gt; So if this is not working for you, it&#39;s due to some other c=\r\nhange you&#39;ve \n&gt; made to the default configuration. Also, if you want to get=\r\n pages on \n&gt; other hostnames, like &#39;www-2.cs.cmu.edu&#39;, you will have to add=\r\n \n&gt; additional directives.\n&gt; \n&gt; &gt; Here is my crawl order:\n&gt; &gt; \n&gt; &gt; 17 Admin=\r\n 20090927015119 settings logs checkpoints state scratch 0 0 0 100 4096 6553=\r\n6 0 true seeds.txt true ACCEPT true result.txt false true Mozilla/5.0 (comp=\r\natible; heritrix/@1.14.3@ +http://192.168.0.1) test@... ignore false 5.0 30=\r\n000 3000 300 30 900 1 0 0 org.archive.crawler.frontier.HostnameQueueAssignm=\r\nentPolicy false false false true true 3000 100 -1 org.archive.crawler.front=\r\nier.UnitCostAssignmentPolicy 300000 50 org.archive.crawler.util.BdbUriUniqF=\r\nilter false true false false false true 21600 86400 false true false true s=\r\nha1 true 1200 20000 0 0 false true open ISO-8859-1 true sha1 true true true=\r\n true false true true true true false true true true true true index.html %=\r\n2E . true mirror 1023 255 false true LONG true true false true -1 true true=\r\n false true true\n&gt; \n&gt; An actual crawl order would include XML that helps in=\r\nterpret these \n&gt; values. You must have copied this out of some view that hi=\r\ndes that XML. \n&gt; If you need to share an order.xml, you should probably ope=\r\nn it directly \n&gt; from the filesystem in a text editor.\n&gt; \n&gt; - Gordon @ IA\n&gt;=\r\n \n&gt; \n&gt; &gt; Thank you in advance.\n&gt; &gt; \n&gt; &gt; Chuan\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In archive-=\r\ncrawler@yahoogroups.com, Gordon Mohr &lt;gojomo@&gt; wrote:\n&gt; &gt;&gt; shichuanwuhan@ w=\r\nrote:\n&gt; &gt;&gt;&gt; Hi all,\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; I am using version 1.14.3. My final goal is=\r\n to get all the URLs of professors&#39; mainpages on one host i.e www.cs.cmu.ed=\r\nu. So firstly, I plan to fetch all pages that are within the host. However,=\r\n I fail to do that.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; I tried both the traditional &#39;hostscope&#39; an=\r\nd recommended &#39;decidingscope&#39; but I still cannot achieve my goal. As Englis=\r\nh is not my native language, maybe I misunderstand something in &#39;user manua=\r\nl&#39;. Would someone kindly answer several questions?\n&gt; &gt;&gt; The included &#39;decid=\r\ning-default&#39; profile should work for the purpose of \n&gt; &gt;&gt; getting all pages=\r\n on a single web host.\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt; 1.My seed is simply: &#39;http://www.cs.cmu.e=\r\ndu/&#39;. Is it right?\n&gt; &gt;&gt; Supplying that as a seed, with the deciding-default=\r\n settings, should \n&gt; &gt;&gt; cause the crawler to:\n&gt; &gt;&gt;\n&gt; &gt;&gt; (1) start by visiti=\r\nng &quot;http://www.cs.cmu.edu/&quot;, examining the outlinks \n&gt; &gt;&gt; of that page for =\r\n&quot;in-scope&quot; URIs\n&gt; &gt;&gt;\n&gt; &gt;&gt; (2) evaluate any URIs that begin &quot;http://www.cs.c=\r\nmu.edu/&quot; as being \n&gt; &gt;&gt; &quot;in-scope&quot; (along with some other rules), and thus =\r\neligible for \n&gt; &gt;&gt; recursive fetching\n&gt; &gt;&gt;\n&gt; &gt;&gt; The default configuration, =\r\nwith only the single seed, will only wander \n&gt; &gt;&gt; off &quot;www.cs.cmu.edu&quot; to f=\r\netch URIs that appears necessary to render \n&gt; &gt;&gt; another page (like inline =\r\nreferences to scripts, images, frames, etc., \n&gt; &gt;&gt; or URLs found in Javascr=\r\nipt that may auto-load).\n&gt; &gt;&gt;\n&gt; &gt;&gt; I see that the &#39;faculty&#39; link from &#39;www.=\r\ncs.cmu.edu&#39; goes to another \n&gt; &gt;&gt; host, &#39;people.cs.cmu.edu. Your crawl will=\r\n not in general visit that \n&gt; &gt;&gt; other host without additional scope custom=\r\nization to say those URLs are \n&gt; &gt;&gt; of interest. Also, it appears that facu=\r\nlty web pages are on a variety of \n&gt; &gt;&gt; hosts (including &#39;www-2&#39; and other =\r\ndepartmental servers).\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt; 2.Is it possible that I simply use &#39;hosts=\r\ncope&#39; to achieve my goal? \n&gt; &gt;&gt; It might be possible, but it is not recomme=\r\nnded -- HostScope is \n&gt; &gt;&gt; deprecated, less efficient and flexible than the=\r\n DecidingScope + \n&gt; &gt;&gt; SurtPrefixedDecideRule mechanism.\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt; 3.If no=\r\nt, how to configure &#39;decidingscope&#39;?\n&gt; &gt;&gt; You probably want to tell your cr=\r\nawl that begins at www.cs.cmu.edu that \n&gt; &gt;&gt; it may accept URIs on other ho=\r\nsts, like &#39;people.cs.cmu.edu&#39; and others, \n&gt; &gt;&gt; as &#39;in-scope&#39;. This involve=\r\ns giving the SurtPrefixedDecideRule more \n&gt; &gt;&gt; acceptable &#39;SURT&#39; (URI-like)=\r\n prefixes.\n&gt; &gt;&gt;\n&gt; &gt;&gt; This can be done either by specifying a file with such=\r\n prefixes as the \n&gt; &gt;&gt; SurtPrefixedDecideRule&#39;s &#39;surt-source-file&#39;, or by a=\r\ndding lines to your \n&gt; &gt;&gt; seeds list that begin &#39;+&#39;.\n&gt; &gt;&gt;\n&gt; &gt;&gt; You can read=\r\n more about SURTs as a means of scoping at:\n&gt; &gt;&gt;\n&gt; &gt;&gt; http://crawler.archiv=\r\ne.org/articles/user_manual/config.html#surtprefixscope\n&gt; &gt;&gt;\n&gt; &gt;&gt; Another st=\r\nrategy might be to start your crawling at the faculty directory:\n&gt; &gt;&gt;\n&gt; &gt;&gt; =\r\nhttp://people.cs.cmu.edu/faculty/index.html\n&gt; &gt;&gt;\n&gt; &gt;&gt; Supplying that URI as=\r\n a seed will cause the &quot;implied scope&quot; to be all \n&gt; &gt;&gt; URLs beginning &quot;http=\r\n://people.cs.cmu.edu/faculty/&quot; -- which should get \n&gt; &gt;&gt; all the other page=\r\ns of the directory, as well. *If* you are confident \n&gt; &gt;&gt; all homepages alw=\r\nays contain the &#39;~&#39; character, you could also add a new \n&gt; &gt;&gt; rule to the l=\r\nist of rules, such as a MatchesRegExpDecideRule, that \n&gt; &gt;&gt; always ACCEPTs =\r\nany URI with a &#39;~&#39; character. That would get the \n&gt; &gt;&gt; directory, and all t=\r\nhe linked pages with &#39;~&#39; anywhere in their URI (and \n&gt; &gt;&gt; quite probably ot=\r\nher pages, at other hosts and universities, when their \n&gt; &gt;&gt; URIs with &#39;~&#39; =\r\nare discovered).\n&gt; &gt;&gt;\n&gt; &gt;&gt; Hope this helps,\n&gt; &gt;&gt;\n&gt; &gt;&gt; - Gordon @ IA\n&gt; &gt;&gt;\n&gt; =\r\n&gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; ------------------------------------\n&gt; &gt; \n&gt; &gt; Yahoo! =\r\nGroups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}