{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":516443375,"authorName":"ngzikai92","from":"&quot;ngzikai92&quot; &lt;ngzikai92@...&gt;","profile":"ngzikai92","replyTo":"LIST","senderId":"UZK2TspLRd4NwvuhyJvdbQY6UwNxzYkdPmab3I3jl6NcUNH1QNHXveCc5UgWb8YdqtTh82SGXJ8TKZ7f19Q1XA7mNkYQQu2E","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Crawling Layer by Layer","postDate":"1327648064","msgId":7575,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpmdGlnMCthbW82QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRGMjEyRjNCLjkwMTA2MDFAYXJjaGl2ZS5vcmc+"},"prevInTopic":7572,"nextInTopic":0,"prevInTime":7574,"nextInTime":7576,"topicId":7518,"numMessagesInTopic":6,"msgSnippet":"Thanks for the help, especially for providing an example to better illustrate the tradeoff. However, I don t think that is a concern for me, my main objective","rawEmail":"Return-Path: &lt;ngzikai92@...&gt;\r\nX-Sender: ngzikai92@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 96158 invoked from network); 27 Jan 2012 07:07:47 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m16.grp.sp2.yahoo.com with QMQP; 27 Jan 2012 07:07:47 -0000\r\nX-Received: from unknown (HELO ng12-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.148)\n  by mta6.grp.sp2.yahoo.com with SMTP; 27 Jan 2012 07:07:47 -0000\r\nX-Received: from [98.137.0.83] by ng12.bullet.mail.gq1.yahoo.com with NNFMP; 27 Jan 2012 07:07:47 -0000\r\nX-Received: from [69.147.65.149] by tg3.bullet.mail.gq1.yahoo.com with NNFMP; 27 Jan 2012 07:07:47 -0000\r\nX-Received: from [98.137.34.119] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 27 Jan 2012 07:07:47 -0000\r\nDate: Fri, 27 Jan 2012 07:07:44 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jftig0+amo6@...&gt;\r\nIn-Reply-To: &lt;4F212F3B.9010601@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;ngzikai92&quot; &lt;ngzikai92@...&gt;\r\nSubject: Re: Crawling Layer by Layer\r\nX-Yahoo-Group-Post: member; u=516443375; y=7pKTTjnt31EJebguffVWbHZAi1B-6FbESmPY5FZhZdxB-RTR\r\nX-Yahoo-Profile: ngzikai92\r\n\r\nThanks for the help, especially for providing an example to better illustra=\r\nte the tradeoff.\n\nHowever, I don&#39;t think that is a concern for me, my main =\r\nobjective is to be able to seperate the found URIs on a layer by layer basi=\r\ns.\n\nRegards,\nZi Kai\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;g=\r\nojomo@...&gt; wrote:\n&gt;\n&gt; Yes, Heritrix considers certain implied prerequisites=\r\n as if they were \n&gt; &#39;discovered&#39; when considering the URI that required the=\r\nm. Thus they \n&gt; could be 1-2 &#39;P&#39; hops, in the simple hops-accounting.\n&gt; \n&gt; =\r\nIf you place a PrerequisiteAcceptDecideRule near the (or at the very) \n&gt; en=\r\nd of your scope DecideRules, it will &#39;rescue&#39; any necessary &#39;P&#39;s that \n&gt; yo=\r\nur earlier rules REJECTed. The default H3 configuration includes such \n&gt; a =\r\nrule, just in case.\n&gt; \n&gt; Your technique will allow you to clearly separate =\r\nURIs into categories \n&gt; by their hops from your starting points.\n&gt; \n&gt; But, =\r\nsince some URIs/paths can only be retrieved slowly (due to either \n&gt; server=\r\n slowness or the requirements of being polite), this sort of \n&gt; crawling wi=\r\nll tend to proceed much more slowly.\n&gt; \n&gt; (For example: Consider you have 1=\r\n seed. It has 100 links to 100 \n&gt; different sites, and 100 links to the sam=\r\ne site. The first 101 outlinks \n&gt; -- 1 to each site -- can be crawled very =\r\nfast, assuming all sites are \n&gt; up. The next 99 from the last site, when cr=\r\nawling politely, will take \n&gt; 100 times as long, and you won&#39;t be done with=\r\n &quot;level 1&quot; until then.)\n&gt; \n&gt; That may be fine for your purpose; I just want=\r\ned to further \n&gt; illustrate/emphasize the tradeoff.\n&gt; \n&gt; - Gordon\n&gt; \n&gt; \n&gt; \n=\r\n&gt; On 1/25/12 10:57 PM, ngzikai92 wrote:\n&gt; &gt; Hi,\n&gt; &gt;\n&gt; &gt; First of all, thank=\r\ns alot for the replies.\n&gt; &gt;\n&gt; &gt; In order to facilitate the crawling of laye=\r\nr by layer, I have been playing with the hops filter for awhile now.\n&gt; &gt;\n&gt; =\r\n&gt; Because I wanted to crawl layer by layer, I have been configuring my hop =\r\nfilter with a value of &#39;1&#39;. My idea is to start with a seed url, and crawl =\r\nit with a hop filter of 1 (meaning the uris will be 1 layer down from the s=\r\need), extract the results of the crawl.log, and start a new crawl with thos=\r\ne links I extracted from the first crawl as the seeds.\n&gt; &gt;\n&gt; &gt; I have been =\r\nusing the TooManyHopsDecideRule. However, I something  interesting happens =\r\nwhen I tried it out. When I limit the hops to 1, my crawl.log has a lot of =\r\nlinks with a status code of &#39;-63&#39; (which means a prerequisite could not be =\r\nresolved) even though they are 1 hop away from the link. I retried crawling=\r\n the same seed, this time with a hop limit of 2 and interestingly, the uris=\r\n which I was unable to crawl previously could be crawled now. This lead me =\r\nto believe (correct me if I am wrong) that DNS requests are also considered=\r\n a &#39;hop&#39;. My question is that is are there any configurations (or perhaps s=\r\nome other decide rule?) that I can use so that such uris could also be craw=\r\nled?\n&gt; &gt;\n&gt; &gt; Once again, thanks for the help give.\n&gt; &gt;\n&gt; &gt; Regards,\n&gt; &gt; Zi =\r\nKai\n&gt; &gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@&gt;  wrote=\r\n:\n&gt; &gt;&gt;\n&gt; &gt;&gt; The one thing I would add is that the &#39;layers&#39; as encountered b=\r\ny the\n&gt; &gt;&gt; crawler may not be the ones you are most interested in, because =\r\nthere\n&gt; &gt;&gt; are many link-hop paths from your seed to each URI, and which pa=\r\nth is\n&gt; &gt;&gt; followed first by the crawler is affected by lots of factors.\n&gt; =\r\n&gt;&gt;\n&gt; &gt;&gt; For example, you might reach one URI via an &#39;LLLL&#39; path -- four\n&gt; &gt;=\r\n&gt; navigational outinks in a row -- as reported in the crawl.log.\n&gt; &gt;&gt;\n&gt; &gt;&gt; =\r\nBut, that doesn&#39;t mean there isn&#39;t also a shorter &#39;LL&#39; path that *could*\n&gt; =\r\n&gt;&gt; have been followed. Only that for the crawler, with all of its various\n&gt;=\r\n &gt;&gt; ordering tendencies and the delays it encountered, happened to discover=\r\n\n&gt; &gt;&gt; the &#39;LLLL&#39; path first. (If the first &#39;L&#39; of that short &#39;LL&#39; path was =\r\non\n&gt; &gt;&gt; a slow host, or on a host that already had hours or days worth of U=\r\nRIs\n&gt; &gt;&gt; queued up, then by the time it is crawled, the other &#39;LLLL&#39; path h=\r\nas\n&gt; &gt;&gt; already finished. Thus the &#39;LL&#39; discovery path is rejected as provi=\r\nding\n&gt; &gt;&gt; a non-unique URI, and never enters the crawler queues/crawl.log.)=\r\n\n&gt; &gt;&gt;\n&gt; &gt;&gt; If you want a &#39;web graph&#39; of all paths (or all shortest paths) b=\r\netween\n&gt; &gt;&gt; URIs, that&#39;s usually calculated via a big post-crawl analysis.\n=\r\n&gt; &gt;&gt;\n&gt; &gt;&gt; - Gordon\n&gt; &gt;&gt;\n&gt; &gt;&gt; On 1/20/12 12:04 PM, Noah Levitt wrote:\n&gt; &gt;&gt;&gt; =\r\nHello Ng Zi Kai,\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; You can determine the number of hops from seed=\r\n by looking at the\n&gt; &gt;&gt;&gt; path-from-seed, which is the 5th column in crawl.l=\r\nog. The number of\n&gt; &gt;&gt;&gt; letters in that field is the number of hops from se=\r\ned (unless it&#39;s\n&gt; &gt;&gt;&gt; more than 50 hops in which case it will be abbreviate=\r\nd).\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; It is possible to write separate crawl logs for the differe=\r\nnt numbers\n&gt; &gt;&gt;&gt; of hops, but it&#39;s not simple to configure. My suggestion w=\r\nould be to\n&gt; &gt;&gt;&gt; split the crawl log up in a postprocessing step, by examin=\r\ning the\n&gt; &gt;&gt;&gt; path-from-seed for each crawled url.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Noah\n&gt; &gt;&gt;&gt;\n&gt;=\r\n &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; On 2012-01-12 19:09 , ngzikai92 wrote:\n&gt; &gt;&gt;&gt;&gt; Hi,\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; =\r\nThis might be a little confusing, and I am just starting out.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;=\r\n If we think about Heritrix, we can see it like a &#39;tree structure&#39;\n&gt; &gt;&gt;&gt;&gt; w=\r\nhere the seed url is at the top layer, the additional urls that\n&gt; &gt;&gt;&gt;&gt; are =\r\nfound from the root at the second layer, and the urls that are\n&gt; &gt;&gt;&gt;&gt; found=\r\n from the the additional urls at the third layer, and so on.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; =\r\nCurrently, based on what I can get from Heritrix&#39;s crawl.log, I am\n&gt; &gt;&gt;&gt;&gt; o=\r\nnly able to see the list of urls Heritrix found/visits as a list,\n&gt; &gt;&gt;&gt;&gt; bu=\r\nt I am not able to determine the layers of the &#39;tree structure&#39;\n&gt; &gt;&gt;&gt;&gt; from=\r\n there.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; I would like to know, if it is possible to customise =\r\nHeritrix in\n&gt; &gt;&gt;&gt;&gt; such a way that I can crawl layer by layer and separatin=\r\ng the urls\n&gt; &gt;&gt;&gt;&gt; obtained from each layer into separate log files?\n&gt; &gt;&gt;&gt;&gt;\n=\r\n&gt; &gt;&gt;&gt;&gt; Much thanks and regards, Ng Zi Kai\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; -----=\r\n-------------------------------\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt; &gt;&gt;&gt;&gt;\n&gt; =\r\n&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; ------------------------------------\n&gt; &gt;&gt;&gt;\n&gt; =\r\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --------=\r\n----------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}