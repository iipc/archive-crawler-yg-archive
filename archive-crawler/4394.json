{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":91078969,"authorName":"Jigar Patel","from":"Jigar Patel &lt;jigar_bca@...&gt;","profile":"jigar_bca","replyTo":"LIST","senderId":"dU5Th3YzGu9HVf4y_ju7WkYOvWwYbn7DHClCj8-e6JefoFKZAdfcaNiY-rIchAiv5ClndG8MHbm6w0PawIvJvK7vDHzI92qW","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling - Speed issue","postDate":"1183122915","msgId":4394,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU2OTMzLjEzOTcwLnFtQHdlYjUwMzExLm1haWwucmUyLnlhaG9vLmNvbT4=","inReplyToHeader":"PDQ2ODQwREMxLjQwNzA2MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":4392,"nextInTopic":0,"prevInTime":4393,"nextInTime":4395,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"Hi Gordon, Presently I am using Deciding Scope and I am getting excellent speed initially... But as time goes on speed decrease significantly... My seed file","rawEmail":"Return-Path: &lt;jigar_bca@...&gt;\r\nX-Sender: jigar_bca@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 12824 invoked from network); 29 Jun 2007 13:16:17 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m43.grp.scd.yahoo.com with QMQP; 29 Jun 2007 13:16:17 -0000\r\nReceived: from unknown (HELO web50311.mail.re2.yahoo.com) (206.190.39.202)\n  by mta13.grp.scd.yahoo.com with SMTP; 29 Jun 2007 13:16:17 -0000\r\nReceived: (qmail 15219 invoked by uid 60001); 29 Jun 2007 13:15:16 -0000\r\nX-YMail-OSG: Qzxgzg0VM1nVyeSxwHP.kuDhTtMOXoi196coJG1V\r\nReceived: from [203.199.114.33] by web50311.mail.re2.yahoo.com via HTTP; Fri, 29 Jun 2007 06:15:15 PDT\r\nDate: Fri, 29 Jun 2007 06:15:15 -0700 (PDT)\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;46840DC1.4070607@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-524406944-1183122915=:13970&quot;\r\nContent-Transfer-Encoding: 8bit\r\nMessage-ID: &lt;56933.13970.qm@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Jigar Patel &lt;jigar_bca@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling - Speed issue\r\nX-Yahoo-Group-Post: member; u=91078969; y=dL1bRkP2p0EyIB3gh7yKPQFquxep8HO_xHJHYfRCkD4jPl45\r\nX-Yahoo-Profile: jigar_bca\r\n\r\n\r\n--0-524406944-1183122915=:13970\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nHi Gordon,\n\nPresently I am using Deciding Scope and I am getting excellent speed initially...\nBut as time goes on speed decrease significantly...\n\nMy seed file contains 1 Million domains...\n\nSo please lte me know why does it happen ?\n\nPresently I am having following settings in BdbFrontier...\n\nqueue-assignment-policy :: SurtAuthorityQueueAssignmentPolicy\nhold-queues :: true\nbalance-replenish-amount :: 100\ncost-policy :: UnitCostAssignmentPolicy\nsnooze-deactivate-ms :: 1000\nuri-included-structure :: BloomUriUniqFilter\n\nPlease let me know If I have configured something wrong...\n\nThanks and Regards,\n\nJigar Patel\n\n\n\n\n\n\nGordon Mohr &lt;gojomo@...&gt; wrote:                                  lists@... wrote:\n &gt; HCC seems to offer a number of answers to this kind of configuration but \n &gt; getting hold of documentation regarding it (try \n &gt; http://crawler.sourceforge.net/hcc ) is a slightly challenging.  Is HCC \n &gt; still in active use? If so, can someone point me to a tad more doco.?\n \n The proper link is:\n \n http://crawler.archive.org/hcc\n \n (The crawler.archive.org homepage sidebar link was wrong but should be \n fixed now.)\n \n Sorry, there isn&#39;t much more in the way of documentation.\n \n HCC is used at the Internet Archive to help manage the multiple crawlers \n doing episodic, independent jobs for the Archive-It service, but not to \n manage sets of cooperating crawlers on a large crawl.\n \n &gt; As an aside - How many toes should a Heritrix on a quad core/cpu system \n &gt; be able to mange if it is assumed that memory/io/bandwidth are removed \n &gt; from the equation?\n \n We know no hard-and-fast rules; our technique has been &#39;add more as long \n as they seem to increase throughput&#39;. We usually use 150-250 on \n dual-core machines.\n \n - Gordon @ IA\n \n &gt; Regards,\n &gt; Graeme\n &gt; \n &gt; Gordon Mohr wrote:\n &gt;&gt;\n &gt;&gt; Jigar Patel wrote:\n &gt;&gt; &gt; Thanks a lot Gordon,\n &gt;&gt; &gt;\n &gt;&gt; &gt; You solved my problem.\n &gt;&gt; &gt;\n &gt;&gt; &gt; One more thing I want to know that,\n &gt;&gt; &gt; Can I use same settings for two different independent machines for \n &gt;&gt; distributed crawling ?\n &gt;&gt;\n &gt;&gt; Yes; that is the usual case for distributed crawling: each of the\n &gt;&gt; crawlers has the exact same initial configuration, *except* for the\n &gt;&gt; HashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to\n &gt;&gt; decline to process URIs mapped to the others.\n &gt;&gt;\n &gt;&gt; There is no automatic facility for sharing the settings; you must\n &gt;&gt; manually copy the order.xml (and possibly other files) between crawlers.\n &gt;&gt; Also, there is no automatic facility for cross-feeding the URIs each\n &gt;&gt; crawler rejects. You must watch the crawl.log (or diversions logs from\n &gt;&gt; the mapper) and decide if the URIs should be fed to the sibling crawlers.\n &gt;&gt;\n &gt;&gt; - Gordon @ IA\n &gt;&gt;\n &gt;&gt; &gt; Please tell me how does it work and coordinate with different machine ?\n &gt;&gt; &gt;\n &gt;&gt; &gt; Thanks\n &gt;&gt; &gt;\n &gt;&gt; &gt; Jigar\n &gt;&gt; &gt;\n &gt;&gt; &gt; Gordon Mohr &lt;gojomo@... &lt;mailto:gojomo%40archive.org&gt;&gt; wrote:\n &gt;&gt; &gt; Jigar Patel wrote:\n &gt;&gt; &gt;&gt; Presently I am running two heritrix instances on the same machine on\n &gt;&gt; &gt;&gt; different port...\n &gt;&gt; &gt;\n &gt;&gt; &gt; In general, you only want to use distributed crawling, with URIs\n &gt;&gt; &gt; partitioned across separate cooperating crawlers, to spread a crawl \n &gt;&gt; over\n &gt;&gt; &gt; multiple independent machines. If using a single machine, a single\n &gt;&gt; &gt; crawler instance will be more efficient.\n &gt;&gt; &gt;\n &gt;&gt; &gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule\n &gt;&gt; &gt;&gt; I added HashCrawlMapper at two places as you suggested\n &gt;&gt; &gt;&gt; I made same configuration setting and seed file at each place.\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt; But as I run my job it gives me following error in seed file and\n &gt;&gt; &gt;&gt; nothing was crawled.\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt; Heritrix(-5002)-Blocked by custom prefetch processor\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt; Please let me know why I am getting such error...\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt; Is anything missing ?\n &gt;&gt; &gt;\n &gt;&gt; &gt; This is the expected crawl.log result for URIs that are considered by a\n &gt;&gt; &gt; crawler, but mapped to be handled by one of the others in the group of\n &gt;&gt; &gt; crawlers. With a proper configuration, some but not all lines in your\n &gt;&gt; &gt; crawl.log will have this code.\n &gt;&gt; &gt;\n &gt;&gt; &gt; For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and\n &gt;&gt; &gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; \n &gt;&gt; of &#39;2&#39;.\n &gt;&gt; &gt;\n &gt;&gt; &gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, \n &gt;&gt; but\n &gt;&gt; &gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the\n &gt;&gt; &gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the\n &gt;&gt; &gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.\n &gt;&gt; &gt;\n &gt;&gt; &gt; - Gordon @ IA\n &gt;&gt; &gt;\n &gt;&gt; &gt;&gt; Regards,\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt; Jigar Patel\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt; --- In archive-crawler@yahoogroups.com \n &gt;&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Gordon Mohr &lt;gojomo@...&gt;\n &gt;&gt; &gt;&gt; wrote:\n &gt;&gt; &gt;&gt;&gt; nt_bdr wrote:\n &gt;&gt; &gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n &gt;&gt; &gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we\n &gt;&gt; &gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600\n &gt;&gt; &gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8\n &gt;&gt; &gt;&gt; machines\n &gt;&gt; &gt;&gt;&gt; (&gt;1 billion URLs fetched).\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; For background see some previous threads including:\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2909 \n &gt;&gt; &lt;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&gt;\n &gt;&gt; &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/3060 \n &gt;&gt; &lt;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&gt;\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; Roughly how we do it:\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; - Use BloomFilterUriUniqFilter with its defaults -- which devotes\n &gt;&gt; &gt;&gt;&gt; about 500MB to this structure and keeps the false-positive\n &gt;&gt; &gt;&gt; (mistakenly\n &gt;&gt; &gt;&gt;&gt; believed to have been previously-scheduled) rate under 1-in-4-\n &gt;&gt; &gt;&gt; million up\n &gt;&gt; &gt;&gt;&gt; through 125 million URIs discovered.\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+\n &gt;&gt; &gt;&gt; heap\n &gt;&gt; &gt;&gt;&gt; - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in\n &gt;&gt; &gt;&gt; queues\n &gt;&gt; &gt;&gt;&gt; named by the reversed-host (com,example,) rather than usual host\n &gt;&gt; &gt;&gt;&gt; (example.com)\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor\n &gt;&gt; &gt;&gt; chain:\n &gt;&gt; &gt;&gt;&gt; * Once, immediately before the PreconditionEnforcer. This one\n &gt;&gt; &gt;&gt; has\n &gt;&gt; &gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any\n &gt;&gt; &gt;&gt; scheduled\n &gt;&gt; &gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)\n &gt;&gt; &gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has\n &gt;&gt; &gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any\n &gt;&gt; &gt;&gt; discovered\n &gt;&gt; &gt;&gt;&gt; outlinks before they are scheduled.)\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a\n &gt;&gt; &gt;&gt; number 0\n &gt;&gt; &gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and\n &gt;&gt; &gt;&gt; all\n &gt;&gt; &gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers,\n &gt;&gt; &gt;&gt; n).\n &gt;&gt; &gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the\n &gt;&gt; &gt;&gt; SURT\n &gt;&gt; &gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI\n &gt;&gt; &gt;&gt; is\n &gt;&gt; &gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to\n &gt;&gt; &gt;&gt; a\n &gt;&gt; &gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally.\n &gt;&gt; &gt;&gt;&gt; Depending on the character of your crawl, you may want to feed\n &gt;&gt; &gt;&gt; these\n &gt;&gt; &gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore\n &gt;&gt; &gt;&gt; them.\n &gt;&gt; &gt;&gt;&gt; The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key\n &gt;&gt; &gt;&gt; before\n &gt;&gt; &gt;&gt;&gt; mapping -- used to ensure that all subdomains of example.com are\n &gt;&gt; &gt;&gt; treated\n &gt;&gt; &gt;&gt;&gt; the same as example.com for mapping purposes. The first match of\n &gt;&gt; &gt;&gt; this\n &gt;&gt; &gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small\n &gt;&gt; &gt;&gt;&gt; example would be:\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything\n &gt;&gt; &gt;&gt; through\n &gt;&gt; &gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it\n &gt;&gt; &gt;&gt; uses\n &gt;&gt; &gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns\n &gt;&gt; &gt;&gt; that\n &gt;&gt; &gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs,\n &gt;&gt; &gt;&gt; like\n &gt;&gt; &gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level\n &gt;&gt; &gt;&gt;&gt; domains.)\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; - All crawlers are launched with the same configuration,\n &gt;&gt; &gt;&gt; including\n &gt;&gt; &gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate.\n &gt;&gt; &gt;&gt; Seeds\n &gt;&gt; &gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early\n &gt;&gt; &gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed\n &gt;&gt; &gt;&gt; are\n &gt;&gt; &gt;&gt;&gt; done so by an external process/scripts.\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;&gt; - Gordon @ IA\n &gt;&gt; &gt;&gt;&gt;\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt; Yahoo! Groups Links\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;&gt;\n &gt;&gt; &gt;\n &gt;&gt; &gt;\n &gt;&gt; &gt;\n &gt;&gt; &gt;\n &gt;&gt; &gt;\n &gt;&gt; &gt;\n &gt;&gt; &gt; ---------------------------------\n &gt;&gt; &gt; Food fight? Enjoy some healthy debate\n &gt;&gt; &gt; in the Yahoo! Answers Food & Drink Q&A.\n &gt;&gt;\n &gt;&gt;  \n &gt; \n \n \n     \n                       \n\n       \n---------------------------------\nGot a little couch potato? \nCheck out fun summer activities for kids.\r\n--0-524406944-1183122915=:13970\r\nContent-Type: text/html; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nHi Gordon,&lt;br&gt;&lt;br&gt;Presently I am using Deciding Scope and I am getting excellent speed initially...&lt;br&gt;But as time goes on speed decrease significantly...&lt;br&gt;&lt;br&gt;My seed file contains 1 Million domains...&lt;br&gt;&lt;br&gt;So please lte me know why does it happen ?&lt;br&gt;&lt;br&gt;Presently I am having following settings in BdbFrontier...&lt;br&gt;&lt;br&gt;queue-assignment-policy :: SurtAuthorityQueueAssignmentPolicy&lt;br&gt;hold-queues :: true&lt;br&gt;balance-replenish-amount :: 100&lt;br&gt;cost-policy :: UnitCostAssignmentPolicy&lt;br&gt;snooze-deactivate-ms :: 1000&lt;br&gt;uri-included-structure :: BloomUriUniqFilter&lt;br&gt;&lt;br&gt;Please let me know If I have configured something wrong...&lt;br&gt;&lt;br&gt;Thanks and Regards,&lt;br&gt;&lt;br&gt;Jigar Patel&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;&lt;i&gt;Gordon Mohr &lt;gojomo@...&gt;&lt;/i&gt;&lt;/b&gt; wrote:&lt;blockquote class=&quot;replbq&quot; style=&quot;border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;&quot;&gt;     &lt;!-- Network content --&gt;           &lt;div id=&quot;ygrp-text&quot;&gt;             &lt;div&gt;&lt;a href=&quot;mailto:lists%40graemes.com&quot;&gt;lists@graemes.&lt;wbr&gt;com&lt;/a&gt; wrote:&lt;br&gt; &gt; HCC seems to offer a number of answers to this kind of configuration but &lt;br&gt; &gt; getting hold of documentation regarding it (try &lt;br&gt; &gt; &lt;a href=&quot;http://crawler.sourceforge.net/hcc&quot;&gt;http://crawler.&lt;wbr&gt;sourceforge.&lt;wbr&gt;net/hcc&lt;/a&gt; ) is a slightly challenging.  Is HCC &lt;br&gt; &gt; still in active use? If so, can someone point me to a tad more doco.?&lt;br&gt; &lt;br&gt; The proper link is:&lt;br&gt; &lt;br&gt; &lt;a href=&quot;http://crawler.archive.org/hcc&quot;&gt;http://crawler.&lt;wbr&gt;archive.org/&lt;wbr&gt;hcc&lt;/a&gt;&lt;br&gt; &lt;br&gt; (The crawler.archive.&lt;wbr&gt;org homepage sidebar link was wrong but should be &lt;br&gt; fixed now.)&lt;br&gt; &lt;br&gt; Sorry, there isn&#39;t much more in the way of documentation.&lt;br&gt; &lt;br&gt; HCC is used at the Internet Archive to help manage\n the multiple crawlers &lt;br&gt; doing episodic, independent jobs for the Archive-It service, but not to &lt;br&gt; manage sets of cooperating crawlers on a large crawl.&lt;br&gt; &lt;br&gt; &gt; As an aside - How many toes should a Heritrix on a quad core/cpu system &lt;br&gt; &gt; be able to mange if it is assumed that memory/io/bandwidth are removed &lt;br&gt; &gt; from the equation?&lt;br&gt; &lt;br&gt; We know no hard-and-fast rules; our technique has been &#39;add more as long &lt;br&gt; as they seem to increase throughput&#39;. We usually use 150-250 on &lt;br&gt; dual-core machines.&lt;br&gt; &lt;br&gt; - Gordon @ IA&lt;br&gt; &lt;br&gt; &gt; Regards,&lt;br&gt; &gt; Graeme&lt;br&gt; &gt; &lt;br&gt; &gt; Gordon Mohr wrote:&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; Jigar Patel wrote:&lt;br&gt; &gt;&gt; &gt; Thanks a lot Gordon,&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; You solved my problem.&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; One more thing I want to know that,&lt;br&gt; &gt;&gt; &gt; Can I use same settings for two different independent machines for &lt;br&gt; &gt;&gt; distributed crawling ?&lt;br&gt; &gt;&gt;&lt;br&gt;\n &gt;&gt; Yes; that is the usual case for distributed crawling: each of the&lt;br&gt; &gt;&gt; crawlers has the exact same initial configuration, *except* for the&lt;br&gt; &gt;&gt; HashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to&lt;br&gt; &gt;&gt; decline to process URIs mapped to the others.&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; There is no automatic facility for sharing the settings; you must&lt;br&gt; &gt;&gt; manually copy the order.xml (and possibly other files) between crawlers.&lt;br&gt; &gt;&gt; Also, there is no automatic facility for cross-feeding the URIs each&lt;br&gt; &gt;&gt; crawler rejects. You must watch the crawl.log (or diversions logs from&lt;br&gt; &gt;&gt; the mapper) and decide if the URIs should be fed to the sibling crawlers.&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; - Gordon @ IA&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt; Please tell me how does it work and coordinate with different machine ?&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; Thanks&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; Jigar&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt;\n &gt; Gordon Mohr &lt;&lt;a href=&quot;mailto:gojomo%40archive.org&quot;&gt;gojomo@archive.&lt;wbr&gt;org&lt;/a&gt; &lt;mailto:gojomo%&lt;wbr&gt;40archive.&lt;wbr&gt;org&gt;&gt; wrote:&lt;br&gt; &gt;&gt; &gt; Jigar Patel wrote:&lt;br&gt; &gt;&gt; &gt;&gt; Presently I am running two heritrix instances on the same machine on&lt;br&gt; &gt;&gt; &gt;&gt; different port...&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; In general, you only want to use distributed crawling, with URIs&lt;br&gt; &gt;&gt; &gt; partitioned across separate cooperating crawlers, to spread a crawl &lt;br&gt; &gt;&gt; over&lt;br&gt; &gt;&gt; &gt; multiple independent machines. If using a single machine, a single&lt;br&gt; &gt;&gt; &gt; crawler instance will be more efficient.&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule&lt;br&gt; &gt;&gt; &gt;&gt; I added HashCrawlMapper at two places as you suggested&lt;br&gt; &gt;&gt; &gt;&gt; I made same configuration setting and seed file at each place.&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt; But as I run my\n job it gives me following error in seed file and&lt;br&gt; &gt;&gt; &gt;&gt; nothing was crawled.&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt; Heritrix(-5002)&lt;wbr&gt;-Blocked by custom prefetch processor&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt; Please let me know why I am getting such error...&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt; Is anything missing ?&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; This is the expected crawl.log result for URIs that are considered by a&lt;br&gt; &gt;&gt; &gt; crawler, but mapped to be handled by one of the others in the group of&lt;br&gt; &gt;&gt; &gt; crawlers. With a proper configuration, some but not all lines in your&lt;br&gt; &gt;&gt; &gt; crawl.log will have this code.&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and&lt;br&gt; &gt;&gt; &gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; &lt;br&gt; &gt;&gt; of &#39;2&#39;.&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a\n URI is mapped to &#39;1&#39;, &lt;br&gt; &gt;&gt; but&lt;br&gt; &gt;&gt; &gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the&lt;br&gt; &gt;&gt; &gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the&lt;br&gt; &gt;&gt; &gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; - Gordon @ IA&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt;&gt; Regards,&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt; Jigar Patel&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt; --- In &lt;a href=&quot;mailto:archive-crawler%40yahoogroups.com&quot;&gt;archive-crawler@&lt;wbr&gt;yahoogroups.&lt;wbr&gt;com&lt;/a&gt; &lt;br&gt; &gt;&gt; &lt;mailto:archive-&lt;wbr&gt;crawler%40yahoog&lt;wbr&gt;roups.com&gt;&lt;wbr&gt;, Gordon Mohr &lt;gojomo@...&gt;&lt;br&gt; &gt;&gt; &gt;&gt; wrote:&lt;br&gt; &gt;&gt; &gt;&gt;&gt; nt_bdr wrote:&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?&lt;br&gt; &gt;&gt; &gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we&lt;br&gt; &gt;&gt; &gt;&gt;&gt; would\n like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600&lt;br&gt; &gt;&gt; &gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8&lt;br&gt; &gt;&gt; &gt;&gt; machines&lt;br&gt; &gt;&gt; &gt;&gt;&gt; (&gt;1 billion URLs fetched).&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; For background see some previous threads including:&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; &lt;a href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/2909&lt;/a&gt; &lt;br&gt; &gt;&gt; &lt;&lt;a href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/2909&lt;/a&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; &lt;a href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/3060&lt;/a&gt; &lt;br&gt; &gt;&gt; &lt;&lt;a\n href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/3060&lt;/a&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; Roughly how we do it:&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; - Use BloomFilterUriUniqF&lt;wbr&gt;ilter with its defaults -- which devotes&lt;br&gt; &gt;&gt; &gt;&gt;&gt; about 500MB to this structure and keeps the false-positive&lt;br&gt; &gt;&gt; &gt;&gt; (mistakenly&lt;br&gt; &gt;&gt; &gt;&gt;&gt; believed to have been previously-schedule&lt;wbr&gt;d) rate under 1-in-4-&lt;br&gt; &gt;&gt; &gt;&gt; million up&lt;br&gt; &gt;&gt; &gt;&gt;&gt; through 125 million URIs discovered.&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+&lt;br&gt; &gt;&gt; &gt;&gt; heap&lt;br&gt; &gt;&gt; &gt;&gt;&gt; - Use SurtAuthorityAssign&lt;wbr&gt;mentPolicy, so URIs are grouped in&lt;br&gt; &gt;&gt; &gt;&gt; queues&lt;br&gt; &gt;&gt; &gt;&gt;&gt; named by the reversed-host\n (com,example,&lt;wbr&gt;) rather than usual host&lt;br&gt; &gt;&gt; &gt;&gt;&gt; (example.com)&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor&lt;br&gt; &gt;&gt; &gt;&gt; chain:&lt;br&gt; &gt;&gt; &gt;&gt;&gt; * Once, immediately before the PreconditionEnforce&lt;wbr&gt;r. This one&lt;br&gt; &gt;&gt; &gt;&gt; has&lt;br&gt; &gt;&gt; &gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any&lt;br&gt; &gt;&gt; &gt;&gt; scheduled&lt;br&gt; &gt;&gt; &gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)&lt;br&gt; &gt;&gt; &gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has&lt;br&gt; &gt;&gt; &gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any&lt;br&gt; &gt;&gt; &gt;&gt; discovered&lt;br&gt; &gt;&gt; &gt;&gt;&gt; outlinks before they are scheduled.)&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a&lt;br&gt; &gt;&gt; &gt;&gt; number 0&lt;br&gt; &gt;&gt;\n &gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and&lt;br&gt; &gt;&gt; &gt;&gt; all&lt;br&gt; &gt;&gt; &gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers,&lt;br&gt; &gt;&gt; &gt;&gt; n).&lt;br&gt; &gt;&gt; &gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the&lt;br&gt; &gt;&gt; &gt;&gt; SURT&lt;br&gt; &gt;&gt; &gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI&lt;br&gt; &gt;&gt; &gt;&gt; is&lt;br&gt; &gt;&gt; &gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to&lt;br&gt; &gt;&gt; &gt;&gt; a&lt;br&gt; &gt;&gt; &gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally.&lt;br&gt; &gt;&gt; &gt;&gt;&gt; Depending on the character of your crawl, you may want to feed&lt;br&gt; &gt;&gt; &gt;&gt; these&lt;br&gt; &gt;&gt; &gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore&lt;br&gt; &gt;&gt; &gt;&gt; them.&lt;br&gt; &gt;&gt; &gt;&gt;&gt; The &#39;reduce-prefix-&lt;wbr&gt;pattern&#39; may be used to trim the\n queue key&lt;br&gt; &gt;&gt; &gt;&gt; before&lt;br&gt; &gt;&gt; &gt;&gt;&gt; mapping -- used to ensure that all subdomains of example.com are&lt;br&gt; &gt;&gt; &gt;&gt; treated&lt;br&gt; &gt;&gt; &gt;&gt;&gt; the same as example.com for mapping purposes. The first match of&lt;br&gt; &gt;&gt; &gt;&gt; this&lt;br&gt; &gt;&gt; &gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small&lt;br&gt; &gt;&gt; &gt;&gt;&gt; example would be:&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*&lt;wbr&gt;)|[&#92;w,]{9}&lt;wbr&gt;)&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything&lt;br&gt; &gt;&gt; &gt;&gt; through&lt;br&gt; &gt;&gt; &gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it&lt;br&gt; &gt;&gt; &gt;&gt; uses&lt;br&gt; &gt;&gt; &gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns&lt;br&gt; &gt;&gt; &gt;&gt; that&lt;br&gt; &gt;&gt; &gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs,&lt;br&gt; &gt;&gt;\n &gt;&gt; like&lt;br&gt; &gt;&gt; &gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level&lt;br&gt; &gt;&gt; &gt;&gt;&gt; domains.)&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; - All crawlers are launched with the same configuration,&lt;br&gt; &gt;&gt; &gt;&gt; including&lt;br&gt; &gt;&gt; &gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate.&lt;br&gt; &gt;&gt; &gt;&gt; Seeds&lt;br&gt; &gt;&gt; &gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early&lt;br&gt; &gt;&gt; &gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed&lt;br&gt; &gt;&gt; &gt;&gt; are&lt;br&gt; &gt;&gt; &gt;&gt;&gt; done so by an external process/scripts.&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&gt; - Gordon @ IA&lt;br&gt; &gt;&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt; Yahoo! Groups Links&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt; &gt;&gt;&lt;br&gt; &gt;&gt;\n &gt;&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt;&lt;br&gt; &gt;&gt; &gt; ------------&lt;wbr&gt;---------&lt;wbr&gt;---------&lt;wbr&gt;---&lt;br&gt; &gt;&gt; &gt; Food fight? Enjoy some healthy debate&lt;br&gt; &gt;&gt; &gt; in the Yahoo! Answers Food &amp; Drink Q&amp;A.&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt;  &lt;br&gt; &gt; &lt;br&gt; &lt;br&gt; &lt;/div&gt;     &lt;/div&gt;          &lt;!--End group\n email --&gt;  &lt;/blockquote&gt;&lt;br&gt;&lt;p&gt;&#32;\n      &lt;hr size=1&gt;Got a little couch potato? &lt;br&gt;\nCheck out fun &lt;a href=&quot;http://us.rd.yahoo.com/evt=48248/*http://search.yahoo.com/search?fr=oni_on_mail&p=summer+activities+for+kids&cs=bz&quot;&gt;summer activities for kids.&lt;/a&gt;\r\n--0-524406944-1183122915=:13970--\r\n\n"}}