{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"rn8i_oY-ell-jQ3AgACau7W4VMWFVbO8Kt7wuVYQwwzcLgGz_JoUcjFffEoZPRcqBRKSyInK7-zMakOr4latpIbavHVYDuM","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] unnecessary bottleneck in FrontierScheduler?","postDate":"1170290289","msgId":3769,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1QzEzNjcxLjUwMDA0QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDIwMDcwMTMxMTg1MjA0LkdBMjUyMjRAZHV2ZWwuaXIuaWl0LmVkdT4=","referencesHeader":"PDIwMDcwMTMxMTg1MjA0LkdBMjUyMjRAZHV2ZWwuaXIuaWl0LmVkdT4="},"prevInTopic":3767,"nextInTopic":0,"prevInTime":3768,"nextInTime":3770,"topicId":3767,"numMessagesInTopic":2,"msgSnippet":"... This is an area where we ve also noticed synchronization holdups, and there is surely room for improvement. If I recall correctly, the FrontierScheduler","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 97609 invoked from network); 1 Feb 2007 00:38:05 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m36.grp.scd.yahoo.com with QMQP; 1 Feb 2007 00:38:05 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta4.grp.scd.yahoo.com with SMTP; 1 Feb 2007 00:38:05 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id D63AC141997C5\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 31 Jan 2007 16:38:04 -0800 (PST)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 22952-05-18 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 31 Jan 2007 16:38:02 -0800 (PST)\r\nReceived: from [192.168.1.203] (c-67-180-203-212.hsd1.ca.comcast.net [67.180.203.212])\n\tby mail.archive.org (Postfix) with ESMTP id 82CE2141997C1\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 31 Jan 2007 16:38:02 -0800 (PST)\r\nMessage-ID: &lt;45C13671.50004@...&gt;\r\nDate: Wed, 31 Jan 2007 16:38:09 -0800\r\nUser-Agent: Thunderbird 1.5.0.9 (X11/20070104)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;20070131185204.GA25224@...&gt;\r\nIn-Reply-To: &lt;20070131185204.GA25224@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] unnecessary bottleneck in FrontierScheduler?\r\nX-Yahoo-Group-Post: member; u=137285340; y=Yq-DlVGUqQ0IrwwUkkLJgFVnh12elLAKW5z4a8elD4xq\r\nX-Yahoo-Profile: gojomo\r\n\r\nEric wrote:\n&gt; When running heritrix with about 50 ToeThreads I&#39;m running into a\n&gt; bottleneck in FrontierScheduler that seems unnecessary, or at least\n&gt; misplaced.  On line 76 as the final step of innerProcess it does\n&gt; synchronized(this) while it loops through all the\n&gt; curi.getOutCandidates() and calls Frontier.schedule() on each of them.\n&gt; First, there should at least be a check to see if there are any\n&gt; candidates before requiring the lock.  Second, I don&#39;t think this is\n&gt; the right place to be synchronizing.  Looking at\n&gt; WorkQueueFrontier.schedule() which handles this in most configs, it&#39;s\n&gt; passing these candidates along to the UriUniqFilter (of which at least\n&gt; the Bdb one is thread-safe).  We know that the UriUniqFilter can\n&gt; become a bottleneck in large crawls, so we definitely shouldn&#39;t make\n&gt; all the threads wait while we access it, especially if it is\n&gt; thread-safe anyways...we should let Bdb itself do any necessary\n&gt; synchronization.  \n&gt; \n&gt; Of course, this would mean making sure other things down the line are\n&gt; thread-safe.  UriUniqFilter in turn calls WorkQueueFrontier.receive()\n&gt; if they&#39;re unique, which most significantly calls\n&gt; WorkQueueFrontier.sendToQueue() which I think for the Bdb one should\n&gt; be thread-safe, but even if we made that synchronized, that would be\n&gt; better than synchronizing before we even access the UriUniqFilter\n&gt; since that can be slow.\n\nThis is an area where we&#39;ve also noticed synchronization holdups, and \nthere is surely room for improvement.\n\nIf I recall correctly, the FrontierScheduler lock was introduced in \nreaction to some performance anomalies, rather than for thread safety.\n\nWith large lists of of outlinks to schedule, finer-grained locks were \nbeing acquired and released many times to finish the scheduling step of \na single CrawlURI. But, with other CrawlURIs also in the same step, and \nwith native Java synchronization locks not being &#39;fair&#39;, a large number \nof threads were all observed midway through scheduling of their \noutlinks, with some threads being repeatedly overtaken and staying in \nthis Processor for ludicrous amounts of time. (All threads stuck halfway \nthrough outlink scheduling is also a near worst-case scenario for memory \nusage, as its peak memory usage is between the application of scoping \nrules and the CrawlURI&#39;s final disposition, where all the in-memory \noutlink URI objects can be discarded.)\n\nWith the coarse lock, at least once the scheduling of a group of \noutlinks begins, it will finish before another thread intercedes. To \nsome extent, this moves the bottleneck to acquiring the coarse \nFrontierScheduler lock, but at the time that seemed superior to the \nalternative.\n\nThat may no longer be the right choice, and there is likely a better \napproach not yet even considered.\n\nWe will undoubtedly take a look at this at some point, but would welcome \nany suggestions, results from testing, or patches in the meantime. Does \nyour performance improve if you eliminate the FrontierScheduler \nsynchronization?\n\n- Gordon @ IA\n\n\n"}}