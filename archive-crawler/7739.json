{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"rYlU3stDg7eqh7ApfdZTgDN23jMBQvAoeJXOv7zwuU2F1qLyRKjis4QsHLbBC5ZPNF2eDaKXip67P0_og64ozQryxLlC0dE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Stucked in 690 million discovered.","postDate":"1344381094","msgId":7739,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwMjFBMEE2LjUwMDAyMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDUwMTJDQjc4LjQwODAzMDVAYmF5YXJlYS5uZXQ+","referencesHeader":"PGp1dTkwNyt1Zm1pQGVHcm91cHMuY29tPiA8NTAxMkNCNzguNDA4MDMwNUBiYXlhcmVhLm5ldD4="},"prevInTopic":7733,"nextInTopic":7740,"prevInTime":7738,"nextInTime":7740,"topicId":7731,"numMessagesInTopic":6,"msgSnippet":"There s no inherent limit in either H1 or H3 of the number of URIs a crawl can discover: if properly configured, with enough disk space, it will keep","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 28532 invoked from network); 7 Aug 2012 23:11:37 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m11.grp.sp2.yahoo.com with QMQP; 7 Aug 2012 23:11:37 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta2.grp.sp2.yahoo.com with SMTP; 7 Aug 2012 23:11:37 -0000\r\nX-Received: (qmail 90208 invoked by uid 0); 7 Aug 2012 23:11:35 -0000\r\nX-Received: from 70.36.143.78 (HELO silverbook.local) (70.36.143.78)\n  by relay00.pair.com with SMTP; 7 Aug 2012 23:11:35 -0000\r\nX-pair-Authenticated: 70.36.143.78\r\nMessage-ID: &lt;5021A0A6.5000205@...&gt;\r\nDate: Tue, 07 Aug 2012 16:11:34 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:14.0) Gecko/20120713 Thunderbird/14.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;juu907+ufmi@...&gt; &lt;5012CB78.4080305@...&gt;\r\nIn-Reply-To: &lt;5012CB78.4080305@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Stucked in 690 million discovered.\r\nX-Yahoo-Group-Post: member; u=137285340; y=W7k_dhYO42Wfm_iruYlsp8lH39hel-DruFRapHhB2EdH\r\nX-Yahoo-Profile: gojomo\r\n\r\nThere&#39;s no inherent limit in either H1 or H3 of the number of URIs a \ncrawl can discover: if properly configured, with enough disk space, it \nwill keep crawling... just slower and slower, as lookups on ever-larger \ndisk structures take longer.\n\nHowever, in the standard configuration (using BdbUriUniqFilter), this \ncan be get slow indeed, so broader crawls expected to grow into the tens \nor hundreds of millions of URIs often use the BloomUriUniqFilter \ninstead. Bloom filters only use a fixed amount of main memory... and so \ncan &#39;fill up&#39;: beyond their designed insert-size, their false-positive \nrate (saying a URI was seen when it wasn&#39;t) will climb, eventually to 100%.\n\nIf you swapped in our BloomFilter option, and didn&#39;t change its default \nconfiguration (which used ~500MB of RAM and is size-optimized for 125 \nmillion discovered URIs and 1-in-4-million false-positive rate), that \nmight explain what you&#39;re seeing. (I believe even at almost 5x designed \nsize, *some* URIs would still be considered new, but the numbers you&#39;ve \nincluded don&#39;t make it clear if the &#39;discovered&#39; count is frozen or just \ngrowing very slowly.)\n\nIf that&#39;s not the problem, other things to consider:\n\n� maybe you really have discovered all URIs allowed by your scope and \ntransitively linked from your seeds -- are you sure the expected &quot;2 \nmillion&quot; domains are both (a) allowed by your scope; and (b) reachable \nby outlink paths that are at each hop allowed by your scope as well?\n\n� some sort of subtle error or data corruption which has allowed \ncrawling to proceed but interfered with the enqueuing of new URIs. I \ndon&#39;t know of any known errors that would cause this, but it&#39;s worth \nchecking the alerts/error-logs/heritrix_out.log for any suspicious \nExceptions/Errors.\n\nIf it is the saturated BloomFilter issue, you would want to relaunch \nwith a filter sized appropriately for your expected crawl size. (It \ncan&#39;t be resized mid-crawl, and unless you were separately redundanly \nlogging outlinks you&#39;ll have to revisit/re-extract many pages to \ndiscover the links that were discarded as false duplicates.) In H1, the \nfilter-sizing parameters must be set via system properties -- see \nBloomUriUniqFilter(). In H3, it&#39;s a bit easier: the component can be \nswapped out or configured in the same way as others.\n\nIf your crawl is even larger than your RAM allows a properly-sized \nfilter, you might want to split it into separate non-overlapping crawls, \ndividing the URI space between separate machines (or serial runs). See \ndiscussion on the list and project wiki around &#39;HashCrawlMapper&#39; for \nsome ways to achieve this. (Again, it&#39;s a bit easier in H3.)\n\n- Gordon\n\nOn 7/27/12 10:10 AM, John Lekashman wrote:\n&gt;\n&gt;\n&gt; Hi,\n&gt; You still on Heretrix 1?\n&gt;\n&gt; I know that H 1 had a problem of an upper limit of around 700M urls per\n&gt; crawler.\n&gt; Split the crawl with a hashmapper.\n&gt;\n&gt; Don&#39;t know if H 3 has that problem, I always split those as well, to get\n&gt; things done\n&gt; in finite time.\n&gt;\n&gt; John\n&gt;\n&gt; On 7/27/12 7:35 AM, Elverton wrote:\n&gt;&gt;\n&gt;&gt; Hello everybody.\n&gt;&gt;\n&gt;&gt; Well, I&#39;m having a big trouble this time. Before I explain the\n&gt;&gt; problem, here is the system configuration:\n&gt;&gt;\n&gt;&gt; - 24 GB RAM\n&gt;&gt; - Intel(R) Xeon(R) CPU E5520 @ 2.27GHz\n&gt;&gt; - 1.8TB hard disk for Heritrix. (I don&#39;t use warc in this crawl. My\n&gt;&gt; only target is to know how many (approx.) URLs a domain has.) The\n&gt;&gt; usage of the disk is: used 500GB, free 1.3TB.\n&gt;&gt; - 16GB java heap size for heritrix.\n&gt;&gt; - Java 1.7.0_05\n&gt;&gt;\n&gt;&gt; Here is the Heritrix configuration that I consider helpful to the\n&gt;&gt; problem:\n&gt;&gt;\n&gt;&gt; - bdb-cache-percent = 25\n&gt;&gt; - frontier = BdbFrontier\n&gt;&gt; - max-delay-ms = 10000\n&gt;&gt; - min-delay-ms = 2000\n&gt;&gt; - respect-crawl-delay-up-to-secs = 300\n&gt;&gt; - max-retries = 10\n&gt;&gt; - retry-delay-seconds = 30\n&gt;&gt; - timeout-seconds = 1200\n&gt;&gt; - sotimeout-ms = 20000\n&gt;&gt;\n&gt;&gt; % ----------------------------------------------------------\n&gt;&gt;\n&gt;&gt; So, my problem is: the crawl stucked in 690 million discovered.\n&gt;&gt; (Queued it&#39;s around 520 million and downloaded is around 170 million).\n&gt;&gt;\n&gt;&gt; The strange thing is the download/uri rate.\n&gt;&gt;\n&gt;&gt; Docs/s(avg): 53.2(60.77)\n&gt;&gt; KB/s(avg): 2069(3205)\n&gt;&gt;\n&gt;&gt; It continues, in some way, good in theory (about 3 or 4 million uri\n&gt;&gt; crawled per day if you have 53.2 uri&#39;s during all day), but the real\n&gt;&gt; crawled per day is below 500.000 (discovered).\n&gt;&gt;\n&gt;&gt; Looking at some number in the last five days:\n&gt;&gt; Queued Downloaded\n&gt;&gt; 541054381 133121289\n&gt;&gt; 535322185 138522175\n&gt;&gt; 530280577 143176680\n&gt;&gt; 525907149 147086865\n&gt;&gt; 520568517 151604201\n&gt;&gt;\n&gt;&gt; Notice that the queued decreases at the &quot;same&quot; rate that downloaded\n&gt;&gt; increases. The problem could be getting URIs to the queue. A possible\n&gt;&gt; is the URIs discovered now had be crawled before, and doesn&#39;t go the\n&gt;&gt; the queue anymore. But the domain I&#39;m crawling has about 2 million\n&gt;&gt; domains and I got only 70.000, so there&#39;re many URI&#39;s to be crawled\n&gt;&gt; yet. :)\n&gt;&gt;\n&gt;&gt; Other possibility I thought could be a swap problem (too much I/O).\n&gt;&gt; For my surprise (using vmstat), the swpd is 0.\n&gt;&gt;\n&gt;&gt; Another problem could be know if a URI was crawled already.\n&gt;&gt; Before the URI goes to the frontier, heritrix verifies it in a queue,\n&gt;&gt; using the hash technique. If the crawling is big enough, the search\n&gt;&gt; get slower, even using hash, because there are many URI&#39;s for a key in\n&gt;&gt; hash table.\n&gt;&gt;\n&gt;&gt; But, I really don&#39;t know the exactly problem. Anyone had this problem\n&gt;&gt; or could point a direction?\n&gt;&gt;\n&gt;&gt; Thanks,\n&gt;&gt; Elverton.\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}