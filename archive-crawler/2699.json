{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"OByCodZ21yaGX26KdgIBKsFZ_jIUdM9o5uzRrbO7aWruCh8URrEVpZRH_2ZWMp-Rl7ZBTl744kWVY-q04jKABg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Metadata Harvesting","postDate":"1140714470","msgId":2699,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzRkRFQkU2LjMwMzA4MDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGR0a2Q1OCtta2FkQGVHcm91cHMuY29tPg==","referencesHeader":"PGR0a2Q1OCtta2FkQGVHcm91cHMuY29tPg=="},"prevInTopic":2698,"nextInTopic":2702,"prevInTime":2698,"nextInTime":2700,"topicId":2698,"numMessagesInTopic":3,"msgSnippet":"... I don t know of a metadata only harvester though I m sure one exists somewhere (I once had a link to an RDF crawler but can t put my hands on it currently.","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 10312 invoked from network); 23 Feb 2006 17:08:02 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m27.grp.scd.yahoo.com with QMQP; 23 Feb 2006 17:08:02 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta1.grp.scd.yahoo.com with SMTP; 23 Feb 2006 17:08:01 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k1NFx2113840\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Feb 2006 07:59:02 -0800\r\nMessage-ID: &lt;43FDEBE6.3030800@...&gt;\r\nDate: Thu, 23 Feb 2006 09:07:50 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8) Gecko/20051218 SeaMonkey/1.0b\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;dtkd58+mkad@...&gt;\r\nIn-Reply-To: &lt;dtkd58+mkad@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Metadata Harvesting\r\nX-Yahoo-Group-Post: member; u=168599281; y=BHcODQ6oCZZUtlmGCp7mCSByJCujpE75D-weReicVkK_e5mffDEGcbB9\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nseamuslawless wrote:\n&gt;\n&gt; Hi,\n&gt;\n&gt; I want to create a cache of metadata descriptions of internet pages. I\n&gt; need a crawler that will allow me to harvest any metadata tags that\n&gt; exist in a web page describing the content of that page. I don&#39;t want\n&gt; to  harvest and archive the actual resource itself, merely a\n&gt; description of the content. I&#39;m not sure if Heritrix is the way to go\n&gt; for this? Has anyone any recommendations on the best open source\n&gt; crawler to use for this situation?\nI don&#39;t know of a metadata only harvester though I&#39;m sure one exists \nsomewhere (I once had a link to an RDF crawler but can&#39;t put my hands on \nit currently.  That project sounded interesting).\n\nIf you were to use Heritrix for this task, you would need to write your \nown metadata extractors.  The current extractors in Heritrix are \naggressively purposed to the extraction of links only.  Subclassing them \nso they also harvested metadata would probably be an onerous task. \n\nYour new metadata extractors would likely run after the link extractors \nhad done their work and would focus on pulling the META tags from html, \nthe headers from http, the meta info from pdfs and ms*, etc.  You&#39;d \nprobably just need to read in the first 3 or 4k of the document to get \nwhat you need.\n\nYou&#39;d then then need to make your own Writer.  Heritrix writes all \ndownloaded to ARC files.  You&#39;d likely want to turn this functionality \noff -- sounds like you don&#39;t want to keep around the complete download, \njust metadata only -- in favor of a Writer of our own composition that \ninstead writes metadata only (You might take some inspiration from the \ncurrent gzipping Writer with its lead-in line of URL, harvest data, etc.).\n\n(Long term, we&#39;re working on a new format in which to store resources \ncalled WARC.  It will have means for saving metadata, even adding \nmetadata about a resource.  If this format were here now you could just \nuse the &#39;WarcWriter&#39; straight).\n\nIt looks like you&#39;d have to do similiar if you used the nutch -- \nhttp://lucene.apache.org/nutch -- fetcher crawling the web.  You might \nprefer the batch mode in which it runs.   After each batch fetching \nstep, you could run a metadata distiller that extracts meta tags from \nHTML, reads the metadata from PDF, etc.  Again, you&#39;d have to write \ncustom metadata parsers plugins.\n\nAsk more questions.\nSt.Ack\n\n&gt;\n&gt; I&#39;m new to this game so any help you can provide will be greatly\n&gt; appreciated.\n&gt;\n&gt; Shay\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; SPONSORED LINKS\n&gt; Computer security \n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+security&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=BHmcxBg5sKfN9-gcWnJWDg&gt; \n&gt; \tComputer training \n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+training&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=v0JjJWA4s7mLnWQWdFxuTQ&gt; \n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}