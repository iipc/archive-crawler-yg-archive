{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"kris@archive.org","from":"kris@...","profile":"kristsi25","replyTo":"LIST","senderId":"TI81YaT7ZlThZbrLlA6c1Z3CcIMZDQ8DyV7QKoZF2fnknGJ1mW07yir3u7V8SA2bkEFvInwjMw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix Checkpointing High-Level Design","postDate":"1074892448","msgId":259,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEwNzQ4OTI0NDguNDAxMThlYTAzZWNkMUBtYWlsLWRldi5hcmNoaXZlLm9yZz4=","inReplyToHeader":"PDQwMTE3QjJCLjUwNDAwQGFyY2hpdmUub3JnPg==","referencesHeader":"PDQwMERBMjRELjMwMjA0MDNAYXJjaGl2ZS5vcmc+IDw0MDEwODU2OS42MDYwMUBhcmNoaXZlLm9yZz4gPDQwMTE3QjJCLjUwNDAwQGFyY2hpdmUub3JnPg=="},"prevInTopic":258,"nextInTopic":260,"prevInTime":258,"nextInTime":260,"topicId":244,"numMessagesInTopic":13,"msgSnippet":"...  ... It is relatively simple to have the UI mask the fact that we are indeed working with multiple files. Missing files would be easy to","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 4225 invoked from network); 23 Jan 2004 21:16:23 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m19.grp.scd.yahoo.com with QMQP; 23 Jan 2004 21:16:23 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 23 Jan 2004 21:16:23 -0000\r\nReceived: (qmail 1731 invoked by uid 48); 23 Jan 2004 21:14:08 -0000\r\nReceived: from b116-dyn-55.archive.org (b116-dyn-55.archive.org [209.237.240.55]) \n\tby mail-dev.archive.org (IMP) with HTTP \n\tfor &lt;kris@...@localhost&gt;; Fri, 23 Jan 2004 13:14:08 -0800\r\nMessage-ID: &lt;1074892448.40118ea03ecd1@...&gt;\r\nDate: Fri, 23 Jan 2004 13:14:08 -0800\r\nTo: archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;400DA24D.3020403@...&gt; &lt;40108569.60601@...&gt; &lt;40117B2B.50400@...&gt;\r\nIn-Reply-To: &lt;40117B2B.50400@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nUser-Agent: Internet Messaging Program (IMP) 3.2.1\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.3 required=6.0 tests=NO_REAL_NAME autolearn=no \n\tversion=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: kris@...\r\nSubject: Re: [archive-crawler] Heritrix Checkpointing High-Level Design\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\nQuoting Gordon Mohr &lt;gojomo@...&gt;:\n\n&gt; \n&gt; \n&lt;--- SNIP ---&gt;\n&gt; \n&gt;  &gt;\n&gt; \n&gt;  &gt; So all logs for a crawl will be available under the logging directory?\n&gt;  &gt; We don&#39;t want to rotate them out on a period?  Will the UI be expected\n&gt;  &gt; to open massive log files (or rather should the UI just show reporting\n&gt;  &gt; generated off data logged)?\n&gt; \n&gt; Yes, I believe all the logs should collect in the logging directory\n&gt; until there is a pressing need to move them. Then, we might want to\n&gt; implement -- inside or outside the crawler -- a rotate-away capacity.\n&gt; \n&gt; Such rotation out to another volume shouldn&#39;t have much effect\n&gt; on checkpointing. The UI&#39;s facilities for live-log browsing would\n&gt; have to be made tolerant of such log disappearances.\n\nIt is relatively simple to have the UI mask the fact that we are indeed working\nwith multiple files. Missing files would be easy to ignore. \n\n&gt;  &gt;&gt;When user wants to resume from a checkpoint, they can\n&gt;  &gt;&gt;browse a list of all known checkpoints (or point crawler\n&gt;  &gt;&gt;to a previously unknown checkpoint). Checkpoint is\n&gt;  &gt;&gt;first loaded in &#39;paused&#39; mode, allowing state to be\n&gt;  &gt;&gt;viewed and paramters to be tuned. Then, crawl can be\n&gt;  &gt;&gt;resumed on request.\n&gt;  &gt;\n&gt;  &gt; At what points during crawling can this above operation be done?  At any\n&gt;  &gt; point?  Or just at pause after startup?  Will it be possible for\n&gt;  &gt; operator to load a checkpoint while crawler is running (And if so, how\n&gt;  &gt; does this work)?\n&gt; \n&gt; The software should be in a &quot;clear&quot; state before a resume-\n&gt; from-checkpoint is attempted: no active crawling, no\n&gt; paused crawl state (or at least no paused crawl state you\n&gt; care to save). This could be immediately after launch, or\n&gt; it could be after some other crawling is paused/saved/cleared.\n&gt; \n&gt; Loading a checkpoint should usually bring the crawler to a\n&gt; &quot;crawl paused&quot; state that is effectively a replica of the\n&gt; state at the time of the checkpoint. Ideally, you could load\n&gt; a checkpoint, use the software admin UI to examine it to see\n&gt; if it really is the one you&#39;d like to resume, perhaps repeat\n&gt; this several times, tinker with some settings, and then hit\n&gt; &#39;begin&#39; to pick up where it left off.\n&gt; \nAnother option here is that when we resume from a checkpoint it is handled like\nany other job. Thus if the crawler was already running, it would simply be\nprepared and put into the pending queue.\n\nThus the process would be something like:\nLoad checkpoint, edit settings (optional), submit to CrawlJobHandler, job\nresumes when it is it&#39;s turn (if no job is running, that would be at once).\n&gt; \n&gt;  &gt;&gt;The checkpoint begins: each notable component of\n&gt;  &gt;&gt;the system -- implementers of the Checkpointable\n&gt;  &gt;&gt;interface -- are sent the prepare(checkpointNumber,\n&gt;  &gt;&gt;storeDirectory) message. As necessary, they pass\n&gt;  &gt;&gt;this to their subcomponents.\n&gt;  &gt;\n&gt;  &gt; How will you find all implementers of the Checkpointable interface?  Can\n&gt;  &gt; any old POJO implement Checkpointable or is it only processors?\n&gt; \n&gt; The CrawlController (which implements Checkpointable) is sent the\n&gt; relevant messages. It propagates these to all components it\n&gt; believes needs checkpointing. Those may further propagate the\n&gt; messages. Any POJO can implement the interface, but that&#39;s no\n&gt; guarantee it will be called; there&#39;s got to be a chain of\n&gt; intentional calls from the CrawlController down.\n\nFor consistency with existing code this should be done with the Event model.\n\n&gt;  &gt; (Is there a facilty for pausing a crawl or stopping a crawl?  If so, how\n&gt;  &gt; is that done?)\n&gt; \n&gt; The existing pause/terminate facility will be touched up as necessary;\n&gt; I believe it currently sets a flag indicating that a pause or termination\n&gt; has been requested, and lets the CrawlController control thread react.\n\nThat is correct. The CrawlController basically stops new URIs from being\nprocessed and eventually halt&#39;s the crawl. Currently if there is a thread that\n&#39;hangs&#39; it will hang the entire pausing, making resumes impossible.\n\n&gt; \n&gt;  &gt; Is it completely up to the Checkpointable implementer how they checkpoint?\n&gt; \n&gt; Yes, though by convention, they should ensure all their state goes\n&gt; into the designated checkpoint directory.\n\nI suggest that by default the checkpointing directory be a subdirectory of the\njob directory. Thus it would be easy to see what job any checkpoint belongs to.\nThis should then be configurable in the crawl order.\n\n- Kris\n\n\n\n\n&gt; \n&gt;  &gt;&gt;Generally, the checkpointing of an object involves:\n&gt;  &gt;&gt; (1) Writing its important in-memory state to\n&gt;  &gt;&gt;     one or more files.\n&gt;  &gt;&gt;\n&gt;  &gt;&gt; (2) Duplicating any on-disk state to the checkpoint\n&gt;  &gt;&gt;     directory. (In some cases, this may be possible\n&gt;  &gt;&gt;     with filesystem hard-links rather than actual\n&gt;  &gt;&gt;     copies.)\n&gt;  &gt;&gt;\n&gt;  &gt;\n&gt;  &gt; Do you have examples of the above to illustrate how it would work?\n&gt; \n&gt; A trivial example of (1) would be the ARC writer knowing what\n&gt; sequence-number to assign the next ARC file to begin. When asked\n&gt; to checkpoint itself, it would write that bit of state to a file.\n&gt; Similarly, any module collecting a in-memory histogram of\n&gt; interesting resource features would dump its current data in\n&gt; a recoverable fashion to a file.\n&gt; \n&gt; For (2), an example would be the Frontier&#39;s overall pending queue\n&gt; or per-host queues. These might already substantially be on disk,\n&gt; with a small amount in memory. I believe our existing disk-backed\n&gt; Queues can be quickly checkpointed into three files with a minimum\n&gt; of disk writing by:\n&gt;     (a) creating hard links to the up-to-2 constituent backing\n&gt;         disk files (&quot;flip files&quot;)\n&gt;     (b) writing a third file which contains the current lengths of\n&gt;         those backing files, the current pointer to the &#39;head&#39; entry\n&gt;         in one of those files, and the portions of the queue which\n&gt;         live in memory.\n&gt; As activity continues after a checkpoint, the &quot;flip files&quot; grow\n&gt; but are never overwritten, just discarded when their contents are\n&gt; no longer needed. Thus, the hard links will keep the checkpoint\n&gt; data (as well as some extra cruft) alive under a different filename.\n&gt; When a resume becomes necessary the relevant excerpts of the files\n&gt; will be restored.\n&gt; \n&gt;  &gt;&gt;To resume from a checkpoint, the CrawlController would\n&gt;  &gt;&gt;receive a resume-request with an origin directory. It\n&gt;  &gt;&gt;would reconstitute the parts of the crawl, primarily by\n&gt;  &gt;&gt;constructing new instances which read their state from\n&gt;  &gt;&gt;the origin directory, copying data as necessary to the\n&gt;  &gt;&gt;&quot;running&quot; disk space. (A resume should not alter the\n&gt;  &gt;&gt;stored checkpoint in any way.)\n&gt;  &gt;\n&gt;  &gt; The origin directory points at the checkpoint we want to resume from?\n&gt; \n&gt; Yes.\n&gt; \n&gt;  &gt;&gt;CHECKPOINT FRAMEWORK\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;Each ToeThread wraps its per-URI processing with:\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;   crawlLock.acquireShared(); // crawlLock is a shared-exclusive\n&gt;  &gt;&gt;                              // (AKA &#39;readwrite&#39;) lock\n&gt;  &gt;&gt;   // all processing\n&gt;  &gt;&gt;   crawlLock.releaseShared();\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;(This lock may be refined later to leave out early\n&gt;  &gt;&gt;processing stages, possibly up through fetching,\n&gt;  &gt;&gt;which can be harmlessly considered to never have\n&gt;  &gt;&gt;begun.)\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;The CrawlController controlThread, when it detects\n&gt;  &gt;&gt;a checkpoint has been requested, runs a checkpoint\n&gt;  &gt;&gt;rountine which is roughly:\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;   crawlLock.acquireExclusive();\n&gt;  &gt;&gt;   versionId++;\n&gt;  &gt;&gt;   prepare(versionId, checkpointDirectory); // actually does the\n&gt; checkpointing, passing\n&gt;  &gt;&gt;                                            // prepare() calls to\n&gt; subcomponents\n&gt;  &gt;&gt;   commit(versionId, checkpointDirectory);  // marks the checkpoint as\n&gt; complete, cleans up\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;\n&gt;  &gt;&gt;   crawlLock.releaseExclusive();\n&gt;  &gt;\n&gt;  &gt; Are you missing a resume here?\n&gt; \n&gt; Missing a resume() example, yes -- but in the checkpointing itself, no\n&gt; resume() is necessary: the prepare() and commit() do not destroy any\n&gt; of the crawl state.\n&gt; \n&gt; Resuming is actually much like starting a crawl for the first time,\n&gt; a close analogue to CrawlController.initialize() and\n&gt; CrawlController.startCrawl(), but I&#39;m not sure yet how the details will\n&gt; work out.\n&gt; \n&gt; Great questions! Keep them coming.\n&gt; \n&gt; \n&gt; \n&gt; - Gordon\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; To visit your group on the web, go to:\n&gt; http://groups.yahoo.com/group/archive-crawler/\n&gt;  \n&gt; To unsubscribe from this group, send an email to:\n&gt; archive-crawler-unsubscribe@yahoogroups.com\n&gt;  \n&gt; Your use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n\n\n\n\n"}}