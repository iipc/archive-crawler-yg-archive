{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"M41BnCs_7LosCsVoF3e4YdJwtC-twFcik_5CE36jB4MS_rpP8JYwUtzx-qxMmE81YfNAhVUPnLJxoGXrXGgGD1LjClr__uI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Heritix doesn&#39;t observe limit on number of documents","postDate":"1375777012","msgId":8282,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUyMDBCMEY0LjIwOTA0MDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGt0a3FvcStmczFsQGVHcm91cHMuY29tPg==","referencesHeader":"PGt0a3FvcStmczFsQGVHcm91cHMuY29tPg=="},"prevInTopic":8278,"nextInTopic":8285,"prevInTime":8281,"nextInTime":8283,"topicId":8278,"numMessagesInTopic":5,"msgSnippet":"... It s unclear what you ve tried. There s no setting in the standard configuration which would exactly limit the crawl to 10 pages from each seed and","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 72134 invoked by uid 102); 6 Aug 2013 08:16:54 -0000\r\nX-Received: from unknown (HELO mtaq6.grp.bf1.yahoo.com) (10.193.84.37)\n  by m2.grp.bf1.yahoo.com with SMTP; 6 Aug 2013 08:16:54 -0000\r\nX-Received: (qmail 8833 invoked from network); 6 Aug 2013 08:16:53 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mtaq6.grp.bf1.yahoo.com with SMTP; 6 Aug 2013 08:16:53 -0000\r\nX-Received: (qmail 93984 invoked by uid 0); 6 Aug 2013 08:16:52 -0000\r\nX-Received: from 70.36.143.106 (HELO silverbook.local) (70.36.143.106)\n  by relay01.pair.com with SMTP; 6 Aug 2013 08:16:52 -0000\r\nX-pair-Authenticated: 70.36.143.106\r\nMessage-ID: &lt;5200B0F4.2090409@...&gt;\r\nDate: Tue, 06 Aug 2013 01:16:52 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:17.0) Gecko/20130620 Thunderbird/17.0.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ktkqoq+fs1l@...&gt;\r\nIn-Reply-To: &lt;ktkqoq+fs1l@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritix doesn&#39;t observe limit on number of\n documents\r\nX-Yahoo-Group-Post: member; u=137285340; y=t_HtmH13Ci674HHnRtKc7sGcU1zscw7ArsTrIMglHL8c\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 8/3/13 11:02 PM, Arian wrote:\n&gt; Hi Everyone,\n&gt;\n&gt; I&#39;m using Heritrix 3.1.1 to crawl the contents of a number of sites.\n&gt; I want it to get 10 pages from each seed, not including pages from\n&gt; outlinks of the seed. I set the number of documents limit in the\n&gt; configuration file to 10, and it passed that. Set it to 20, passed it\n&gt; again. If that&#39;s not the right tweak, where should I set the limit\n&gt; for it to work? About leaving out external pages, I have 100+ sites\n&gt; to crawl, so I can&#39;t set regexes matching every single URL (and don&#39;t\n&gt; know if there&#39;s a better way rather than that.)\n&gt;\n&gt; I&#39;d appreciate any help.\n\nIt&#39;s unclear what you&#39;ve tried. There&#39;s no setting in the standard \nconfiguration which would exactly limit the crawl to 10 &quot;pages&quot; &quot;from \neach seed&quot; and &quot;not including outlinks of the seed&quot;. (Those descriptions \ncould mean a variety of things.)\n\n(If the crawl seems to be getting URIs from more sites than you \nexpected, you should check out the FAQ entry at:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/unexpected+offsite+content \n)\n\nThe &#39;CrawlLimitEnforcer&#39; which you may have seen sets count/size/time \nlimits for the entire crawl, not individual sites.\n\nThe optional QuotaEnforcer processor can be added to the &#39;prefetch&#39; \nchain, and then can reject URIs after certain configured counts of URIs \nper hostname/server/&#39;group&#39; are reached. (Server = hostname+port; \n&#39;group&#39; is usually the same as server unless using some advanced \nfrontier configuration.)\n\nQuotaEnforcer is probably what you want... is this what you tried?\n\n(An alternate way to get a similar effect is by adjusting the \n&#39;queueTotalBudget&#39; frontier setting, which stops crawling URIs from a \nqueue -- same as &#39;group&#39; above -- after a certain amount of effort is \nexpended, with the usual policy being roughly one URI equals one unit of \neffort. But, QuotaEnforcer is likely the better approach unless you need \nto understand the frontier internals.)\n\n- Gordon\n\n\n"}}