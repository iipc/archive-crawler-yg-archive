{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":6903103,"authorName":"Tom Emerson","from":"Tom Emerson &lt;Tree@...&gt;","profile":"tree02139","replyTo":"LIST","senderId":"dm_3kPHp0pPpxWO8TjG_YcmVbx52S75ICgkOG2qwMb3oF0tmlUMkSbL1ln-WX8AuDKF4xi9kBWMqyodPL0Hbp_E0R8SznTY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Duplicate detection in large scale crawls - thinking out loud","postDate":"1131555066","msgId":2347,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDE3MjY2LjEwNDkwLjYyNTAxNy41MzAyNjVAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","inReplyToHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBNzg1NjM1QHNrYXJmdXIuYm9rLmxvY2FsPg==","referencesHeader":"PDQ5NGFmZTFjMDUxMTA4MjMyOXE1OTJiOGZjNW01NTVmMDhiNGQ3ZTEwYWQwQG1haWwuZ21haWwuY29tPgk8MDY3OERCMTk2OEVBQzc0MDlDQzNEMEFCN0ExMUI4NEE3ODU2MzVAc2thcmZ1ci5ib2subG9jYWw+"},"prevInTopic":2340,"nextInTopic":2349,"prevInTime":2346,"nextInTime":2348,"topicId":2338,"numMessagesInTopic":7,"msgSnippet":"... Just so I m clear, this is to prevent from downloading the same (unchanged) document at the same URL on subsequent crawls, as opposed to making sure you","rawEmail":"Return-Path: &lt;Tree@...&gt;\r\nX-Sender: Tree@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 7796 invoked from network); 9 Nov 2005 16:51:33 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m24.grp.scd.yahoo.com with QMQP; 9 Nov 2005 16:51:33 -0000\r\nReceived: from unknown (HELO mail2.basistech.net) (199.88.205.1)\n  by mta3.grp.scd.yahoo.com with SMTP; 9 Nov 2005 16:51:33 -0000\r\nReceived: from tiphares.basistech.net ([10.1.3.250]) by mail2.basistech.net with Microsoft SMTPSVC(6.0.3790.1830);\n\t Wed, 9 Nov 2005 11:51:06 -0500\r\nReceived: by tiphares.basistech.net (Postfix, from userid 5007)\n\tid BC1C38401C4; Wed,  9 Nov 2005 11:51:06 -0500 (EST)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\nMessage-ID: &lt;17266.10490.625017.530265@...&gt;\r\nDate: Wed, 9 Nov 2005 11:51:06 -0500\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;0678DB1968EAC7409CC3D0AB7A11B84A785635@...&gt;\r\nReferences: &lt;494afe1c0511082329q592b8fc5m555f08b4d7e10ad0@...&gt;\n\t&lt;0678DB1968EAC7409CC3D0AB7A11B84A785635@...&gt;\r\nX-Mailer: VM 7.19 under Emacs 21.2.1\r\nReturn-Path: tree@...\r\nX-OriginalArrivalTime: 09 Nov 2005 16:51:06.0979 (UTC) FILETIME=[C75ADB30:01C5E54D]\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Tom Emerson &lt;Tree@...&gt;\r\nReply-To: tree@...\r\nSubject: Re: [archive-crawler] Duplicate detection in large scale crawls - thinking out loud\r\nX-Yahoo-Group-Post: member; u=6903103; y=6vC99sIZS-tlr8qPYzryPslxwHtqUpiBrdIXQsyKlPfjbA_u\r\nX-Yahoo-Profile: tree02139\r\n\r\nKristinn Sigurdsson writes:\n&gt; I&#39;m currently doing the 4th complete .is crawl. The amount of data gathered\n&gt; has got me thinking about detecting and eliminating duplicate documents in\n&gt; subsequent crawls. \n\nJust so I&#39;m clear, this is to prevent from downloading the same\n(unchanged) document at the same URL on subsequent crawls, as opposed\nto making sure you only have one copy of (say) the LINUX HOWTO\ndocuments in your entire collection?\n\n&gt; A basic idea for handling duplicate detection. Add a new processor that\n&gt; maintains its own database (Berkley DB or other). In this database we record\n&gt; the URL fingerprint (ideally canonicalized but that is more difficult since\n&gt; that is done in the frontier at the moment, Michael any thoughts on this?)\n&gt; and the content hash (plus possibly some additional meta-data such as time\n&gt; of first discovery and last change). This is indexed by the URL fingerprint.\n&gt; The processor is applied after the FetchHTTP processor (and only on HTTP\n&gt; documents). It looks the URI up and compares the content hashes, aborts\n&gt; further processing of unchanged documents. The DB is updated as needed. All\n&gt; very simple and straightforward.\n\nThis would work fine. I wonder if it wouldn&#39;t make sense to add a\nspecial kind of ARC record that notated the existence of an identical\ncopy elsewhere?\n\n&gt; However, text/* accounts for ~265GB while everything else ~615 (there are\n&gt; some rounding errors in this, but the scales are clear). We know that text\n&gt; compresses well, typically around 80%. We also know that &#39;everything else&#39;\n&gt; means mostly images and other, already compressed, material that does not\n&gt; compress well. Since this material accounts for 70% of the downloaded data\n&gt; (and probably even more of the stored, compressed data) but only 30% of the\n&gt; downloaded documents, it is clear that eliminiating duplicates. \n\nThis begs the question if it wouldn&#39;t make sense to have a single copy\nof some files that are referred to by multiple URLs. For example, how\nmany copies of various Linux ISOs do you want to keep around?\n\n    -tree\n\n-- \nTom Emerson                                          Basis Technology Corp.\nSoftware Architect                                 http://www.basistech.com\n &quot;You can&#39;t fake quality any more than you can fake a good meal.&quot; (W.S.B.)\n\n"}}