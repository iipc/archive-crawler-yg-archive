{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256901594,"authorName":"jonathansiddharth","from":"&quot;jonathansiddharth&quot; &lt;jonathansiddharth@...&gt;","profile":"jonathansiddharth","replyTo":"LIST","senderId":"ebQcukfx7yWT3TzQx5uef170_5p507-H4qLOTReAhTlKFt2Akvwe2pG1mPXM1Dky7zc_jaC9Osbp5jZWvmRvAQGtOZ5QIHv3eE80IS8KrjkN-ZmQYQfk03ia","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: failed to get char replay sequence","postDate":"1149203090","msgId":2901,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU1bnJxaSs5M3QzQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0N0YwNUI3LjgwNDA4MDBAYXJjaGl2ZS5vcmc+"},"prevInTopic":2898,"nextInTopic":2902,"prevInTime":2900,"nextInTime":2902,"topicId":2891,"numMessagesInTopic":8,"msgSnippet":"Yes that is all I m crawling(wikipedia pages..and archiving just the history pages of featured articles. Im starting the crawl from the featured aticles home","rawEmail":"Return-Path: &lt;jonathansiddharth@...&gt;\r\nX-Sender: jonathansiddharth@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 36940 invoked from network); 1 Jun 2006 23:05:22 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m9.grp.scd.yahoo.com with QMQP; 1 Jun 2006 23:05:22 -0000\r\nReceived: from unknown (HELO n13b.bullet.sc5.yahoo.com) (66.163.187.180)\n  by mta6.grp.scd.yahoo.com with SMTP; 1 Jun 2006 23:05:22 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.163.187.122] by n13.bullet.sc5.yahoo.com with NNFMP; 01 Jun 2006 23:04:53 -0000\r\nReceived: from [66.218.69.5] by t3.bullet.sc5.yahoo.com with NNFMP; 01 Jun 2006 23:04:53 -0000\r\nReceived: from [66.218.66.89] by t5.bullet.scd.yahoo.com with NNFMP; 01 Jun 2006 23:04:53 -0000\r\nDate: Thu, 01 Jun 2006 23:04:50 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;e5nrqi+93t3@...&gt;\r\nIn-Reply-To: &lt;447F05B7.8040800@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;jonathansiddharth&quot; &lt;jonathansiddharth@...&gt;\r\nSubject: Re: failed to get char replay sequence\r\nX-Yahoo-Group-Post: member; u=256901594; y=w1ZFXBScVkPn7ANgyDEGq_tzIgTolgVfzTUu7BtMIWRZcySnMnITomsNzBM\r\nX-Yahoo-Profile: jonathansiddharth\r\n\r\nYes that is all I&#39;m crawling(wikipedia pages..and archiving just the\nhistor=\r\ny pages of featured articles. Im starting the crawl from the\nfeatured aticl=\r\nes home page).\n\nI don&#39;t think the crawl hangs. Its just that the FRontier q=\r\nueue is so\nlong that progress in % appears slow. Looking at the crawl log i=\r\nn real\ntime shows that it is crawling. \n\nBut it seems to stop abruptly afte=\r\nr crawling for say 7 to 8 hrs. The\nestimated time was 4 days. With only say=\r\n 5% of the job done.\nThe log says the job is &quot;finished&quot; ,no exception or al=\r\nert.\n\nI dont understand one thing. The log says the job is finished. Yet\nea=\r\nch time I look at its crawl report it is updated. The URIs\ndiscovered keeps=\r\n increasing. Does it mean that when it says crawl job\ndone it means you are=\r\n done crawling. But the % progress that is shown\ntells you how many more UR=\r\nLs are left to explore? In other words, your\ncrawl can end when you&#39;re prog=\r\nress on the console page is just 5%. But\nthe URLs already in the queue keep=\r\n getting downloaded?\n\nI&#39;m sorry but this really confused me.\n\nthanks,\nJonat=\r\nhan\n\n--- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@...&gt; wrot=\r\ne:\n&gt;\n&gt; For sure its hung?  You seem to be crawling wikipedia this time and =\r\nin \n&gt; your last mail.  Is this all you are crawling?  If so, retries becaus=\r\ne \n&gt; of the below connection timeouts with the Heritrix increasing interval=\r\n \n&gt; between retries may look like the crawler is hung.  Check the frontier =\r\n\n&gt; report.  \n&gt; \n&gt; Is there nothing in heritrix_out.log?  You could send the=\r\n process a \n&gt; SIGQUIT signal -- $ kill -3 HERITRIX_PROCESS_ID -- and it&#39;ll =\r\nsend a \n&gt; thread dump to heritrix_out.log.  Study the output.  Wait a while=\r\n.  \n&gt; Redo. Compare the stack traces.  Any progress?  If none or you can&#39;t =\r\n\n&gt; make sense of it, send it on over. Maybe we can see something in thread =\r\n\n&gt; dump tea leaves.\n&gt; \n&gt; St.Ack\n&gt; \n&gt; \n&gt; jonathansiddharth wrote:\n&gt; &gt; Thanks=\r\n for the reply St.Ack.\n&gt; &gt; I think you&#39;re right. Although I got these alert=\r\ns, I dont think the\n&gt; &gt; crawl terminated because of that.\n&gt; &gt; Because on re=\r\n-running the crawl it again mysteriously stopped after a\n&gt; &gt; (long) time ev=\r\nen though I&#39;m sure the crawl was not completed. It was\n&gt; &gt; only some 5% don=\r\ne. And this time there were no alerts.\n&gt; &gt; However I saw some errors in the=\r\n local-errors.log of this sort\n&gt; &gt;\n&gt; &gt; 2006-06-01T04:42:01.909Z    -2      =\r\n    -\n&gt; &gt; http://en.wikipedia.org/wiki/J%C3%BCrgen_Habermas LL\n&gt; &gt; http://e=\r\nn.wikipedia.org/wiki/Bernard_Williams no-type #032 - - -\n&gt; &gt; le:SocketTimeo=\r\nutException@HTTP\n&gt; &gt; java.net.SocketTimeoutException: connect timed out: ti=\r\nmeout set at\n&gt; &gt; 20000ms.\n&gt; &gt;       at\n&gt; &gt;\norg.archive.crawler.fetcher.Heri=\r\ntrixProtocolSocketFactory.createSocket(HeritrixProtocolSocketFactory.java:1=\r\n42)\n&gt; &gt;       at\n&gt; &gt;\norg.apache.commons.httpclient.HttpConnection.open(Http=\r\nConnection.java:707)\n&gt; &gt;       at\n&gt; &gt;\norg.apache.commons.httpclient.HttpMet=\r\nhodDirector.executeWithRetry(HttpMethodDirector.java:382)\n&gt; &gt;       at\n&gt; &gt;\n=\r\norg.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDi=\r\nrector.java:168)\n&gt; &gt;       at\n&gt; &gt;\norg.apache.commons.httpclient.HttpClient.=\r\nexecuteMethod(HttpClient.java:396)\n&gt; &gt;       at\n&gt; &gt;\norg.apache.commons.http=\r\nclient.HttpClient.executeMethod(HttpClient.java:324)\n&gt; &gt;       at \n&gt; &gt; org.=\r\narchive.crawler.fetcher.FetchHTTP.innerProcess(FetchHTTP.java:408)\n&gt; &gt;     =\r\n  at \n&gt; &gt; org.archive.crawler.framework.Processor.process(Processor.java:10=\r\n3)\n&gt; &gt;       at\n&gt; &gt;\norg.archive.crawler.framework.ToeThread.processCrawlUri=\r\n(ToeThread.java:306)\n&gt; &gt;       at\norg.archive.crawler.framework.ToeThread.r=\r\nun(ToeThread.java:153)\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; There were 2 errors like this.\n&gt; &gt; I=\r\n doubt if this caused the crawl to end though. Since there were two\n&gt; &gt; of =\r\nthese and if it was really terminal the first one should have\n&gt; &gt; brought t=\r\nhe crawl to an end.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; is there anyother condition for which the c=\r\nrawler gracefully gives up.\n&gt; &gt; Like if there are too many URLs in the fron=\r\ntier queue.\n&gt; &gt; I&#39;m assuming a heap overflow would have an explicit error m=\r\nessage\n&gt; &gt; indicating that.\n&gt; &gt;\n&gt; &gt; (My estimated crawl job time was 4 days=\r\n)\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; thanks,\n&gt; &gt; Jonathan\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --- In a=\r\nrchive-crawler@yahoogroups.com, Michael Stack &lt;stack@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; j=\r\nonathansiddharth wrote:\n&gt; &gt; &gt; &gt; Hi,\n&gt; &gt; &gt; &gt;         This alert/exception ab=\r\nruptly terminated my crawl job.\nCould\n&gt; &gt; &gt; &gt; someone tell me what it is an=\r\nd how it can be avoided?\n&gt; &gt; &gt;\n&gt; &gt; &gt; The below is an old faithful.  See\n&gt; &gt;=\r\n &gt;\n&gt; &gt;\nhttp://sourceforge.net/tracker/index.php?func=3Ddetail&aid=3D1218961=\r\n&group_id=3D73833&atid=3D539099.\n\n&gt; &gt;\n&lt;http://sourceforge.net/tracker/index=\r\n.php?func=3Ddetail&aid=3D1218961&group_id=3D73833&atid=3D539099.&gt;\n&gt; &gt;\n&gt; &gt; &gt;=\r\n It rears its head from time to time but we&#39;ve not been able to\nfigure\n&gt; &gt; =\r\n&gt; why it happens nor how to reliably reproduce.  I&#39;m guessing if you\n&gt; &gt; cr=\r\nawl\n&gt; &gt; &gt; that same page again in wikipedia, you&#39;ll succeed (least it did j=\r\nust\n&gt; &gt; now\n&gt; &gt; &gt; for me when I tried it).  I&#39;m surprised it terminated you=\r\nr crawl. \n&gt; &gt; &gt; Usually we just fail extraction on a particular page and ju=\r\nst\nmove to\n&gt; &gt; &gt; the next in the queue.\n&gt; &gt; &gt;\n&gt; &gt; &gt; St.Ack\n&gt; &gt; &gt; P.S. I&#39;ve =\r\njust added more logging around this exception.   Perhaps\n&gt; &gt; it&#39;ll\n&gt; &gt; &gt; tu=\r\nrn up the needed clue.\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; SPONSORED LINK=\r\nS\n&gt; &gt; Computer security \n&gt; &gt;\n&lt;http://groups.yahoo.com/gads?t=3Dms&k=3DCompu=\r\nter+security&w1=3DComputer+security&w2=3DComputer+training&c=3D2&s=3D46&.si=\r\ng=3DBHmcxBg5sKfN9-gcWnJWDg&gt;\n\n&gt; &gt; \tComputer training \n&gt; &gt;\n&lt;http://groups.yah=\r\noo.com/gads?t=3Dms&k=3DComputer+training&w1=3DComputer+security&w2=3DComput=\r\ner+training&c=3D2&s=3D46&.sig=3Dv0JjJWA4s7mLnWQWdFxuTQ&gt;\n\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n--=\r\n----------------------------------------------------------------------\n&gt; &gt; =\r\nYAHOO! GROUPS LINKS\n&gt; &gt;\n&gt; &gt;     *  Visit your group &quot;archive-crawler\n&gt; &gt;   =\r\n    &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt; &gt;       =\r\n \n&gt; &gt;     *  To unsubscribe from this group, send an email to:\n&gt; &gt;        a=\r\nrchive-crawler-unsubscribe@yahoogroups.com\n&gt; &gt;      \n&lt;mailto:archive-crawle=\r\nr-unsubscribe@yahoogroups.com?subject=3DUnsubscribe&gt;\n&gt; &gt;        \n&gt; &gt;     * =\r\n Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt; &gt;       Serv=\r\nice &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n----------------------=\r\n--------------------------------------------------\n&gt; &gt;\n&gt;\n\n\n\n\n\n\n"}}