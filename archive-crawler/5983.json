{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"2LtbZao8bwMY_SvHoBFkRuudr9YHQySCZAieRCmovw2qd6VZaeHAdevKFKgbK6H2JsbyJpvFea3RHuvn11HFrvVzKBwJk6I","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Re: Can I split seeds for a HashCrawlMapper crawl?","postDate":"1250197957","msgId":5983,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBODQ4MUM1LjEwMzA5QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGg1dmIyNCtuMnUxQGVHcm91cHMuY29tPg==","referencesHeader":"PGg1dmIyNCtuMnUxQGVHcm91cHMuY29tPg=="},"prevInTopic":5982,"nextInTopic":5987,"prevInTime":5982,"nextInTime":5984,"topicId":5971,"numMessagesInTopic":8,"msgSnippet":"... If crawler 2 s scope is built up from its unique seedlist, which seems to be the case in your design, then it will not necessarily accept the same URIs as","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 20828 invoked from network); 13 Aug 2009 21:13:39 -0000\r\nX-Received: from unknown (69.147.108.201)\n  by m5.grp.sp2.yahoo.com with QMQP; 13 Aug 2009 21:13:39 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta2.grp.re1.yahoo.com with SMTP; 13 Aug 2009 21:13:39 -0000\r\nX-Received: (qmail 61439 invoked from network); 13 Aug 2009 21:12:38 -0000\r\nX-Received: from 67.188.14.54 (HELO ?192.168.1.114?) (67.188.14.54)\n  by relay00.pair.com with SMTP; 13 Aug 2009 21:12:38 -0000\r\nX-pair-Authenticated: 67.188.14.54\r\nMessage-ID: &lt;4A8481C5.10309@...&gt;\r\nDate: Thu, 13 Aug 2009 14:12:37 -0700\r\nUser-Agent: Thunderbird 2.0.0.22 (Windows/20090605)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;h5vb24+n2u1@...&gt;\r\nIn-Reply-To: &lt;h5vb24+n2u1@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Can I split seeds for a HashCrawlMapper\n crawl?\r\nX-Yahoo-Group-Post: member; u=137285340; y=QKLb1AAgTLHQxfx_mlkMswar3STFNfQvYBNOXFeOm5Ym\r\nX-Yahoo-Profile: gojomo\r\n\r\n\n\njoehung302 wrote:\n&gt; Gordon,\n&gt; \n&gt; Thanks for the response. I did come up with a custom program to divide the 20MM seed list myself. I am now running a proof instance with less than 1MM seed and so far it seems to work well.\n&gt; \n&gt; Here is my next question. When diverting the crawl, let&#39;s say the diversion is from crawler 1 to cralwer 2. If I start crawler 1 & 2 with different seeds (because of the split), would crawler 2 accept the URLs from crawler 1? \n&gt; \n&gt; I know the diverted URLs has 2 parts of information: target URL and seed. Apparently the target URL should be hashed to crawler 2, but the seed won&#39;t be in crawler&#39;s 2 seed list...\n\nIf crawler 2&#39;s scope is built up from its unique seedlist, which seems \nto be the case in your design, then it will not necessarily accept the \nsame URIs as crawler 1&#39;s unique seedlist-derived scope.\n\nHOWEVER, in practice, this may not be an issue for the seed-level \ndomains. The mapping is done not by full URL, but hostname or even \nbetter, &quot;TopmostAssignedSurt&quot;. This is a reduction of the domain, using \na series of rules derived from the Mozilla &#39;public suffix list&#39;, to the \ncomponent that was sold/assigned from a domain registrar.\n\nThat is, all subdomains of .archive.org reduce to the \nassignment-level-domain archive.org, and the assignment-level-SURT \n(org,archive,). Thus in a crawl split they all get mapped to the same \ncrawler -- and thus every URI that was scope-allowed by a seed should \nland at the same crawler as the seed.\n\nThe bigger issue is if you apply scoping *before* diversion -- if \nLinksScoper is before your outlinks HashCrawlMapper. Then crawler 1 \nrejects the crawler-2-seed-allowed-URI for being out-of-scope before \nit&#39;s diverted. It&#39;s nice to apply scoping as before as much storage/IO \nas possible as it is usually quick and deterministic... but in your \ncase, you probably want scoping deferred until the URI reaches the \ncrawler that knows whether it was seed-allowed.\n\n- Gordon @ IA\n\n\n&gt; Cheers,\n&gt; -Joe\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt; It would be useful if (for example) HashCrawlMapper had a main() that \n&gt;&gt; let it be used outside the crawler to pre-split lists... however the way \n&gt;&gt; it&#39;s currently dependent on the frontier&#39;s configured queue-key policy \n&gt;&gt; would require some extra complication on how such a hypothetical utility \n&gt;&gt; was invoked.\n&gt;&gt;\n&gt;&gt; A roundabout way to achieve the same effect might be to set up a dummy \n&gt;&gt; crawler whose first HashCrawlMapper does &#39;check-uri&#39; but also has a \n&gt;&gt; erroneous &#39;local-name&#39;. Feed it all 20MM URLs -- and every one will land \n&gt;&gt; in one of the diversion logs, because none will be bucketed to the bad \n&gt;&gt; &#39;local-name&#39;. This dummy crawl wouldn&#39;t need any other things (like \n&gt;&gt; seed-based scopes) that might be choking on a large seed list.\n&gt;&gt;\n&gt;&gt; Note that if each of the 12 crawlers only is initialized with a small \n&gt;&gt; portion of the seed list, when they discover deep URIs on hosts assigned \n&gt;&gt; to other nodes they might mistakenly rule them out-of-scope, though in \n&gt;&gt; practice this might not be an issue for well-connected sites.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; joehung302 wrote:\n&gt;&gt;&gt; I&#39;m using Heritrix 1.14.3.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Let&#39;s say I have \n&gt;&gt;&gt; 1. one big seed list consisting of 1MM seeds. \n&gt;&gt;&gt; 2. 2 crawler instances to implement HashCrawlMapper. \n&gt;&gt;&gt; 3. The crawl scope is domain + 1 (implemented through OnDomainDecideRule with &quot;seeds-as-surt-prefixes&quot;==true and &quot;also-check-via&quot;==true). \n&gt;&gt;&gt;\n&gt;&gt;&gt; Can I split the seeds using the same HashCrawlMapper rule so that each crawler would only get seeds that are within its scope? Would there be any difference if I use the same 1MM seeds for both crawlers?\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; The reason why I want to do this is, I have 20MM seeds among 12 crawlers. I&#39;ve tested with one instance handling 20MM seeds and it doesn&#39;t seem to work. If I can split the seeds so that each cralwer starts with URLs that belong to themselves it should make the crawl process easier....\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks,\n&gt;&gt;&gt; -Joe\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}