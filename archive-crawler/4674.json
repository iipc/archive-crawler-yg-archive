{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"zjrHJGe5BIgrKh8zD09MejUtbrMd0BJ39MtGMZ57hLKlqd0ZCiTOfBwZL-ht0fZlwY5qZdnVjCi-6Ogshww-AzKzbpT6M9o","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Looking for way to break large crawl into two or more simultaneous crawls","postDate":"1194646991","msgId":4674,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3MzRERENGLjkwNTAzMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGZndmloMityNzZxQGVHcm91cHMuY29tPg==","referencesHeader":"PGZndmloMityNzZxQGVHcm91cHMuY29tPg=="},"prevInTopic":4671,"nextInTopic":0,"prevInTime":4673,"nextInTime":4675,"topicId":4671,"numMessagesInTopic":2,"msgSnippet":"... If you politeness (or the target server s performance) is the gating factor, and you re going to adjust dual crawlers to be just as polite, what s the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 84183 invoked from network); 9 Nov 2007 22:23:11 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m55.grp.scd.yahoo.com with QMQP; 9 Nov 2007 22:23:11 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta16.grp.scd.yahoo.com with SMTP; 9 Nov 2007 22:23:10 -0000\r\nX-Received: (qmail 4720 invoked from network); 9 Nov 2007 22:23:10 -0000\r\nX-Received: from unknown (HELO ?192.168.1.30?) (unknown)\n  by unknown with SMTP; 9 Nov 2007 22:23:10 -0000\r\nX-pair-Authenticated: 76.102.230.209\r\nMessage-ID: &lt;4734DDCF.9050300@...&gt;\r\nDate: Fri, 09 Nov 2007 14:23:11 -0800\r\nUser-Agent: Thunderbird 2.0.0.6 (Windows/20070728)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;fgvih2+r76q@...&gt;\r\nIn-Reply-To: &lt;fgvih2+r76q@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Looking for way to break large crawl into two\n or more simultaneous crawls\r\nX-Yahoo-Group-Post: member; u=137285340; y=HAB8ZmWTS7vtCvsjoU8ayXXL-v4_wY_R61vcaTkSYV3W\r\nX-Yahoo-Profile: gojomo\r\n\r\nmjjjhjemj wrote:\n&gt; I am performing a very large domain crawl that is going extremely slow\n&gt; as many of the pages are dynamically created pages from database\n&gt; driven sites. As a result the crawl is preceding along well below the\n&gt; politeness settings configured. The crawl is presently gathering data\n&gt; at 6KB/sec. Typically the rates are anywhere from 5KB/sec to 10KB/sec\n&gt; on the database driven sites within the domain. Very slow.\n&gt; \n&gt; My question is: Is there a way to split the crawl in two or more\n&gt; simultaneous crawls that could reference the same history so that\n&gt; pages would not be duplicated by each subcrawl (two or more).\n&gt; \n&gt; I could configure the politeness settings so the summed crawls would\n&gt; not exceed the overall demand that I have set for the single crawl\n&gt; configuration.\n\nIf you politeness (or the target server&#39;s performance) is the gating \nfactor, and you&#39;re going to adjust dual crawlers to be just as polite, \nwhat&#39;s the benefit of the split?\n\nWe typically split crawls so that each crawler has a unique set of sites \nto visit -- so they never visit each other&#39;s URIs and don&#39;t need to \nshare history info. (They may want to share discovered URIs meant for \nthe other crawler.)\n\nHowever, the benefit of a split is primarily when there are so many \nsites to crawl in parallel that machine resources -- CPU, memory, IO -- \nare the gating factor. It doesn&#39;t sound like that&#39;s the case for you.\n\nAm I missing some other factor?\n\n- Gordon @ IA\n\n"}}