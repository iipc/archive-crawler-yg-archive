{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":325624130,"authorName":"Noah Levitt","from":"Noah Levitt &lt;nlevitt@...&gt;","profile":"nlevitt0","replyTo":"LIST","senderId":"hJu1nJdOlTrVLXOndSwpCee4aYN9VpSMK8INkwWxpAraI6Dh3ycSa_0f1E0vFSJj-kjGBtOSjp_NtMd6GmmjMLNV4xVmEiYh","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] &quot;Balanced&quot; crawl frontier","postDate":"1215044935","msgId":5343,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NkMxRDQ3LjUwODA4MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGc0aDF1cCt1aHAxQGVHcm91cHMuY29tPg==","referencesHeader":"PGc0aDF1cCt1aHAxQGVHcm91cHMuY29tPg=="},"prevInTopic":5342,"nextInTopic":5347,"prevInTime":5342,"nextInTime":5344,"topicId":5342,"numMessagesInTopic":5,"msgSnippet":"Heritrix does a pretty good job of running in a balanced manner by default. The list of urls yet to be crawled (the frontier) is divided into queues by host,","rawEmail":"Return-Path: &lt;nlevitt@...&gt;\r\nX-Sender: nlevitt@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 45382 invoked from network); 3 Jul 2008 00:28:58 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m35.grp.scd.yahoo.com with QMQP; 3 Jul 2008 00:28:58 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta18.grp.scd.yahoo.com with SMTP; 3 Jul 2008 00:28:58 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 8C50C19BE00\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  2 Jul 2008 17:17:10 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 3ifPYxK809G3 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed,  2 Jul 2008 17:17:10 -0700 (PDT)\r\nX-Received: from noah-levitts-computer.local (adsl-64-160-39-28.dsl.snfc21.pacbell.net [64.160.39.28])\n\tby mail.archive.org (Postfix) with ESMTPSA id F33841095D0\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed,  2 Jul 2008 17:17:09 -0700 (PDT)\r\nMessage-ID: &lt;486C1D47.5080805@...&gt;\r\nDate: Wed, 02 Jul 2008 17:28:55 -0700\r\nUser-Agent: Thunderbird 2.0.0.14 (Macintosh/20080421)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g4h1up+uhp1@...&gt;\r\nIn-Reply-To: &lt;g4h1up+uhp1@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Noah Levitt &lt;nlevitt@...&gt;\r\nSubject: Re: [archive-crawler] &quot;Balanced&quot; crawl frontier\r\nX-Yahoo-Group-Post: member; u=325624130; y=e4luvaIrZCZGknZeCHB174TxxMSzuDDG_bU1yrlomUveS-c\r\nX-Yahoo-Profile: nlevitt0\r\n\r\nHeritrix does a pretty good job of running in a &quot;balanced&quot; manner by \ndefault. The list of urls yet to be crawled (the frontier) is divided \ninto queues by host, and there is a delay between fetches of each url in \na single queue, so hosts/queues generally aren&#39;t starved out. Have you \nencountered this problem in a real crawl?\n\nNoah\n\nnickbirren wrote:\n&gt; Is there a way with Heritrix to define a job with many starting\n&gt; points, one per site, then have the job run in a &quot;balanced&quot; manner so\n&gt; that each site gets its share of bandwidth / processing. I&#39;m trying to\n&gt; &quot;avoid starvation&quot; of sites that do not have many links in them on\n&gt; each page by other sites who do, and if I&#39;m not mistaken, will &quot;take\n&gt; over&quot; the crawl frontier after a few iterations.\n&gt;\n&gt; My other option is to run many crawl engines, one per site, but this\n&gt; seems cumbersome and is probably less scaleable.\n&gt;\n&gt; Thanks,\n&gt; -Nick\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;   \n\n"}}