{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"lekash","from":"lekash &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"u_TH9_lexthAwudv6mXjtT8TkjftTM_uYnPfRTp152CWxN3QXTkB8iQEi5hKFXF8HvwOCUhtda02XkkEvXmfQD76","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Crawling 100 million pages","postDate":"1212790755","msgId":5302,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NDlCN0UzLjcwOTA1MDRAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PGI3NmUxZGYwODA2MDYxNTExajQ4ZDM3NjM0d2EzNzc4M2QwMjk2MjJmOTJAbWFpbC5nbWFpbC5jb20+","referencesHeader":"PDQ4NDk5QzMxLjkwNTA3MDJAYmF5YXJlYS5uZXQ+CSA8NDg0OWFjOTQuMTFiZDcyMGEuNDZkNy43MjQ4U01UUElOX0FEREVEQG14Lmdvb2dsZS5jb20+IDxiNzZlMWRmMDgwNjA2MTUxMWo0OGQzNzYzNHdhMzc3ODNkMDI5NjIyZjkyQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5301,"nextInTopic":0,"prevInTime":5301,"nextInTime":5303,"topicId":5292,"numMessagesInTopic":11,"msgSnippet":"Hi there, A bit focused per crawler. But relatively broad over the set. John","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 69293 invoked from network); 6 Jun 2008 22:19:50 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m43.grp.scd.yahoo.com with QMQP; 6 Jun 2008 22:19:50 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta18.grp.scd.yahoo.com with SMTP; 6 Jun 2008 22:19:50 -0000\r\nX-Received: from [192.168.0.12] (72.20.109.026.bayarea.net [72.20.109.26])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id m56MJoKw097822\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 6 Jun 2008 15:19:50 -0700 (PDT)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;4849B7E3.7090504@...&gt;\r\nDate: Fri, 06 Jun 2008 15:19:15 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.11) Gecko/20050727\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;48499C31.9050702@...&gt;\t &lt;4849ac94.11bd720a.46d7.7248SMTPIN_ADDED@...&gt; &lt;b76e1df0806061511j48d37634wa37783d029622f92@...&gt;\r\nIn-Reply-To: &lt;b76e1df0806061511j48d37634wa37783d029622f92@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: lekash &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Crawling 100 million pages\r\nX-Yahoo-Group-Post: member; u=90724651; y=9DkLVAFtqQwH8DTSfd-Vo7hrub47sWPS-9TKPehPKeU3\r\nX-Yahoo-Profile: lekash\r\n\r\nHi there,\nA bit focused per crawler.\nBut relatively broad over the set.\n\nJohn\n\nAnmol Bhasin wrote:\n\n&gt; Hi John,\n&gt;\n&gt; Did you go for a focussed crawl or a broad-scope crawl? I believe\n&gt; Heritrix-slows down considerably in focussed crawling with a sequence\n&gt; of decision-filters applied to queued URLs also, if we used SURT based\n&gt; Prefixes then the politeness considerations slow down crawling\n&gt; further.\n&gt;\n&gt; Anmol\n&gt;\n&gt; On Fri, Jun 6, 2008 at 2:30 PM, Leo Dagum &lt;leo_dagum@... \n&gt; &lt;mailto:leo_dagum%40yahoo.com&gt;&gt; wrote:\n&gt; &gt; Hi John,\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; What hardware does a crawler instance run on and what is your \n&gt; Internet b/w?\n&gt; &gt; 385M/3m from a single crawler instance is very impressive.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; - leo\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ________________________________\n&gt; &gt;\n&gt; &gt; From: archive-crawler@yahoogroups.com \n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt; &gt; [mailto:archive-crawler@yahoogroups.com \n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;] On Behalf Of lekash\n&gt; &gt; Sent: Friday, June 06, 2008 1:21 PM\n&gt; &gt; To: archive-crawler@yahoogroups.com \n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt; &gt; Subject: Re: [archive-crawler] Crawling 100 million pages\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Hi there,\n&gt; &gt;\n&gt; &gt; I&#39;m like to very much encourage being careful about &#39;speeding up \n&gt; heritrix&#39;.\n&gt; &gt; Politeness is really important on the net. I keep doubling my inter\n&gt; &gt; crawl delay every\n&gt; &gt; year, and still people have problems.\n&gt; &gt;\n&gt; &gt; Its not a heretrix limit you will be looking at. Its a hardware\n&gt; &gt; capacity limit.\n&gt; &gt; (Though the most I&#39;ve gotten a single crawler to do is\n&gt; &gt; 385M over a three month period.)\n&gt; &gt; A crawler group, I&#39;ve gotten to 6B /year.\n&gt; &gt; So, 100 M /month is well within the operating range.\n&gt; &gt;\n&gt; &gt; The 1.x crawler seems to have a limit around 680M for a single crawler,\n&gt; &gt; like the queued numbers stop going up then. Never got there, so don&#39;t\n&gt; &gt; know what would happen. I haven&#39;t run a multi-billion one on the 2.x \n&gt; line,\n&gt; &gt; maybe next year.\n&gt; &gt;\n&gt; &gt; The things that run out of space are memory for java heap space, and\n&gt; &gt; something funny with the database. Re-writing scratch files in that\n&gt; &gt; seems to be what slows it down later, despite a still wide frontier.\n&gt; &gt;\n&gt; &gt; And of course, urls are not a consistent measure, as size varies widely.\n&gt; &gt; e.g. .gov sites are 10 times as dense in text/pdf as .com sites,\n&gt; &gt; and .com sites have all the visuals and movies. Depends on what\n&gt; &gt; you are crawling.\n&gt; &gt;\n&gt; &gt; John\n&gt; &gt;\n&gt; &gt; hijbul_bd wrote:\n&gt; &gt;\n&gt; &gt;&gt; Dear All\n&gt; &gt;&gt;\n&gt; &gt;&gt; I would like to crawl 100 million pages with in a month for crawling\n&gt; &gt;&gt; research. As far i know some research crawler(IRLbot(6 billion pages in\n&gt; &gt;&gt; 41 days), polybot(120 millions pages in 19 days)) can download huge\n&gt; &gt;&gt; pages in short amount of time wich is not open source. In 2005\n&gt; &gt;&gt; according to some blog site Heritrix can download about 20 miilion\n&gt; &gt;&gt; pages in a month. What is the speed of current Heritrix version and How\n&gt; &gt;&gt; can I speed up heritrix to download 100 million or at least 50 million\n&gt; &gt;&gt; pages with in a month. Are there any ohter open source crawler which\n&gt; &gt;&gt; can do this?\n&gt; &gt;&gt;\n&gt; &gt;&gt; Thanks in Advance\n&gt; &gt;&gt; Hijbul Alam\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt; -- \n&gt; Courage doesn&#39;t always roar. Sometimes courage is the quiet voice at\n&gt; the end of the day saying, &quot;I will try again tomorrow&quot;\n&gt;\n&gt; Anmol Bhasin\n&gt; SSE Data Platform\n&gt; www.linkedin.com\n&gt;\n&gt;  \n\n\n"}}