{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":325624130,"authorName":"Noah Levitt","from":"Noah Levitt &lt;nlevitt@...&gt;","profile":"nlevitt","replyTo":"LIST","senderId":"ZgihoP-yYPYzbJ-q5uJOjHV8m5Zdb3kjfbZ-_jjt6piwoQa9DVmufiFL29Zg9dJkKNYv4IsNkstB0_TNMKZds9eBoH3sOQJH","spamInfo":{"isSpam":false,"reason":"2"},"subject":"Re: [archive-crawler] blacklist surt","postDate":"1323829719","msgId":7442,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFRTgwOUQ3LjcwMTAxMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGpjNThodCt1Z3VwQGVHcm91cHMuY29tPg==","referencesHeader":"PGpjNThodCt1Z3VwQGVHcm91cHMuY29tPg=="},"prevInTopic":7428,"nextInTopic":7445,"prevInTime":7441,"nextInTime":7443,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"Hello David, ... filter the seed list as well as discovered URIs? If so, how can I do this. Yes, in order to do this you need to enable recheckScope on the ","rawEmail":"Return-Path: &lt;nlevitt@...&gt;\r\nX-Sender: nlevitt@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 9908 invoked from network); 14 Dec 2011 02:28:40 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m7.grp.sp2.yahoo.com with QMQP; 14 Dec 2011 02:28:40 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.224.6)\n  by mta6.grp.sp2.yahoo.com with SMTP; 14 Dec 2011 02:28:40 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 8AFF6684107B;\n\tTue, 13 Dec 2011 18:28:40 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id pwNQjVIuGe0B; Tue, 13 Dec 2011 18:28:39 -0800 (PST)\r\nX-Received: from [208.70.27.155] (desktop-nlevitt.sf.archive.org [208.70.27.155])\n\tby mail.archive.org (Postfix) with ESMTPSA id 46B086840165;\n\tTue, 13 Dec 2011 18:28:39 -0800 (PST)\r\nMessage-ID: &lt;4EE809D7.7010100@...&gt;\r\nDate: Tue, 13 Dec 2011 18:28:39 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.2.23) Gecko/20110922 Thunderbird/3.1.15\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: david_pane1 &lt;dpane@...&gt;\r\nReferences: &lt;jc58ht+ugup@...&gt;\r\nIn-Reply-To: &lt;jc58ht+ugup@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:2:2:0:1\r\nFrom: Noah Levitt &lt;nlevitt@...&gt;\r\nSubject: Re: [archive-crawler] blacklist surt\r\nX-Yahoo-Group-Post: member; u=325624130; y=8tWhDVe3tIrJ4D0u2ZbjumO6KESGgfdVitRZvaoymnpG3w\r\nX-Yahoo-Profile: nlevitt\r\n\r\nHello David,\n\n &gt; 1) Can the crawler be configured to use the blaklist surt list to \nfilter the seed list as well as discovered URIs? If so, how can I do this.\n\nYes, in order to do this you need to enable recheckScope on the \npreselector bean. Without that setting, urls are checked against the \nscope after they&#39;ve been discovered, which is a step that seeds do not \ngo through. With recheckScope urls are checked against the scope at the \nbeginning of their run through the processing chain. So most of the time \nthis is a redundant check, but not in the case of seeds. (Another case \nwhere recheckScope is useful is when the crawl operator changes the \nscope rules during the course of the crawl.)\n\n &gt; 2) Although I had an IP address in my SURT file, the crawler still \nhad some URIs crawled a domain at that address. Is there something \nincorrect about my configuration or SURT file format?\n\nSurts are only checked against the url, so a surt with an ip address \nwill not match a url with a domain name, regardless of what the domain \nresolves to. There is no existing DecideRule in heritrix to block by ip \naddress. One could be written though. (If you go down that path you \nshould look at e.g. FetchHTTP to see how to obtain the resolved ip \naddress. The recheckScope setting might be useful here too, because the \ndomain name may not have been looked up for a given url at the time it&#39;s \ndiscovered.)\n\nNoah\n\nOn 12/12/2011 08:02 AM, david_pane1 wrote:\n&gt;\n&gt; I am trying to create and use a large blacklist (1 million ).\n&gt;\n&gt; I created a surt file with lines in the format similar to both of these:\n&gt;\n&gt; +http://(128.2.42.10\n&gt; +http://(com,domain,\n&gt;\n&gt;\n&gt;      &lt;!-- ...but REJECT those from a configurable (initially empty) set of REJECT SURTs... --&gt;\n&gt;      &lt;bean class=&quot;org.archive.modules.deciderules.surt.SurtPrefixedDecideRule&quot;&gt;\n&gt;            &lt;property name=&quot;decision&quot; value=&quot;REJECT&quot;/&gt;\n&gt;            &lt;property name=&quot;seedsAsSurtPrefixes&quot; value=&quot;false&quot;/&gt;\n&gt;            &lt;property name=&quot;surtsDumpFile&quot; value=&quot;${launchId}/negative-surts.dump&quot; /&gt;\n&gt;            &lt;property name=&quot;surtsSource&quot;&gt;\n&gt;             &lt;bean class=&quot;org.archive.spring.ConfigFile&quot;&gt;\n&gt;              &lt;property name=&quot;path&quot; value=&quot;myblacklist.surt&quot; /&gt;\n&gt;             &lt;/bean&gt;\n&gt;            &lt;/property&gt;\n&gt;\n&gt;\n&gt;\n&gt; 1) Can the crawler be configured to use the blaklist surt list to filter the seed list as well as discovered URIs?  If so, how can I do this.\n&gt;\n&gt; 2) Although I had an IP address in my SURT file, the crawler still had some URIs crawled a domain at that address.  Is there something incorrect about my configuration or SURT file format?\n&gt;\n&gt; --David\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@...&gt;  wrote:\n&gt;&gt; On 11/7/11 12:46 PM, David Pane wrote:\n&gt;&gt;&gt; Thank you Gordon.\n&gt;&gt;&gt;\n&gt;&gt;&gt; We would also like to have a blacklist of unwanted sites. This blacklist\n&gt;&gt;&gt; would possibly contain millions hosts. Do you have any experience in the\n&gt;&gt;&gt; resources needed for this? Would this slow down the speed of the crawl?\n&gt;&gt; I&#39;ve not done anything with a blacklist that big. You certainly wouldn&#39;t\n&gt;&gt; want to use a list of regexes!\n&gt;&gt;\n&gt;&gt; The SURT-based DecideRules use a sorted list of prefixes, all in memory,\n&gt;&gt; and have roughly O(log n) lookup. Perhaps that would work for your\n&gt;&gt; purposes, if you have a big RAM machine; you should do some tests and\n&gt;&gt; back-of-the-envelope calculations to check for sure. If the blacklist\n&gt;&gt; entries are always hosts, some other hash-based structure might work\n&gt;&gt; with even less RAM overhead and O(1) lookup.\n&gt;&gt;\n&gt;&gt;&gt; In our small crawls that we have been running, we found that although we\n&gt;&gt;&gt; have setup for a breadth first crawl, days into the crawl and 10&#39;s of\n&gt;&gt;&gt; million of pages crawled, the crawler has still only crawled a small\n&gt;&gt;&gt; percentage of seeds (under 10% of a 2 million host seed list - 1200\n&gt;&gt;&gt; threads). It appears that most of the seeds are in a separate queue so\n&gt;&gt;&gt; would cycling through the queues (balance-replenish-amount to a lower\n&gt;&gt;&gt; amount maybe 100) help cover all of the queues?\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n\n"}}