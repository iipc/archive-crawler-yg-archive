{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"John Lekashman","from":"John Lekashman &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"lAQODKY4JoxmL0Jw1iB_lIqym7ZYigHZUQqknwOq13Px2_6eC4qMaHdaNTpqHypc2t2QHkNyuyvovAsc8mavMyrTTghuPsldaFE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Question about H3 crawl management","postDate":"1326241833","msgId":7508,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMENEODI5LjkwMzA4MDFAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PDRGMENDOUQzLjIwNTAxMDRAY3MuY211LmVkdT4=","referencesHeader":"PDRGMEM3NjRFLjQwMDA3MDBAY3MuY211LmVkdT4gPDRGMEM4QjQ4LjQwMjAwMDlAYmF5YXJlYS5uZXQ+IDw0RjBDQzlEMy4yMDUwMTA0QGNzLmNtdS5lZHU+"},"prevInTopic":7507,"nextInTopic":7509,"prevInTime":7507,"nextInTime":7509,"topicId":7505,"numMessagesInTopic":7,"msgSnippet":"Hi David, Why would ssl have anything to do with it? Trying to read the UI from a script isn t the best way, but good luck if you must. A script: Now, this is","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 67315 invoked from network); 11 Jan 2012 00:31:03 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m2.grp.sp2.yahoo.com with QMQP; 11 Jan 2012 00:31:03 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta3.grp.sp2.yahoo.com with SMTP; 11 Jan 2012 00:31:03 -0000\r\nX-Received: from john-lekashmans-macbook-pro.local (173-164-186-205-SFBA.hfc.comcastbusiness.net [173.164.186.205])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id q0B0UX5Z094254;\n\tTue, 10 Jan 2012 16:30:34 -0800 (PST)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;4F0CD829.9030801@...&gt;\r\nDate: Tue, 10 Jan 2012 16:30:33 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: David Pane &lt;dpane@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;4F0C764E.4000700@...&gt; &lt;4F0C8B48.4020009@...&gt; &lt;4F0CC9D3.2050104@...&gt;\r\nIn-Reply-To: &lt;4F0CC9D3.2050104@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------030503070005000309060509&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: John Lekashman &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Question about H3 crawl management\r\nX-Yahoo-Group-Post: member; u=90724651; y=oVsGngFwEiafuWICY0xm774bE-sm0NFzncazyIT0NfA2\r\nX-Yahoo-Profile: lekash\r\n\r\n\r\n--------------030503070005000309060509\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nHi David,\nWhy would ssl have anything to do with it?\nTrying to read the UI from a script isn&#39;t the best way,\nbut good luck if you must.\n\nA script:\nNow, this is pretty rough, and if you are generating various\nversions of crawl.log, you have to deal with it.\n\n# # of pages.  Run wc on crawl log and add &#39;em up.\n#!/bin/bash\n\nrm /tmp/$sometmpfilename\nfor i in a1 a2 a3 a4 a5     # or `cat my_list_of_crawlers`\ndo\n    ssh $i wc /home/crawl/logs/crawl.log | awk &#39;{print $1}&#39; &gt;&gt; \n/tmp/$sometmpfilename\n    echo &quot;+&quot; &gt;&gt; /tmp/$sometmpfilename\ndone\n\necho &quot;+p&quot; &gt;&gt;  /tmp/$sometmpfilename\n\ncat /tmp/$sometmpfilename | dc\n\nAnd what you do with the output of cat, well, you figure that out.\n\n# Total size of crawled data.  Pretty much the same thing, although you \nlook at the third field in crawl.log\nInstead of wc use something like this:  the grep -v removes the &#39;-&#39; from \n0 length records, which would confuse\nthe output.\n\nssh $i cat crawl.log | awk &#39;{print $3}&#39; | grep -v  &#39;-&#39;\n\n\nOn 1/10/12 3:29 PM, David Pane wrote:\n&gt;\n&gt; Thank you for your responses John.\n&gt;\n&gt; Can you be more specific about your thoughts on writing a script to\n&gt; generate these? I have tried to generate the mime report and crawl\n&gt; summary reports using perl LWP, but haven&#39;t figured out how to deal with\n&gt; the SSL certificates.\n&gt;\n&gt; --David\n&gt;\n&gt; &gt;&gt; 5) Although I can capture the below statistics manually, can you \n&gt; suggest\n&gt; &gt;&gt; a way that I can automatically generate/collect the following \n&gt; statistics\n&gt; &gt;&gt; from the crawl. I would like to generate this data at least once every\n&gt; &gt;&gt; 24 hours and possibly as often as every hour.\n&gt; &gt;&gt;\n&gt; &gt; Well, you could write a script to do it.\n&gt; &gt;\n&gt; &gt;&gt; a) Total size of crawled data.\n&gt; &gt;&gt; b) total number of pages crawled (mime-type: text/html).\n&gt; &gt;&gt;\n&gt;\n&gt; \n\n\r\n--------------030503070005000309060509\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta content=&quot;text/html; charset=ISO-8859-1&quot;\n      http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#FFFFFF&quot; text=&quot;#000000&quot;&gt;\n    Hi David,&lt;br&gt;\n    Why would ssl have anything to do with it?&lt;br&gt;\n    Trying to read the UI from a script isn&#39;t the best way, &lt;br&gt;\n    but good luck if you must.&lt;br&gt;\n    &lt;br&gt;\n    A script:&lt;br&gt;\n    Now, this is pretty rough, and if you are generating various &lt;br&gt;\n    versions of crawl.log, you have to deal with it.&lt;br&gt;\n    &lt;br&gt;\n    # # of pages.&nbsp; Run wc on crawl log and add &#39;em up.&nbsp; &lt;br&gt;\n    #!/bin/bash&lt;br&gt;\n    &lt;br&gt;\n    rm /tmp/$sometmpfilename&lt;br&gt;\n    for i in a1 a2 a3 a4 a5&nbsp;&nbsp;&nbsp;&nbsp; # or `cat my_list_of_crawlers`&lt;br&gt;\n    do&lt;br&gt;\n    &nbsp;&nbsp; ssh $i wc /home/crawl/logs/crawl.log | awk &#39;{print $1}&#39; &gt;&gt;\n    /tmp/$sometmpfilename&lt;br&gt;\n    &nbsp;&nbsp; echo &quot;+&quot; &gt;&gt; /tmp/$sometmpfilename&lt;br&gt;\n    done&lt;br&gt;\n    &nbsp;&nbsp; &lt;br&gt;\n    echo &quot;+p&quot; &gt;&gt;&nbsp; /tmp/$sometmpfilename &lt;br&gt;\n    &lt;br&gt;\n    cat /tmp/$sometmpfilename | dc&lt;br&gt;\n    &lt;br&gt;\n    And what you do with the output of cat, well, you figure that out.&lt;br&gt;\n    &lt;br&gt;\n    # Total size of crawled data.&nbsp; Pretty much the same thing, although\n    you look at the third field in crawl.log&lt;br&gt;\n    Instead of wc use something like this:&nbsp; the grep -v removes the &#39;-&#39;\n    from 0 length records, which would confuse&lt;br&gt;\n    the output.&lt;br&gt;\n    &lt;br&gt;\n    ssh $i cat crawl.log | awk &#39;{print $3}&#39; | grep -v&nbsp; &#39;-&#39;&lt;br&gt;\n    &lt;br&gt;\n    &lt;br&gt;\n    On 1/10/12 3:29 PM, David Pane wrote:\n    &lt;blockquote cite=&quot;mid:4F0CC9D3.2050104@...&quot; type=&quot;cite&quot;&gt;\n      &lt;span style=&quot;display:none&quot;&gt;&nbsp;&lt;/span&gt;\n      \n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;Thank you for your responses John.&lt;br&gt;\n              &lt;br&gt;\n              Can you be more specific about your thoughts on writing a\n              script to &lt;br&gt;\n              generate these? I have tried to generate the mime report\n              and crawl &lt;br&gt;\n              summary reports using perl LWP, but haven&#39;t figured out\n              how to deal with &lt;br&gt;\n              the SSL certificates.&lt;br&gt;\n              &lt;br&gt;\n              --David&lt;br&gt;\n              &lt;br&gt;\n              &gt;&gt; 5) Although I can capture the below statistics\n              manually, can you suggest&lt;br&gt;\n              &gt;&gt; a way that I can automatically generate/collect\n              the following statistics&lt;br&gt;\n              &gt;&gt; from the crawl. I would like to generate this\n              data at least once every&lt;br&gt;\n              &gt;&gt; 24 hours and possibly as often as every hour.&lt;br&gt;\n              &gt;&gt;&lt;br&gt;\n              &gt; Well, you could write a script to do it.&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt;&gt; a) Total size of crawled data.&lt;br&gt;\n              &gt;&gt; b) total number of pages crawled (mime-type:\n              text/html).&lt;br&gt;\n              &gt;&gt;&lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- end group email --&gt;\n    &lt;/blockquote&gt;\n    &lt;br&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------030503070005000309060509--\r\n\n"}}