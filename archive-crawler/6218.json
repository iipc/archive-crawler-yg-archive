{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":212342429,"authorName":"Pranay Pandey","from":"Pranay Pandey &lt;sspranay@...&gt;","profile":"sspranay","replyTo":"LIST","senderId":"j455WR9Ny6_6Tui3ZUgmP2EvQjZWN0Va-bo7wcg5yPuvMU3LZvqyYhKh5qEZQzNPTw5p_W8ltRgR40CgFTGTeL9THLS0BfiFyg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [archive-crawler] more fetches using parallel queues in H3.0","postDate":"1260801156","msgId":6218,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDkwODk4Ljk3MTU1LnFtQHdlYjQzMTM3Lm1haWwuc3AxLnlhaG9vLmNvbT4=","inReplyToHeader":"PEJMVTEzNi1XMTZCMTA5NEJCM0JCOEE1QzY0QjA3RDlDOEIwQHBoeC5nYmw+"},"prevInTopic":6210,"nextInTopic":0,"prevInTime":6217,"nextInTime":6219,"topicId":6197,"numMessagesInTopic":4,"msgSnippet":"I have data for over 20 crawls (each ran for about a day) with very much the same crawler-beans settings. I used H3-beta and as said earlier I had only 1 site","rawEmail":"Return-Path: &lt;sspranay@...&gt;\r\nX-Sender: sspranay@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 13753 invoked from network); 14 Dec 2009 14:32:37 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m12.grp.re1.yahoo.com with QMQP; 14 Dec 2009 14:32:37 -0000\r\nX-Received: from unknown (HELO web43137.mail.sp1.yahoo.com) (216.252.121.67)\n  by mta2.grp.sp2.yahoo.com with SMTP; 14 Dec 2009 14:32:37 -0000\r\nX-Received: (qmail 97477 invoked by uid 60001); 14 Dec 2009 14:32:37 -0000\r\nMessage-ID: &lt;90898.97155.qm@...&gt;\r\nX-YMail-OSG: BUmiPagVM1nGZXS3aUtQOkM0_q69aLEps1o1EzXsPmiftTy7fi8BxYdbuadW8vrt22U.rbhuzCnl5zAOEB6e6__T33agEmXaSxkXf7_ZTGSn0b9jpKENikobARuW5jYhgB6mnaCq1LjUsgs2419OBregkavZzd4bxygW6JEnLOWO_G6MSoQZgtLyD9IYGNlTSztQPbeOPVl6q.A3meWmByNNsRLQ6AAvcFa9Yui2lcYyIh7V.CDHEHlQ4XowW0bD7jeBMLju2nRZ8N9hlHRSGKTABg_GpdyWTWIl.0QjZ4aQ0XsJvnIX7D4Df7RFOWIIPWNC_NznIjlfuK6_mODo778T3.SgmjIYxhMqrJ4OqUgPuJzpi77VosB3xMkCha7ugBUsCRPDdyI3nY.oX_imPSy4HYIuofiJECmWLKoywhGLT7Vta0DUF_gpUjCLtvd5sKHvZ9LdZKe6KFwi7TjRBnqhgQKlEnazcBd_COCk5j_q425KrqmlXkipw.D.I5urISzIROgWZCL6cmuvXaDAh2o2fkh3f0KFWycwbAPbVP_l2m4RSGFWD5vuBzncqXljMuIncXpdNmSKmEkgH6mL9l7RGOGaGQLyc8_bKVOTP.u7FqBRhlnwjRLQawtonnCJdbxWlqekusPcBENiwN2qqFyC2WYhhYMiZc5YCeqxASBt8fMNFvFCyHYfSlmHXfaJGBsosneqtwkNJfpJy0hz63MA.r6LsuW963GtVEHkzJ39tDseTbsFO0HzMXBxz9J5uMXKnMokZUwcH_4Hf9hCCcP9.ymxbxDh4uiB94xjFp_.PkRuP6La14XQ4XqGP7uItI8NvFtMlE8h4RZRKAnSVPR7MNehDj4fXoWfyxYNp8s3eS2kMc4IXWP367yJ.Aj1ZylIKNzQHU_9znO0e4aJjTE3_MPOmCWHB682zg3SoogFoqM8TaiuKSVpyqXZHyS_iAkShCPVGtP57iu.MaJXd2afiec3dgGunuNMr1Vjj43HeUv.tVPf3bXkF0jVCgamCgrRwgy1dDNxHyxQ0OC6fIYPVGczfsOJPg7KNIdQpKWQzQ8UznlBdbSL38Jx0AkXOfuN4yQHQ1PqtKZdnkLzWMfwx4s4XbafGinmF3zcOTTdrMv5uvdlclyK9RYLrcn3qu8A2DBDkIpawEC.5IhJc.fa9p48SWZvmWnOtRJDPtVvaEhayrpq9zOnGADnc2LDUMB5TbfnBkyJXRbctBgwoJBAvkObkJWyqT_trRfo3lItIRMyrWb06dCh5hkjsKgxAyjaLimx9HvKQeLMxtSsnInfXcbJS79cvaGhnfOAGXhhVN98wG6VExOBTQ84VjEtXS3iwvTS_2pvem4X.3sWCLQDHCSCnhPtySgJs1jiDCN.ifyzjuUaYldPVlohLS.hWvYRrgQZls1Xo1tBKQOXBgG2Lh.qm3O.sEUO.5NAPGwAsUTA8_NoZypJ8P6IAQWE5FtU0gru3J8GbYiwONlCcs.QSavwcaFi_z8-\r\nX-Received: from [204.194.77.3] by web43137.mail.sp1.yahoo.com via HTTP; Mon, 14 Dec 2009 06:32:36 PST\r\nX-Mailer: YahooMailClassic/9.0.19 YahooMailWebService/0.8.100.260964\r\nDate: Mon, 14 Dec 2009 06:32:36 -0800 (PST)\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;BLU136-W16B1094BB3BB8A5C64B07D9C8B0@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-31407505-1260801156=:97155&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Pranay Pandey &lt;sspranay@...&gt;\r\nSubject: RE: [archive-crawler] more fetches using parallel queues in H3.0\r\nX-Yahoo-Group-Post: member; u=212342429; y=vCcoPHElaw8UiRmxpRbANC3BxfcykKsuFJhYu1UPLPt3YEk\r\nX-Yahoo-Profile: sspranay\r\n\r\n\r\n--0-31407505-1260801156=:97155\r\nContent-Type: text/plain; charset=utf-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n\n\nI have data for over 20 crawls (each ran for about a day) with very much =\r\nthe same crawler-beans settings.\nI used H3-beta and as said earlier I had o=\r\nnly 1 site to crawl.\n\nOut of those 20+ runs, only 1 has a download rate of =\r\n7.37 docs/sec. Rest all are around 0.5 docs/sec. Number of parallel queue u=\r\nsed =3D 10, allocated maxHeap =3D 2G\nrobots.txt is not a problem, we have t=\r\nhe permission.\n\nShould I be trying ab &#39;Apache-benchmarking&#39; to see if the t=\r\nerrible-slowness has to do with the server capability?\n\nI encountered the s=\r\name slow download speed issue with H3-RC1 and final H3 release.\nAny pointer=\r\ns?\n\nThanks!\nPranay\n-\n\n\n--- On Sat, 12/12/09, Dodda Tika &lt;dodda.tika@hotmail=\r\n.com&gt; wrote:\n\nFrom: Dodda Tika &lt;dodda.tika@...&gt;\nSubject: RE: [archi=\r\nve-crawler] more fetches using parallel queues in H3.0\nTo: archive-crawler@=\r\nyahoogroups.com\nDate: Saturday, December 12, 2009, 3:05 AM\n\n\n\n\n\n\n\n=C2=A0\n\n\n=\r\n\n  \n\n\n    \n      \n      \n      \n\n\nHello,\n\nI too am observing that the downl=\r\noad is too slow. Any pointers as to how do I speed it up? Its not using the=\r\n full bandwidth/processor capability available.\n\nRegards\nPraveen\n\n\n\n\n\n\nTo: =\r\narchive-crawler@ yahoogroups. com\nFrom: sspranay@yahoo. com\nDate: Wed, 9 De=\r\nc 2009 11:52:40 -0800\nSubject: [archive-crawler] more fetches using paralle=\r\nl queues in H3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=C2=A0\n\n\n\n  \n\n\n    \n      \n      \n      =\r\nHi,\n\nI have a site that allows my crawler to be least polite. The site is h=\r\nuge and also capable of handling large traffic. \nI increased the number of =\r\nparallel queues from 1 to 10, but no visible improvement in crawl speed - s=\r\ntill around 0.28 docs/sec against 0.19 docs/sec with 1 queue. \n\nI have set =\r\nthe heapSize to 2GB and at no point of time more than 1G was being used.\nI =\r\nhave also loosened all the other politeness factors.\nIf not 10 times, the c=\r\nrawl should have been faster by at least 5 times with 10 queues. \nAny clues=\r\n?\n\nThanks,\nPranay\n\n\n\n      \n\n\n    \n     \n\n    \n    \n\n\n\n\n\n\n   \t\t \t   \t\t  \nNe=\r\nw Windows 7: Simplify what you do everyday. Find the right PC for you.\n\n\n  =\r\n  \n     \n\n    \n    \n\n\n \n\n\n\n  \n\n\n\n\n\n\n      \r\n--0-31407505-1260801156=:97155\r\nContent-Type: text/html; charset=utf-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;table cellspacing=3D&quot;0&quot; cellpadding=3D&quot;0&quot; border=3D&quot;0&quot; &gt;&lt;tr&gt;&lt;td valign=3D&quot;=\r\ntop&quot; style=3D&quot;font: inherit;&quot;&gt;&lt;br&gt;&lt;br&gt;I have data for over 20 crawls (each =\r\nran for about a day) with very much the same crawler-beans settings.&lt;br&gt;I u=\r\nsed H3-beta and as said earlier I had only 1 site to crawl.&lt;br&gt;&lt;br&gt;Out of t=\r\nhose 20+ runs, only 1 has a download rate of 7.37 docs/sec. Rest all are ar=\r\nound 0.5 docs/sec. Number of parallel queue used =3D 10, allocated maxHeap =\r\n=3D 2G&lt;br&gt;robots.txt is not a problem, we have the permission.&lt;br&gt;&lt;br&gt;Shoul=\r\nd I be trying ab &#39;Apache-benchmarking&#39; to see if the terrible-slowness has =\r\nto do with the server capability?&lt;br&gt;&lt;br&gt;I encountered the same slow downlo=\r\nad speed issue with H3-RC1 and final H3 release.&lt;br&gt;Any pointers?&lt;br&gt;&lt;br&gt;Th=\r\nanks!&lt;br&gt;Pranay&lt;br&gt;&lt;div&gt;&lt;div&gt;-&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;br&gt;--- On &lt;b&gt;Sat, 12/12/=\r\n09, Dodda Tika &lt;i&gt;&lt;dodda.tika@...&gt;&lt;/i&gt;&lt;/b&gt; wrote:&lt;br&gt;&lt;blockqu=\r\note style=3D&quot;border-left: 2px solid rgb(16, 16, 255); margin-left: 5px;\n pa=\r\ndding-left: 5px;&quot;&gt;&lt;br&gt;From: Dodda Tika &lt;dodda.tika@...&gt;&lt;br&gt;Su=\r\nbject: RE: [archive-crawler] more fetches using parallel queues in H3.0&lt;br&gt;=\r\nTo: archive-crawler@yahoogroups.com&lt;br&gt;Date: Saturday, December 12, 2009, 3=\r\n:05 AM&lt;br&gt;&lt;br&gt;&lt;div id=3D&quot;yiv1551354240&quot;&gt;\n\n\n\n\n\n&lt;span style=3D&quot;display: none;=\r\n&quot;&gt;&nbsp;&lt;/span&gt;\n\n\n\n    &lt;div id=3D&quot;ygrp-text&quot;&gt;\n      \n      \n      &lt;p&gt;\n\n\nHel=\r\nlo,&lt;br&gt;&lt;br&gt;I too am observing that the download is too slow. Any pointers a=\r\ns to how do I speed it up? Its not using the full bandwidth/processor capab=\r\nility available.&lt;br&gt;&lt;br&gt;Regards&lt;br&gt;Praveen&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;=\r\nhr id=3D&quot;stopSpelling&quot;&gt;To: archive-crawler@ yahoogroups. com&lt;br&gt;From: sspra=\r\nnay@yahoo. com&lt;br&gt;Date: Wed, 9 Dec 2009 11:52:40 -0800&lt;br&gt;Subject: [archive=\r\n-crawler] more fetches using parallel queues in H3.0&lt;br&gt;&lt;br&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=\r\n\n\n&lt;span&gt;&nbsp;&lt;/span&gt;\n\n\n&lt;div id=3D&quot;ecxygrp-mlmsg&quot;&gt;\n  &lt;div id=3D&quot;ecxygrp-msg=\r\n&quot;&gt;\n\n\n    &lt;div id=3D&quot;ecxygrp-text&quot;&gt;\n      \n      \n      &lt;table border=3D&quot;0&quot; =\r\ncellpadding=3D&quot;0&quot; cellspacing=3D&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=3D&quot;font-family: in=\r\nherit; font-style: inherit; font-variant: inherit; font-weight: inherit; fo=\r\nnt-size: inherit; font-size-adjust: inherit; font-stretch: inherit;&quot; valign=\r\n=3D&quot;top&quot;&gt;Hi,&lt;br&gt;&lt;br&gt;I have a site that allows my crawler to be least polite=\r\n. The site is huge and also capable of handling large traffic. &lt;br&gt;I increa=\r\nsed the number of parallel queues from 1 to 10, but no visible improvement =\r\nin crawl speed - still around 0.28 docs/sec against 0.19 docs/sec with 1 qu=\r\neue. &lt;br&gt;&lt;br&gt;I have set the heapSize to 2GB and at no point of time more th=\r\nan 1G was being used.&lt;br&gt;I have also loosened all the other politeness fact=\r\nors.&lt;br&gt;If not 10 times, the crawl should have been faster by at least 5 ti=\r\nmes with 10 queues. &lt;br&gt;Any clues?&lt;br&gt;&lt;br&gt;Thanks,&lt;br&gt;Pranay&lt;br&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/=\r\ntbody&gt;&lt;/table&gt;&lt;br&gt;\n\n      &lt;br&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;co=\r\nlor: rgb(255, 255, 255); height: 0pt;&quot;&gt;&lt;/div&gt;\n\n\n\n\n\n\n   \t\t \t   \t\t  &lt;br&gt;&lt;hr&gt;N=\r\new Windows 7: Simplify what you do everyday. &lt;a rel=3D&quot;nofollow&quot; target=3D&quot;=\r\n_blank&quot; href=3D&quot;http://windows.microsoft.com/shop&quot;&gt;Find the right PC for yo=\r\nu.&lt;/a&gt;\n&lt;/div&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n\n\n\n \n\n\n&lt;/div&gt;&lt;/blockquote&gt;&lt;/td&gt;&lt;/tr&gt;&lt;=\r\n/table&gt;&lt;br&gt;\n\n\n\n      \r\n--0-31407505-1260801156=:97155--\r\n\n"}}