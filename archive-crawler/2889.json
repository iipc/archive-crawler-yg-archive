{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"IIUzfb4Wgw1ds72GQwqJZYg2PDDKcaOwfOggtLBbILvYp78IIXftsC9xpNULD3uxtoxj2odIp72wzb49QHg9FeSC8DDDIxw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Can we add content filter ?","postDate":"1149033049","msgId":2889,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0N0NEQTU5LjUwMjAzMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGU1Z2NocStlcWxrQGVHcm91cHMuY29tPg==","referencesHeader":"PGU1Z2NocStlcWxrQGVHcm91cHMuY29tPg=="},"prevInTopic":2888,"nextInTopic":0,"prevInTime":2888,"nextInTime":2890,"topicId":2887,"numMessagesInTopic":3,"msgSnippet":"... There is not a built-in facility for this, though the intent of the design is to allow it. It would require some light Java programming. The first approach","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 7454 invoked from network); 30 May 2006 23:50:29 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m34.grp.scd.yahoo.com with QMQP; 30 May 2006 23:50:29 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta8.grp.scd.yahoo.com with SMTP; 30 May 2006 23:50:29 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id C679614156945\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 30 May 2006 16:50:28 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 15246-07-72 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 30 May 2006 16:50:28 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 741AD14156942\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 30 May 2006 16:50:28 -0700 (PDT)\r\nMessage-ID: &lt;447CDA59.5020306@...&gt;\r\nDate: Tue, 30 May 2006 16:50:49 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e5gchq+eqlk@...&gt;\r\nIn-Reply-To: &lt;e5gchq+eqlk@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Can we add content filter ?\r\nX-Yahoo-Group-Post: member; u=137285340; y=Dhxwd6wQJr2qjvs-p2EVtw6s_dhDM8UZLCpfxkmAaIN4\r\nX-Yahoo-Profile: gojomo\r\n\r\nanand_akela wrote:\n&gt; Hi,\n&gt; \n&gt; I am learning to use this tool. I understood that you can add filters\n&gt; for URIs, but, I am wondering if there any way to add keywords filter\n&gt; that can look-up content of the webpage and save the information only\n&gt; if there is keyword match.\n\nThere is not a built-in facility for this, though the intent of the \ndesign is to allow it. It would require some light Java programming.\n\nThe first approach that comes to mind would be to create a custom \nProcessor that performs the keyword analysis you want, and sets an \nattribute on the CrawlURI based on that analysis.\n\nThen, create a Filter consulting that attribute and returns &#39;true&#39; or \n&#39;false&#39; accordingly. By setting that Filter on the ARCWriterProcessor, \nonly some CrawlURIs will be handled by the Processor and thus written to \nARCs.\n\n&gt; Another question... Is there an easy-to-use tool that can split the\n&gt; big ARC file into html files per URL?\n\nThis is often requested and we have at times developed internal scripts \nto do this, but haven&#39;t thought any of general enough use to release. I \nalso believe others&#39; work in this area has been referenced on this list \npreviously, though I don&#39;t have the references handy.\n\nThe challenges with any such transformation are at the very least:\n\n(1) The URL namespace does not neatly map into the directory/filesystem \nnamespace, despite the superficial similarities and the fact that often \nan URL space is backed by a filesystem space. Most crawls will have some \nURLs requiring exceptional treatment.\n(2) The goal often underlying this desire is to be able to browse \ncaptures with a web browser pointed at local files. However, for this to \nwork the content itself may need extensive rewriting, for absolute and \nrelative URLs.\n(3) The modifications required for (1) and (2) are lossy -- losing \noriginal caputre information when headers are discarded and original \nURLs coerced into locally-usable versions -- and no one set of \nmodification rules seems to suit all users and source material.\n\nWe&#39;d welcome contributions to the Heritrix or archive-access projects \naddressing this need, but as the IA&#39;s usual mode is (1) exact \npreservation of the HTTP responses and (2) display via the Wayback or \nother active replay-application, such &#39;unrolling&#39; of ARCs to disk has \nrarely been a priority for us internally.\n\nHope this helps,\n\n- Gordon @ IA\n\n\n"}}