{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"uiF9E6x0dBrqN053ROC3g5OLe1DvqkChmCskEN4qlz5KCr1_ILkUH9ImC8MM9DKs5dPiCzQj3SnFraHCVztnrE7QPArlbSsBuqjHvp9a3w","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] persistent crawls","postDate":"1094137076","msgId":942,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBEMDhAbG9raS5ib2suaGkuaXM+","inReplyToHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDI5QzRFMTdAbG9raS5ib2suaGkuaXM+"},"prevInTopic":941,"nextInTopic":943,"prevInTime":941,"nextInTime":943,"topicId":934,"numMessagesInTopic":14,"msgSnippet":"... That sounds great and brings me to a related topic. There really should be a nice way of marking CrawlURIs as duplicate or something similar. This flag ","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 76407 invoked from network); 2 Sep 2004 14:57:28 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m3.grp.scd.yahoo.com with QMQP; 2 Sep 2004 14:57:28 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 2 Sep 2004 14:57:28 -0000\r\nReceived: (qmail 20231 invoked by uid 100); 2 Sep 2004 14:47:53 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (kris@...@130.208.152.83)\n  by mail-dev.archive.org with RC4-MD5 encrypted SMTP; 2 Sep 2004 14:47:53 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Thu, 2 Sep 2004 14:57:56 -0000\r\nMessage-ID: &lt;D958110B273CD511ACC100B0D079AA4E01960D08@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nIn-Reply-To: &lt;D958110B273CD511ACC100B0D079AA4E029C4E17@...&gt;\r\nImportance: Normal\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1441\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.4 required=7.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] persistent crawls\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\n&gt;\n&gt;\n&gt;-----Original Message-----\n&gt;From: stack [mailto:stack@...] \n&gt;S=\r\nent: 2. september 2004 14:44\n&gt;To: archive-crawler@yahoogroups.com\n&gt;Subject:=\r\n Re: [archive-crawler] persistent crawls\n&gt;\n&gt;Tom Emerson wrote:\n&gt;\n&gt;&gt;Phil Whi=\r\nte writes:\n&gt;&gt;[...]\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt;&gt;As a result, it&#39;s necessarily a longer term p=\r\nroject and I&#39;d prefer to \n&gt;&gt;&gt;not have my DSL pegged out for the next 3 year=\r\ns or so.  8)\n&gt;&gt;&gt;    \n&gt;&gt;&gt;\n&gt;&gt;[...]\n&gt;&gt;\n&gt;&gt;There has been a lot of research done=\r\n on how to select URLs for\n&gt;&gt;subsequent crawling: the major search engines =\r\ncertainly don&#39;t recrawl\n&gt;&gt;their entire catalog on a regular basis. Searchin=\r\ng on Google (you&#39;ll\n&gt;&gt;find papers by Sergei Brin and Larry Page, who both w=\r\norked on this\n&gt;&gt;problem) or on CiteSeer will show a bunch. However, for you=\r\nr task this\n&gt;&gt;is probably overkill.\n&gt;&gt;\n&gt;&gt;One hack comes to mind, which may =\r\nor may not work:\n&gt;&gt;\n&gt;&gt;In the Expert Settings for the crawl you can add &quot;Acc=\r\nept&quot; headers to\n&gt;&gt;the request. It turns out that the way I implemented this=\r\n allows you\n&gt;&gt;to add *any* header to the request. The upshot is that you co=\r\nuld try\n&gt;&gt;adding an &#39;If-Modified-Since:&#39; header to the subsequent crawls, g=\r\niving\n&gt;&gt;the date of your initial crawl. It isn&#39;t perfect, but it may help.\n=\r\n&gt;&gt;\n&gt;&gt;You could also write a script that extracts all the URLs and then\n&gt;&gt;se=\r\nnds a HEAD request to determine which ones have changed... I was\n&gt;&gt;thinking=\r\n of writing something like this, but have not gotten around to\n&gt;&gt;it.\n&gt;&gt;  \n&gt;=\r\n&gt;\n&gt;If the &#39;If-Modified-Since&#39; header add doesn&#39;t work, we did a little \n&gt;pl=\r\nanning yesterday and the feature &#39;[ 941072 ] Allow operator-configured \n&gt;mi=\r\nd-HTTP-fetch filters&#39; is to be done for an October 1st-ish release \n&gt;(1.2).=\r\n  This feature would introduce filters after the headers have been \n&gt;downlo=\r\naded but before we start in on the body.  Filters will say yes or \n&gt;no on w=\r\nhether to proceed.  Let me take on the above as a test this \n&gt;feature needs=\r\n to pass (Another will be a mime-type filter).\n&gt;\n&gt;(If anyone is interested =\r\nin features scheduled for 1.2, see the RFE list \n&gt;by priority.  At least th=\r\ne highest priority items with the next lowest \n&gt;priority items to be done o=\r\npportunistically).\n&gt;\n\nThat sounds great and brings me to a related topic. T=\r\nhere really should be a\nnice way of marking CrawlURIs as &quot;duplicate&quot; or som=\r\nething similar. This flag\nshould then be used by extractors to (optionally?=\r\n) preclude link extraction.\nOf course if stopped before the body is downloa=\r\nded there wont be anything to\ndo link extraction, but eventually we&#39;ll have=\r\n a processor that compares it\nto existing content hashes.\n\nOnce we can note=\r\n duplicate records in the ARCs the ArcWriter would consult\nthis value as we=\r\nll.\n\n- Kris\n\n\n"}}