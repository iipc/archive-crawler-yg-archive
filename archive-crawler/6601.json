{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"mGnQF-3wcxS_URCu77k5DPEV-VwWdb5t_BqdO86XggjeOiv3rqETNf6n6AahE1ItpaUwZTGziLNTFcpdRFah479dcEercmk","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix for writing extracted URIs rather than ARCs","postDate":"1277502508","msgId":6601,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDMjUyNDJDLjQwMjA0MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDI1N0NENDgwLTQyRTQtNEM5MS1CRjA3LUJEQzZGRENBODhEQ0BnbWFpbC5jb20+","referencesHeader":"PDIyNDk1MkI5LTYxOUYtNDE1MS04OTY1LTQ3MTA5RkQwRDE1RUBnbWFpbC5jb20+IDxCM0E5QTU0NzlDQ0I0REU5ODI2N0ZFNTI1OUFBMjUxQkBFV0FMYXB0b3AxPiA8MjU3Q0Q0ODAtNDJFNC00QzkxLUJGMDctQkRDNkZEQ0E4OERDQGdtYWlsLmNvbT4="},"prevInTopic":6598,"nextInTopic":6615,"prevInTime":6600,"nextInTime":6602,"topicId":6596,"numMessagesInTopic":6,"msgSnippet":"The crawl.log does offer each URI that was visited, and includes as reference the exact URI from which it was discovered. It doesn t include *all*","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 22796 invoked from network); 25 Jun 2010 21:48:30 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m14.grp.re1.yahoo.com with QMQP; 25 Jun 2010 21:48:30 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.re1.yahoo.com with SMTP; 25 Jun 2010 21:48:30 -0000\r\nX-Received: (qmail 52792 invoked from network); 25 Jun 2010 21:48:29 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 25 Jun 2010 21:48:29 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4C25242C.4020405@...&gt;\r\nDate: Fri, 25 Jun 2010 14:48:28 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.9) Gecko/20100317 Thunderbird/3.0.4\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Federico Maggi &lt;federico.maggi@...&gt;\r\nReferences: &lt;224952B9-619F-4151-8965-47109FD0D15E@...&gt; &lt;B3A9A5479CCB4DE98267FE5259AA251B@EWALaptop1&gt; &lt;257CD480-42E4-4C91-BF07-BDC6FDCA88DC@...&gt;\r\nIn-Reply-To: &lt;257CD480-42E4-4C91-BF07-BDC6FDCA88DC@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix for writing extracted URIs rather\n than ARCs\r\nX-Yahoo-Group-Post: member; u=137285340; y=_DaEgFLuYRL25eIaYdf-N_Ol5vDxiu2XZoCK8j9HrNJ-\r\nX-Yahoo-Profile: gojomo\r\n\r\nThe crawl.log does offer each URI that was visited, and includes as \nreference the exact URI from which it was discovered. It doesn&#39;t include \n*all* outlinks/inlinks; just those that were actually followed. (So, \nwhich paths it shows is sensitive to some fairly arbitrary \nordering/timing issues.)\n\nIf in fact you want every link, or in a particular format, you&#39;ll need \nto try other approaches.\n\nIf you choose to write crawled content as &#39;WARC&#39; files, which is the \ndefault in our recommended Heritrix-3 (3.0.0+) configuration, a \n&#39;metadata&#39; record will be written for each crawled URI (alongside the \n&#39;response&#39; and &#39;request&#39; records). This contains a rudimentary listing \nof the outlinks found from each URI that, with a little massaging, might \nmeet your needs.\n\nTo get output exactly as you described...\n\nORIGIN-URI-1\tDESTINATION-URI-1\nORIGIN-URI-1\tDESTINATION-URI-2\nORIGIN-URI-1\tDESTINATION-URI-3\nORIGIN-URI-2\tDESTINATION-URI-4\netc.\n\n...you would need to write your own dump code. In the Heritrix \narchitecture, this would most likely involve supplying your own \n&#39;Processor&#39; subclass, and placing it into the list (aka &#39;chain&#39;) of \nProcessors at a suitable position -- after all link-extraction is \nperformed. (You might additionally position this Processor before any \nlink-rejecting &#39;scoping&#39; occurs, if you want *all* outlinks, or after \nscoping, if you only want those that otherwise fit your crawl definition.)\n\nThis Processor can be written in Java as long as it&#39;s available from the \nclasspath at the time of JVM startup. It can also be written in several \nother scripting languages that work on the JVM.\n\nThe same general strategy works in either the latest Heritrix-1 release \n(1.14.4) or Heritrix-3 (3.0.0). However, H3 is a little more welcoming \nto this kind of incremental extension: it&#39;s easier to specify arbitrary \nclasses in your configuration, and more bundled scripting languages \n(Beanshell, Groovy, JavaScript). The downside of H3 is that you have to \nbe comfortable editing a largish XML configuration file, filled with \nJava terminology, to customize crawls.\n\nHope this helps,\n\n- Gordon @ IA\n\n\nOn 6/25/10 4:43 AM, Federico Maggi wrote:\n&gt; On Jun 24, 2010, at 9:54 PM, Gabriel Vasile wrote:\n&gt;\n&gt;&gt; The information you want is in the crawl.log\n&gt;\n&gt; \tit seems to me that entries are inserted into crawl.log only when a certain URI is visited, not when it&#39;s found.\n&gt;\n&gt; This is &quot;OK&quot;, since I&#39;d have to wait for the entire crawl to finish in order to see all the URI encountered.\n&gt;\n&gt; Isn&#39;t there a way to log URIs right after they got extracted (and put in queue)?\n&gt;\n&gt; Thanks!\n&gt; -- Fede\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}