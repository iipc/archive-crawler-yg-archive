{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"kris@archive.org","from":"kris@...","profile":"kristsi25","replyTo":"LIST","senderId":"fk_Kgwdverbzb5c8MOb-YK5s4vy3H1R4SSydoKdIupE6gIbQJgEBvAvUOeG8244HcBypy1sRQw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] Advice needed on how to (properly) structure                new Heritrix modify and delete functionality","postDate":"1153231652","msgId":3078,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIyMzguMTMwLjIwOC4xNTIuODAuMTE1MzIzMTY1Mi5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg=="},"prevInTopic":3076,"nextInTopic":3080,"prevInTime":3077,"nextInTime":3079,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"See my thoughts below (I ve trimmed the message somewhat for clarity)... ... That doesn t sound too bad for snapshot crawls. The ARFrontier has never crawled","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 76835 invoked from network); 18 Jul 2006 14:07:33 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m35.grp.scd.yahoo.com with QMQP; 18 Jul 2006 14:07:33 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta7.grp.scd.yahoo.com with SMTP; 18 Jul 2006 14:07:33 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 4D54B14156B12\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Jul 2006 07:07:35 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 05295-01 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 18 Jul 2006 07:07:32 -0700 (PDT)\r\nReceived: from mail.archive.org (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id DB97614156D3D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Jul 2006 07:07:32 -0700 (PDT)\r\nReceived: from 130.208.152.80\n        (SquirrelMail authenticated user kris)\n        by mail.archive.org with HTTP;\n        Tue, 18 Jul 2006 07:07:32 -0700 (PDT)\r\nMessage-ID: &lt;2238.130.208.152.80.1153231652.squirrel@...&gt;\r\nDate: Tue, 18 Jul 2006 07:07:32 -0700 (PDT)\r\nTo: archive-crawler@yahoogroups.com\r\nUser-Agent: SquirrelMail/1.4.6\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nX-Priority: 3 (Normal)\r\nImportance: Normal\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: kris@...\r\nSubject: RE: [archive-crawler] Advice needed on how to (properly) structure \n               new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=163406187; y=IpL6WNgFfjscoK6WO-Y84dmMYOcRTwnVnmy6e2KBDCLgh2vl\r\nX-Yahoo-Profile: kristsi25\r\n\r\nSee my thoughts below (I&#39;ve trimmed the message somewhat for clarity)...\n\n&gt; &gt; Just to make sure we are on the same page I&#39;m going to\n&gt; reword your situation.\n&gt; &gt;\n&gt; &gt; You have an index of crawled content and you wish to update\n&gt; it in an\n&gt; &gt; incremental fashion.\n&gt; &gt;\n&gt; &gt; A question here; how long does it take to do a complete\n&gt; crawl? How big is it?\n&gt; &gt;\n&gt;\n&gt; This largely depends on our customer. Our current maximum is\n&gt; 7.5 million urls. The maximum is expected to be 20 million or more.\n\nThat doesn&#39;t sound too bad for snapshot crawls. The ARFrontier has never\ncrawled anything remotely that large.\n\n\n&gt; &gt;\n&gt; &gt; Incremental crawling on a truly large scale is bloody hard. As you\n&gt; &gt; note the BdbFrontier becomes very heavy during your crawls, yet it\n&gt; &gt; only needs to keep a small fingerprint of crawled URIs. An\n&gt; incremental\n&gt; &gt; frontier needs to keep rich objects for each URI (crawled\n&gt; or not) and\n&gt; &gt; is going to be accessing on-disk content far more randomly (read:\n&gt; &gt; higher access latency and decreased cache performance) then\n&gt; a snapshot frontier.\n&gt; &gt;\n&gt;\n&gt; I fully understand the tradeoffs. However, in general, crawl\n&gt; *rate* has never been a problem for our application; indeed\n&gt; we get dinged for crawling too fast much of the time (so we\n&gt; have had to adopt much more stringent politeness parameters\n&gt; than the defaults in many cases, and introduced burst\n&gt; throttling code as well).\n&gt;\n&gt; I can say this: Our customers are largely crawling intranets\n&gt; - very large intranets, but these *are* definitely\n&gt; constrained by domain. As a ballpark, I would say that the\n&gt; maximum number of domains we would care about would be about\n&gt; 5,000. That&#39;s still probably way too many, though, if you are\n&gt; keeping one queue per DB. What do you think?\n&gt;\n&gt; Based on your description, maybe the best approach is to\n&gt; analyze the behavior of ARF and try to optimize it for our\n&gt; particular usage pattern.\n\nThat is definitely an option. If Bdb still has the issues with multiple\ndatabases in a single environment, it would probably be worth looking at\nchanging that to be more like the BdbFrontier.\n\n\n&gt; &gt; I&#39;m going to suggest you take a look at the (still unreleased)\n&gt; &gt; DeDuplicator. A prerelease version can be found here:\n&gt; &gt; http://vefsofnun.bok.hi.is/deduplicator/.\n&gt; &gt; &lt;http://vefsofnun.bok.hi.is/deduplicator/.&gt;  The DeDuplicator was\n&gt; &gt; developed by me and has been tested at netarkivet.dk and is\n&gt; nearing a formal release.\n&gt; &gt;\n&gt; &gt; The idea behind it is simple. Perform a regular snapshot. Once\n&gt; &gt; completed create a deduplication index (Lucene is used for\n&gt; this). On\n&gt; &gt; subsequent crawls the deduplication index is used to\n&gt; discard duplicate\n&gt; &gt; data (duplicate detection is done by comparing the SHA-1 content\n&gt; &gt; digests that Heritrix&#39;s FetchHTTP processor creates).\n&gt; &gt;\n&gt;\n&gt; So, effectively, you are building a second index that you can\n&gt; query against for the SHA-1.\n\nRight. There are many advantages (for archival crawling) in this. In\nparticular we can constrain this index to just the files we are interested\nin (I&#39;m unsure if that would apply in any way to your work) and it allows\nus to use the BdbFrontier which is known to scale up to tens of millions\nof documents per crawl.\n\nThe only real downside to using such a secondary index for you would be\ndetecting when documents disappear. This isn&#39;t an issue for those of us\ndoing archival crawling, but you&#39;d want to know that a snapshot is no\nlonger running into a certain document. This would probably mean that your\nseedlist would have to be all successfully crawled documents from the last\ncrawl if using the same methodology as the DeDuplicator.\n\nThere is no &#39;smart&#39; comparison going on here. In fact there is nothing too\nterribly clever at all. The DeDuplicator was created as a way to\nsignificantly reduce the volume of duplicate data. In this it succeeds but\nsince duplicate detection is not perfect it is not 100% efficient. The\nroute it takes (creating a second index between crawls) is in\nacknowledgement of how difficult it is to build a decent incremental\nfrontier.\n\nIt is worth remembering that Heritrix (using BdbFrontier) tends to run\nreally fast initially and then gradually slow down. This is can be because\nof politeness kicking in as fewer sites remain, but is often in response\nto the BDB managed data structures growing larger and consequentially\nrequiring more I/O. Especially once the &#39;already included&#39; structure\nstarts overflowing to disk. Remember, it needs to be checked for each now\nURL extracted form every file.\n\nJust take my word for this, it is worth considering if you can achieve\nyour goals without writing a new frontier. Most likely by having some\nseparate index built between crawls that is consulted during the crawl by\ncustom processors. The DeDuplicator may or may not meet your needs, but\nsomething along those lines is an order of magnitude easier to write and\ndebug than a new high performance frontier.\n\n\n&gt; In our case it&#39;s perfectly acceptable to have the writer make\n&gt; the decision as to whether the document has &quot;changed&quot;. Is\n&gt; this possible?\n&gt; Does the old SHA-1 signature and the new one both appear\n&gt; available to the writer at that time?\n\nUsing the ARFrontier, yes, kind of. The ChangeEvaluator reads the old and\nnew hashes, based on them it makes its decision and records it in the\ncrawl URI. It then overwrites the &#39;old&#39; hash with the &#39;new&#39; one. This\nfunctionality could be handled by the writers but it would seem simpler to\nhave them simply pick up on the flag that the ChangeEvaluator sets.\n\nWith the DeDup the old hash is only available to classes that can access\nthe index.\n\n\n&gt; It sounds to me like you&#39;d be using the DeDuplicator to\n&gt; prefilter potential duplicates, which could save time spent\n&gt; extracting urls and queueing them and stuff, but may not be\n&gt; that significant if there were few changes.\n\nNot as such. For documents that may contain links it is very important not\nto filter them out before link extraction or we risk losing &#39;deeper&#39;\ncontent that may have changed (for example if we are crawling an index\npage it may never change but that doesn&#39;t mean that the pages it refers to\nremain unchanged).\n\n\n&gt; &gt; A large scale incremental frontier would definitely be\n&gt; interesting, but\n&gt; &gt; would also be hard to achieve for reasons cited above.\n&gt; &gt;\n&gt; &gt; If you decide to go this route I wish you all the luck in\n&gt; the world but I\n&gt; &gt; do note the following:\n&gt; &gt;\n&gt; &gt; 1. If you do not use some type of adaptive strategy to optimize your\n&gt; &gt; crawling you are no better off then doing repeated snapshot\n&gt; crawls with\n&gt; &gt; the DeDuplicator\n&gt;\n&gt; If the adaptive frontier has the ability to &quot;schedule&quot; recrawls of\n&gt; specific URI&#39;s then I think it&#39;s still a win, simply because\n&gt; you would\n&gt; not need to recrawl URI&#39;s very often that did not change frequently.\n&gt; Does it have a notion of scheduling, or is it built simply\n&gt; like a queue?\n\nIt is built as a priority queue. The WaitEvalutor adjusts an interval on\nthe CrawlURI depending on whether or not it has changed. This wait\ninterval is used by the ARF to decide when next to crawl the URI.\nScheduling directives override this to ensure prereqs are always fetched.\n\nBut yes, the whole idea of the ARFrontier and associated processors was to\nuse an adaptive revisiting strategy to only crawl URLs &#39;as needed&#39;. It\nnever really worked properly due to the fact that change detection is so\nsketchy. If it works for you then that is great.\n\n\n&gt; &gt; 2. If you do use some adaptive strategies the complexity of\n&gt; the frontier\n&gt; &gt; will go up making it harder to scale it to multimillion\n&gt; document crawls.\n&gt; &gt;\n&gt;\n&gt; Where do you see the scaling issues come in? Just due to\n&gt; random access vs. sequential?\n\nA snapshot frontier grows, meaning that initially there is less data and\nconsequentially more speed. For each iteration it restarts. An incremental\nfrontier grows and stays large. Future iterations are going to be slowed\nby the overall &#39;heavy&#39; frontier with its large data structures. One of the\nmain problems with the BdbFrontier right now is with the already included\ndata (as noted above). Once your are over 30 million documents\n_discovered_ I/O because of it becomes a problem. I believe work continues\non improving this but it serves to highlight the main problem of\nincremental frontiers; you&#39;ll always have that huge &#39;already&#39; included\nstructure!\n\n\n&gt; Do you see any reason that I shouldn&#39;t work on ARFrontier in-place\n&gt; (rather than creating a whole new frontier)?\n\nNone as such. What you are trying to achieve is pretty much in line with\nwhat the ARFrontier was meant to do. If you can get it to scale up that\nwould be great if we ever find a more reliable way of detecting change in\nHTML pages.\n\nBest,\nKris\n\n\n"}}