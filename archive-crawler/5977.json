{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"FQyJca-va76C2yvIpeLWdpIzFFDLVCvMLXvd9Xzxp7Aij6S8X831Fcl6O6NTay00ysgxykt52ULQHs9g7aX9IWC-IgBFQjORm7JjouEp","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: domain scope with millions of seeds","postDate":"1250122005","msgId":5977,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg1dmxlbCtwNWdtQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRBODMyNzEyLjcwNDAyMDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":5974,"nextInTopic":5984,"prevInTime":5976,"nextInTime":5978,"topicId":5969,"numMessagesInTopic":5,"msgSnippet":"... Gordon, Yes. This is exactly the kind of crawl we d like to do after years of using Heritrix. We knew that we can crawl as many pages as we want using","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 56909 invoked from network); 13 Aug 2009 00:07:28 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m3.grp.re1.yahoo.com with QMQP; 13 Aug 2009 00:07:28 -0000\r\nX-Received: from unknown (HELO n6-vm6.bullet.mail.sp2.yahoo.com) (67.195.135.102)\n  by mta1.grp.re1.yahoo.com with SMTP; 13 Aug 2009 00:07:28 -0000\r\nX-Received: from [67.195.134.239] by n6.bullet.mail.sp2.yahoo.com with NNFMP; 13 Aug 2009 00:06:47 -0000\r\nX-Received: from [69.147.65.150] by t4.bullet.mail.sp2.yahoo.com with NNFMP; 13 Aug 2009 00:06:47 -0000\r\nX-Received: from [98.137.35.12] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 13 Aug 2009 00:06:47 -0000\r\nDate: Thu, 13 Aug 2009 00:06:45 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h5vlel+p5gm@...&gt;\r\nIn-Reply-To: &lt;4A832712.7040205@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: domain scope with millions of seeds\r\nX-Yahoo-Group-Post: member; u=132996324; y=pvvm-spCzUs7iT5BIpwIgw86TBrKgzwIVViNPjFwBfAjXisZgg\r\nX-Yahoo-Profile: joehung302\r\n\r\n\n&gt; \n&gt; It seems your general goal is &quot;get a lot (or everything) from \n&gt; seed=\r\ns/distinguished-domains, a little from everything else&quot;.\n&gt; \n\nGordon,\n\nYes. =\r\nThis is exactly the kind of crawl we&#39;d like to do after years of using Heri=\r\ntrix.\n\nWe knew that we can crawl as many pages as we want using Heritrix. T=\r\noday we use 12 crawlers to crawl 6B pages and I can comfortably say that we=\r\n can linearly scale that model by adding more cralwer instances, up to 20B =\r\npages.\n\nBut the reality is every business has its own value-added and there=\r\n really need to be a way to justify the Internet data (6B pages &gt;=3D 60TB c=\r\nompressed archives, text only) becuase it costs a lot to maintain and proce=\r\nss.\n\nThat&#39;s why the strategy you mentioned,\n&quot;get a lot (or everything) from=\r\n seeds/distinguished-domains, a little from everything else&quot;.\nis important =\r\nfor a real business. The business would pick and maintain the seeds (many M=\r\nMs) and only reach out *a little* for the *relevant* information. For a ver=\r\ntical search application (like us), it is important to communicate which pa=\r\nrt of the Internet you provide because it doesn&#39;t really make sense to cove=\r\nr all Internet after all. A seed list with many MMs USLs is a good start.\n\n=\r\nIdeally the crawl strategy I&#39;d like to implement is:\n1) Be able to crawl *a=\r\nlmost* all pages from the domain derived from seeds.\n2) Add the one-level o=\r\nut *domains* (not just links) and put them into the scope (not seeds, other=\r\nwise it becomes broad crawl)\n\nWith this we can start to maintain a meaningf=\r\nul seedlist for our business and will be able to explain the value-added wi=\r\nth our search application.\n\nCheers,\n-Joe\n\n\n\n\n\n\n\n\n"}}