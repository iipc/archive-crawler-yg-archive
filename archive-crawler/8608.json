{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"4DBtI1xEXKlKeev1xwDhYPM1Cmn4ID1WGlJ-wdAUFodap_NQ2_YZEmi5sjW4nEEzHPqBHvGHXfCk-2L7fd04zUPDg14sxpE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] how to speed up crawls?","postDate":"1411061626","msgId":8608,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU0MUIxNzdBLjMwNjA4MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDU0MUFBQkRDLjUwMDAwMDZAZ214LmRlPg==","referencesHeader":"PDUzRjlENzMxLjUwMzA0MDlAZ214LmRlPiA8NTNGQUQ4NjAuNjAxMDIwN0BhcmNoaXZlLm9yZz4gPDUzRkFFOEZFLjkwOTA0MDRAZ214LmRlPiA8NTNGQkEyODcuMzAwMDMwN0BhcmNoaXZlLm9yZz4gPDUzRkMxMzQzLjUwODA5MDJAZ214LmRlPiA8NTQwODI2MzcuMTAxMDYwMUBnbXguZGU+IDw1NDFBQUJEQy41MDAwMDA2QGdteC5kZT4="},"prevInTopic":8607,"nextInTopic":0,"prevInTime":8607,"nextInTime":8609,"topicId":8589,"numMessagesInTopic":10,"msgSnippet":"Thanks for the followup! I d completely forgotten about that 1st-path-segment-based seeding of queue-assignment when parallelQueues  1, even though I see via","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 7860 invoked by uid 102); 18 Sep 2014 17:33:48 -0000\r\nX-Received: from unknown (HELO mtaq2.grp.bf1.yahoo.com) (10.193.84.33)\n  by m2.grp.bf1.yahoo.com with SMTP; 18 Sep 2014 17:33:48 -0000\r\nX-Received: (qmail 31143 invoked from network); 18 Sep 2014 17:33:48 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (98.139.170.167)\n  by mtaq2.grp.bf1.yahoo.com with SMTP; 18 Sep 2014 17:33:48 -0000\r\nX-Received: (qmail 13003 invoked by uid 0); 18 Sep 2014 17:33:47 -0000\r\nX-Received: from 50.1.86.69 (HELO probook.local) (50.1.86.69)\n  by relay00.pair.com with SMTP; 18 Sep 2014 17:33:47 -0000\r\nX-pair-Authenticated: 50.1.86.69\r\nMessage-ID: &lt;541B177A.3060805@...&gt;\r\nDate: Thu, 18 Sep 2014 10:33:46 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;53F9D731.5030409@...&gt; &lt;53FAD860.6010207@...&gt; &lt;53FAE8FE.9090404@...&gt; &lt;53FBA287.3000307@...&gt; &lt;53FC1343.5080902@...&gt; &lt;54082637.1010601@...&gt; &lt;541AABDC.5000006@...&gt;\r\nIn-Reply-To: &lt;541AABDC.5000006@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nSubject: Re: [archive-crawler] how to speed up crawls?\r\nX-Yahoo-Group-Post: member; u=137285340; y=xfzP7RWhLgYjzc9YuF_UxKQnQUkHekIxeqK7gBjz75U-\r\nX-Yahoo-Profile: gojomo\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\n\r\nThanks for the followup! I&#39;d completely forgotten about that \n1st-path-segment-based seeding of queue-assignment when parallelQueues &gt; \n1, even though I see via github&#39;s blame it was I who made that change in \n2009.\n\nI hope you found it easy to change, with either a local patch or your \nown subclass which overrides bucketBasis(). (And as a side reminder: \nit&#39;s even possible to add & use such subclasses via local script files, \nrather than new JARs, using the Spring IoC dynamic-languages support: \n&lt;http://docs.spring.io/spring/docs/3.2.11.RELEASE/spring-framework-reference/htmlsingle/#dynamic-language&gt;.)\n\n- Gordon\n\nOn 9/18/14, 2:54 AM, &#39;Markus.Mirsberger&#39; markus.mirsberger@... \n[archive-crawler] wrote:\n&gt; Hi Gordon,\n&gt;\n&gt;\n&gt; just want to let you know that I figured out what the problem was.\n&gt; The problem is that the parallelQueues are attached to the first\n&gt; path-segment which (at least in my cases) doesnt make any sense.\n&gt;\n&gt; For example I have a website with around 10Mio pages and about 9Mio of\n&gt; them are in the same first path-segment and this means in the same queue.\n&gt; Or if you try to crawl wikipedia. All articles are in the wiki path\n&gt; which means only 1 queue with this logic.\n&gt;\n&gt; I changed the method which returns the bucket-number so that it randomly\n&gt; selects a bucket and now everything runs as I expected it ;)\n&gt;\n&gt; Regards,\n&gt; Markus\n&gt;\n&gt; Am 04.09.2014 um 15:43 schrieb &#39;Markus.Mirsberger&#39;\n&gt; markus.mirsberger@... [archive-crawler]:\n&gt;&gt; Hi :)\n&gt;&gt;\n&gt;&gt; I still didnt find a solution to this problem.\n&gt;&gt; Can it be the problem that I am using Heritrix 3.1.1?\n&gt;&gt; Is it possible that JVM or server settings are causing Heritrix to use\n&gt;&gt; just one queue?\n&gt;&gt;\n&gt;&gt; so just in case this might be the cause.....here are my current JVM\n&gt;&gt; settings:\n&gt;&gt;\n&gt;&gt; export JAVA_OPTS=&quot;\n&gt;&gt; -Xms3072m\n&gt;&gt; -Xmx19000m\n&gt;&gt; -XX:NewSize=256m\n&gt;&gt; -XX:SurvivorRatio=1\n&gt;&gt; -XX:TargetSurvivorRatio=2\n&gt;&gt; -XX:MaxTenuringThreshold=3\n&gt;&gt; -XX:+UseConcMarkSweepGC\n&gt;&gt; -XX:+CMSScavengeBeforeRemark\n&gt;&gt; -XX:PretenureSizeThreshold=256m\n&gt;&gt; -XX:CMSFullGCsBeforeCompaction=1\n&gt;&gt; -XX:+UseCMSInitiatingOccupancyOnly\n&gt;&gt; -XX:CMSInitiatingOccupancyFraction=70\n&gt;&gt; -XX:CMSTriggerPermRatio=80\n&gt;&gt; -XX:CMSMaxAbortablePrecleanTime=6000\n&gt;&gt; -XX:+CMSConcurrentMTEnabled\n&gt;&gt; -XX:+UseParNewGC\n&gt;&gt; -XX:ConcGCThreads=4\n&gt;&gt; -XX:ParallelGCThreads=8&quot;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Thanks,\n&gt;&gt; Markus\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; On 26.08.2014 11:55, &#39;Markus.Mirsberger&#39;markus.mirsberger@...\n&gt;&gt; [archive-crawler] wrote:\n&gt;&gt;&gt; On 26.08.2014 03:54, Gordon Mohrgojomo@...  [archive-crawler] wrote:\n&gt;&gt;&gt;&gt; On 8/25/14, 12:42 AM, &#39;Markus.Mirsberger&#39;markus.mirsberger@...\n&gt;&gt;&gt;&gt; [archive-crawler] wrote:\n&gt;&gt;&gt;&gt;&gt; I already set parallelQueues to 20 before but this didn&#39;t increase the\n&gt;&gt;&gt;&gt;&gt; number of active queues/threads as you could see.\n&gt;&gt;&gt;&gt; Hmm. That should be the only change necessary: from that point on, if\n&gt;&gt;&gt;&gt; you&#39;re using one of the standard QueueAssignmentPolicies, any URIs\n&gt;&gt;&gt;&gt; discovered on a single site should be scattered over 20 queues instead\n&gt;&gt;&gt;&gt; of 1. Unlike your prior stats snapshot showing only 1 queue with\n&gt;&gt;&gt;&gt; millions of URIs, you&#39;d then see 20 queues.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Was &#39;parallelQueues&#39; set to a higher value from the very beginning of\n&gt;&gt;&gt;&gt; the crawl? Once a URI is sent to a queue, changing this setting\n&gt;&gt;&gt;&gt; mid-crawl doesn&#39;t cause any immediate change. The URI still needs to\n&gt;&gt;&gt;&gt; come off its original queue, one at a time. Then, depending on the\n&gt;&gt;&gt;&gt; &#39;deferToPrevious&#39; setting, it might get processed without reconsidering\n&gt;&gt;&gt;&gt; its queue (deferToPrevious=true), or be immediately re-evaluated\n&gt;&gt;&gt;&gt; (deferToPrevious=false) and thus potentially re-enqueued rather than\n&gt;&gt;&gt;&gt; fetched.\n&gt;&gt;&gt; Yes &#39;parallelQueues&#39; was set to 20 at the very beginning of the crawl.\n&gt;&gt;&gt; This is my basic setting in the profile Iam using for every Crawl.\n&gt;&gt;&gt; The same for &#39;deferToPrevious&#39; ... this was set to &#39;false&#39; at the beginning.\n&gt;&gt;&gt; Just to be sure this might not be the reason for my problem. I am still\n&gt;&gt;&gt; using Heritrix 3.1.1\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; What I did now was setting deferToPrevious to &quot;true&quot;. At least this\n&gt;&gt;&gt;&gt;&gt; increased the number of parallel threads to 2-3.\n&gt;&gt;&gt;&gt;&gt; But I still dont get more than 4kb/sec :(\n&gt;&gt;&gt;&gt; That sounds exactly backwards. If deferToPrevious=true, you&#39;ll get\n&gt;&gt;&gt;&gt; *less* reassignment-among-queues and thus less long-run speedup. Only\n&gt;&gt;&gt;&gt; new URIs being discovered will get spread over 20 queues.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Note, though, that with deferToPrevious=false, and changing\n&gt;&gt;&gt;&gt; parallelQueues mid-crawl, there is often then a tight busy-loop as\n&gt;&gt;&gt;&gt; 19-out-of-each-20 URIs on the original single queue pop off and *don&#39;t*\n&gt;&gt;&gt;&gt; get crawled, but instead get reenqueued over other queues. This might\n&gt;&gt;&gt;&gt; stick out as a period of higher CPU and local IO usage, without much (or\n&gt;&gt;&gt;&gt; any) fetch speedup. However, as the giant queue shrinks rapidly, and\n&gt;&gt;&gt;&gt; other queues grow, you should then start to see a higher request rate\n&gt;&gt;&gt;&gt; over time, approaching the 20x speedup (if there aren&#39;t other CPU/IO\n&gt;&gt;&gt;&gt; bottlenecks).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; So you&#39;d want to give the deferToPrevious=false setting some time to\n&gt;&gt;&gt;&gt; have its effect on queue-distribution.\n&gt;&gt;&gt; As I mentioned in the comment before ... deferToPrevious was set to\n&gt;&gt;&gt; false at the beginning of the crawl and I ended up with only one queue.\n&gt;&gt;&gt;&gt;&gt; I think it is not necessary to use sheets in my case. I crawl every\n&gt;&gt;&gt;&gt;&gt; domain in an own crawljob and dont allow the job to crawl other domains.\n&gt;&gt;&gt;&gt; There were 14 other empty queues in your stats cut-and-paste... just be\n&gt;&gt;&gt;&gt; aware when rerunning this crawl, if a global parallelQueues=20 setting\n&gt;&gt;&gt;&gt; is in effect, they&#39;ll all be equally hammered.\n&gt;&gt;&gt; Yes I am aware this. Every Job is crawling only one single domain by\n&gt;&gt;&gt; request of the domain&#39;s owner. So the speed of the crawl will be\n&gt;&gt;&gt; adjusted in agreement with the owner.\n&gt;&gt;&gt; And both the crawling server and the server with the domain on it are\n&gt;&gt;&gt; almost sleeping and definitly not the bottlenecks here ... maybe if I\n&gt;&gt;&gt; ever make all the queues really running parallel :)\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks,\n&gt;&gt;&gt; Markus\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - Gordon\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Regards,\n&gt;&gt;&gt;&gt;&gt; Markus\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; On 25.08.2014 13:32, Gordon Mohrgojomo@...  [archive-crawler] wrote:\n&gt;&gt;&gt;&gt;&gt;&gt; The core politeness assumptions of Heritrix have been that URIs for a\n&gt;&gt;&gt;&gt;&gt;&gt; single &#39;site&#39; should go into a single logical queue, and that only one\n&gt;&gt;&gt;&gt;&gt;&gt; URI should be in process from any one queue at a time.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; That&#39;s what you&#39;re running into, even after minimizing the other\n&gt;&gt;&gt;&gt;&gt;&gt; politeness delays between fetches.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; If you&#39;re sure that more-aggressive crawling is alright (as seems to be\n&gt;&gt;&gt;&gt;&gt;&gt; the case with your target site with owner permission), you can cause the\n&gt;&gt;&gt;&gt;&gt;&gt; usual queue-assignment to instead distribute a single site&#39;s URIs over\n&gt;&gt;&gt;&gt;&gt;&gt; many related queues. Then, if (for example) using 5 queues, 5 requests\n&gt;&gt;&gt;&gt;&gt;&gt; can be in process in parallel, resulting in up to 5X faster crawling.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; The relevant setting is &#39;parallelQueues&#39; on the QueueAssignmentPolicy.\n&gt;&gt;&gt;&gt;&gt;&gt; See some notes here:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Crawl+Operators#H3DevNotesforCrawlOperators-QueueAssignmentPolicies:%27parallelQueues%27and%27deferToPrevious%27settings\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; As it&#39;s usually the case that you want most sites to be crawled in the\n&gt;&gt;&gt;&gt;&gt;&gt; normal polite manner, even when there are one or more sites that can be\n&gt;&gt;&gt;&gt;&gt;&gt; crawled more aggressively, it often makes sense to only set a higher\n&gt;&gt;&gt;&gt;&gt;&gt; &#39;parallelQueues&#39; value in a &quot;sheet override&quot; to affect some subset of\n&gt;&gt;&gt;&gt;&gt;&gt; sites. Thus, that&#39;s one of the example overridden settings in the sheets\n&gt;&gt;&gt;&gt;&gt;&gt; example at:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Sheets\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; - Gordon\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; On 8/24/14, 5:14 AM, &#39;Markus.Mirsberger&#39;markus.mirsberger@...\n&gt;&gt;&gt;&gt;&gt;&gt; [archive-crawler] wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; are there any ways to speed up an active crawl?\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; I am always crawling only one single domain and I was told by the owner\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; that it is ok when I crawl it with up to 10Url/sec. As You can see in\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; the pasted jobdata ... a little bit more than 1 Url/sec is all I can get.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; I tried already several things like rising the amount of queues and\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; threads, shorten the breaks between the requests but nothing really works.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; I also wonder that there is only one active thread and also only one\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; queue in use.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Do you have any suggestions where in the config I can turn on the turbo?:)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks in advance,\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Markus\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           Job is Active: RUNNING\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Totals\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           738743 downloaded + 5950664 queued = 6689408 total\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           44 GiB crawled (44 GiB novel, 0 B dupByHash, 0 B notModified)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Alerts\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           /none/\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Rates\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           1.16 URIs/sec (2.11 avg); 84 KB/sec (131 avg)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Load\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           1 active of 100 threads; 1 congestion ratio; 5950664 deepest queue;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           5950664 average depth\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Elapsed\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           4d1h7m37s131ms\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Threads\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           100 threads: 99 ABOUT_TO_GET_URI, 1 ABOUT_TO_BEGIN_PROCESSOR; 99\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           noActiveProcessor, 1 extractorHtml\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; *Frontier*\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           RUN - 15 URI queues: 1 active (1 in-process; 0 ready; 0 snoozed); 0\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           inactive; 0 ineligible; 0 retired; 14 exhausted\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Memory\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;           10369079 KiB used; 15480904 KiB current heap; 16270464 KiB max heap\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Yahoo Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Yahoo Groups Links\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Yahoo Groups Links\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; Yahoo Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}