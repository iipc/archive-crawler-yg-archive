{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":267551907,"authorName":"anand_akela","from":"&quot;anand_akela&quot; &lt;anand.akela@...&gt;","profile":"anand_akela","replyTo":"LIST","senderId":"qYlroEwyIhR7fP764T93VBlJL2idpldfUjpeRT3pg1aTQGbC1V7gBdw4hkiawg1zH1OgESomJVFV4ghSaFtraf5G9zHvd7NE1ktqPaIU3QGL","spamInfo":{"isSpam":false,"reason":"6"},"subject":"PLEASE IGNORE: Has anyone tried Libarc library on Suse Linux?","postDate":"1149315913","msgId":2908,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU1cmEwOSs2ODRrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGU1cWJ1NitocHNxQGVHcm91cHMuY29tPg=="},"prevInTopic":2907,"nextInTopic":0,"prevInTime":2907,"nextInTime":2909,"topicId":1286,"numMessagesInTopic":8,"msgSnippet":"I found the problem, hacked the misc.h and was able to compile and run the arcdump utility. Somehow strerror_r was returning a char*  , but configure script","rawEmail":"Return-Path: &lt;anand.akela@...&gt;\r\nX-Sender: anand.akela@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 59543 invoked from network); 3 Jun 2006 06:25:18 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m27.grp.scd.yahoo.com with QMQP; 3 Jun 2006 06:25:18 -0000\r\nReceived: from unknown (HELO n14c.bullet.sc5.yahoo.com) (66.163.187.205)\n  by mta5.grp.scd.yahoo.com with SMTP; 3 Jun 2006 06:25:18 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.163.187.121] by n14.bullet.sc5.yahoo.com with NNFMP; 03 Jun 2006 06:25:13 -0000\r\nReceived: from [66.218.69.2] by t2.bullet.sc5.yahoo.com with NNFMP; 03 Jun 2006 06:25:13 -0000\r\nReceived: from [66.218.66.84] by t2.bullet.scd.yahoo.com with NNFMP; 03 Jun 2006 06:25:13 -0000\r\nDate: Sat, 03 Jun 2006 06:25:13 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;e5ra09+684k@...&gt;\r\nIn-Reply-To: &lt;e5qbu6+hpsq@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;anand_akela&quot; &lt;anand.akela@...&gt;\r\nSubject: PLEASE IGNORE: Has anyone tried Libarc library on Suse Linux?\r\nX-Yahoo-Group-Post: member; u=267551907; y=LxmtYILP9VlswrPnRUCQURRvuQ9Oat0TAB7Y5eZ0u_v_YYDukvw\r\nX-Yahoo-Profile: anand_akela\r\n\r\nI found the problem, hacked the misc.h and was able to compile and run\nthe =\r\narcdump utility.\n\nSomehow strerror_r was returning a char*  , but configure=\r\n script was\ntreat as if strerror_r was returning inteager. This was resulti=\r\nng in\nfollowing error :-\n\nmake  all-recursive\nmake[1]: Entering directory `=\r\n/home/aakela/heritrix-1.8.0/libarc-0.2.0&#39;\nMaking all in src\nmake[2]: Enteri=\r\nng directory `/home/aakela/heritrix-1.8.0/libarc-0.2.0/src&#39;\nsource=3D&#39;arc_f=\r\nile.cpp&#39; object=3D&#39;arc_file.o&#39; libtool=3Dno &#92;\ndepfile=3D&#39;.deps/arc_file.Po&#39;=\r\n tmpdepfile=3D&#39;.deps/arc_file.TPo&#39; &#92;\ndepmode=3Dgcc3 /bin/sh ../depcomp &#92;\ng+=\r\n+ -DHAVE_CONFIG_H -I. -I. -I..   -I/usr/include  -g -O2 -c -o\narc_file.o `t=\r\nest -f &#39;arc_file.cpp&#39; || echo &#39;./&#39;`arc_file.cpp\nmisc.h: In constructor `lib=\r\narc::errno_exception::errno_exception(int)&#39;:\nmisc.h:33: error: invalid conv=\r\nersion from `char*&#39; to `int&#39;\nmake[2]: *** [arc_file.o] Error 1\nmake[2]: Lea=\r\nving directory `/home/aakela/heritrix-1.8.0/libarc-0.2.0/src&#39;\nmake[1]: *** =\r\n[all-recursive] Error 1\nmake[1]: Leaving directory `/home/aakela/heritrix-1=\r\n.8.0/libarc-0.2.0&#39;\nmake: *** [all] Error 2\n\n\n--- In archive-crawler@yahoogr=\r\noups.com, &quot;anand_akela&quot;\n&lt;anand.akela@...&gt; wrote:\n&gt;\n&gt; Has anyone tried Libar=\r\nc library on Suse Linux? I am getting errors\n&gt; when I am trying to make the=\r\n library. I am interested in arcdump\n&gt; utility and wondering if anyone has =\r\nbuilt and tried on Suse Linux\n&gt; recently.\n&gt; \n&gt; Thanks\n&gt; \n&gt; --- In archive-c=\r\nrawler@yahoogroups.com, Tom Emerson &lt;Tree@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Marco Baroni wri=\r\ntes:\n&gt; &gt; &gt; As a linguist, I have been interested in downloading large\namoun=\r\nts of \n&gt; &gt; &gt; text from the web for various forms of statistical analyses.\n&gt;=\r\n &gt; \n&gt; &gt; Indeed: we&#39;re using Heritrix for exactly this purpose.\n&gt; &gt; \n&gt; &gt; &gt; N=\r\now, as I should get some funds to buy a few dedicated servers, I \n&gt; &gt; &gt; wou=\r\nld like to get started with some more ambitious crawl, and\n&gt; heritrix \n&gt; &gt; =\r\n&gt; looks like the ideal choice.\n&gt; &gt; \n&gt; &gt; It is, in the latest releases. And =\r\nyou can do not necessarily need\n&gt; &gt; significant hardware to run your crawls=\r\n, though post-processing will\n&gt; &gt; be dependent on local processing power.\n&gt;=\r\n &gt; \n&gt; &gt; &gt; 1) I understand that there is a way to stop fetching documents\n&gt; =\r\nthat do \n&gt; &gt; &gt; not match certain content-types with a filter. Is there a wa=\r\ny to\n&gt; insert \n&gt; &gt; &gt; such a filter via the standard WUI?\n&gt; &gt; \n&gt; &gt; Yes, you =\r\ncan specify filters at various stages of the process from\n&gt; &gt; within the WU=\r\nI. In my data crawls I define three different filters in\n&gt; &gt; the &quot;Filters&quot; =\r\npage:\n&gt; &gt; \n&gt; &gt; 1. crawl-order:scope:exclude-filter:filters\n&gt; &gt;       URIReg=\r\nExpFilter\n&gt; &gt; \n&gt; &gt; 2. fetch-processors:HTTP:midfetch-filters\n&gt; &gt;       Cont=\r\nentTypeRegExpFilter\n&gt; &gt; \n&gt; &gt; 3. write-processors:Archiver:filters\n&gt; &gt;      =\r\n ContentTypeRegExpFilter\n&gt; &gt; \n&gt; &gt; The first is configured to restrict the s=\r\net of file extensions that\n&gt; &gt; are treated as being in-scope:\n&gt; &gt; \n&gt; &gt;\n&gt;\n.*=\r\n(?i)&#92;.(a|ai|aif|aifc|aiff|asc|au|avi|bcpio|bin|bmp|bz2|c|cdf|cgi|cgm|class|=\r\ncpio|cpp?|cpt|csh|css|cxx|dcr|dif|dir|djv|djvu|dll|dmg|dms|doc|dtd|dv|dvi|d=\r\nxr|eps|etx|exe|ez|gif|gram|grxml|gtar|h|hdf|hqx|ice|ico|ics|ief|ifb|iges|ig=\r\ns|iso|jnlp|jp2|jpe|jpeg|jpg|js|kar|latex|lha|lzh|m3u|mac|man|mathml|me|mesh=\r\n|mid|midi|mif|mov|movie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|nc|o|oda|o=\r\ngg|pbm|pct|pdb|pdf|pgm|pgn|pic|pict|pl|png|pnm|pnt|pntg|ppm|ppt|ps|py|qt|qt=\r\ni|qtif|ra|ram|ras|rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|sgml|sh|shar|silo|sit|s=\r\nkd|skm|skp|skt|smi|smil|snd|so|spl|src|srpm|sv4cpio|sv4crc|svg|swf|t|tar|tc=\r\nl|tex|texi|texinfo|tgz|tif|tiff|tr|tsv|ustar|vcd|vrml|vxml|wav|wbmp|wbxml|w=\r\nml|wmlc|wmls|wmlsc|wrl|xbm|xht|xhtml|xls|xml|xpm|xsl|xslt|xwd|xyz|z|zip)$\n&gt;=\r\n &gt; \n&gt; &gt; Note that this excludes &quot;.au&quot;, which can be problematic if you are\n=\r\n&gt; &gt; fetching documents from domains in Australia, since the DNS request\n&gt; &gt;=\r\n gets filtered and this is a required prerequisite, so the entire site\n&gt; &gt; =\r\ngets filtered.\n&gt; &gt; \n&gt; &gt; The second and third have the same RegExp, and rest=\r\nrict the content to\n&gt; &gt; text/html:\n&gt; &gt; \n&gt; &gt; (?i)text/html.*\n&gt; &gt; \n&gt; &gt; If you=\r\n don&#39;t include the write-processor filter then you will end up\n&gt; &gt; with an =\r\nARC file containing partial content interrupted by the\n&gt; &gt; mid-fetch filter=\r\n.\n&gt; &gt; \n&gt; &gt; If you want to download and save other content types then you wi=\r\nll\n&gt; &gt; obviously need to modify these regular expressions appropriately.\n&gt; =\r\n&gt; \n&gt; &gt; &gt; 2) Similarly, is it possible to restrict the documents to be\n&gt; wri=\r\ntten to \n&gt; &gt; &gt; the arc files to a certain maximum and minimum size?\n&gt; &gt; \n&gt; =\r\n&gt; No, I don&#39;t believe so. It would be pretty easy to write a new type of\n&gt; =\r\n&gt; filter and add it to the write-processor. You would not want to do\n&gt; &gt; th=\r\nis as a mid-fetch filter though, because not all servers include the\n&gt; &gt; Co=\r\nntent-Length: header.\n&gt; &gt; \n&gt; &gt; I filter on this during post-processing: at =\r\nthat point you know both\n&gt; &gt; the size of the returned markup and, after rem=\r\noving the markup, the\n&gt; &gt; size of the remaining data. I limit my saved data=\r\n to the extracted\n&gt; &gt; size, not the size including markup since I&#39;ve found =\r\nthat in the\n&gt; &gt; contemporary web the markup absolutely dwarfs the content i=\r\nn many\n&gt; &gt; cases.\n&gt; &gt; \n&gt; &gt; &gt; 3) In order to extract pure text from the docu=\r\nments, I plan to\nwrite \n&gt; &gt; &gt; scripts that invoke arcdump to get file type =\r\nand contents out of\nthe \n&gt; &gt; &gt; arcs, and then call appropriate tools such a=\r\ns pdftotex.\nActually, for \n&gt; &gt; &gt; now I would be quite happy just extracting=\r\n text from the html\nfiles. \n&gt; &gt; &gt; Before I re-invent the wheel, is there al=\r\nready some tool/module\nto do \n&gt; &gt; &gt; this?\n&gt; &gt; \n&gt; &gt; I&#39;ve written this a few =\r\ntimes, using my libarc C++ library. Currently\n&gt; &gt; I strip markup with extre=\r\nme prejudice, run a language/content\n&gt; &gt; identifier on the remainder, trans=\r\ncode the original to UTF-8 from the\n&gt; &gt; detected encoding, then dump the HT=\r\nML (*not* the extracted text) to\n&gt; &gt; disk that is within my size constraint=\r\ns. At the end of this phase I\n&gt; &gt; have hundreds of thousands to millions of=\r\n HTML files that fit my size\n&gt; &gt; and language constraints. For my work I ne=\r\ned to preserve as much\n&gt; &gt; logical structure in the original texts as possi=\r\nble, so simple HTML\n&gt; &gt; stripping does not work for me. Instead I use an op=\r\nen source tool\n&gt; &gt; called Vilistextum to extract the text from the HTML in =\r\nall of the\n&gt; &gt; extracted files.\n&gt; &gt; \n&gt; &gt; Once that is done we have lots of =\r\nplain text to use for various\n&gt; &gt; linguistics work. By way of example I&#39;ve =\r\nused this technique to\n&gt; &gt; collect a corpus of over 2 GB of plain (i.e., po=\r\nst-processed)\n&gt; &gt; Traditional Chinese text, and the crawl is still going.\n&gt;=\r\n &gt; \n&gt; &gt; We&#39;ve done the same for about 200-300 MB of post-processed French,\n=\r\n&gt; &gt; Italian, Vietnamese, and Tagalog (well, an order or magnitude less\n&gt; &gt; =\r\nTagalog).\n&gt; &gt; \n&gt; &gt; &gt; 4) Am I right in thinking that heritrix will not downl=\r\noad the same\n&gt; url \n&gt; &gt; &gt; twice (within the same crawling job)?\n&gt; &gt; \n&gt; &gt; Ye=\r\ns.\n&gt; &gt; \n&gt; &gt; This scratches the surface. For the Tagalog and Vietnamese craw=\r\nls we\n&gt; &gt; seeded using techniques described in your BootCat papers, and we&#39;=\r\nve\n&gt; &gt; thought about how to integrate BootCat type processes into Heritrix\n=\r\n&gt; &gt; proper, but for now we do it separately using some Java apps utilizing\n=\r\n&gt; &gt; the Google API and our language identifier technology.\n&gt; &gt; \n&gt; &gt;     -tr=\r\nee\n&gt; &gt; \n&gt; &gt; -- \n&gt; &gt; Tom Emerson                                          Ba=\r\nsis\n&gt; Technology Corp.\n&gt; &gt; Software Architect                              =\r\n  \n&gt; http://www.basistech.com\n&gt; &gt;   &quot;Beware the lollipop of mediocrity: lic=\r\nk it once and you suck\nforever&quot;\n&gt; &gt;\n&gt;\n\n\n\n\n\n\n"}}