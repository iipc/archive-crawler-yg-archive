{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"h9DWQQ9d2bZ3Wi3yOqBwrTPjfukEIO86aBgNrlBuCfAkwFWN8Z7l7tohqOYxMqi-3NqWQ19ntiHdw4jGDzlE9lULBQL_rJE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Notes about recent major CVS commits","postDate":"1068063961","msgId":163,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNGQTk1Q0Q5LjgwNjAzMDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":0,"nextInTopic":0,"prevInTime":162,"nextInTime":164,"topicId":163,"numMessagesInTopic":1,"msgSnippet":"Here s a guide to the major areas of change that were checked into CVS the other day. A number of things will initially be slower and somewhat flaky after","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 45056 invoked from network); 5 Nov 2003 20:26:02 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m15.grp.scd.yahoo.com with QMQP; 5 Nov 2003 20:26:02 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 5 Nov 2003 20:26:02 -0000\r\nReceived: (qmail 20092 invoked by uid 100); 5 Nov 2003 20:19:53 -0000\r\nReceived: from b116-dyn-42.archive.org (HELO archive.org) (gojomo@...@209.237.240.42)\n  by mail-dev.archive.org with SMTP; 5 Nov 2003 20:19:53 -0000\r\nMessage-ID: &lt;3FA95CD9.8060307@...&gt;\r\nDate: Wed, 05 Nov 2003 12:26:01 -0800\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.5) Gecko/20031013 Thunderbird/0.3\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nSubject: Notes about recent major CVS commits\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-4.5 required=6.0\n\ttests=AWL,BAYES_01,USER_AGENT_MOZILLA_UA\n\tversion=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nHere&#39;s a guide to the major areas of change that were checked into\nCVS the other day.\n\nA number of things will initially be slower and somewhat flaky\nafter these commits, but major areas of inconsistency and complexity\nhave been improved.\n\nAlso, the initial disk-based crawl-state data structures can begin\nto be tested/used for supporting indefinite crawl jobs, limited only\nby disk space. The largest &quot;pending&quot; queue is now always disk-based;\nthe alreadyIncluded lookup table can be switched to use backing disk\nby editting the Frontier.initialize() method. (Preallocating a large\ndisk-based table adds a significant startup delay.)\n\nComments/questions sought.\n\nMajor areas of change:\n\n(1) Schedule/Store/Selector -&gt; Frontier\n\n     These classes -- the Scheduler, Store, and Selector -- were\n     very tightly coupled anyway, and bled functionality into\n     each other. Trying to keep them as individually swappable\n     components was too ambitious, at least for now. By aggregating\n     them into a single Frontier, we move more towards the Mercator\n     way of doing things.\n\n     However, the &quot;Selector&quot; functionality for determining\n     which discovered URIs should be crawled does still separate\n     out nicely; see below about CrawlScope and Postselector.\n\n     The Frontier now also completely internalizes any\n     timing-related politeness settings. (Previously, the idea\n     was that one of the swappable modules decided these, but then\n     hinted them to the Frontier for enforcement.)\n\n     The two most memory-intensive portions of the Frontier --\n     the queue(s) of pending URIs, and the &quot;alreadyIncluded&quot;\n     structure to prevent the same URI from being twice\n     instantiated/scheduled -- now can be provided by data\n     structures which use disk-backing once a fixed amount\n     of memory is used. See especially the...\n\n         org.archive.util.DiskBackedQueue\n         org.archive.util.CachingDiskLongFPSet\n\n     ...classes, and their superclasses and siblings. These are\n     still lightly tested and unoptimized.\n\n     The per-host queues which prevent more than one outstanding\n     request to any one host have not yet been diskified, but\n     that should be straightforward.  The CrawlServer cache, of\n     server & host specific values, has not yet been diskified,\n     and will require a different approach.\n\n(2) Introduction of a &quot;master&quot; thread\n\n     Previously, a number of worker ToeThreads are spawned, and\n     they each ask the Frontier (Scheduler/Store/Selector)\n     for a CrawlURI to tackle. To do this, they acquire a lock\n     on a critical section of code which hands out CrawlURIs.\n     When nothing was ready, many threads would all wait for\n     a signal that something becomes ready, and then an arbitrary\n     one will win the next item. When it is done, it must\n     acquire a lock on the critical code for inserting its\n     results back into the Frontier datastructures and logs.\n\n     It would be easier to understand this crucial handoff\n     process if these critical sections of code only ever run in\n     a distinguished master thread. This also opens the potential\n     (but not yet the implementation) that the interfaces between\n     the Frontier and ToeThreads can be queues.\n\n     Additionally, this has made it easier to preference\n     certain threads with work -- for example, always assign\n     work to the lowest-numbered available thread. This makes\n     it easier to collect interpretable information on thread\n     load over time (such as maximum threads in action at any\n     one time).\n\n     Currently, when running without a web UI, the &quot;main&quot; thread\n     that launches the program passes into the CrawlController\n     and becomes this master thread -- though under administrative\n     control, other explicitly created threads could fill this\n     role in the future.\n\n(3) Return of CrawlScope\n\n     The &quot;CrawlScope&quot; class, which had gone away, is back as\n     a place to centralize all the inclusion/exclusion tests\n     which are applied to define which URIs get crawled, and\n     which do not. Previously, the role of defining what\n     was icnluded was as filters on the &quot;Selector&quot; module. By\n     having this be a distinct object, it can be more easily\n     reused at several stages, such as both before URIs are\n     queued inside the Frontier, and again after they are\n     released -- in case the operator has updated the scope\n     in the meantime. It may also be easier (though not simple)\n     to design sophisticated experimental scopes, such as those\n     which decide whether to crawl in a certain direction based\n     on page content.\n\n     A Scope in the &#39;basic&#39; package implements a generic\n     set of inclusion/exclusion rules that should be sufficient\n     for 90%+ of our crawls. See its class comment for details.\n\n(4) New Processor modules, and cancelFurtherProcessing() change\n\n     A Preselector processor now optionally reapplies the scope\n     to URIs each time their processing begins. A new Postselector\n     processor looks at all the URIs that previous modules have\n     extracted, and decides which should be handed to the Frontier.\n\n     Previously, when one Processor needed to prevent further\n     normal processing of a URI (for example, because it failed\n     a necessary test), it used CrawlURI.cancelFurtherProcessing(),\n     which caused the CrawlURI to skip forward over all other\n     Processors to being re-inserted into the Frontier. In most\n     cases, however, there are &quot;cleanup and maintenance&quot; Processors\n     which should run even in these cases. So now, a skipToProcessor()\n     is used instead, consulting the CrawlController for the\n     distinguished &quot;Postprocessor&quot; used for such skip-aheads.\n     (In the default arrangement of Processors, the CrawlStateUpdater\n     is the postprocessor, and this is set via a defined attribute\n     in the order.xml.)\n\n(5) Logging/debugging updates:\n\n     The &quot;uri-processing&quot; log is now simply &quot;crawl.log&quot;, as much\n     to avoid starting with the same letter as &quot;uri-errors&quot; as\n     any other reason. This log now contains 2 lines of info\n     per URI -- the second line showing the symbolic path to this\n     URI from a seed, as a series of single-letters describing\n     the hop-type and then the immediate-precursor URI. For example:\n\n20031030164828155     1         63 #7 dns:www.washingtonpost.com text/dns\n   LPRP http://www.washingtonpost.com/robots.txt\n\n     The &quot;LPRP&quot; indicates a link-precondition-referral-precondition\n     path to this URI, most recently from the robots.txt URI\n     shown. Pathological patterns stick out, and in fact it is\n     by looking at these symbolic path strings that the transitive\n     inclusion filter decides how many hops out to seek included-\n     by-reference resources.\n\n     (This information may be folded into a single line in the\n     future, but for now you can [ grep -v &quot;^ &quot; ] to strip these\n     lines from crawl.log when unwanted. The other logs follow\n     this same convention, with any lines beginning with whitespace\n     optional additional information about the preceding regular\n     log line.)\n\n     The &quot;crawl-errors&quot; log is now more accurate named\n     &quot;runtime-errors&quot;, since it collect unexpected runtime\n     exceptions. A new &quot;local-errors&quot; log has been added, to\n     capture Processor-local exceptions, as for example when an\n     Extractor module catches its own semi-expected errors and\n     wants to log a stack trace.\n\n     The &quot;progress-statistics&quot; log has been compressed to only\n     show columns we are using (or will imminently use). Some of\n     the values in this lag are still dubious, not quite properly\n     calculated.\n\n(6) Order.xml settings scatter has been minimized\n\n     XML Element attributes are used whenever possible, at as high\n     a level as is practical, giving the shortest possible XPaths\n     to configuration values.\n\n(7) Giant seed lists better supported\n\n     For crawls of type &quot;broad&quot;, the seed input file is read as\n     needed via a custom iterator, rather than always being brought\n     entirely into memory. In practice, the whole file is iterated\n     over once, copying seeds over to the disk-based pending queues.\n     This should enable multi-million seed crawls in small fixed\n     running memory.\n\n     Focused crawls -- limited to a medium-sized set of URI-prefixes\n     or hosts/domains -- still require the seed/host list to be in\n     memory.\n\n(8) HTTP aborts on elapsed time, total length implemented\n\n     Via the RecordingInputStream facility, which is inserted\n     into the HTTPClient socket-communication streams, maximum\n     thresholds of elapsed time and total length can be enforced.\n     See FetchHTTP.innerProcess() for an example of how it&#39;s set\n     up (and exceeded thresholds are caught).\n\n     Because it is harder to adjust the way the HTTPClient\n     library reads the initial response-line and headers, and\n     the HTTPClient library currently uses an awkward way to\n     handle connection timeouts, proper behavior on this initial\n     communication isn&#39;t yet implemented: timeouts and limits\n     only apply to the response content-body.\n\n(9) Lots of other renaming and relocation\n\n     All in the interest of having smaller, more clearly-related\n     packages and clearer/up-to-date names.\n\n- Gordon\n\n\n\n"}}