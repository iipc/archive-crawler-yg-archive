{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"01KKLTwATN9Qov7IqVvZ7aNg7DXYXWx1g5Z3-e3hJqM5_s970wfG19v8DsB9YN2D-X-FRdz8VnSYe4VnwV1DEA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Best approach to 7M seeds","postDate":"1126032750","msgId":2170,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzMURFNTZFLjcwNTA5MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDlkMWU0NTI1MDUwOTA2MTEyNzEzMGI4MmY4QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PDlkMWU0NTI1MDUwODE3MTAyMzQ4Y2Q2MmE4QG1haWwuZ21haWwuY29tPgkgPDlkMWU0NTI1MDUwOTAyMTI1ODQ5ZmQzNzNhQG1haWwuZ21haWwuY29tPgkgPDE3MTc2LjQ1MTc1LjkyOTMwNy45Mzg5MTdAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4JIDw5ZDFlNDUyNTA1MDkwMjEzMjI1Njc4MzRiN0BtYWlsLmdtYWlsLmNvbT4JIDw0MzE4QzEwQS4xMDMwNTA1QGFyY2hpdmUub3JnPgkgPDlkMWU0NTI1MDUwOTAyMTUwNDc1NmI0ZDQ1QG1haWwuZ21haWwuY29tPgkgPDQzMThENTYwLjEwMjA2MDVAYXJjaGl2ZS5vcmc+CSA8OWQxZTQ1MjUwNTA5MDIxNjE2MzE5YTNAbWFpbC5nbWFpbC5jb20+CSA8NDMxOEUzQzYuOTA4MDMwOUBhcmNoaXZlLm9yZz4JIDw5ZDFlNDUyNTA1MDkwMjE5MTg3N2Y2YmM5Y0BtYWlsLmdtYWlsLmNvbT4gPDlkMWU0NTI1MDUwOTA2MTEyNzEzMGI4MmY4QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2169,"nextInTopic":2171,"prevInTime":2169,"nextInTime":2171,"topicId":2116,"numMessagesInTopic":25,"msgSnippet":"... heritrix_out.log (The file that captures all errant stdout/stderr emissions). ... That is a small number of downloads.  I m guessing whats killing you is ","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 10136 invoked from network); 6 Sep 2005 19:01:23 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m26.grp.scd.yahoo.com with QMQP; 6 Sep 2005 19:01:23 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta4.grp.scd.yahoo.com with SMTP; 6 Sep 2005 19:01:23 -0000\r\nReceived: (qmail 11602 invoked by uid 100); 6 Sep 2005 18:31:18 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.8?) (stack@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 6 Sep 2005 18:31:18 -0000\r\nMessage-ID: &lt;431DE56E.7050907@...&gt;\r\nDate: Tue, 06 Sep 2005 11:52:30 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.8) Gecko/20050513 Debian/1.7.8-1\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;9d1e4525050817102348cd62a8@...&gt;\t &lt;9d1e4525050902125849fd373a@...&gt;\t &lt;17176.45175.929307.938917@...&gt;\t &lt;9d1e45250509021322567834b7@...&gt;\t &lt;4318C10A.1030505@...&gt;\t &lt;9d1e45250509021504756b4d45@...&gt;\t &lt;4318D560.1020605@...&gt;\t &lt;9d1e45250509021616319a3@...&gt;\t &lt;4318E3C6.9080309@...&gt;\t &lt;9d1e4525050902191877f6bc9c@...&gt; &lt;9d1e45250509061127130b82f8@...&gt;\r\nIn-Reply-To: &lt;9d1e45250509061127130b82f8@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-91.3 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Best approach to 7M seeds\r\nX-Yahoo-Group-Post: member; u=168599281; y=wdlcEajkXv1GyYYyaHsrGCFf5eFkJIEVbq1BzeJsdQmHhmFfJKz2ovrW\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nMatt Ittigson wrote:\n\n&gt; On 9/2/05, Matt Ittigson &lt;cydatamatt@...&gt; wrote:\n&gt; &gt; On 9/2/05, stack &lt;stack@...&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; ... you can watch the URLs as they go through the scope.  Will \n&gt; help you\n&gt; &gt; &gt; debug.\n&gt; &gt;\n&gt; &gt; Done exactly as suggested.\n&gt;\n&gt; Where would I go about watching this?  It seems pertinent given the\n&gt; information below.\n\nheritrix_out.log (The file that captures all errant stdout/stderr \nemissions).\n\n&gt;\n&gt; &gt; &gt; (You&#39;ll also likely need the aforementioned alternate queue assignment\n&gt; &gt; &gt; policy).\n&gt; &gt; &gt; Good luck,\n&gt; &gt;\n&gt; &gt; 1,146,463 URLs loaded via the importUris JMX command with the heap\n&gt; &gt; climbing to 1,869,072 KB.  I just started the job and I&#39;ll keep y&#39;all\n&gt; &gt; apprised of the memory situation as it runs.\n&gt;\n&gt; Before running out of memory, the final statiscal count was 151889\n&gt; downloads out of 4473197 potential (the bar graph on the summary\n&gt; page).  \n\nThat is a small number of downloads.  I&#39;m guessing whats killing you is \nthe number of queues.  Whats the frontier report say about queues?\n\nDid you use the suggested alternate queue policy so there isn&#39;t a queue \nper host?\n\nYou could also set the hold-queues to be true so that heritrix focuses \non a server giving it a chance to crawl a queue to completion.\n\n&gt; The problem, as I see it, is that the 4.4M number should never\n&gt; have been that high.  After I loaded the 1.1M seeds, the number of\n&gt; potential pages was around twice that (I&#39;m guessing because of DNS?).\n\nYeah.  And robots.\n\n&gt; Because the attempt was to configure Heritrix to not download any\n&gt; pages but the seeds, clearly something has gone wrong.  The extra\n&gt; pages being scheduled is what is driving the memory needs up.  \n\nSee above.\n\n&gt; As I\n&gt; see it, Heritrix&#39;s memory should already be close to the max necessary\n&gt; after the seed load, assuming this method works.\n&gt;\n&gt; I set the parameter:\n&gt;\n&gt; org.archive.crawler.deciderules.DecideRuleSequence.level=FINE\n&gt;\n&gt; But there doesn&#39;t appear to be any useful information in the\n&gt; heritrix_out.log file (I guess I could be looking for the wrong thing,\n&gt; since the file is 2.8G).  \n\nI suggested you turn on this parameter to help you debug your scope.  I \nwas thinking you could watch the deciding scope make decisions as it \nprocessed URLs.  Once it was doing as you wanted, you could turn off the \ndebugging and launch your production crawl.\n\nBut, I should have tried it first myself before passing you the \nsuggestion.  Looks like the logger has wrong name in the \nDecideRuleSequence.  You need to add following to heritrix.properties to \nsee deciderules output:\n\norg.archive.crawler.filter.OrFilter.level=FINE\n\n(I&#39;ve since fixed it so \norg.archive.crawler.deciderules.DecideRuleSequence.level=FINE works).\n\nHere&#39;s what you should see in the log once above is enabled:\n\n09/06/2005 18:47:04 +0000 FINE \norg.archive.crawler.deciderules.DecideRuleSequence decisionFor Rule \nrejectByDefault of decide-rules decided REJECT on http://www.duboce.net/\n09/06/2005 18:47:06 +0000 FINE \norg.archive.crawler.deciderules.DecideRuleSequence decisionFor Rule \nacceptIfSurtPrefixed of decide-rules decided ACCEPT on \nhttp://www.duboce.net/\n09/06/2005 18:47:07 +0000 FINE \norg.archive.crawler.deciderules.DecideRuleSequence decisionFor Rule \nrejectIfTooManyHops of decide-rules decided PASS on http://www.duboce.net/\n09/06/2005 18:47:08 +0000 FINE \norg.archive.crawler.deciderules.DecideRuleSequence decisionFor Rule \nrejectIfPathological of decide-rules decided PASS on http://www.duboce.net/\n09/06/2005 18:47:09 +0000 FINE \norg.archive.crawler.deciderules.DecideRuleSequence decisionFor Rule \nrejectIfTooManyPathSegs of decide-rules decided PASS on \nhttp://www.duboce.net/\n09/06/2005 18:47:09 +0000 FINE \norg.archive.crawler.deciderules.DecideRuleSequence decisionFor Decision \nof decide-rules was ACCEPT\n\nBut why is the log 2.8G in size?  Whats in it? (Other than OOMEs)\n\nSt.Ack\n\n&gt; Suggestions for next efforts?\n&gt;\n&gt; Thanks for your help.\n&gt;\n&gt; -matt\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}