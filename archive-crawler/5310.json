{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"3kL0xfhpzXJ0MOuR3M3g1_qF6ELQILhrUA2vSfg5-evTxvm3F-xYPw_DZwX9iELgEO1QU0hogLtBJX-6bVSEiRirfeY-bC4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Web page for Project &quot;Web Spam Detection for      Heritrix&quot; is on Heritrix Wiki","postDate":"1213639219","msgId":5310,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NTZBQTMzLjEwNTA0MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ1NTYyLjc2LjE5MS4xMzUuODIuMTIxMzYzNjYwOC5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg==","referencesHeader":"PGczMjI3Yysyc3RuQGVHcm91cHMuY29tPiA8NDU1NjIuNzYuMTkxLjEzNS44Mi4xMjEzNjM2NjA4LnNxdWlycmVsQG1haWwuYXJjaGl2ZS5vcmc+"},"prevInTopic":5309,"nextInTopic":5312,"prevInTime":5309,"nextInTime":5311,"topicId":5308,"numMessagesInTopic":4,"msgSnippet":"... FYI, a major goal of this project is to have an effective Javascript/DOM simulation *inside the crawler*, for both advanced link-extraction and detection","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 26824 invoked from network); 16 Jun 2008 18:00:16 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m42.grp.scd.yahoo.com with QMQP; 16 Jun 2008 18:00:16 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta16.grp.scd.yahoo.com with SMTP; 16 Jun 2008 18:00:16 -0000\r\nX-Received: (qmail 52326 invoked from network); 16 Jun 2008 18:00:15 -0000\r\nX-Received: from unknown (HELO ?192.168.1.15?) (unknown)\n  by unknown with SMTP; 16 Jun 2008 18:00:15 -0000\r\nX-pair-Authenticated: 67.180.197.118\r\nMessage-ID: &lt;4856AA33.1050404@...&gt;\r\nDate: Mon, 16 Jun 2008 11:00:19 -0700\r\nUser-Agent: Thunderbird 2.0.0.14 (Windows/20080421)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g3227c+2stn@...&gt; &lt;45562.76.191.135.82.1213636608.squirrel@...&gt;\r\nIn-Reply-To: &lt;45562.76.191.135.82.1213636608.squirrel@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Web page for Project &quot;Web Spam Detection for\n      Heritrix&quot; is on Heritrix Wiki\r\nX-Yahoo-Group-Post: member; u=137285340; y=1hHYpRWdpgOWPGzAd0A5FF1_joosjzNXTHP9MXR4x_yb\r\nX-Yahoo-Profile: gojomo\r\n\r\nigor@... wrote:\n&gt; Hi Ping,\n&gt; \n&gt; It is great that you are undertaking this project. Here are some of my\n&gt; initial thoughts.\n&gt; \n&gt; - In general, I would do analysis off-line and not during the crawl.\n&gt; \n&gt; - See if Browser Monkeys will scale for this project. If yes, then you get\n&gt; you javascript engine, events simulations, dom and all other wonders of a\n&gt; browser for &quot;free&quot;.\n\nFYI, a major goal of this project is to have an effective Javascript/DOM \nsimulation *inside the crawler*, for both advanced link-extraction and \ndetection of spammy redirects/JS-cloaking, so offline analysis and \nreliance on externally-controlled browsers (aka &quot;Browser Monkeys&quot;) was \nruled out.\n\nComparing the behavior of the inside-the-crawler JS/DOM with the \nfull-fledged Firefox JS/DOM would be an valuable test of the integrated \ncode&#39;s functionality.\n\n&gt; - If you decide to fetch two pages for cloaking detection be sure that\n&gt; fetches are coming from different ip addresses/ranges. Also, be sure that\n&gt; for &quot;browser&quot; fetches you don&#39;t make robots.txt requests. This is a\n&gt; telltale to spammers that you are bot regardless of the IP address,\n&gt; user-agent and other http request headers. This kind of dual fetching can\n&gt; easily be done with Heritrix&#39;s proxy setting and probably a beanshell\n&gt; processor.\n\nGood points. At a first level, I hope Ping&#39;s analysis code can do two \nchecks from a single fetch -- plain link extraction, compared to the \nresults of letting JS run. Then, for catching another level of tricks, \nsecondary fetches via alternate IPs and User-Agents would be added.\n\n&gt; - As you already pointed out, dual fetching can be expensive. However,\n&gt; sampling is probably a good way to go given that spammers are usually\n&gt; ambitious/greedy. So, fetching a couple of pages per host will probably be\n&gt; good enough. I recommend slash page plus one other link of it.\n\nYes, or some proportion of a site, or perhaps a proportion of any \nURL-prefix that has a lot of content. (EG, when \nwww.example.com/dirA/dirB/* becomes very common, it earns a test.)\n\n&gt; - As a part of a wish list, it would be good to check how often webmasters\n&gt; implement beneficial cloaking (removing session ids, parameters and etc.).\n&gt; Also, if Flash has been a new spamming medium.\n\nInteresting idea... and if there is significant beneficial cloaking, can \nwe determine some heurisitics to distinguish it from bad cloaking?\n\n- Gordon @ IA\n\n"}}