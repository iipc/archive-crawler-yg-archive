{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (@Internet Archive)","from":"&quot;Gordon Mohr (@Internet Archive)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"ycuwPsHFFIiC6N72FUtXnXAY3H0M09ZrSicWGeYzpQuZkuovMT4bPstoG9rpYd5m1zAAXGHd2I7pch12YAH3l_MFFt_jgaIbxfqJmFd4MrmpOPFQhvDErgTXqJD0","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Large scale crawling with Heritrix","postDate":"1085153670","msgId":433,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwQUUyMTg2LjEwNjAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBDNzFAbG9raS5ib2suaGkuaXM+","referencesHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBDNzFAbG9raS5ib2suaGkuaXM+"},"prevInTopic":432,"nextInTopic":434,"prevInTime":432,"nextInTime":434,"topicId":432,"numMessagesInTopic":9,"msgSnippet":"It s good to see the limits tested in a new way! Some comments interspersed... ... I would also check to see if any of the worker ToeThreads seem indefinitely ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 61134 invoked from network); 21 May 2004 15:34:52 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m19.grp.scd.yahoo.com with QMQP; 21 May 2004 15:34:52 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 21 May 2004 15:34:52 -0000\r\nReceived: (qmail 24594 invoked by uid 100); 21 May 2004 15:27:46 -0000\r\nReceived: from adsl-67-119-26-227.dsl.snfc21.pacbell.net (HELO ?10.0.10.13?) (gojomo@...@67.119.26.227)\n  by mail-dev.archive.org with SMTP; 21 May 2004 15:27:46 -0000\r\nMessage-ID: &lt;40AE2186.1060303@...&gt;\r\nDate: Fri, 21 May 2004 08:34:30 -0700\r\nUser-Agent: Mozilla Thunderbird 0.6 (X11/20040502)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;D958110B273CD511ACC100B0D079AA4E01960C71@...&gt;\r\nIn-Reply-To: &lt;D958110B273CD511ACC100B0D079AA4E01960C71@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: **\r\nX-Spam-Status: No, hits=2.6 required=6.0 tests=AWL,RCVD_IN_DYNABLOCK,\n\tRCVD_IN_SORBS autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: &quot;Gordon Mohr (@Internet Archive)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Large scale crawling with Heritrix\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nIt&#39;s good to see the limits tested in a new way! Some comments interspersed...\n\nKristinn Sigurdsson wrote:\n&gt; Initial progress was quite impressive, running at over 50 documents per\n&gt; second initially. Eventually it started to gradually drop and now several\n&gt; days later it stands at around 17 documents per second. I&#39;m unsure of the\n&gt; reason for this gradual decline. It may be related to increasing sizes of\n&gt; various data structures. \n\nI would also check to see if any of the worker ToeThreads seem indefinitely\nstuck on single URIs, effectively shrinking the working pool. Are you using\nfetch timeouts?\n\n&gt; Disk use by the disk bound queues however has been much greater then I\n&gt; anticipated. With said 11 million URLs waiting in the queues they now take\n&gt; up about 16 GB. This comes out at about 1.6 KB per URI. This will turn out\n&gt; to be the limiting factor for my current crawl since the disk in question\n&gt; only has another 3 GB free so it will be exhausted soon.\n\nThe default Java object serialization we use is very bloated. On the bright\nside, it compresses well: in excess of 95% using gzip in my tests. So at a\nslight cost of CPU a gzip option could be offered (in the DiskByteQueue)\nfor big disk savings where necessary.\n\n&gt; Of course a crawl of that scope is not possible until the list of already\n&gt; seen URIs can be disk backed.  With the current method of using 4 byte\n&gt; fingerprints for each encountered URI 1 GB of memory can hold around 28\n&gt; million URIs. Even with a machine with 4 GB RAM would not be able to scale\n&gt; up to even a full .is TLD crawl.\n\nA disk-based already-seen option is available, but hasn&#39;t been well-tested\nor profiled, and you have to edit Frontier source code to enable it.\n(See Frontier.initialize(), specifically the commented-out CachingDiskLongFPSet\nsegment.) One of my bugs to tackle is enabling this option from the UI,\nso it gets real testing.\n\n- Gordon\n\n"}}