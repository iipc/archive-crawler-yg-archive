{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"C9sEvU7WpmlCtoDS6X-tLUawLv2ts7BaNj9lStUfmY3a9YafOezY4VR2ndmz651wnOrQ9CYAzWQbecLBpKcX_sH9Qulo7Vor5tFMamlO","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Can I split seeds for a HashCrawlMapper crawl?","postDate":"1250111364","msgId":5976,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg1dmIyNCtuMnUxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRBODMyQTU0LjUwOTAzMDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":5975,"nextInTopic":5978,"prevInTime":5975,"nextInTime":5977,"topicId":5971,"numMessagesInTopic":8,"msgSnippet":"Gordon, Thanks for the response. I did come up with a custom program to divide the 20MM seed list myself. I am now running a proof instance with less than 1MM","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 54897 invoked from network); 12 Aug 2009 21:09:57 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.re1.yahoo.com with QMQP; 12 Aug 2009 21:09:57 -0000\r\nX-Received: from unknown (HELO n41b.bullet.mail.sp1.yahoo.com) (66.163.168.155)\n  by mta3.grp.sp2.yahoo.com with SMTP; 12 Aug 2009 21:09:57 -0000\r\nX-Received: from [69.147.65.147] by n41.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2009 21:09:26 -0000\r\nX-Received: from [98.137.35.13] by t10.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2009 21:09:26 -0000\r\nDate: Wed, 12 Aug 2009 21:09:24 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h5vb24+n2u1@...&gt;\r\nIn-Reply-To: &lt;4A832A54.5090305@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: Can I split seeds for a HashCrawlMapper crawl?\r\nX-Yahoo-Group-Post: member; u=132996324; y=qAKQEHZQ02H66659AZmKe5cp4gmYv1Dn4Wk1biZ4_ZV6HJRpDA\r\nX-Yahoo-Profile: joehung302\r\n\r\nGordon,\n\nThanks for the response. I did come up with a custom program to di=\r\nvide the 20MM seed list myself. I am now running a proof instance with less=\r\n than 1MM seed and so far it seems to work well.\n\nHere is my next question.=\r\n When diverting the crawl, let&#39;s say the diversion is from crawler 1 to cra=\r\nlwer 2. If I start crawler 1 & 2 with different seeds (because of the split=\r\n), would crawler 2 accept the URLs from crawler 1? \n\nI know the diverted UR=\r\nLs has 2 parts of information: target URL and seed. Apparently the target U=\r\nRL should be hashed to crawler 2, but the seed won&#39;t be in crawler&#39;s 2 seed=\r\n list...\n\nCheers,\n-Joe\n--- In archive-crawler@yahoogroups.com, Gordon Mohr =\r\n&lt;gojomo@...&gt; wrote:\n&gt;\n&gt; It would be useful if (for example) HashCrawlMapper=\r\n had a main() that \n&gt; let it be used outside the crawler to pre-split lists=\r\n... however the way \n&gt; it&#39;s currently dependent on the frontier&#39;s configure=\r\nd queue-key policy \n&gt; would require some extra complication on how such a h=\r\nypothetical utility \n&gt; was invoked.\n&gt; \n&gt; A roundabout way to achieve the sa=\r\nme effect might be to set up a dummy \n&gt; crawler whose first HashCrawlMapper=\r\n does &#39;check-uri&#39; but also has a \n&gt; erroneous &#39;local-name&#39;. Feed it all 20M=\r\nM URLs -- and every one will land \n&gt; in one of the diversion logs, because =\r\nnone will be bucketed to the bad \n&gt; &#39;local-name&#39;. This dummy crawl wouldn&#39;t=\r\n need any other things (like \n&gt; seed-based scopes) that might be choking on=\r\n a large seed list.\n&gt; \n&gt; Note that if each of the 12 crawlers only is initi=\r\nalized with a small \n&gt; portion of the seed list, when they discover deep UR=\r\nIs on hosts assigned \n&gt; to other nodes they might mistakenly rule them out-=\r\nof-scope, though in \n&gt; practice this might not be an issue for well-connect=\r\ned sites.\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; joehung302 wrote:\n&gt; &gt; I&#39;m using Heritrix =\r\n1.14.3.\n&gt; &gt; \n&gt; &gt; Let&#39;s say I have \n&gt; &gt; 1. one big seed list consisting of 1=\r\nMM seeds. \n&gt; &gt; 2. 2 crawler instances to implement HashCrawlMapper. \n&gt; &gt; 3.=\r\n The crawl scope is domain + 1 (implemented through OnDomainDecideRule with=\r\n &quot;seeds-as-surt-prefixes&quot;=3D=3Dtrue and &quot;also-check-via&quot;=3D=3Dtrue). \n&gt; &gt; \n=\r\n&gt; &gt; Can I split the seeds using the same HashCrawlMapper rule so that each =\r\ncrawler would only get seeds that are within its scope? Would there be any =\r\ndifference if I use the same 1MM seeds for both crawlers?\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; The=\r\n reason why I want to do this is, I have 20MM seeds among 12 crawlers. I&#39;ve=\r\n tested with one instance handling 20MM seeds and it doesn&#39;t seem to work. =\r\nIf I can split the seeds so that each cralwer starts with URLs that belong =\r\nto themselves it should make the crawl process easier....\n&gt; &gt; \n&gt; &gt; Thanks,\n=\r\n&gt; &gt; -Joe\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; ------------------------------------\n=\r\n&gt; &gt; \n&gt; &gt; Yahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}