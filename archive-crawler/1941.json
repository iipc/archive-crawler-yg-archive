{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":215515511,"authorName":"Mike Schwartz","from":"Mike Schwartz &lt;mschwartz@...&gt;","profile":"mfschwartz","replyTo":"LIST","senderId":"hBG8t1wP3wUONc30dPQthT34dHKQr-VjOWz4z2geBj0OZJeB08tOJZizoaGfrVXHFpuNKPTyOSn4Zi0elFDkpYs2uoRvy_LtPxY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"mapping between crawled and seed URLs","postDate":"1118436265","msgId":1941,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMi4xLjIuMi4yMDA1MDYxMDE0NDM0MS4wMmJhMjM5MEB2aG9zdDYuYXRvbWljc2VydmVycy5jb20+"},"prevInTopic":0,"nextInTopic":0,"prevInTime":1940,"nextInTime":1942,"topicId":1941,"numMessagesInTopic":1,"msgSnippet":"Hi, A few months ago I contributed a class (RecoveryLogMapper) that reads the Heritrix recover.gz file and builds maps between seed and crawled URLs.  I ve","rawEmail":"Return-Path: &lt;mschwartz@...&gt;\r\nX-Sender: mschwartz@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 65167 invoked from network); 10 Jun 2005 20:44:19 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m29.grp.scd.yahoo.com with QMQP; 10 Jun 2005 20:44:19 -0000\r\nReceived: from unknown (HELO vhost6.atomicservers.com) (216.58.160.194)\n  by mta5.grp.scd.yahoo.com with SMTP; 10 Jun 2005 20:44:18 -0000\r\nReceived: from dev4lt.localmatters.com ([64.78.237.253])\n\t(authenticated (0 bits))\n\tby vhost6.atomicservers.com (8.11.6/8.11.6) with ESMTP id j5AKiHB13211\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 10 Jun 2005 14:44:17 -0600\r\nMessage-Id: &lt;6.2.1.2.2.20050610144341.02ba2390@...&gt;\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.2.1.2\r\nDate: Fri, 10 Jun 2005 14:44:25 -0600\r\nTo: archive-crawler@yahoogroups.com\r\nMime-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;=====================_1733836921==.ALT&quot;\r\nX-eGroups-Msg-Info: 1:12:0\r\nX-eGroups-From: Mike Schwartz &lt;mschwartz@...&gt;\r\nFrom: Mike Schwartz &lt;mschwartz@...&gt;\r\nSubject: mapping between crawled and seed URLs\r\nX-Yahoo-Group-Post: member; u=215515511; y=YSuWB_6uo17qJcC8KBTSe37Z2bloUjrNty9-UO2TeHSwE8wNnw\r\nX-Yahoo-Profile: mfschwartz\r\n\r\n\r\n--=====================_1733836921==.ALT\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\n\r\nHi,\n\nA few months ago I contributed a class (RecoveryLogMapper) that reads the \nHeritrix recover.gz file and builds maps between seed and crawled \nURLs.  I&#39;ve discovered some problems with this approach, which make me \nthink I should implement a module that builds a separate (optional) log \ninstead of trying to reuse the recover log.  Below I summarize the goals \nand problems.  I&#39;d appreciate any thoughts/suggestions about this.\n\nGoal: ability to determine, after a crawl has run, (a) which URI&#39;s were \ncrawled via a particular seed URI, and (b) the seed URI for a given crawled \nURI.  Note that what I want for these things is a partitioning of the URIs \nbased on scope -- all URLs related to seed A, all related to seed B, etc., \nas opposed to the literal discovery paths.  The distinction I&#39;m making \nshould become clear in problem description (a) below.\n\nProblems with how I currently implemented RecoveryLogMapper:\na) the recover.gz log shows the sequence of frontier additions based on \noverall scope tests, rather than tests relative to individual seed \nURIs.  As an example, the log will show an F+  entry like:\nF+ http://forms.real.com/.../RealPlayer10GOLD_bb.exe LL \nhttp://www.sundanceyachts.com/newsite/vidpage.html\nbecause while crawling www.sundanceyachts.com it found a link to the above \nreal.com URL, and real.com is a seed in my crawl -- i.e., it&#39;s in scope for \nthe overall crawl, but not in (domain) scope for sundanceyachts.  Because \nof this, my recover log mapper will report that the real.com URL comes from \nthe sundanceyachts.com seed when what I really want to know is that it&#39;s in \nscope of the real.com seed.\nb) redirected seed URLs show up as new seeds in the recover log, without an \nindication of the seed from whence they came.  I need to know that \nseed.  As an example, when I run a crawl starting with the seed \nhttp://www.saab-zentrum-hamburg.de , that page redirects to \nhttp://www.saab-web.de/index3.php, at which point the following line gets \nadded to recover.gz :\nF+ http://www.saab-web.de/index3.php?cid=5318703\nIn order to be able to trace this redirected page back to the seed from \nwhence it originated, I want a log that shows the redirected page as well \nas the redirecting seed page.  Note - I believe this is related to (or \npossibly the same as) the problem mentioned at \nhttps://sourceforge.net/tracker/index.php?func=detail&aid=1217290&group_id=73833&atid=539099 \n.\nProblem (a) derives from the fact that scope tests (like that done in \nDomainScope.focusAccepts() ) are done relative to all seeds in a crawl, so \nhave no concept of in scope relative to a particular seed.  In turn this \nprobably represents a difference in the way we crawl from the way most \npeople crawl -- most people want to collect all the content related to all \nseeds in the crawl into a single bucket, while we need to collect content \ninto individual buckets, each relative to an individual seed.  I suppose we \ncould solve this by just running single seed crawls, but that would \ndrastically reduce crawl parallelism and increase operational hassles \n(having to create tons of little crawl jobs).\n\nProblem (b) derives from the fact that apparently no one to date has needed \nto track redirections back to the seed that caused them, which in turn is \nrelated to the point I made above about our needing to collect content on a \nseed-by-seed basis, rather than globally for all seeds in a \ncrawl.  Concretely, we need the \nhttp://forms.real.com/.../RealPlayer10GOLD_bb.exe URL to end up in the \nreal.com bucket, not the sundanceyachts.com bucket (or, one overall bucket).\n\nThoughts on how to rework the code to accomodate our requirements:\n\nI think the solution is to stop trying to repurpose the existing recover.gz \nfile for our purposes - it&#39;s not collecting quite the right event info, and \nsince it&#39;s eventually not going to be useful to other people (because of \nthe new checkpoint mechanism) I think I should just create an optional new \nlog that logs exactly the events I need.\n\nHow/where to put this?  In a 1:1 email exchange St.Ack had suggested \nsubclassing BdbFrontier, but this seems possibly problematic for two reasons:\n1) if we do that we&#39;re locked into a particular Frontier \nimplementation.  I&#39;m not clear whether that&#39;s a real problem because \nBdbFrontier seems to be becoming the core implementation for Frontiers and \nmaybe we&#39;d never need to be able to use a different one.  Any thoughts on \nthis would be appreciated.\n2) currently there is just one part of the code where URIs are added to the \nrecover log, in WorkQueueFrontier.receive (the call to \nAbstractFrontier.doJournalAdded), so that would seem to be a good place to \nadd support for my new log (but in a subclassed implementation).  But by \nthe time that part of the code is reached the new URI (in the case of a \nredirected seed) is already considered a seed, so if I were to implement \nthe new logging mechanism as part of the Frontier type hierarchy I would \nnot be able to log seed redirections.  Instead, the place in the code where \nthis knowledge is known is in LinkScoper.considerAsSeed() (which is called \nby LinkScoper.innerProcess).\nSo, my current thinking is that this new log Could be written during the \noutlink iteration inside LinkScoper.innerProcess -- but unless I&#39;m mistaken \nthat seems to be a part of the Heritrix core, not something that was \nintended to be a pluggable module... which calls the question of how I \ncould make this new log be optional, fitting into the settings mechanism.\n\nI still don&#39;t understand the code flow well enough to know the right way to \naccomplish the above.  Any suggestions would be appreciated.\n\nThanks\n\n\nMike Schwartz\nVP of Technology Strategy\nLocal Matters, Inc.\n\n1517 Blake Street, Floor Two\nDenver, CO  80202\nmschwartz@...\nwww.localmatters.com\nO 303-572-1122 x 214\nF 303-572-1123  \r\n--=====================_1733836921==.ALT\r\nContent-Type: text/html; charset=&quot;us-ascii&quot;\r\n\r\n&lt;html&gt;\n&lt;body&gt;\n&lt;font size=3&gt;Hi,&lt;br&gt;&lt;br&gt;\nA few months ago I contributed a class (RecoveryLogMapper) that reads the\nHeritrix recover.gz file and builds maps between seed and crawled\nURLs.&nbsp; I&#39;ve discovered some problems with this approach, which make\nme think I should implement a module that builds a separate (optional)\nlog instead of trying to reuse the recover log.&nbsp; Below I summarize\nthe goals and problems.&nbsp; I&#39;d appreciate any thoughts/suggestions\nabout this.&lt;br&gt;&lt;br&gt;\n&lt;u&gt;Goal&lt;/u&gt;: ability to determine, after a crawl has run, (a) which URI&#39;s\nwere crawled via a particular seed URI, and (b) the seed URI for a given\ncrawled URI.&nbsp; Note that what I want for these things is a\npartitioning of the URIs based on scope -- all URLs related to seed A,\nall related to seed B, etc., as opposed to the literal discovery\npaths.&nbsp; The distinction I&#39;m making should become clear in problem\ndescription (a) below.&lt;br&gt;&lt;br&gt;\n&lt;u&gt;Problems with how I currently implemented RecoveryLogMapper&lt;/u&gt;: \n&lt;dl&gt;\n&lt;dd&gt;a) the recover.gz log shows the sequence of frontier additions based\non &lt;i&gt;overall &lt;/i&gt;scope tests, rather than tests relative to individual\nseed URIs.&nbsp; As an example, the log will show an F+&nbsp; entry like: \n&lt;dl&gt;\n&lt;dd&gt;F+\n&lt;a href=&quot;http://forms.real.com/.../RealPlayer10GOLD_bb.exe&quot; eudora=&quot;autourl&quot;&gt;\nhttp://forms.real.com/.../RealPlayer10GOLD_bb.exe&lt;/a&gt; LL\n&lt;a href=&quot;http://www.sundanceyachts.com/newsite/vidpage.html&quot; eudora=&quot;autourl&quot;&gt;\nhttp://www.sundanceyachts.com/newsite/vidpage.html&lt;/a&gt; \n&lt;/dl&gt;\n&lt;dd&gt;because while crawling\n&lt;a href=&quot;http://www.sundanceyachts.com/&quot; eudora=&quot;autourl&quot;&gt;\nwww.sundanceyachts.com&lt;/a&gt; it found a link to the above real.com URL, and\nreal.com is a seed in my crawl -- i.e., it&#39;s in scope for the &lt;i&gt;overall\n&lt;/i&gt;crawl, but not in (domain) scope for sundanceyachts.&nbsp; Because of\nthis, my recover log mapper will report that the real.com URL comes from\nthe sundanceyachts.com seed when what I really want to know is that it&#39;s\nin scope of the real.com seed.\n&lt;dd&gt;b) redirected seed URLs show up as new seeds in the recover log,\nwithout an indication of the seed from whence they came.&nbsp; I need to\nknow that seed.&nbsp; As an example, when I run a crawl starting with the\nseed\n&lt;a href=&quot;http://www.saab-zentrum-hamburg.de/&quot; eudora=&quot;autourl&quot;&gt;\nhttp://www.saab-zentrum-hamburg.de&lt;/a&gt; , that page redirects to\n&lt;a href=&quot;http://www.saab-web.de/index3.php&quot; eudora=&quot;autourl&quot;&gt;\nhttp://www.saab-web.de/index3.php&lt;/a&gt;, at which point the following line\ngets added to recover.gz : \n&lt;dl&gt;\n&lt;dd&gt;F+\n&lt;a href=&quot;http://www.saab-web.de/index3.php?cid=5318703&quot; eudora=&quot;autourl&quot;&gt;\nhttp://www.saab-web.de/index3.php?cid=5318703&lt;/a&gt; \n&lt;/dl&gt;\n&lt;dd&gt;In order to be able to trace this redirected page back to the seed\nfrom whence it originated, I want a log that shows the redirected page as\nwell as the redirecting seed page.&nbsp; Note - I believe this is related\nto (or possibly the same as) the problem mentioned at\n&lt;a href=&quot;https://sourceforge.net/tracker/index.php?func=detail&amp;aid=1217290&amp;group_id=73833&amp;atid=539099&quot; eudora=&quot;autourl&quot;&gt;\nhttps://sourceforge.net/tracker/index.php?func=detail&amp;aid=1217290&amp;group_id=73833&amp;atid=539099&lt;/a&gt;\n . \n&lt;/dl&gt;Problem (a) derives from the fact that scope tests (like that done\nin DomainScope.focusAccepts() ) are done relative to all seeds in a\ncrawl, so have no concept of in scope relative to a particular\nseed.&nbsp; In turn this probably represents a difference in the way we\ncrawl from the way most people crawl -- most people want to collect all\nthe content related to all seeds in the crawl into a single bucket, while\nwe need to collect content into individual buckets, each relative to an\nindividual seed.&nbsp; I suppose we could solve this by just running\nsingle seed crawls, but that would drastically reduce crawl parallelism\nand increase operational hassles (having to create tons of little crawl\njobs).&lt;br&gt;&lt;br&gt;\nProblem (b) derives from the fact that apparently no one to date has\nneeded to track redirections back to the seed that caused them, which in\nturn is related to the point I made above about our needing to collect\ncontent on a seed-by-seed basis, rather than globally for all seeds in a\ncrawl.&nbsp; Concretely, we need the\n&lt;a href=&quot;http://forms.real.com/.../RealPlayer10GOLD_bb.exe&quot; eudora=&quot;autourl&quot;&gt;\nhttp://forms.real.com/.../RealPlayer10GOLD_bb.exe&lt;/a&gt; URL to end up in\nthe real.com bucket, not the sundanceyachts.com bucket (or, one overall\nbucket).&lt;br&gt;&lt;br&gt;\n&lt;u&gt;Thoughts on how to rework the code to accomodate our requirements&lt;/u&gt;:\n&lt;br&gt;&lt;br&gt;\nI think the solution is to stop trying to repurpose the existing\nrecover.gz file for our purposes - it&#39;s not collecting quite the right\nevent info, and since it&#39;s eventually not going to be useful to other\npeople (because of the new checkpoint mechanism) I think I should just\ncreate an optional new log that logs exactly the events I need.&lt;br&gt;&lt;br&gt;\nHow/where to put this?&nbsp; In a 1:1 email exchange St.Ack had suggested\nsubclassing BdbFrontier, but this seems possibly problematic for two\nreasons: \n&lt;dl&gt;\n&lt;dd&gt;1) if we do that we&#39;re locked into a particular Frontier\nimplementation.&nbsp; I&#39;m not clear whether that&#39;s a real problem because\nBdbFrontier seems to be becoming the core implementation for Frontiers\nand maybe we&#39;d never need to be able to use a different one.&nbsp; Any\nthoughts on this would be appreciated.\n&lt;dd&gt;2) currently there is just one part of the code where URIs are added\nto the recover log, in WorkQueueFrontier.receive (the call to\nAbstractFrontier.doJournalAdded), so that would seem to be a good place\nto add support for my new log (but in a subclassed implementation).&nbsp;\nBut by the time that part of the code is reached the new URI (in the case\nof a redirected seed) is already considered a seed, so if I were to\nimplement the new logging mechanism as part of the Frontier type\nhierarchy I would not be able to log seed redirections.&nbsp; Instead,\nthe place in the code where this knowledge is known is in\nLinkScoper.considerAsSeed() (which is called by LinkScoper.innerProcess). \n&lt;/dl&gt;So, my current thinking is that this new log Could be written during\nthe outlink iteration inside LinkScoper.innerProcess -- but unless I&#39;m\nmistaken that seems to be a part of the Heritrix core, not something that\nwas intended to be a pluggable module... which calls the question of how\nI could make this new log be optional, fitting into the settings\nmechanism.&lt;br&gt;&lt;br&gt;\nI still don&#39;t understand the code flow well enough to know the right way\nto accomplish the above.&nbsp; Any suggestions would be\nappreciated.&lt;br&gt;&lt;br&gt;\nThanks&lt;br&gt;&lt;br&gt;\n&lt;/font&gt;&lt;x-sigsep&gt;&lt;p&gt;&lt;/x-sigsep&gt;\n&lt;font face=&quot;arial&quot; size=2&gt;Mike Schwartz&lt;br&gt;\nVP of Technology Strategy&lt;br&gt;\nLocal Matters, Inc.&lt;br&gt;\n&nbsp;&lt;br&gt;\n1517 Blake Street, Floor Two&lt;br&gt;\nDenver, CO&nbsp; 80202&lt;br&gt;\nmschwartz@...&lt;br&gt;\n&lt;a href=&quot;http://www.localmatters.com/&quot; eudora=&quot;autourl&quot;&gt;\nwww.localmatters.com&lt;br&gt;\n&lt;/a&gt;O 303-572-1122 x 214&lt;br&gt;\nF 303-572-1123&lt;/font&gt;&lt;font size=3&gt; &lt;/font&gt;&lt;/body&gt;\n&lt;/html&gt;\n\r\n--=====================_1733836921==.ALT--\r\n\n"}}