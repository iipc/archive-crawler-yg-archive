{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"2ZsCa259SvMyr13P7jk2H83x-RiRY7Hkk3gPNpjviifI4Pu3V88Ll2_kNmVOspdpPUH6WNQvZlI1HUGAkRE3mY-erb88Mf4","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] crawl problem","postDate":"1164829236","msgId":3571,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1NkRFMjM0LjgwNjA1MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDY0MjU1NS44MTA4Mi5xbUB3ZWI1MzUxMy5tYWlsLnlhaG9vLmNvbT4=","referencesHeader":"PDY0MjU1NS44MTA4Mi5xbUB3ZWI1MzUxMy5tYWlsLnlhaG9vLmNvbT4="},"prevInTopic":3569,"nextInTopic":0,"prevInTime":3570,"nextInTime":3572,"topicId":3558,"numMessagesInTopic":6,"msgSnippet":"... The completion percentage is only a very rough *under*estimate of the crawl s progress, as it only considers already-discovered URIs. When many new URIs","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 36244 invoked from network); 29 Nov 2006 19:36:02 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m26.grp.scd.yahoo.com with QMQP; 29 Nov 2006 19:36:02 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta2.grp.scd.yahoo.com with SMTP; 29 Nov 2006 19:36:01 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 7E1DA141569B5\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 29 Nov 2006 11:34:21 -0800 (PST)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 00929-05-52 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 29 Nov 2006 11:34:19 -0800 (PST)\r\nReceived: from [192.168.1.203] (c-71-198-60-165.hsd1.ca.comcast.net [71.198.60.165])\n\tby mail.archive.org (Postfix) with ESMTP id 251D6141561FD\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 29 Nov 2006 11:34:19 -0800 (PST)\r\nMessage-ID: &lt;456DE234.8060508@...&gt;\r\nDate: Wed, 29 Nov 2006 11:40:36 -0800\r\nUser-Agent: Thunderbird 1.5.0.5 (X11/20060728)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;642555.81082.qm@...&gt;\r\nIn-Reply-To: &lt;642555.81082.qm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] crawl problem\r\nX-Yahoo-Group-Post: member; u=137285340; y=YJ0SNxv9JoAJgX-DqK5ZT0i1m_jMCm_SfZAtiI0QbkR4\r\nX-Yahoo-Profile: gojomo\r\n\r\nJanhavi Sheode wrote:\n&gt; Hello All,\n&gt; \n&gt; I need some help. I am trying to run a crawl and it reaches 90% and then drops back again to 77% !! When i check the seeds report, the last entry is the same for the past one hour. I checked the logs and the crawler seems to be stuck with the same link. Is there any way to get out of this problem? is there a method to skip that particular link at which the crawler is getting stuck. I did try to read the documentation but am rather confused. I would highly appreciate any kind of help with issue.\n\nThe completion percentage is only a very rough *under*estimate of the \ncrawl&#39;s progress, as it only considers already-discovered URIs. When \nmany new URIs are discovered, the completion percentage can drop \nsignificantly.\n\nIt is quite possible that the crawl has completed except for URIs that \nare all on the same host (and thus internal crawler queue), and that \nhost is nonresponsive.\n\nFor a URI on a nonresponsive host, Heritrix will issue &#39;max-retries&#39; \nattempts (default:30) each separated by &#39;retry-delay-seconds&#39; seconds \n(default:900) before giving up on a URI as failed, and moving to the \nnext in the same host-queue. That&#39;s over 7 hours to completely fail a \nsingle URI; the rationale is that there are often transient network or \nserver problems that (1) affect an entire host at a time; and (2) you \ndon&#39;t want to cause you to skip URIs.\n\nYou can adjust those retry & delay parameters, or simply notice that a \ncrawl is done except for a problem host or hosts and decide to manually \nend the crawl.\n\nIf this isn&#39;t the case, how large is the crawl (in seeds and completed \nURIs) and what does the &#39;frontier report&#39; look like? It will give clues \nas to whether a single or small number of host-queues are the only \nlagging work.\n\nHope this helps,\n\n- Gordon @ IA\n\n\n"}}