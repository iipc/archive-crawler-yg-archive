{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"l1kRSZMDLsZzbwavDOF3t5JDUjey0jhHS7v_ZaXboh6rArRWcErnuSg32BR_MZvDAc--Qe4Yxp5cgg4AduVosWorT3hOhq8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Re: Heritrix performing crawls at very low rates","postDate":"1309392596","msgId":7195,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFMEJCRUQ0LjMwNjAwMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGl1ZzNvYys5cWkxQGVHcm91cHMuY29tPg==","referencesHeader":"PGl1ZzNvYys5cWkxQGVHcm91cHMuY29tPg=="},"prevInTopic":7194,"nextInTopic":0,"prevInTime":7194,"nextInTime":7196,"topicId":7191,"numMessagesInTopic":4,"msgSnippet":"... Probably, though if your needs evolve to require very special rules for what to get, how often, and what to do with it, then it may require significant","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 84113 invoked from network); 30 Jun 2011 00:09:58 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m3.grp.sp2.yahoo.com with QMQP; 30 Jun 2011 00:09:58 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.sp2.yahoo.com with SMTP; 30 Jun 2011 00:09:58 -0000\r\nX-Received: (qmail 67206 invoked by uid 0); 30 Jun 2011 00:09:57 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 30 Jun 2011 00:09:57 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4E0BBED4.3060004@...&gt;\r\nDate: Wed, 29 Jun 2011 17:09:56 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.18) Gecko/20110616 Thunderbird/3.1.11\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;iug3oc+9qi1@...&gt;\r\nIn-Reply-To: &lt;iug3oc+9qi1@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Heritrix performing crawls at very low\n rates\r\nX-Yahoo-Group-Post: member; u=137285340; y=hACHwWNa__ayjG7yw7Py-f0tVfud78eRlexquECN0xYU\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 6/29/11 1:59 PM, cosmicpioneer2011 wrote:\n&gt; Thanks Gordon, What we want to ideally achieve is to have a focused\n&gt; crawler to crawl only specific portions of the web or a.k.a directed\n&gt; crawl which we think can be achieved by writing a RegExp filter plugin.\n&gt;\n&gt; Do you think heritrix can scale out well for our usecase where we have\n&gt; needs to do a directed/focused crawl ?\n\nProbably, though if your needs evolve to require very special rules for \nwhat to get, how often, and what to do with it, then it may require \nsignificant non-default configuration.\n\n&gt; Also attached is the xml settings file which we use for directed crawls.\n&gt; Can you suggest any modifications to improve crawl rates.\n&gt;\n&gt; http://f1.grp.yahoofs.com/v1/QIQLTjfsFvXQiibub_-pvI_POMQwbxbhRFAKbLg52-ZoIaUXowI9na8r9zW2M3VZ-9PlfW24TK62zHW1Lx_cmctQuoY7edQkxSJoySGPcg/CrawlOrder_xmlsettings.xml\n&gt; &lt;http://f1.grp.yahoofs.com/v1/QIQLTjfsFvXQiibub_-pvI_POMQwbxbhRFAKbLg52-ZoIaUXowI9na8r9zW2M3VZ-9PlfW24TK62zHW1Lx_cmctQuoY7edQkxSJoySGPcg/CrawlOrder_xmlsettings.xml%20&gt;\n\nFYI, this link had expired but I found your upload via the group \nuploaded files area.\n\nA few things I noted:\n\n- you can make &#39;recorder-in-buffer-bytes&#39; much larger so fewer resources \nuse scratch disk space between fetching and final writing. (We&#39;ve \nincreased the default to 512K in Heritrix3.)\n\n- generally the &#39;PrerequisiteAcceptDecideRule&#39; should come last, just in \ncase any earlier rules REJECTed a necessary prerequisite. (Not an issue \nyet, as you have no parameterized REJECTing rules, but could become one \nlater.)\n\n- your regexes seem a bit confused, almost like a little \n&#39;glob&#39;-expansion is mixed in (where &#39;*&#39; means &#39;any-character-run&#39; rather \nthan &#39;0-or-more-of-preceding&#39;). We accept expressions as implemented by \nthe Java &#39;Pattern&#39; class, which are more-or-less the same as in Perl and \nelsewhere. But also, you might be able to convert to the &#39;SURT&#39; \nreordered URI prefixes we prefer for scope-matching.\n\n- if your machine has multiple independent disks, spreading the major IO \ntargets across them (the &#39;state&#39;, &#39;arcs&#39;, &#39;scratch&#39; directories) can \noffer some speedup. (If your disks are already in a shared array, no need.)\n\n- you have ARC compression off. Unless you&#39;re CPU-saturated or your data \nis uncompressible, it&#39;s often faster to compress-then-write than \nwrite-uncompressed.\n\nFinally, I see that your politeness and crawler-identification settings \nare really only appropriate for crawling your own closely-affiliated \nsites. (You&#39;ve disabled all politeness pauses; set multiple parallel \nqueues/connections per host; and used a non-functional User-Agent \ncontact URL and &#39;From&#39; email address.) If such settings are used on \nunaffiliated sites without permission, you&#39;ll quickly earn enemies, \nhate-mail, network-level-blocking, and you could possibly even face \nlegal prosecution if your traffic flood disables the site for other \nusers. A recklessly-configured crawler can very easily become a \nDenial-of-Service attack.\n\n&gt; this is our system configuration:\n&gt; Windows 2008 server [Intel(R) Xeon 2.40Ghz (dual core), Installed RAM\n&gt; (16g), 64-bit OS ]\n&gt;\n&gt; We are using 200 threads @ 2 g JVM heap.\n\nThat&#39;s a suitable crawling machine. If it&#39;s not being used for anything \nelse simultaneously, you could give the Java heap more of the 16GB RAM.\nWe and many other major users of Heritrix only use (and test or \nprioritize-fixes for) Linux, but Heritrix generally works wherever \nrecent Java does.\n\n- Gordon @ IA\n\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; Regexes can be wildly different in their performance characteristics\n&gt;  &gt; with just a single character difference; you may want to profile and\n&gt;  &gt; optimize your regexes.\n&gt;  &gt;\n&gt;  &gt; Also, long lists of regexes to apply in sequence can be costly for\n&gt;  &gt; ruling things in or out; that&#39;s why we try to use simpler\n&gt;  &gt; prefix-matching whenever possible.\n&gt;  &gt;\n&gt;  &gt; We&#39;ve had crawls on relatively modest machines (2 cores, ~4GB RAM, 4\n&gt;  &gt; disks) that collect 150-200 URIs/sec at initial launch but then settle\n&gt;  &gt; to 30-60 URIs/sec after days or weeks of running.\n&gt;  &gt;\n&gt;  &gt; What you might get depends a lot on your system specs, and the general\n&gt;  &gt; &#39;shape&#39;/settings of your crawl. Tweaking the settings can make a big\n&gt;  &gt; difference, and there&#39;s also a lot more room for optimization in the\n&gt;  &gt; code if any team needed to really squeeze the most out of a small\n&gt;  &gt; hardware budget. (We have often chosen &#39;more machines&#39; over &#39;more coding\n&gt;  &gt; optimizations&#39; when we need greater throughput.)\n&gt;  &gt;\n&gt;  &gt; - Gordon @ IA\n&gt;  &gt;\n&gt;  &gt; On 6/29/11 9:48 AM, cosmicpioneer2011 wrote:\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Hi,\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; We have been using heritrix for a while now and we like it so far with\n&gt;  &gt; &gt; all the pluggable modules and ease of extensibility. We see a big\n&gt;  &gt; &gt; difference in the crawl rate. We ran two parallel crawls one with a\n&gt; seed\n&gt;  &gt; &gt; url and no RegExp Decide filters and another with RegExp Decide rule\n&gt;  &gt; &gt; filters and we see that the crawler with filters performs crawl rate at\n&gt;  &gt; &gt; 6 to 9 URIs per second compared to the crawl without the decide filter\n&gt;  &gt; &gt; performs at 60 URIs/sec (on Avg). And we would like to scale to\n&gt;  &gt; &gt; performing the crawl at a higher rate since our needs are more.\n&gt; & gt; &gt;\n&gt;  &gt; &gt; Our aim should be to get to &gt;500 URIs/sec which translates to 1.8\n&gt;  &gt; &gt; Million/Hr. What is the max crawl rate that can be achived by heritrix\n&gt;  &gt; &gt; crawler.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Thanks, -Vinoth.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt;\n&gt;\n&gt;\n&gt; \n\n"}}