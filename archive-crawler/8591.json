{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":496150545,"authorName":"Markus.Mirsberger","from":"&quot;Markus.Mirsberger&quot; &lt;markus.mirsberger@...&gt;","profile":"mirschi74","replyTo":"LIST","senderId":"PS4QIqU0PUhz_UifTSvHMyq7jiTHwEivzRIt4EI_pL6TlSEN9V2vUso7eVIRmktYo7rXDx7Ve2ueb7DZoYTbs61gtUdXmYuIz-m3fmxH2Mrh0GGqKQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] how to speed up crawls?","postDate":"1408952574","msgId":8591,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUzRkFFOEZFLjkwOTA0MDRAZ214LmRlPg==","inReplyToHeader":"PDUzRkFEODYwLjYwMTAyMDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PDUzRjlENzMxLjUwMzA0MDlAZ214LmRlPiA8NTNGQUQ4NjAuNjAxMDIwN0BhcmNoaXZlLm9yZz4="},"prevInTopic":8590,"nextInTopic":8592,"prevInTime":8590,"nextInTime":8592,"topicId":8589,"numMessagesInTopic":10,"msgSnippet":"Hi Gordon, thanks for the answer. I already set parallelQueues to 20 before but this didn t increase the number of active queues/threads as you could see. What","rawEmail":"Return-Path: &lt;markus.mirsberger@...&gt;\r\nX-Sender: markus.mirsberger@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 74420 invoked by uid 102); 25 Aug 2014 07:42:59 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m10.grp.bf1.yahoo.com with SMTP; 25 Aug 2014 07:42:59 -0000\r\nX-Received: (qmail 4447 invoked from network); 25 Aug 2014 07:42:59 -0000\r\nX-Received: from unknown (HELO mout.gmx.net) (98.139.170.166)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 25 Aug 2014 07:42:59 -0000\r\nX-Received: from [192.168.88.115] ([183.89.50.93]) by mail.gmx.com (mrgmx101)\n with ESMTPSA (Nemesis) id 0LcFTN-1Wf1vw3KpJ-00jWHg for\n &lt;archive-crawler@yahoogroups.com&gt;; Mon, 25 Aug 2014 09:42:58 +0200\r\nMessage-ID: &lt;53FAE8FE.9090404@...&gt;\r\nDate: Mon, 25 Aug 2014 14:42:54 +0700\r\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;53F9D731.5030409@...&gt; &lt;53FAD860.6010207@...&gt;\r\nIn-Reply-To: &lt;53FAD860.6010207@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Provags-ID:  V03:K0:ukrK2T2mc9T1MxLxzl95pGc1cMpZC0aUplCIUYjziwZZHw7lYmy\n M2+EfwwJE0L1V1t3yBJuPI9Pa9XSp681I47qe2Hp4VNNBy9BAPLvfyfqsv0nxZXb2A2+9FW\n kRhp4DhTjqjjROwlianHiUCv6ZzeTjx5g0jr8mVNYUQBIxYw8a7IK+lYlSAFR8zc3VpjmCl\n udrnvO0f7DejklhZ5kJgA==\r\nX-UI-Out-Filterresults: notjunk:1;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nSubject: Re: [archive-crawler] how to speed up crawls?\r\nX-Yahoo-Group-Post: member; u=496150545; y=QX3PMPmNLfCwgehbsUUhM0yFcADtPBFrYbyANraExRgYWhOIDqY0z4XHw__mfyhAWcZ6OiruG9T26tw\r\nX-Yahoo-Profile: mirschi74\r\nFrom: &quot;Markus.Mirsberger&quot; &lt;markus.mirsberger@...&gt;\r\n\r\nHi Gordon,\n\nthanks for the answer.\n\nI already set parallelQueues to 20 before but this didn&#39;t increase the \nnumber of active queues/threads as you could see.\nWhat I did now was setting deferToPrevious to &quot;true&quot;. At least this \nincreased the number of parallel threads to 2-3.\nBut I still dont get more than 4kb/sec :(\n\nI think it is not necessary to use sheets in my case. I crawl every \ndomain in an own crawljob and dont allow the job to crawl other domains.\n\n\nRegards,\nMarkus\n\nOn 25.08.2014 13:32, Gordon Mohr gojomo@... [archive-crawler] wrote:\n&gt; The core politeness assumptions of Heritrix have been that URIs for a\n&gt; single &#39;site&#39; should go into a single logical queue, and that only one\n&gt; URI should be in process from any one queue at a time.\n&gt;\n&gt; That&#39;s what you&#39;re running into, even after minimizing the other\n&gt; politeness delays between fetches.\n&gt;\n&gt; If you&#39;re sure that more-aggressive crawling is alright (as seems to be\n&gt; the case with your target site with owner permission), you can cause the\n&gt; usual queue-assignment to instead distribute a single site&#39;s URIs over\n&gt; many related queues. Then, if (for example) using 5 queues, 5 requests\n&gt; can be in process in parallel, resulting in up to 5X faster crawling.\n&gt;\n&gt; The relevant setting is &#39;parallelQueues&#39; on the QueueAssignmentPolicy.\n&gt; See some notes here:\n&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Crawl+Operators#H3DevNotesforCrawlOperators-QueueAssignmentPolicies:%27parallelQueues%27and%27deferToPrevious%27settings\n&gt;\n&gt; As it&#39;s usually the case that you want most sites to be crawled in the\n&gt; normal polite manner, even when there are one or more sites that can be\n&gt; crawled more aggressively, it often makes sense to only set a higher\n&gt; &#39;parallelQueues&#39; value in a &quot;sheet override&quot; to affect some subset of\n&gt; sites. Thus, that&#39;s one of the example overridden settings in the sheets\n&gt; example at:\n&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Sheets\n&gt;\n&gt; - Gordon\n&gt;\n&gt; On 8/24/14, 5:14 AM, &#39;Markus.Mirsberger&#39; markus.mirsberger@...\n&gt; [archive-crawler] wrote:\n&gt;&gt;\n&gt;&gt; Hi,\n&gt;&gt;\n&gt;&gt; are there any ways to speed up an active crawl?\n&gt;&gt; I am always crawling only one single domain and I was told by the owner\n&gt;&gt; that it is ok when I crawl it with up to 10Url/sec. As You can see in\n&gt;&gt; the pasted jobdata ... a little bit more than 1 Url/sec is all I can get.\n&gt;&gt;\n&gt;&gt; I tried already several things like rising the amount of queues and\n&gt;&gt; threads, shorten the breaks between the requests but nothing really works.\n&gt;&gt; I also wonder that there is only one active thread and also only one\n&gt;&gt; queue in use.\n&gt;&gt;\n&gt;&gt; Do you have any suggestions where in the config I can turn on the turbo?:)\n&gt;&gt;\n&gt;&gt; Thanks in advance,\n&gt;&gt; Markus\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;      Job is Active: RUNNING\n&gt;&gt;\n&gt;&gt; Totals\n&gt;&gt;      738743 downloaded + 5950664 queued = 6689408 total\n&gt;&gt;      44 GiB crawled (44 GiB novel, 0 B dupByHash, 0 B notModified)\n&gt;&gt; Alerts\n&gt;&gt;      /none/\n&gt;&gt; Rates\n&gt;&gt;      1.16 URIs/sec (2.11 avg); 84 KB/sec (131 avg)\n&gt;&gt; Load\n&gt;&gt;      1 active of 100 threads; 1 congestion ratio; 5950664 deepest queue;\n&gt;&gt;      5950664 average depth\n&gt;&gt; Elapsed\n&gt;&gt;      4d1h7m37s131ms\n&gt;&gt; Threads\n&gt;&gt;      100 threads: 99 ABOUT_TO_GET_URI, 1 ABOUT_TO_BEGIN_PROCESSOR; 99\n&gt;&gt;      noActiveProcessor, 1 extractorHtml\n&gt;&gt; *Frontier*\n&gt;&gt;      RUN - 15 URI queues: 1 active (1 in-process; 0 ready; 0 snoozed); 0\n&gt;&gt;      inactive; 0 ineligible; 0 retired; 14 exhausted\n&gt;&gt; Memory\n&gt;&gt;      10369079 KiB used; 15480904 KiB current heap; 16270464 KiB max heap\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo Groups Links\n&gt;\n&gt;\n&gt;\n\n\n"}}