{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"chnf-xlwleoyEdxfXPQqccAhRB9fs0aGWcB3s1XaNY2PmhjcKLJ5YZznBbeAeEleM0iJmDA1XZGgjvUvHMZlrve5eWDg8y0","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] memory amount","postDate":"1233126074","msgId":5655,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5ODAwMkJBLjYwMDA2MDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGM1MDczNjZmMDkwMTI3MjIxMnIxYThkYTEyOGdiNzEyZDU4MDc4MDRjYzA2QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGM1MDczNjZmMDkwMTI3MDI0MGoyZGZjOWJieTM4ODY4ZjZmNGNlMWUyOWZAbWFpbC5nbWFpbC5jb20+CSA8NDk3RkVFRUYuNjA0MDEwNkBhcmNoaXZlLm9yZz4gPGM1MDczNjZmMDkwMTI3MjIxMnIxYThkYTEyOGdiNzEyZDU4MDc4MDRjYzA2QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5654,"nextInTopic":0,"prevInTime":5654,"nextInTime":5656,"topicId":5652,"numMessagesInTopic":4,"msgSnippet":"... That may not help, unless you also change the BDB cache-percent setting. Each crawl s database environment will grow, as long as the crawl is still","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 26050 invoked from network); 28 Jan 2009 07:01:16 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m52.grp.scd.yahoo.com with QMQP; 28 Jan 2009 07:01:16 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta18.grp.scd.yahoo.com with SMTP; 28 Jan 2009 07:01:16 -0000\r\nX-Received: (qmail 87116 invoked from network); 28 Jan 2009 07:01:14 -0000\r\nX-Received: from 70.137.133.158 (HELO ?10.0.13.7?) (70.137.133.158)\n  by relay00.pair.com with SMTP; 28 Jan 2009 07:01:14 -0000\r\nX-pair-Authenticated: 70.137.133.158\r\nMessage-ID: &lt;498002BA.6000600@...&gt;\r\nDate: Tue, 27 Jan 2009 23:01:14 -0800\r\nUser-Agent: Thunderbird 2.0.0.19 (Windows/20081209)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c507366f0901270240j2dfc9bby38868f6f4ce1e29f@...&gt;\t &lt;497FEEEF.6040106@...&gt; &lt;c507366f0901272212r1a8da128gb712d5807804cc06@...&gt;\r\nIn-Reply-To: &lt;c507366f0901272212r1a8da128gb712d5807804cc06@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] memory amount\r\nX-Yahoo-Group-Post: member; u=137285340; y=BacjdOs9fWCsbjkxc_o83mNhQGmpDteJdkj-eqAU6_K9\r\nX-Yahoo-Profile: gojomo\r\n\r\ntakeru sasaki wrote:\n&gt; Thank you for your help.\n&gt; \n&gt; Default setting is -Xmx256m, I know it is for single crawl.\n&gt; I will try with 256*(same time crawls) MB memory.\n&gt; Thank you very much!\n\nThat may not help, unless you also change the BDB cache-percent setting.\n\nEach crawl&#39;s database environment will grow, as long as the crawl is \nstill progressing, until it reaches 60% of heap. With more than one \nactive crawl, that means together they will try to grow beyond available \nmemory.\n\n&gt; And new question,\n&gt; I created jobs for each &#39;site&#39; group.\n&gt; For example, engadget.com-CrawlJob, apple.com-CrawlJob, ....\n&gt; Is it bad way?\n\nThere is significant memory overhead per crawl. We usually combine many \nsites into a single crawl, and only separate materials -- if necessary \n-- as part of postprocessing/indexing. This also ensures that any \nresources referred to by multiple sites (offsite IMGs, JS, frames, etc.) \nare only crawled once. If you can use once combined crawl, I recommend \nthat.\n\n- Gordon @ IA\n\n&gt; \n&gt; takeru\n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}