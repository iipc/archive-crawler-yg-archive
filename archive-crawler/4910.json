{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"64afXABt0H6ZKkR72GYQM7OgACz0etYHcyE1c5-sOJ0Aj4Q6geAh8DGQTmLF3ukWzFTQPLtcTCtj44vp7D8NOFR4xZ1i4Y9H","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Best approach question","postDate":"1200697373","msgId":4910,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3OTEzMDFELjEwNzAyMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEEwNzdERjdDRjc3MzM1NERCMjg1Mjc5MDMwRjZBMUE1MzFCODE5QE9YWUdFTi5zaXJzaS5wdnQ+","referencesHeader":"PEEwNzdERjdDRjc3MzM1NERCMjg1Mjc5MDMwRjZBMUE1MzFCODE5QE9YWUdFTi5zaXJzaS5wdnQ+"},"prevInTopic":4908,"nextInTopic":4912,"prevInTime":4909,"nextInTime":4911,"topicId":4906,"numMessagesInTopic":6,"msgSnippet":"I think that it will be OK, but I am not sure where the breaking point is :) Maybe running a test crawl is a good idea. If you want to write a new decide rule","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 77553 invoked from network); 18 Jan 2008 23:02:56 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m54.grp.scd.yahoo.com with QMQP; 18 Jan 2008 23:02:56 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta16.grp.scd.yahoo.com with SMTP; 18 Jan 2008 23:02:56 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id AF1F748497\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 18 Jan 2008 15:02:55 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id UWtKcgPmrKmf for &lt;archive-crawler@yahoogroups.com&gt;;\n\tFri, 18 Jan 2008 15:02:55 -0800 (PST)\r\nX-Received: from [192.168.1.107] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 374164837E\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 18 Jan 2008 15:02:55 -0800 (PST)\r\nMessage-ID: &lt;4791301D.1070204@...&gt;\r\nDate: Fri, 18 Jan 2008 15:02:53 -0800\r\nUser-Agent: Thunderbird 2.0.0.9 (Windows/20071031)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;A077DF7CF773354DB285279030F6A1A531B819@...&gt;\r\nIn-Reply-To: &lt;A077DF7CF773354DB285279030F6A1A531B819@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Best approach question\r\nX-Yahoo-Group-Post: member; u=137477665; y=mdJroJUz_83V2AwL6qoOYhdhgfX86JPIoJiYpHxZ8ruaPPikSw\r\nX-Yahoo-Profile: iranitovic\r\n\r\nI think that it will be OK, but I am not sure where the breaking point \nis :) Maybe running a test crawl is a good idea.\n\nIf you want to write a new decide rule you can subclass the \nSurtPrefixedDecideRule and override the &#39;evaluate&#39; method.\n\nMaybe something like this (not tested):\n\nprotected boolean evaluate(Object object) {\n  if ( (object instanceof CandidateURI) &&\n          ((Boolean) getUncheckedAttribute(null, ATTR_ALSO_CHECK_VIA))\n                     .booleanValue()) {\n     if(evaluate(((CandidateURI)object).getVia())) {\n             return true;\n      }\n   }\n   String candidateSurt;\n   candidateSurt = SurtPrefixSet.getCandidateSurt(object);\n\n// Drop everything after the last &#39;/&#39;\n   candidateSurt = candidateSurt.replaceFirst(&quot;^(.*/)[^/]*$&quot;, &quot;$1&quot;);\t\n\n   if (candidateSurt == null) {\n           return false;\n   }\n\n// Check if we have exact match\n   return getPrefixes().contains(candidateSurt);\n}\n\n\nTake care,\ni.\n\n\nTravis Jensen wrote:\n&gt; \n&gt; Hi,\n&gt; \n&gt; Thanks for your reply, Igor.  \n&gt; \n&gt; Would this still be a preferred way if I have 1500 of these URLs?  I\n&gt; would worry about the performance hit of every crawled URL needing to go\n&gt; through an average of 750 regex matches to find the one that will match.\n&gt; \n&gt; tj\n&gt; \n&gt; -----Original Message-----\n&gt; From: archive-crawler@yahoogroups.com\n&gt; [mailto:archive-crawler@yahoogroups.com] On Behalf Of Igor Ranitovic\n&gt; Sent: Friday, January 18, 2008 12:26 PM\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Subject: Re: [archive-crawler] Best approach question\n&gt; \n&gt; \n&gt; I don&#39;t think that you need to run separate jobs for each seed.\n&gt; \n&gt; For example, is you have two seeds as:\n&gt; \n&gt; http://www.foo.com/baz/bar2.html\n&gt; http://www.foo.com/zab/bar2.html\n&gt; \n&gt; you can have scope as:\n&gt; \n&gt; +http://www.foo.com/baz/bar2.html\n&gt; +http://www.foo.com/zab/bar2.html\n&gt; \n&gt; and this decide rule:\n&gt; \n&gt; &lt;newObject name=&quot;seedsCurrentFolderOnly&quot; \n&gt; class=&quot;org.archive.crawler.deciderules.MatchesListRegExpDecideRule&quot;&gt;\n&gt;   &lt;string name=&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n&gt;   &lt;string name=&quot;list-logic&quot;&gt;OR&lt;/string&gt;\n&gt;   &lt;stringList name=&quot;regexp-list&quot;&gt;\n&gt;      &lt;string&gt;(?i)http://www.foo.com/baz/.*/&lt;/string&gt;\n&gt;      &lt;string&gt;(?i)http://www.foo.com/zab/.*/&lt;/string&gt;\n&gt;    &lt;/stringList&gt;\n&gt; &lt;/newObject&gt;\n&gt; \n&gt; \n&gt; Creating the scope and MatchesListRegExpDecideRule can be done easily \n&gt; with little scripting.\n&gt; \n&gt; I hope this helps.\n&gt; \n&gt; Take care,\n&gt; i.\n&gt; \n&gt; Travis Jensen wrote:\n&gt;&gt; Hi,\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; I&#39;m looking for some feedback as to a best approach to a crawling \n&gt;&gt; problem I need solve.  I have a list of URLs, some of which I only\n&gt; want \n&gt;&gt; to crawl that URL, some of which I want to crawl that whole domain \n&gt;&gt; (&quot;foo.com&quot;), some I want to crawl only that server (&quot;www.foo.com&quot;),\n&gt; some \n&gt;&gt; I want to crawl anything below the given URL, and some I want to crawl\n&gt; \n&gt;&gt; everything in the URL&#39;s folder.\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; The first four are out-of-the-box from my understanding of Heritrix 2,\n&gt; \n&gt;&gt; which is great.  The last one doesn&#39;t seem to be, so I&#39;ve been looking\n&gt; \n&gt;&gt; at options on how to implement it.  Just to clearly define the\n&gt; problem, \n&gt;&gt; if I&#39;m given a seed URL of:\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; http://www.foo.com/baz/bar.html\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; then I want to match:\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; http://www.foo.com/baz/bar2.html,\n&gt; http://www.foo.com/baz/somethingelse.html\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; but I don&#39;t want to match:\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; http://www.foo.com/apage.html,\n&gt; http://www.foo.com/baz/bang/anotherpage.html\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; The two methods I&#39;ve found to do this would be to define a regex of\n&gt; the \n&gt;&gt; matching URL and use a MatchesRegExpDecideRule or to create my own \n&gt;&gt; DecideRule (call it FolderOnlyDecideRule).\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; If I use the MatchesRegExpDecideRule, will I have to create a\n&gt; different \n&gt;&gt; job for each seed URL (because the regex will be different)?  Or is it\n&gt; \n&gt;&gt; possible to say &quot;this job uses this seed URL with this regex&quot;?\n&gt; Dealing \n&gt;&gt; with a different job for each URL seems painful.\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; If the FolderOnlyDecideRule is the way to go, can I get some pointers\n&gt; on \n&gt;&gt; how to go about implementing it.  I&#39;ve been looking through the \n&gt;&gt; DecideRules and it hasn&#39;t &quot;clicked&quot; yet. :)\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; Thanks.\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt;\n&gt;&gt; tj\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}