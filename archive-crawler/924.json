{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":6903103,"authorName":"Tom Emerson","from":"Tom Emerson &lt;Tree@...&gt;","profile":"tree02139","replyTo":"LIST","senderId":"hZJMCzWvQJm41mff6lrbAB9I_u6ueI0JE4P9UOdlUd8snaiQzPWdnf0fUJCDofhiXSpuIJjY30mRgLhl1xbtAK22dW3rN94","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Is it possible to write crawled files to disk instead of arc","postDate":"1093950870","msgId":924,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDE2NjkyLjIzOTU4LjQ0NjgyOS45MDI4NzFAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","inReplyToHeader":"PGNoMTQ3dCtscWZAZUdyb3Vwcy5jb20+","referencesHeader":"PGNoMTQ3dCtscWZAZUdyb3Vwcy5jb20+"},"prevInTopic":918,"nextInTopic":927,"prevInTime":923,"nextInTime":925,"topicId":918,"numMessagesInTopic":4,"msgSnippet":"... When you disabled the ARC writer you disabled any way for the crawler to save its output, which is why you didn t see anything. You will need to write your","rawEmail":"Return-Path: &lt;Tree@...&gt;\r\nX-Sender: Tree@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 3193 invoked from network); 31 Aug 2004 11:14:56 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m2.grp.scd.yahoo.com with QMQP; 31 Aug 2004 11:14:56 -0000\r\nReceived: from unknown (HELO mailserver.basistech.com) (199.88.205.4)\n  by mta1.grp.scd.yahoo.com with SMTP; 31 Aug 2004 11:14:56 -0000\r\nReceived: from postfix.basistech.com ([10.1.3.65] RDNS failed) by mailserver.basistech.com with Microsoft SMTPSVC(6.0.3790.0);\n\t Tue, 31 Aug 2004 07:14:30 -0400\r\nReceived: by postfix.basistech.com (Postfix, from userid 5007)\n\tid 84FAD2D123E; Tue, 31 Aug 2004 07:14:30 -0400 (EDT)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\nMessage-ID: &lt;16692.23958.446829.902871@...&gt;\r\nDate: Tue, 31 Aug 2004 07:14:30 -0400\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;ch147t+lqf@...&gt;\r\nReferences: &lt;ch147t+lqf@...&gt;\r\nX-Mailer: VM 7.18 under Emacs 21.2.1\r\nReturn-Path: tree@...\r\nX-OriginalArrivalTime: 31 Aug 2004 11:14:30.0751 (UTC) FILETIME=[AFC5C6F0:01C48F4B]\r\nX-eGroups-Remote-IP: 199.88.205.4\r\nFrom: Tom Emerson &lt;Tree@...&gt;\r\nReply-To: tree@...\r\nSubject: Re: [archive-crawler] Is it possible to write crawled files to disk instead of arc\r\nX-Yahoo-Group-Post: member; u=6903103\r\nX-Yahoo-Profile: tree02139\r\n\r\nBruce writes:\n&gt; I am looking at using arcdump to work with the data inside arc files \n&gt; but it would be much easier for me if I could work with the raw \n&gt; files outside of the .arc file.  I tried disabling the arc writer in \n&gt; settings but then Heritrix seems to dispose of the downloaded co \n&gt; ntent without storing.  Any help would be greatly appreciated.  \n\nWhen you disabled the ARC writer you disabled any way for the crawler\nto save its output, which is why you didn&#39;t see anything.\n\nYou will need to write your own Processor that saves the files\ndirectly to disk instead of writing them into an ARC file. Doing this\nshouldn&#39;t be too difficult, but it can be tricky:\n\n1) Depending on your scope, there could be tens of thousands of files\n   being written. Many file systems balk (to put it politely) at\n   having thousands of files in a single directory.\n\n2) You need to come up with a naming scheme for the generated files,\n   since you cannot rely on the name in the URI for a couple of\n   reasons:\n\n   - Duplicate file names: imagine how many index.html files you will\n     get on a non-trivial crawl.\n\n   - Dynamically generated pages with long names: there may not be a\n     file name to use in the URI. For example, a page from a Bugzilla\n     installation (to pick a contrived example) might be\n     &lt;http://bugzilla.some.com/show_bug.cgi?id=42&gt; What do you call\n     this file when you write it to disk.\n\nThese are not unsurmountable problems, but the ARC format mitigates\nthe problems.\n\nWith that said, I have a little utility that uses my libarc library to\nextract the text/html documents from a set of ARC files into a\ndirectory structure where 1000 files are written in consecutively\nnumbered directories, and the file names are similarly synthesized\nfrom a prefix and number. If people are interested I can make this\navailable.\n\n    -tree\n\n-- \nTom Emerson                                          Basis Technology Corp.\nSoftware Architect                                 http://www.basistech.com\n  &quot;Beware the lollipop of mediocrity: lick it once and you suck forever&quot;\n\n"}}