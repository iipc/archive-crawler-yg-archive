{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"90Ugf3so7c49DNjQlifqQqDTtnBf4M9fkVpc1kjHL9Id8rCM1Ny0H5lLZhBnXpJTM6LHo-1nFZ78H96sjpxfSg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] How do I Change crawler behaviour?","postDate":"1140733021","msgId":2711,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzRkUzNDVELjcwMDA5MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGYxZGExYjhhMDYwMjIzMTQxMHQ3ZDcxNmM1NGk3NWZkZTVjNjZiYWFiZWExQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGYxZGExYjhhMDYwMjIzMTEzOGk0MzJkNmQ5ZWlmNjBlMTYwOWE3MjRlNGZjQG1haWwuZ21haWwuY29tPgkgPDQzRkUxQjQ5LjIwODAxMDJAYXJjaGl2ZS5vcmc+IDxmMWRhMWI4YTA2MDIyMzE0MTB0N2Q3MTZjNTRpNzVmZGU1YzY2YmFhYmVhMUBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":2710,"nextInTopic":2716,"prevInTime":2710,"nextInTime":2712,"topicId":2706,"numMessagesInTopic":5,"msgSnippet":"... The selftest uses ARCReader (as does Chronica).  Check out content of this package: ","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 35735 invoked from network); 23 Feb 2006 22:17:30 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m9.grp.scd.yahoo.com with QMQP; 23 Feb 2006 22:17:30 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta3.grp.scd.yahoo.com with SMTP; 23 Feb 2006 22:17:30 -0000\r\nReceived: (qmail 26370 invoked by uid 100); 23 Feb 2006 22:09:23 -0000\r\nReceived: from adsl-71-130-102-77.dsl.pltn13.pacbell.net (HELO ?192.168.1.197?) (stack@...@71.130.102.77)\n  by mail-dev.archive.org with SMTP; 23 Feb 2006 22:09:23 -0000\r\nMessage-ID: &lt;43FE345D.7000905@...&gt;\r\nDate: Thu, 23 Feb 2006 14:17:01 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8) Gecko/20051219 SeaMonkey/1.0b\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f1da1b8a0602231138i432d6d9eif60e1609a724e4fc@...&gt;\t &lt;43FE1B49.2080102@...&gt; &lt;f1da1b8a0602231410t7d716c54i75fde5c66baabea1@...&gt;\r\nIn-Reply-To: &lt;f1da1b8a0602231410t7d716c54i75fde5c66baabea1@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-95.8 required=7.0 tests=AWL,HTTP_ESCAPED_HOST,\n\tUSER_IN_WHITELIST autolearn=no version=2.63\r\nX-eGroups-Msg-Info: 2:12:4:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] How do I Change crawler behaviour?\r\nX-Yahoo-Group-Post: member; u=168599281; y=WG-k-hxzTjHfsUFU8PitcRy7bYgmLD04kwKZCtycu8fe83VEUv4JcWjz\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nThirumalai Veerasamy wrote:\n&gt; Thanks for the quick response.\n&gt; \n&gt; In case I need to go for ARC format, do we have any tool to read that \n&gt; file and give me the required file from ARC? I just checked chronica. Do \n&gt; we have any other tool ?\n&gt; \n\nThe selftest uses ARCReader (as does Chronica).  Check out content of \nthis package: \nhttp://crawler.archive.org/xref/org/archive/crawler/selftest/package-summary.html. \n  See in particular SelfTestCase and the #initialize method: \nhttp://crawler.archive.org/xref/org/archive/crawler/selftest/SelfTestCase.html#191. \n  This is a subclass of junit TestCase with utility methods that allow \nyou ask about content of an ARC.\n\nSt.Ack\n\n\n&gt; \n&gt; \n&gt; On 2/23/06, *stack* &lt;stack@... &lt;mailto:stack@...&gt;&gt; wrote:\n&gt; \n&gt;     Thirumalai Veerasamy wrote:\n&gt;     &gt;  Hi,\n&gt;     &gt;\n&gt;     &gt;    I wanted to crawl through my entire web application.\n&gt;     &gt;\n&gt;     &gt;   The reasons\n&gt;     &gt;\n&gt;     &gt;   1. Test for broken link\n&gt;     &gt;   2. Record the crawling as script and play it again\n&gt;     &gt;   3. Find out the elapsed time for individual pages\n&gt;     &gt;\n&gt;     &gt;   I wrote a custom component using apache httpconnection. Now I wanted\n&gt;     &gt;  to save each web page, so that I can go through individual page and\n&gt;     &gt;  verify whether the page is display what it is supposed to display.\n&gt;     &gt;\n&gt;     &gt;   I found archive crawler can be used to save the pages using\n&gt;     &gt;  MirrorWriterProcessor. I also feel that archive crawler is bit heavy\n&gt;     &gt;  app to crawl just 200-400 pages of my application.\n&gt; \n&gt;     Possibly though the crawler &#39;selftests&#39; by crawling a special\n&gt;     &#39;selftest&#39;\n&gt;     webapp.  We then do examination of the produced ARC files to see that\n&gt;     what was supposed to be downloaded was (and that what was not supposed\n&gt;     to be downloaded, wasn&#39;t).  You build out the selftest by adding junit\n&gt;     based &#39;SelfTests&#39;. Not elegant but works.\n&gt;     &gt;\n&gt;     &gt;   I have few more questions before I can go deep into this crawler.\n&gt;     &gt;\n&gt;     &gt;   1. Can I skip certain pages?\n&gt;     &gt;       i I want to crawl only urls in that particular domain.\n&gt;      &gt; (http://localhost), &lt;http://localhost%29,&gt; &lt; http://localhost%29,&gt;\n&gt;     &gt;       ii. skip urls which satisfies some regular expression.\n&gt;     Yes on above two.  Play with the deciderules in a decidingscope.\n&gt; \n&gt;     &gt;       iii remove some paramter from the url and check whether it is\n&gt;     &gt;  already crawled (the web app add a paramter which doesn&#39;t change the\n&gt;     &gt;  behaviour of the page, but creates different urls for the same page\n&gt;     &gt;  which I don&#39;t want to crawl).\n&gt;     You&#39;d have to add a little code to do this.  See processors in the\n&gt;     postprocessor package.\n&gt; \n&gt;     &gt;    2. Find out the elapsed time for each page.\n&gt;     Time downloading?  Its set by the FetchHTTP fetcher into the CrawlURI\n&gt;     with the key A_FETCH_COMPLETED_TIME.\n&gt; \n&gt; \n&gt;     &gt;    3. Save html, images, etc., into a folder only if it is not already\n&gt;     &gt;  downloaded.\n&gt; \n&gt;     We should not be refetching the same URL twice.  You might want to\n&gt;     enable the hash of downloaded content and add a bit of code to not\n&gt;     write\n&gt;     to the folder items of the same hash (See the &quot;sha1-content&quot; attribute\n&gt;     of FetchHTTP).\n&gt; \n&gt;      &gt;    4. Support for proxy\n&gt;      &gt;\n&gt;     Its present.  See FetchHTTP.\n&gt; \n&gt; \n&gt;     St.Ack\n&gt; \n&gt; \n&gt;     SPONSORED LINKS\n&gt;     Computer security\n&gt;     &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+security&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=BHmcxBg5sKfN9-gcWnJWDg&gt;\n&gt;     \tComputer training\n&gt;     &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+training&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=v0JjJWA4s7mLnWQWdFxuTQ&gt;\n&gt; \n&gt; \n&gt; \n&gt;     ------------------------------------------------------------------------\n&gt;     YAHOO! GROUPS LINKS\n&gt; \n&gt;         *  Visit your group &quot;archive-crawler\n&gt;           &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;            \n&gt;         *  To unsubscribe from this group, send an email to:\n&gt;             archive-crawler-unsubscribe@yahoogroups.com\n&gt;           &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;            \n&gt;         *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;           Service &lt;http://docs.yahoo.com/info/terms/&gt; .\n&gt; \n&gt; \n&gt;     ------------------------------------------------------------------------\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; SPONSORED LINKS\n&gt; Computer security \n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+security&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=BHmcxBg5sKfN9-gcWnJWDg&gt; \n&gt; \tComputer training \n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+training&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=v0JjJWA4s7mLnWQWdFxuTQ&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt; \n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;. \n&gt; \n&gt; \n&gt; ------------------------------------------------------------------------\n&gt; \n\n\n"}}