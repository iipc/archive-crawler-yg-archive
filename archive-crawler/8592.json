{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"ftUt8UIuQ9umUvNwAgYoGhz8tnUGvqdTBTC9A6Q3Cv2TP0TaBX9b7Edq5D5b64J7uQGrTDKJpYxnlstawnUVhWDLliLWldQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] how to speed up crawls?","postDate":"1409000071","msgId":8592,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUzRkJBMjg3LjMwMDAzMDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDUzRkFFOEZFLjkwOTA0MDRAZ214LmRlPg==","referencesHeader":"PDUzRjlENzMxLjUwMzA0MDlAZ214LmRlPiA8NTNGQUQ4NjAuNjAxMDIwN0BhcmNoaXZlLm9yZz4gPDUzRkFFOEZFLjkwOTA0MDRAZ214LmRlPg=="},"prevInTopic":8591,"nextInTopic":8593,"prevInTime":8591,"nextInTime":8593,"topicId":8589,"numMessagesInTopic":10,"msgSnippet":"On 8/25/14, 12:42 AM, Markus.Mirsberger markus.mirsberger@gmx.de ... Hmm. That should be the only change necessary: from that point on, if you re using one","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 35903 invoked by uid 102); 25 Aug 2014 20:54:33 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m9.grp.bf1.yahoo.com with SMTP; 25 Aug 2014 20:54:33 -0000\r\nX-Received: (qmail 7876 invoked from network); 25 Aug 2014 20:54:32 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (98.139.170.166)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 25 Aug 2014 20:54:32 -0000\r\nX-Received: (qmail 7710 invoked by uid 0); 25 Aug 2014 20:54:31 -0000\r\nX-Received: from 142.254.51.72 (HELO probook.local) (142.254.51.72)\n  by relay01.pair.com with SMTP; 25 Aug 2014 20:54:31 -0000\r\nX-pair-Authenticated: 142.254.51.72\r\nMessage-ID: &lt;53FBA287.3000307@...&gt;\r\nDate: Mon, 25 Aug 2014 13:54:31 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;53F9D731.5030409@...&gt; &lt;53FAD860.6010207@...&gt; &lt;53FAE8FE.9090404@...&gt;\r\nIn-Reply-To: &lt;53FAE8FE.9090404@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nSubject: Re: [archive-crawler] how to speed up crawls?\r\nX-Yahoo-Group-Post: member; u=137285340; y=T99UE5NWsF67Hq9mBSHuHGldau4pzsH6PdB5xr27TdfO\r\nX-Yahoo-Profile: gojomo\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\n\r\nOn 8/25/14, 12:42 AM, &#39;Markus.Mirsberger&#39; markus.mirsberger@... \n[archive-crawler] wrote:\n&gt; I already set parallelQueues to 20 before but this didn&#39;t increase the\n&gt; number of active queues/threads as you could see.\n\nHmm. That should be the only change necessary: from that point on, if \nyou&#39;re using one of the standard QueueAssignmentPolicies, any URIs \ndiscovered on a single site should be scattered over 20 queues instead \nof 1. Unlike your prior stats snapshot showing only 1 queue with \nmillions of URIs, you&#39;d then see 20 queues.\n\nWas &#39;parallelQueues&#39; set to a higher value from the very beginning of \nthe crawl? Once a URI is sent to a queue, changing this setting \nmid-crawl doesn&#39;t cause any immediate change. The URI still needs to \ncome off its original queue, one at a time. Then, depending on the \n&#39;deferToPrevious&#39; setting, it might get processed without reconsidering \nits queue (deferToPrevious=true), or be immediately re-evaluated \n(deferToPrevious=false) and thus potentially re-enqueued rather than \nfetched.\n\n&gt; What I did now was setting deferToPrevious to &quot;true&quot;. At least this\n&gt; increased the number of parallel threads to 2-3.\n&gt; But I still dont get more than 4kb/sec :(\n\nThat sounds exactly backwards. If deferToPrevious=true, you&#39;ll get \n*less* reassignment-among-queues and thus less long-run speedup. Only \nnew URIs being discovered will get spread over 20 queues.\n\nNote, though, that with deferToPrevious=false, and changing \nparallelQueues mid-crawl, there is often then a tight busy-loop as \n19-out-of-each-20 URIs on the original single queue pop off and *don&#39;t* \nget crawled, but instead get reenqueued over other queues. This might \nstick out as a period of higher CPU and local IO usage, without much (or \nany) fetch speedup. However, as the giant queue shrinks rapidly, and \nother queues grow, you should then start to see a higher request rate \nover time, approaching the 20x speedup (if there aren&#39;t other CPU/IO \nbottlenecks).\n\nSo you&#39;d want to give the deferToPrevious=false setting some time to \nhave its effect on queue-distribution.\n\n&gt; I think it is not necessary to use sheets in my case. I crawl every\n&gt; domain in an own crawljob and dont allow the job to crawl other domains.\n\nThere were 14 other empty queues in your stats cut-and-paste... just be \naware when rerunning this crawl, if a global parallelQueues=20 setting \nis in effect, they&#39;ll all be equally hammered.\n\n- Gordon\n\n&gt;\n&gt; Regards,\n&gt; Markus\n&gt;\n&gt; On 25.08.2014 13:32, Gordon Mohr gojomo@... [archive-crawler] wrote:\n&gt;&gt; The core politeness assumptions of Heritrix have been that URIs for a\n&gt;&gt; single &#39;site&#39; should go into a single logical queue, and that only one\n&gt;&gt; URI should be in process from any one queue at a time.\n&gt;&gt;\n&gt;&gt; That&#39;s what you&#39;re running into, even after minimizing the other\n&gt;&gt; politeness delays between fetches.\n&gt;&gt;\n&gt;&gt; If you&#39;re sure that more-aggressive crawling is alright (as seems to be\n&gt;&gt; the case with your target site with owner permission), you can cause the\n&gt;&gt; usual queue-assignment to instead distribute a single site&#39;s URIs over\n&gt;&gt; many related queues. Then, if (for example) using 5 queues, 5 requests\n&gt;&gt; can be in process in parallel, resulting in up to 5X faster crawling.\n&gt;&gt;\n&gt;&gt; The relevant setting is &#39;parallelQueues&#39; on the QueueAssignmentPolicy.\n&gt;&gt; See some notes here:\n&gt;&gt;\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Crawl+Operators#H3DevNotesforCrawlOperators-QueueAssignmentPolicies:%27parallelQueues%27and%27deferToPrevious%27settings\n&gt;&gt;\n&gt;&gt; As it&#39;s usually the case that you want most sites to be crawled in the\n&gt;&gt; normal polite manner, even when there are one or more sites that can be\n&gt;&gt; crawled more aggressively, it often makes sense to only set a higher\n&gt;&gt; &#39;parallelQueues&#39; value in a &quot;sheet override&quot; to affect some subset of\n&gt;&gt; sites. Thus, that&#39;s one of the example overridden settings in the sheets\n&gt;&gt; example at:\n&gt;&gt;\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Sheets\n&gt;&gt;\n&gt;&gt; - Gordon\n&gt;&gt;\n&gt;&gt; On 8/24/14, 5:14 AM, &#39;Markus.Mirsberger&#39; markus.mirsberger@...\n&gt;&gt; [archive-crawler] wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt; Hi,\n&gt;&gt;&gt;\n&gt;&gt;&gt; are there any ways to speed up an active crawl?\n&gt;&gt;&gt; I am always crawling only one single domain and I was told by the owner\n&gt;&gt;&gt; that it is ok when I crawl it with up to 10Url/sec. As You can see in\n&gt;&gt;&gt; the pasted jobdata ... a little bit more than 1 Url/sec is all I can get.\n&gt;&gt;&gt;\n&gt;&gt;&gt; I tried already several things like rising the amount of queues and\n&gt;&gt;&gt; threads, shorten the breaks between the requests but nothing really works.\n&gt;&gt;&gt; I also wonder that there is only one active thread and also only one\n&gt;&gt;&gt; queue in use.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Do you have any suggestions where in the config I can turn on the turbo?:)\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks in advance,\n&gt;&gt;&gt; Markus\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;       Job is Active: RUNNING\n&gt;&gt;&gt;\n&gt;&gt;&gt; Totals\n&gt;&gt;&gt;       738743 downloaded + 5950664 queued = 6689408 total\n&gt;&gt;&gt;       44 GiB crawled (44 GiB novel, 0 B dupByHash, 0 B notModified)\n&gt;&gt;&gt; Alerts\n&gt;&gt;&gt;       /none/\n&gt;&gt;&gt; Rates\n&gt;&gt;&gt;       1.16 URIs/sec (2.11 avg); 84 KB/sec (131 avg)\n&gt;&gt;&gt; Load\n&gt;&gt;&gt;       1 active of 100 threads; 1 congestion ratio; 5950664 deepest queue;\n&gt;&gt;&gt;       5950664 average depth\n&gt;&gt;&gt; Elapsed\n&gt;&gt;&gt;       4d1h7m37s131ms\n&gt;&gt;&gt; Threads\n&gt;&gt;&gt;       100 threads: 99 ABOUT_TO_GET_URI, 1 ABOUT_TO_BEGIN_PROCESSOR; 99\n&gt;&gt;&gt;       noActiveProcessor, 1 extractorHtml\n&gt;&gt;&gt; *Frontier*\n&gt;&gt;&gt;       RUN - 15 URI queues: 1 active (1 in-process; 0 ready; 0 snoozed); 0\n&gt;&gt;&gt;       inactive; 0 ineligible; 0 retired; 14 exhausted\n&gt;&gt;&gt; Memory\n&gt;&gt;&gt;       10369079 KiB used; 15480904 KiB current heap; 16270464 KiB max heap\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; Yahoo Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}