{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"fEyJqd9AKzl286x91jx85m2qunZq4P1rpJhOoed7mlulfw9pjAKt7KaITPeyIaXWJ9sF9hpSEeBdsWoiH9GRQ1l8x5SwS6TC","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Max Size Related Configuration","postDate":"1080688164","msgId":299,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwNjlGRTI0LjgwNDAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQwNjlBMkZFLjUwNDA0MDNAYXJjaGl2ZS5vcmc+","referencesHeader":"PGM0YzNyNStjdDVwQGVHcm91cHMuY29tPiA8NDA2OUEyRkUuNTA0MDQwM0BhcmNoaXZlLm9yZz4="},"prevInTopic":297,"nextInTopic":300,"prevInTime":298,"nextInTime":300,"topicId":289,"numMessagesInTopic":9,"msgSnippet":"It seems that Sebs concern is not just polities but the number of bytes downloaded from sites. Some ISPs will charge you arm and leg if you exceed given","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 45720 invoked from network); 30 Mar 2004 23:16:43 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m8.grp.scd.yahoo.com with QMQP; 30 Mar 2004 23:16:43 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta6.grp.scd.yahoo.com with SMTP; 30 Mar 2004 23:16:42 -0000\r\nReceived: (qmail 13905 invoked by uid 100); 30 Mar 2004 23:11:56 -0000\r\nReceived: from b116-dyn-56.archive.org (HELO archive.org) (igor@...@209.237.240.56)\n  by mail-dev.archive.org with SMTP; 30 Mar 2004 23:11:56 -0000\r\nMessage-ID: &lt;4069FE24.8040303@...&gt;\r\nDate: Tue, 30 Mar 2004 15:09:24 -0800\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c4c3r5+ct5p@...&gt; &lt;4069A2FE.5040403@...&gt;\r\nIn-Reply-To: &lt;4069A2FE.5040403@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-0.0 required=6.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Re: Max Size Related Configuration\r\nX-Yahoo-Group-Post: member; u=137477665\r\nX-Yahoo-Profile: iranitovic\r\n\r\nIt seems that Sebs concern is not just polities but the number of bytes downloaded from sites.\nSome ISPs will charge you arm and leg if you exceed given traffic cap in bytes per month.\nFor example, you can buy a plan from a ISP that puts a cap of 100MB of traffic per host per month.\nIf you exceed this cap than you start paying usually for every extra MB of traffic.\nIf you upload a 3-4 MB mp3 song on your site, and it becomes popular, or being crawled often, than \nthe traffic cap can be easily exceeded (read costs money).\nMy suggestion is to write to webmasters of sites of interest and ask for permission to crawled them \nif possible.\n\nTake care.\ni.\n\nKristinn Sigur�sson wrote:\n\n&gt; See below\n&gt; \n&gt; sebastiandelachica wrote:\n&gt; \n&gt;&gt; Hiya Kris,\n&gt;&gt;\n&gt;&gt; Thanks for the hint. That is pretty much where I ended up last nite.\n&gt;&gt;\n&gt;&gt; To clarify, my original intent was to manage xtiple sites from a\n&gt;&gt; single crawl order, but to get the size limit per site, I ended\n&gt;&gt; up &quot;simplifying&quot; my approach and limiting each order to a single\n&gt;&gt; seed. I like the idea of controlling limits on # of downloaded  bytes\n&gt;&gt; on a per domain/host/etc. basis for xtiple seeds from a single order.\n&gt;&gt; I hope that concept becomes part of heritrix at some point in the\n&gt;&gt; future.\n&gt;&gt;\n&gt;&gt; To put things into perspective, my research work is related to the\n&gt;&gt; Digital Library for Earth Sciences (DLESE) http://www.dlese.org. I am\n&gt;&gt; trying to automatically tag online educational resources with\n&gt;&gt; National Science Educational Standards using natural language\n&gt;&gt; processing techniques. The reason for the size limit per site is b/c\n&gt;&gt; I need to avoid upsetting the community that provides the content.\n&gt; \n&gt; We are also aware of the fact that you can&#39;t overload websites and that \n&gt; is why the crawler is very\n&gt; polite. If you look at the settings under &#39;frontier&#39; you will find the \n&gt; following:\n&gt; \n&gt; delay-factor:\n&gt; max-delay-ms:\n&gt; min-delay-ms:\n&gt; min-interval-ms:\n&gt; \n&gt; These settings insure that Heritrix does not hammer a site needlessly. \n&gt; At any given time only\n&gt; one document is being fetched from the same host. The delay-factor says \n&gt; that for every X\n&gt; millisec you spend downloading a document you should wait factor of X \n&gt; before starting the\n&gt; next document download from that same host.  max and min delay allow you \n&gt; to set upper\n&gt; and lower bounds on this factor.\n&gt; \n&gt; Example: Last document took 200 msec and your delay factor is 3. That \n&gt; means that 600 msec\n&gt; will have to elapse before Heritrix starts another download.  If \n&gt; min-delay is set to 800 that will\n&gt; override and Heritrix will wait 800 msec.\n&gt; \n&gt; Even at it&#39;s most aggressive Heritrix will never fetch more then one \n&gt; doc. at a time and with the\n&gt; politeness settings found in the Simple profile you definately don&#39;t \n&gt; need to worry about\n&gt; upsetting anyone.\n&gt; \n&gt; - Kris\n&gt; \n&gt;&gt; Thanks for all the good advice and the Most Excellent Work on\n&gt;&gt; Heritrix!\n&gt;&gt;\n&gt;&gt; Seb\n&gt;&gt;\n&gt;&gt; --- In archive-crawler@yahoogroups.com, Kristinn Sigur�sson\n&gt;&gt; &lt;kris@a...&gt; wrote:\n&gt;&gt; &gt; Hei Seb.\n&gt;&gt; &gt;\n&gt;&gt; &gt; See below\n&gt;&gt; &gt;\n&gt;&gt; &gt; sebastiandelachica wrote:\n&gt;&gt; &gt;\n&gt;&gt; &gt; &gt; Michael,\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; Thanks for the prompt reply. Indeed I upgraded to 0.6.0 over the\n&gt;&gt; &gt; &gt; weekend: amazing how close a 9 looks to a 6 given enough lack of\n&gt;&gt; sleep\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; In English (to the best of my ability), what I am trying to do is\n&gt;&gt; use\n&gt;&gt; &gt; &gt; a separate order for each site I need to crawl. Each order hence\n&gt;&gt; has\n&gt;&gt; &gt; &gt; a single seed. The purpose is to place an upper bound on the\n&gt;&gt; number\n&gt;&gt; &gt; &gt; of &quot;useful&quot; bytes downloaded for each crawl order (about 100K or\n&gt;&gt; 1MB\n&gt;&gt; &gt; &gt; say). In other words, stop crawling the site once we have\n&gt;&gt; downloaded\n&gt;&gt; &gt; &gt; some number of usable bytes.\n&gt;&gt; &gt;\n&gt;&gt; &gt; If you are only crawling one site at a time (using DomainScope) the\n&gt;&gt; &gt; max-bytes-download\n&gt;&gt; &gt; just the thing for you.  It limits the total amount of data\n&gt;&gt; downloaded\n&gt;&gt; &gt; in one CrawlJob. It is\n&gt;&gt; &gt; only if you are crawling multiple domains in the same job (as is\n&gt;&gt; usual)\n&gt;&gt; &gt; that you can&#39;t use it\n&gt;&gt; &gt; for this purpose as it would only cut you off once the total from\n&gt;&gt; all\n&gt;&gt; &gt; domain hit the limit.\n&gt;&gt; &gt;\n&gt;&gt; &gt; Once the limit is hit the current job will end.  At some point we\n&gt;&gt; may\n&gt;&gt; &gt; allow overrides on this\n&gt;&gt; &gt; setting enabling cutoffs on specific domains but that is well into\n&gt;&gt; the\n&gt;&gt; &gt; future.\n&gt;&gt; &gt;\n&gt;&gt; &gt; I hope this is of some use to you.\n&gt;&gt; &gt;\n&gt;&gt; &gt; - Kris\n&gt;&gt; &gt;\n&gt;&gt; &gt; &gt; I thought setting a max size limit on\n&gt;&gt; &gt; &gt; the ARC file would stop logging past that point, but I see now\n&gt;&gt; that\n&gt;&gt; &gt; &gt; it means a slightly different thing. I tried the number of files\n&gt;&gt; &gt; &gt; limit and while it works, I am not sure it matches my intent as\n&gt;&gt; some\n&gt;&gt; &gt; &gt; files may be noticably shorter than others.\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; I am using a DomainScope crawl per the settings in the Profile\n&gt;&gt; used\n&gt;&gt; &gt; &gt; to create the Job.\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; Based on the information you sent me, I need to think about what\n&gt;&gt; &gt; &gt; might serve my purpose. Thanks for pointing me at the right code\n&gt;&gt; in\n&gt;&gt; &gt; &gt; ARCWriter.\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; Seb\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; --- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@a...&gt;\n&gt;&gt; &gt; &gt; wrote:\n&gt;&gt; &gt; &gt; &gt; Thanks for trying Heritrix Seb.\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; See below.\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; sebastiandelachica wrote:\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;I have been playing around with heritrix for a few weeks now\n&gt;&gt; and I\n&gt;&gt; &gt; &gt; am\n&gt;&gt; &gt; &gt; &gt; &gt;in the process of turning it loose on a controlled environment\n&gt;&gt; for\n&gt;&gt; &gt; &gt; &gt; &gt;one of my research strands. I am currently using version\n&gt;&gt; 0.9.0. My\n&gt;&gt; &gt; &gt; &gt; &gt;objective is to limit the amount of data scooped from a site\n&gt;&gt; onto\n&gt;&gt; &gt; &gt; the\n&gt;&gt; &gt; &gt; &gt; &gt;ARC file. I tried using the HTTP Processor max-length-bytes and\n&gt;&gt; &gt; &gt; the\n&gt;&gt; &gt; &gt; &gt; &gt;Archiver max-size-bytes.\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; Do you mean 0.4.0. You say 0.9.0 above.  We just released 0.6.0\n&gt;&gt; on\n&gt;&gt; &gt; &gt; &gt; friday.  Try it if you haven&#39;t already.  Lots of fixes and\n&gt;&gt; &gt; &gt; improvements.\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; If you&#39;re doing a broad crawl, you have the following options\n&gt;&gt; &gt; &gt; available\n&gt;&gt; &gt; &gt; &gt; to you:\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; max-bytes-download\n&gt;&gt; &gt; &gt; &gt; max-document-download\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; These options are not available in a domain scoped crawl which\n&gt;&gt; &gt; &gt; seems to\n&gt;&gt; &gt; &gt; &gt; be what it is you&#39;d like to do.\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; Tell us more about what it is that you&#39;d like.\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; The max-length-bytes options limits size of a particular\n&gt;&gt; download\n&gt;&gt; &gt; &gt; only.\n&gt;&gt; &gt; &gt; &gt; The max-size-bytes  is upper-bound on the size of ARC files\n&gt;&gt; written\n&gt;&gt; &gt; &gt; (See\n&gt;&gt; &gt; &gt; &gt; the code here http://crawler.archive.org/xref/index.html). \n&gt;&gt; &lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt;&gt; &gt; &gt; &lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; Yours,\n&gt;&gt; &gt; &gt; &gt; St.Ack\n&gt;&gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;The Processor max-length-bytes seems to work at some level as\n&gt;&gt; the\n&gt;&gt; &gt; &gt; &gt; &gt;requests are reported as length-truncated in the logs, but the\n&gt;&gt; &gt; &gt; actual\n&gt;&gt; &gt; &gt; &gt; &gt;HTML files still make it into the archive.\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;The Archive max-size-bytes appears to be ignored. Looking at\n&gt;&gt; the\n&gt;&gt; &gt; &gt; &gt; &gt;code, it does not seem to be used by the ARCWriterProcessor\n&gt;&gt; class\n&gt;&gt; &gt; &gt; or\n&gt;&gt; &gt; &gt; &gt; &gt;any other class for that matter...I just starting digging\n&gt;&gt; through\n&gt;&gt; &gt; &gt; the\n&gt;&gt; &gt; &gt; &gt; &gt;code earlier today.\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;I continue my experimentation and code reading, but figured,\n&gt;&gt; I&#39;d\n&gt;&gt; &gt; &gt; &gt; &gt;check in to see if I am missing something very obvious or if\n&gt;&gt; &gt; &gt; anyone\n&gt;&gt; &gt; &gt; &gt; &gt;had experienced similar behavior.\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;Thanks in advance for your time,\n&gt;&gt; &gt; &gt; &gt; &gt;Seb\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;Yahoo! Groups Links\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt; &gt; &gt;\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt; ------------------------------------------------------------------\n&gt;&gt; ------\n&gt;&gt; &gt; &gt; *Yahoo! Groups Links*\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt;     * To visit your group on the web, go to:\n&gt;&gt; &gt; &gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt; &gt; &gt;       \n&gt;&gt; &gt; &gt;     * To unsubscribe from this group, send an email to:\n&gt;&gt; &gt; &gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt; &gt; &gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?\n&gt;&gt; subject=Unsubscribe&gt;\n&gt;&gt; &gt; &gt;       \n&gt;&gt; &gt; &gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt; &gt; &gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;&gt; &gt; &gt;\n&gt;&gt; &gt; &gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt; \n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;. \n&gt; \n&gt; \n\n\n"}}