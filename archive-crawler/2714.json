{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256346715,"authorName":"Adam Fisk","from":"&quot;Adam Fisk&quot; &lt;adamfisk@...&gt;","profile":"afisk3","replyTo":"LIST","senderId":"oIRm_KSabEl05ZpJf8VUVqW8vI26RsyJYE7AkWb3OrNUMdgDEJLOytdwMJ8tpBA6RmcfteiNcmWopI6zmgEQ15Zg0RvriHs","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: OutOfMemoryError on small crawl","postDate":"1140735995","msgId":2714,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR0bGY1citkaDhpQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQzRkUzQzlELjYwNjA0MDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":2713,"nextInTopic":2718,"prevInTime":2713,"nextInTime":2715,"topicId":2709,"numMessagesInTopic":7,"msgSnippet":"Thanks Gordon- Yes -- our heap size is definitely capped at the Java default.  I can t go much higher than 256 on this machine, but I ll see what I get with","rawEmail":"Return-Path: &lt;adamfisk@...&gt;\r\nX-Sender: adamfisk@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 40479 invoked from network); 23 Feb 2006 23:06:57 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m27.grp.scd.yahoo.com with QMQP; 23 Feb 2006 23:06:57 -0000\r\nReceived: from unknown (HELO n7a.bullet.scd.yahoo.com) (66.94.237.41)\n  by mta6.grp.scd.yahoo.com with SMTP; 23 Feb 2006 23:06:57 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.4] by n7.bullet.scd.yahoo.com with NNFMP; 23 Feb 2006 23:06:35 -0000\r\nReceived: from [66.218.66.74] by t4.bullet.scd.yahoo.com with NNFMP; 23 Feb 2006 23:06:35 -0000\r\nDate: Thu, 23 Feb 2006 23:06:35 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;dtlf5r+dh8i@...&gt;\r\nIn-Reply-To: &lt;43FE3C9D.6060402@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Adam Fisk&quot; &lt;adamfisk@...&gt;\r\nSubject: Re: OutOfMemoryError on small crawl\r\nX-Yahoo-Group-Post: member; u=256346715; y=tm9WStCRvXUmg1jGWwebIgw3Y23EoID1uf9Fd9_5K-Au\r\nX-Yahoo-Profile: afisk3\r\n\r\nThanks Gordon-\n\nYes -- our heap size is definitely capped at the Java defau=\r\nlt.  I\ncan&#39;t go much higher than 256 on this machine, but I&#39;ll see what I g=\r\net\nwith that.  I just didn&#39;t want to mask another problem by going ahead\nan=\r\nd cranking the memory, especially for such a small crawl.  Those 20\nwere br=\r\ninging in upwards of 40,000 raw text/html pages, though, so I\nguess memory =\r\nissues are to be expected.\n\nWhere does most of the memory go?  I assume the=\r\n ToeThreads accumulate\na lot of data over time?  We&#39;re going to be doing mu=\r\nch larger crawls\nsoon (hundreds of sites), and we&#39;d prefer not to dedicate =\r\na full\nmachine to this task, but it looks like we might have to.\n\nThanks ag=\r\nain.\n\n-Adam\n\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@.=\r\n..&gt; wrote:\n&gt;\n&gt; It looks like your heap is capped at a maximum size of 64MB =\r\n-- \n&gt; that&#39;s the Java default in the absence of any -Xmx setting in Java \n&gt;=\r\n 1.4 and previous -- so your 2GB of RAM isn&#39;t doing the crawler any \n&gt; good=\r\n.\n&gt; \n&gt; Use of -Xmx is definitely indicated; if the machine is dedicated \n&gt; =\r\nto crawling, and you want the crawler to be able to use all the \n&gt; RAM, -Xm=\r\nx1500m would be justified.\n&gt; \n&gt; (It&#39;s probably possible to crawl 20 hosts i=\r\nn 64MB, if there isn&#39;t \n&gt; an explosion of subdomains, and you use only a sm=\r\nall number of \n&gt; threads -- but I doubt that&#39;s a real constraint you want t=\r\no try to \n&gt; live within.)\n&gt; \n&gt; The &#39;max-depth&#39; and &#39;average-depth&#39; readings=\r\n on that status line \n&gt; (and in the crawler console) refer to the size of q=\r\nueues: \n&gt; &#39;max-depth&#39; is the longest queue in the frontier, &#39;average-depth&#39;=\r\n \n&gt; is the average of all queue sizes.\n&gt; \n&gt; - Gordon @ IA\n\n\n\n\n\n"}}