{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"YncV44typlIXr4MWL8XR7tC0je4YXzIY2unhlx5TMwTE8yXPHUDX3pzChKTyRNtspyFqkmMOEOFqcrBDGYzNSrACQi34kDKB","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] downloading just a few pages from each site","postDate":"1122921452","msgId":2066,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyRUU2QkVDLjIwMjAxMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQyRUU2NEM4LjMwMzA5MDdAbG9jYWxtYXR0ZXJzLmNvbT4=","referencesHeader":"PFBpbmUuTE5YLjQuNTguMDUwODAxMTkzNTU3MC4yNjY0QGVpbnN0ZWluLnNzbG1pdC51bmliby5pdD4gPDQyRUU2NEM4LjMwMzA5MDdAbG9jYWxtYXR0ZXJzLmNvbT4="},"prevInTopic":2065,"nextInTopic":2071,"prevInTime":2065,"nextInTime":2067,"topicId":2064,"numMessagesInTopic":5,"msgSnippet":"Another way to doing this is to use per host overrides (settings tab) and budgeting feature with the default (BdbFrontier) frontier. Take a look at","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 46279 invoked from network); 1 Aug 2005 18:38:17 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m28.grp.scd.yahoo.com with QMQP; 1 Aug 2005 18:38:17 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta4.grp.scd.yahoo.com with SMTP; 1 Aug 2005 18:38:17 -0000\r\nReceived: (qmail 12633 invoked by uid 100); 1 Aug 2005 18:38:13 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.5?) (igor@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 1 Aug 2005 18:38:13 -0000\r\nMessage-ID: &lt;42EE6BEC.2020101@...&gt;\r\nDate: Mon, 01 Aug 2005 11:37:32 -0700\r\nUser-Agent: Mozilla Thunderbird 0.7.3 (Windows/20040803)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;Pine.LNX.4.58.0508011935570.2664@...&gt; &lt;42EE64C8.3030907@...&gt;\r\nIn-Reply-To: &lt;42EE64C8.3030907@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-81.4 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] downloading just a few pages from each site\r\nX-Yahoo-Group-Post: member; u=137477665; y=nhGMohk3KUNcMzDKQkt5IZ_uCDIie3IDhxnAWUPz5tL6jIMLTg\r\nX-Yahoo-Profile: iranitovic\r\n\r\nAnother way to doing this is to use per host overrides (settings tab) and budgeting feature with the \ndefault (BdbFrontier) frontier. Take a look at queue-total-budget, balance-replenish-amount, \nerror-penalty-amount and cost-policy settings.\n\nTake care,\ni.\n\n&gt; Marco,\n&gt; \n&gt; You can use the DomainSensitiveFrontier and set the &quot;max-docs&quot; parameter \n&gt; in the Frontier settings to the maximum number from each website.\n&gt; \n&gt; Rob.\n&gt; \n&gt; Marco Baroni wrote:\n&gt; \n&gt;&gt;Dear All,\n&gt;&gt;\n&gt;&gt;I&#39;ve been looking into the User Manual, but I was not able to find out how\n&gt;&gt;to do the following: I would like to do a crawl in which I do not download\n&gt;&gt;more than N pages from each web-site.\n&gt;&gt;\n&gt;&gt;E.g., a SURT scope crawl of pages from the .de sites, where I do not\n&gt;&gt;download more than 10 pages from each (www)?&#92;.[^&#92;.]+&#92;.de domain (or, if\n&gt;&gt;that is not possible, where I do not download more than 10 pages from the\n&gt;&gt;same IP).\n&gt;&gt;\n&gt;&gt;Is this possible? Is it documented somewhere?\n&gt;&gt;\n&gt;&gt;Thanks a lot.\n&gt;&gt;\n&gt;&gt;Regards,\n&gt;&gt;\n&gt;&gt;Marco\n&gt;&gt;\n&gt; \n&gt; \n\n\n"}}