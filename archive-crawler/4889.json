{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"JytUJrSTmGKDH-lC0MTTU6uofvHo1GAg3Z_Y3iTJ_Qp9o9uJRrc-J9NOcFuTD0iip5Vk0ivHNhLTxRDlKvXYtqiZ3_lvMPE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Large domain crawl best practice depth of crawl","postDate":"1199917630","msgId":4889,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3ODU0QTNFLjYwMDAyMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGZtM2Uycytmb2hrQGVHcm91cHMuY29tPg==","referencesHeader":"PGZtM2Uycytmb2hrQGVHcm91cHMuY29tPg=="},"prevInTopic":4888,"nextInTopic":0,"prevInTime":4888,"nextInTime":4890,"topicId":4888,"numMessagesInTopic":2,"msgSnippet":"... There s no general rule, and I doubt one could be arrived at for all crawls -- the web is so diverse, and it would depend on the seeds you re crawling and","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 61455 invoked from network); 9 Jan 2008 22:27:15 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m36.grp.scd.yahoo.com with QMQP; 9 Jan 2008 22:27:15 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta18.grp.scd.yahoo.com with SMTP; 9 Jan 2008 22:27:15 -0000\r\nX-Received: (qmail 61667 invoked from network); 9 Jan 2008 22:27:10 -0000\r\nX-Received: from unknown (HELO ?192.168.1.28?) (unknown)\n  by unknown with SMTP; 9 Jan 2008 22:27:10 -0000\r\nX-pair-Authenticated: 76.102.230.209\r\nMessage-ID: &lt;47854A3E.6000200@...&gt;\r\nDate: Wed, 09 Jan 2008 14:27:10 -0800\r\nUser-Agent: Thunderbird 2.0.0.9 (Windows/20071031)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;fm3e2s+fohk@...&gt;\r\nIn-Reply-To: &lt;fm3e2s+fohk@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Large domain crawl best practice depth of crawl\r\nX-Yahoo-Group-Post: member; u=137285340; y=gGUaXf9MU2Iz6gv_XusVMJziStZUIBxFMUKkHfGjl_yv\r\nX-Yahoo-Profile: gojomo\r\n\r\nmjjjhjemj wrote:\n&gt; I am crawling a domain with a very large number of hosts and content.\n&gt; It appears that due to time contraints we may not be able to gather\n&gt; all content. Is there a recommended hops from seed that is considered\n&gt; best practice if one does have to limit the crawl? \n\nThere&#39;s no general rule, and I doubt one could be arrived at for all \ncrawls -- the web is so diverse, and it would depend on the seeds you&#39;re \ncrawling and the link structure of the sites visited.\n\n&gt; *I understand that\n&gt; for the number of hops to be consistent across all hosts within a\n&gt; crawl that I would need to make sure there was a seed entry per\n&gt; distinct host. As I discover new hosts I will pause, enter in new\n&gt; seed, and resume crawl. \n\nKeep in mind that hops cutoffs are applied based on the actual (and \nsomewhat arbitrary/timing-dependent) path used to find a URI -- not the \nshortest possible path from all seeds.\n\nSo let&#39;s say you have pages A, B, C, D, E, linked like so:\n\nA -&gt; B -&gt; C -&gt; D -&gt; E\n\nIf A is your seed, and it follows that path, E is 4 hops from A, and if \nyou&#39;ve set a scope-related cutoff at 3, E will be ruled out.\n\nNow let&#39;s say there&#39;s a new seed F:\n\nF -&gt; E\n\nHere, E will be rediscovered, only be 1 hops from E, and be ruled in.\n\nFinally, let&#39;s say there&#39;s a new seed G:\n\nG -&gt; D -&gt; E\n\nIf D has already been crawled, when it is discovered from G, it will \npass scope-testing but not already-included testing. So D&#39;s outlinks, \nincluding E, will not be reevaluated as 2-hop URIs. They got considered \nwhen D was first crawled, with the hops-count known at the time (5 \nhops), and were ruled out.\n\nSo hops-counts are somewhat crude and imprecise as a crawl-limiting tool \ninside a running crawl -- often helpful but not a strict guarantee \nyou&#39;ve retrieved everything reachable by any N-hop path from your seeds.\n\nYou might want to consider instead limits based on queue(host) quotas -- \nstop after the first N URIs from that group are completed. If done \nthrough the &#39;total-budget&#39; mechanism, it even means that additional URIs \ncontinue to queue up, but in a &#39;retired&#39; queue -- so you could change \nyour mind and start crawling that queue more later. (If using the \nQuotaEnforcer, extra URIs are typically cancelled and could be recovered \nfrom logs but don&#39;t wait in queues.)\n\nThe new 2.0 &#39;precedence&#39; features also offer the potential for ordering \nURIs according to a hops-count precedence value inside queues, and \nsetting a precedence-floor below which URIs are held rather than crawled \n-- combining the hops-based-limits with the hold-and-decide-later aspect \nof budgetting. But, the problem of inclusion being affected by arbitrary \npath-discovery ordering remains. Also, this particular precedence-using \nscenario is new and minimally tested; I&#39;m sure there would be \nconfiguration kinks and unexpected behavior when first used on a large \ndiverse crawl.\n\n- Gordon @ IA\n\n\n\n"}}