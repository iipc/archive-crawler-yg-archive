{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (Internet Archive)","from":"&quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"yXTudRYRUrhJsRLnMmG72Ul0U8zUwNsW3Y0ouNEjZFl0DRS33U8MuKPAVgARpcsJQx0bUGCNr1tkfrscPWDI2__Y1YYdXEv3mfENXNbR9NP7z5oJvmPB9lgz8oc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Javascript (was Re: [archive-crawler] Heritrix - use...","postDate":"1086113781","msgId":502,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwQkNDN0Y1LjYwNjAzMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEwODYwNzI1NjIuMTk0OS4xNi5jYW1lbEBwYzc3MC5zYi5zdGF0c2JpYmxpb3Rla2V0LmRrPg==","referencesHeader":"PDAwNDkwMWM0NDUwZiQ0MGY5NmE1MCQwMzAxYThjMEBNZXNhLmNvbT4gPDEwODYwNzI1NjIuMTk0OS4xNi5jYW1lbEBwYzc3MC5zYi5zdGF0c2JpYmxpb3Rla2V0LmRrPg=="},"prevInTopic":501,"nextInTopic":503,"prevInTime":501,"nextInTime":503,"topicId":455,"numMessagesInTopic":37,"msgSnippet":"... Excellent summary of the options for advanced Javascripts extraction. Another idea we ve kicked around, for cases where the site wants to be crawled (or is","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 81052 invoked from network); 1 Jun 2004 18:17:29 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m22.grp.scd.yahoo.com with QMQP; 1 Jun 2004 18:17:29 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta1.grp.scd.yahoo.com with SMTP; 1 Jun 2004 18:17:29 -0000\r\nReceived: (qmail 28203 invoked by uid 100); 1 Jun 2004 18:09:09 -0000\r\nReceived: from b116-dyn-47.archive.org (HELO ?209.237.240.47?) (gojomo@...@209.237.240.47)\n  by mail-dev.archive.org with SMTP; 1 Jun 2004 18:09:09 -0000\r\nMessage-ID: &lt;40BCC7F5.6060300@...&gt;\r\nDate: Tue, 01 Jun 2004 11:16:21 -0700\r\nUser-Agent: Mozilla Thunderbird 0.6 (X11/20040502)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;004901c4450f$40f96a50$0301a8c0@...&gt; &lt;1086072562.1949.16.camel@...&gt;\r\nIn-Reply-To: &lt;1086072562.1949.16.camel@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.0 required=6.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: &quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;\r\nSubject: Javascript (was Re: [archive-crawler] Heritrix - use...\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nLars Clausen wrote:\n&gt; On Sat, 2004-05-29 at 01:55, bruce wrote:\n&gt; \n&gt;&gt;in regards to my last message, we&#39;re starting to climb through the docs for\n&gt;&gt;heritrix.. we don&#39;t initially see anything that jumps out at us as to how\n&gt;&gt;the app handles, or can be configured to handle parsing through\n&gt;&gt;dropdown/select menu structures within a given page...\n&gt; \n&gt; \n&gt; If the URL is visible in the HTML source, Heritrix will find it (and\n&gt; then apply its rules as to whether to follow it or not).  If it is not,\n&gt; as is frequently the case with Javascript-based menus, Heritrix stands\n&gt; little chance of finding out right now.\n&gt; \n&gt; Storing and re-running the Javascript itself is not a problem, the\n&gt; problem is to make sure you get the URLs it refers to.  Two possible\n&gt; solutions we have considered are\n&gt; \n&gt; 1) Run the page through an actual browser that allows crawling the DOM\n&gt; and grab all the URLs encountered.  That&#39;d find those URLs that are\n&gt; created at page loading time, but not such as are created due to user\n&gt; actions.  Not too hard to do, if you can find a browser that interacts\n&gt; well.  Probably very CPU-consuming, though.\n&gt; \n&gt; 2) Make a partial evaluator that attempts to decide what URLs can be\n&gt; created in Javascript.  Most Javascript code I&#39;ve looked at is fairly\n&gt; regular and would be highly amenable to partial evaluation. This would\n&gt; be a major undertaking, and would also require something that\n&gt; understands the DOM.  We&#39;ve considered passing this off to students as a\n&gt; masters or Ph.D. project, we certainly don&#39;t have the resources\n&gt; ourselves.  \n&gt; \n&gt; 3) Do a manual quality check with a proxy server that notices the URLs\n&gt; clicked but not found.  This sounds like a lot of work, but with a\n&gt; limited number of sites it can get big improvements in the workings of\n&gt; the archived copy, as typically only a few essential menu URLs are\n&gt; missing.\n\nExcellent summary of the options for advanced Javascripts extraction.\n\nAnother idea we&#39;ve kicked around, for cases where the site wants to\nbe crawled (or is legally obligated to be crawled): have the site\nprovide a scrubbed version of their web logs. These could have\nall individually identifiable IPs and cookies/session-IDs removed,\nand be sorted/uniqued by visit frequency.\n\nThis would capture a lot of links that might only be reachable after\nJavascript construction, form-queries, offsite links, and even URIs\nembedded in client software. (Indeed, it could collect pages for\nwhich there is not a single link on the whole web, but which are\nlinked-to by software or off-web documents/emails.)\n\nIf this proves generally useful, it could be standardized as a sort\nof complement to &#39;robots.txt&#39;: say, &#39;mappers.txt&#39; or &#39;harvesters.txt&#39;.\nOr perhaps, a tool could be promulgated which converts server logs\ninto OAI-PMH (http://www.openarchives.org/OAI/openarchivesprotocol.html)\naccessible records. (Perhaps this is one of the strategies the\nmod_oai project -- http://www.modoai.org/ -- will use to &quot;get OAI-PMH\nfor free&quot;.)\n\n- Gordon @ IA\n\n"}}