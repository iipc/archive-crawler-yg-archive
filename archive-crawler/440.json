{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"KPBb2UHJnhOnuUw0uQgAU5o9gXbHNNixnSddFMun8qi-Jla4GnlRDtKR35aoD5JXWTU0G12Yqh1j_rDmuW5DSST5UdpMd-fZpP-I8dEGKg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] Re: Large scale crawling with Heritrix","postDate":"1085394746","msgId":440,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBDNzVAbG9raS5ib2suaGkuaXM+","inReplyToHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDI2NUVFNThAbG9raS5ib2suaGkuaXM+"},"prevInTopic":439,"nextInTopic":445,"prevInTime":439,"nextInTime":441,"topicId":432,"numMessagesInTopic":9,"msgSnippet":"Hi Kaisa, I knew my crawl would probably run into physical limits long before such crawler traps as you describe became a problem. I did add the pathological","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 63579 invoked from network); 24 May 2004 10:33:29 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m24.grp.scd.yahoo.com with QMQP; 24 May 2004 10:33:29 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 24 May 2004 10:33:28 -0000\r\nReceived: (qmail 20585 invoked by uid 100); 24 May 2004 10:25:27 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (130.208.152.83)\n  by mail-dev.archive.org with SMTP; 24 May 2004 10:25:27 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Mon, 24 May 2004 10:32:26 -0000\r\nMessage-ID: &lt;D958110B273CD511ACC100B0D079AA4E01960C75@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0014_01C4417A.69192E50&quot;\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nImportance: Normal\r\nIn-Reply-To: &lt;D958110B273CD511ACC100B0D079AA4E0265EE58@...&gt;\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1409\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: ***\r\nX-Spam-Status: No, hits=3.6 required=6.0 tests=AWL,CLICK_BELOW,DOMAIN_BODY,\n\tDRASTIC_REDUCED,HTML_FONTCOLOR_BLUE,HTML_FONTCOLOR_UNKNOWN,\n\tHTML_MESSAGE,HTTP_ENTITIES_HOST autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] Re: Large scale crawling with Heritrix\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n------=_NextPart_000_0014_01C4417A.69192E50\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Kaisa,\n\n \n\nI knew my crawl would probably run into physical limits long =\r\nbefore such\ncrawler traps as you describe became a problem.\n\n \n\nI did add t=\r\nhe pathological path filter that does filter out paths like\n.../img/img/img=\r\n/... but not repeating sequences like\n.../src/some/path/src/some/path/...\n\n=\r\n \n\nI did not apply the path depth filter but it with a sensible setting wou=\r\nld\nlimit such repeating patterns.\n\n \n\nHopefully Igor will chime in on this,=\r\n it=92s really his field. Actually, Igor,\nhow about writing a =91tips and t=\r\nricks=92 section in the user manual?\n\n \n\n- Kris\n\n \n\n \n\n-----Original Messag=\r\ne-----\nFrom: Kaisa Kaunonen [mailto:kaisa.kaunonen@...] \nSent: 22. =\r\nma=ED 2004 10:05\nTo: archive-crawler@yahoogroups.com\nSubject: [archive-craw=\r\nler] Re: Large scale crawling with Heritrix\n\n \n\nHi \n\nHow do you handle dyna=\r\nmic urls (or better urls pointing to dynamic \nfiles). Even large search eng=\r\nines are very careful with them.\n\n*) Reject all urls which contain paramete=\r\nrs longer than 10 \ncharacters, because those parameters are probably sessio=\r\nn ids. Length \n10 is arbitrary, could be 7, 8, 12 or something else? Filter=\r\ns are \neasy to construct if one only knew what &#39;number&#39; works in most cases=\r\n.\n\nhttp://www=85..net/?id=3D12hswjdufh27e6djjsu3778s\n\n*)  Reject urls with =\r\ntoo many parameters. Set maximum number of \nparameters to some value: are 4=\r\n parameters acceptable, but 5 too much \n(or some other limit)?\n\nhttp:/www=\r\n=85...net/?\nproject_id=3D2&ykieli=3Dfi&startfrom=3D0&stopto=3D6&commands=3D=\r\n329&pic=3D\n\n*) Long file paths: how many slashes can you tolerate before th=\r\ney \nlook suspicious? Does the pathological path filter only understand \nsim=\r\nple paths like .../img/img/img where one pattern repeats itself. \nWhat abou=\r\nt longer cycles in paths .../css/fi/forms/css/fi/forms/... \n(Well, if you h=\r\nave cycles then the path is too long anyway.)\n\nhttp://www=85...net/fi/organ=\r\nisaatiot/2380/fi/css\n/fi/forms/fi/css/fi/css/\nfi/css/fi/css/fi/forms/fi/for=\r\nms/form.phtml?\nform_id=3DId585bf028a26d65a9cd670ef16b8921=A7ion=3D314\n\n\nkai=\r\nsa\n\n\n--- In archive-crawler@yahoogroups.com, &quot;Kristinn Sigurdsson&quot; \n&lt;kris@a=\r\n...&gt; wrote:\n&gt; Hi all,\n&gt; \n&gt; This week I&#39;ve been experimenting with running a=\r\n crawl over the \nentire .is\n&gt; TLD. As expected I&#39;ve encountered several pro=\r\nblems and limitations \nthat I\n&gt; thought might of interest to this community=\r\n as this is (as far as I \nknow)\n&gt; the first attempt to run Heritrix on such=\r\n a scale. Crawling has \nbeen done\n&gt; with very recent development builds.\n&gt; =\r\n\n&gt; Many of the limits I&#39;ve encountered are already known but I&#39;m \nreiterati=\r\nng\n&gt; them here for the sake of completeness.\n&gt; \n&gt; The .is TLD has a little =\r\nover 10000 registered second level \ndomains. A list\n&gt; of them with a www pr=\r\nefix was used as a seed list. Instead of one \nof the\n&gt; scopes shipped with =\r\nHeritrix I wrote a custom scope that limited \nthe crawl\n&gt; to hosts with the=\r\n .is suffix (plus the usual transitive includes).\n&gt; \n&gt; The first problem I =\r\nencountered was Heritrix&#39;s excessive use of file\n&gt; handlers. In addition to=\r\n numerous other files used by Heritrix each \none of\n&gt; the 10000+ domains im=\r\nmediately needed its own host queue with two \nfile\n&gt; handlers each. This le=\r\nd me to (with the help of Gordon and Michael) \nto\n&gt; redesign the so called =\r\nDiskBackedQueues so that they only have open \nfiles\n&gt; when they are large e=\r\nnough to warrant it. I.e. when items in them \nare too\n&gt; many to fit into th=\r\ne memory &#39;head&#39; that they have (200 files for \nthe host\n&gt; queues).\n&gt; \n&gt; Thi=\r\ns fix drastically reduced the number of open files and for the \nmost part\n&gt;=\r\n eliminated this problem. With it out of the way it was possible to \nlaunch=\r\n a\n&gt; crawl with this many seeds. \n&gt; \n&gt; Initial progress was quite impressiv=\r\ne, running at over 50 documents \nper\n&gt; second initially. Eventually it star=\r\nted to gradually drop and now \nseveral\n&gt; days later it stands at around 17 =\r\ndocuments per second. I&#39;m unsure \nof the\n&gt; reason for this gradual decline.=\r\n It may be related to increasing \nsizes of\n&gt; various data structures. \n&gt; \n&gt;=\r\n Over the course of the past 3 days the crawl has downloaded nearly \n4 mill=\r\nion\n&gt; documents totaling over 150 GB. In addition some 11 million plus \ndoc=\r\numents\n&gt; have been discovered and are waiting processing. While impressive =\r\n\nthis is\n&gt; still only scratching the top of the .is domain. I estimate that=\r\n \ncurrently\n&gt; documents 3 link hops from the seeds are being processed.\n&gt; \n=\r\n&gt; Memory use was initially my main concern. The crawl is running on a \nmach=\r\nine\n&gt; with 1.5 GB RAM and the JVM max heap size was set to 1.25 GB. To \ndat=\r\ne the\n&gt; JVM has only allocated itself 1 GB and garbage collection still \ndr=\r\nops the\n&gt; memory being used to almost half that.\n&gt; \n&gt; Disk use by the disk =\r\nbound queues however has been much greater \nthen I\n&gt; anticipated. With said=\r\n 11 million URLs waiting in the queues they \nnow take\n&gt; up about 16 GB. Thi=\r\ns comes out at about 1.6 KB per URI. This will \nturn out\n&gt; to be the limiti=\r\nng factor for my current crawl since the disk in \nquestion\n&gt; only has anoth=\r\ner 3 GB free so it will be exhausted soon.\n&gt; \n&gt; Even with much larger disks=\r\n, say 200 GB, crawls will be limited to \nhaving\n&gt; 120-130 million URIs wait=\r\ning. This seems like a huge number until \nyou\n&gt; consider that the .is domai=\r\nn would seem to have at least that many \ndocuments\n&gt; based on this crawl. D=\r\noing a crawl like this on an even larger \nscale would\n&gt; seem to merit tryin=\r\ng to reduce the size (on disk) of these URIs.\n&gt; \n&gt; Of course a crawl of tha=\r\nt scope is not possible until the list of \nalready\n&gt; seen URIs can be disk =\r\nbacked.  With the current method of using 4 \nbyte\n&gt; fingerprints for each e=\r\nncountered URI 1 GB of memory can hold \naround 28\n&gt; million URIs. Even with=\r\n a machine with 4 GB RAM would not be able \nto scale\n&gt; up to even a full .i=\r\ns TLD crawl.\n&gt; \n&gt; \n&gt; \n&gt; Some of my thoughts on dealing with the limits foll=\r\now:\n&gt; \n&gt; Moving the list of encountered URIs to disk seems to be imperative=\r\n. \nBut that\n&gt; will pose even greater demands on disk space so I would sugge=\r\nst \nthat we\n&gt; remain aware of that issue and try (whenever possible) to lim=\r\nit the \nsize of\n&gt; the data being written to disk to that which is actually =\r\nneeded. \nThat may\n&gt; have the additional benefit of speeding I/O operations =\r\n(even if only\n&gt; marginally). Perhaps it should also be possible to split th=\r\ne data \namong many\n&gt; disk (of course this can be done at the OS level to so=\r\nme extent).\n&gt; \n&gt; Multi machine setup might alleviate some of this but for a=\r\nny sort \nof large\n&gt; scale crawl (targeting 100 million +) each machine woul=\r\nd probably be\n&gt; handling tens of millions URIs at least. For truly large sc=\r\nale \ncrawls (like\n&gt; the ones I know IA wants to do) the demands grow even m=\r\nore.\n&gt; \n&gt; Another approach which might limit the problem is the \nimplementa=\r\ntion of the\n&gt; site first crawling strategy. In my crawl all .is domains are=\r\n \ntackled in\n&gt; parallel. This makes the crawl a true breadth first crawl. A=\r\n site \nfirst\n&gt; approach would let Heritrix focus it&#39;s efforts on a limited =\r\nnumber \nof sites\n&gt; at a time. Since the usual pattern for crawling a site i=\r\ns an initial\n&gt; explosion of available URIs followed by a steady decline and=\r\n \neventual\n&gt; exhaustion we would likely not wind up having as huge a number=\r\n of \nURIs\n&gt; waiting (at least not as quickly). This does nothing for the li=\r\nmits \nimposed\n&gt; by the demands on RAM memory though.\n&gt; \n&gt; \n&gt; Kristinn Sigur=\r\n=F0sson\n&gt; Software engineer\n&gt; National and University Library of Iceland\n\n\n=\r\n\n\n\n\nYahoo! Groups Sponsor\n\n\n\nADVERTISEMENT\n \n&lt;http://rd.yahoo.com/SIG=3D129=\r\nltpcep/M=3D295196.4901138.6071305.3001176/D=3Dgroups\n/S=3D1705004924:HM/EXP=\r\n=3D1085306728/A=3D2128215/R=3D0/SIG=3D10se96mf6/*http:/companion\n.yahoo.com=\r\n&gt; click here\n\n\n \n&lt;http://us.adserver.yahoo.com/l?M=3D295196.4901138.6071305=\r\n.3001176/D=3Dgroups/S=3D\n:HM/A=3D2128215/rand=3D124953112&gt; \n\n \n\n  _____  \n\n=\r\nYahoo! Groups Links\n\n*\tTo visit your group on the web, go to:\nhttp://groups=\r\n.yahoo.com/group/archive-crawler/\n  \n*\tTo unsubscribe from this group, send=\r\n an email to:\narchive-crawler-unsubscribe@yahoogroups.com\n&lt;mailto:archive-c=\r\nrawler-unsubscribe@yahoogroups.com?subject=3DUnsubscribe&gt; \n  \n*\tYour use of=\r\n Yahoo! Groups is subject to the Yahoo! Terms of Service\n&lt;http://docs.yahoo=\r\n.com/info/terms/&gt; . \n\n\r\n------=_NextPart_000_0014_01C4417A.69192E50\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;html&gt;\n\n&lt;head&gt;\n&lt;META HTTP-EQUIV=3D&quot;Content-Type&quot; CONTENT=3D&quot;text/html; char=\r\nset=3Diso-8859-1&quot;&gt;\n\n\n&lt;meta name=3DGenerator content=3D&quot;Microsoft Word 10 (f=\r\niltered)&quot;&gt;\n\n&lt;style&gt;\n&lt;!--\n /* Font Definitions */\n @font-face\n\t{font-family:=\r\nWingdings;\n\tpanose-1:5 0 0 0 0 0 0 0 0 0;}\n@font-face\n\t{font-family:Tahoma;=\r\n\n\tpanose-1:2 11 6 4 3 5 4 4 2 4;}\n /* Style Definitions */\n p.MsoNormal, li=\r\n.MsoNormal, div.MsoNormal\n\t{margin:0cm;\n\tmargin-bottom:.0001pt;\n\tfont-size:=\r\n12.0pt;\n\tfont-family:&quot;Times New Roman&quot;;}\na:link, span.MsoHyperlink\n\t{color:=\r\nblue;\n\ttext-decoration:underline;}\na:visited, span.MsoHyperlinkFollowed\n\t{c=\r\nolor:blue;\n\ttext-decoration:underline;}\ntt\n\t{font-family:&quot;Courier New&quot;;}\nsp=\r\nan.EmailStyle18\n\t{font-family:Arial;\n\tcolor:navy;}\n@page Section1\n\t{size:59=\r\n5.3pt 841.9pt;\n\tmargin:72.0pt 90.0pt 72.0pt 90.0pt;}\ndiv.Section1\n\t{page:Se=\r\nction1;}\n /* List Definitions */\n ol\n\t{margin-bottom:0cm;}\nul\n\t{margin-bott=\r\nom:0cm;}\n--&gt;\n&lt;/style&gt;\n\n&lt;/head&gt;\n\n&lt;body lang=3DIS link=3Dblue vlink=3Dblue&gt;\n\n=\r\n&lt;div class=3DSection1&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy fa=\r\nce=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;H=\r\ni Kaisa,&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnav=\r\ny face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:nav=\r\ny&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dna=\r\nvy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:na=\r\nvy&#39;&gt;I knew my crawl would probably run into\nphysical limits long before suc=\r\nh crawler traps as you describe became a\nproblem.&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p cla=\r\nss=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font=\r\n-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p cl=\r\nass=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;fon=\r\nt-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;I did add the pathological pat=\r\nh filter\nthat does filter out paths like .../img/img/img/... but not repeat=\r\ning sequences\nlike .../src/some/path/src/some/path/...&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;=\r\np class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D=\r\n&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n=\r\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=\r\n=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;I did not apply the pat=\r\nh depth filter but\nit with a sensible setting would limit such repeating pa=\r\ntterns.&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy=\r\n face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy=\r\n&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnav=\r\ny face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:nav=\r\ny&#39;&gt;Hopefully Igor will chime in on this, it&#8217;s\nreally his field. Actua=\r\nlly, Igor, how about writing a &#8216;tips and tricks&#8217;\nsection in the=\r\n user manual?&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=\r\n=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;col=\r\nor:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 colo=\r\nr=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;co=\r\nlor:navy&#39;&gt;- Kris&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 col=\r\nor=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;c=\r\nolor:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 co=\r\nlor=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;=\r\ncolor:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;div style=3D&#39;border:none;border-left=\r\n:solid blue 1.5pt;padding:0cm 0cm 0cm 4.0pt&#39;&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font s=\r\nize=3D2 face=3DTahoma&gt;&lt;span lang=3DEN-US style=3D&#39;font-size:\n10.0pt;font-fa=\r\nmily:Tahoma&#39;&gt;-----Original Message-----&lt;br&gt;\n&lt;b&gt;&lt;span style=3D&#39;font-weight:b=\r\nold&#39;&gt;From:&lt;/span&gt;&lt;/b&gt; Kaisa Kaunonen\n[mailto:kaisa.kaunonen@...] &lt;b=\r\nr&gt;\n&lt;b&gt;&lt;span style=3D&#39;font-weight:bold&#39;&gt;Sent:&lt;/span&gt;&lt;/b&gt; 22. ma=ED 2004 10:0=\r\n5&lt;br&gt;\n&lt;b&gt;&lt;span style=3D&#39;font-weight:bold&#39;&gt;To:&lt;/span&gt;&lt;/b&gt;\narchive-crawler@ya=\r\nhoogroups.com&lt;br&gt;\n&lt;b&gt;&lt;span style=3D&#39;font-weight:bold&#39;&gt;Subject:&lt;/span&gt;&lt;/b&gt; [=\r\narchive-crawler] Re:\nLarge scale crawling with Heritrix&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n=\r\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D3 face=3D&quot;Times New Roman&quot;&gt;&lt;span style=3D=\r\n&#39;font-size:\n12.0pt&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;tt&gt;&lt;fon=\r\nt size=3D2 face=3D&quot;Courier New&quot;&gt;&lt;span style=3D&#39;font-size:\n10.0pt&#39;&gt;Hi &lt;/span=\r\n&gt;&lt;/font&gt;&lt;/tt&gt;&lt;font size=3D2 face=3D&quot;Courier New&quot;&gt;&lt;span\nstyle=3D&#39;font-size:1=\r\n0.0pt;font-family:&quot;Courier New&quot;&#39;&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;H=\r\now do you handle dynamic urls (or better urls\npointing to dynamic &lt;/font&gt;&lt;/=\r\ntt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;files). Even large search engines ar=\r\ne very careful\nwith them.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier Ne=\r\nw&quot;&gt;*) Reject all urls which contain parameters longer\nthan 10 &lt;/font&gt;&lt;/tt&gt;&lt;=\r\nbr&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;characters, because those parameters are=\r\n probably\nsession ids. Length &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier Ne=\r\nw&quot;&gt;10 is arbitrary, could be 7, 8, 12 or something\nelse? Filters are &lt;/font=\r\n&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;easy to construct if one only kne=\r\nw what &#39;number&#39;\nworks in most cases.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;&lt;a\nhref=3D&quot;http://www&#8230;..net/?id=3D12hswjdufh27e6djjsu37=\r\n78s&quot;&gt;http://www&#8230;..net/?id=3D12hswjdufh27e6djjsu3778s&lt;/a&gt;&lt;/font&gt;&lt;/tt&gt;&lt;=\r\nbr&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;*)&nbsp; Reject urls with too many =\r\nparameters. Set\nmaximum number of &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courie=\r\nr New&quot;&gt;parameters to some value: are 4 parameters\nacceptable, but 5 too muc=\r\nh &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;(or some other limit)?&lt;/f=\r\nont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;http:/www&#8230;...net/?&lt;=\r\n/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;project_id=3D2&amp;ykieli=3D=\r\nfi&amp;startfrom=3D0&amp;stopto=3D6&amp;commands=3D329&amp;pic=3D&lt;/font&gt;&lt;/t=\r\nt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;*) Long file paths: how many sla=\r\nshes can you\ntolerate before they &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courie=\r\nr New&quot;&gt;look suspicious? Does the pathological path filter\nonly understand &lt;=\r\n/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;simple paths like .../img/im=\r\ng/img where one\npattern repeats itself. &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;=\r\nCourier New&quot;&gt;What about longer cycles in paths\n.../css/fi/forms/css/fi/form=\r\ns/... &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;(Well, if you have cy=\r\ncles then the path is too\nlong anyway.)&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=\r\n=3D&quot;Courier New&quot;&gt;&lt;a\nhref=3D&quot;http://www&#8230;...net/fi/organisaatiot/2380/f=\r\ni/css&quot;&gt;http://www&#8230;...net/fi/organisaatiot/2380/fi/css&lt;/a&gt;&lt;/font&gt;&lt;/tt&gt;=\r\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;/fi/forms/fi/css/fi/css/&lt;/font&gt;&lt;/tt&gt;&lt;br=\r\n&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;fi/css/fi/css/fi/forms/fi/forms/form.phtml=\r\n?&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;form_id=3DId585bf028a26d65=\r\na9cd670ef16b8921=A7ion=3D314&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Co=\r\nurier New&quot;&gt;kaisa&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;-=\r\n-- In archive-crawler@yahoogroups.com,\n&quot;Kristinn Sigurdsson&quot; &lt;/fo=\r\nnt&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&lt;kris@a...&gt; wrote:&lt;/font&gt;=\r\n&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Hi all,&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;=\r\nfont face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier N=\r\new&quot;&gt;&gt; This week I&#39;ve been experimenting with\nrunning a crawl over the &lt;/=\r\nfont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;entire .is&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;t=\r\nt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; TLD. As expected I&#39;ve encountered several=\r\n\nproblems and limitations &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;t=\r\nhat I&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; thought might of =\r\ninterest to this community\nas this is (as far as I &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;fo=\r\nnt face=3D&quot;Courier New&quot;&gt;know)&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New=\r\n&quot;&gt;&gt; the first attempt to run Heritrix on such a\nscale. Crawling has &lt;/fo=\r\nnt&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;been done&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;=\r\nfont face=3D&quot;Courier New&quot;&gt;&gt; with very recent development builds.&lt;/font&gt;&lt;=\r\n/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font fac=\r\ne=3D&quot;Courier New&quot;&gt;&gt; Many of the limits I&#39;ve encountered are\nalready know=\r\nn but I&#39;m &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;reiterating&lt;/font=\r\n&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; them here for the sake of co=\r\nmpleteness.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt=\r\n&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; The .is TLD has a little over 100=\r\n00\nregistered second level &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;=\r\ndomains. A list&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; of them=\r\n with a www prefix was used as a seed\nlist. Instead of one &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;=\r\n\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;of the&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Co=\r\nurier New&quot;&gt;&gt; scopes shipped with Heritrix I wrote a custom\nscope that li=\r\nmited &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;the crawl&lt;/font&gt;&lt;/tt&gt;=\r\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; to hosts with the .is suffix (plus=\r\n the usual\ntransitive includes).&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier =\r\nNew&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; The first pr=\r\noblem I encountered was Heritrix&#39;s\nexcessive use of file&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;t=\r\nt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; handlers. In addition to numerous other f=\r\niles\nused by Heritrix each &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;=\r\none of&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; the 10000+ domai=\r\nns immediately needed its own\nhost queue with two &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;fon=\r\nt face=3D&quot;Courier New&quot;&gt;file&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;=\r\n&gt; handlers each. This led me to (with the help\nof Gordon and Michael) &lt;/=\r\nfont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;to&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font =\r\nface=3D&quot;Courier New&quot;&gt;&gt; redesign the so called DiskBackedQueues so\nthat t=\r\nhey only have open &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;files&lt;/f=\r\nont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; when they are large enoug=\r\nh to warrant it.\nI.e. when items in them &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;are too&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; m=\r\nany to fit into the memory &#39;head&#39; that they\nhave (200 files for &lt;/font&gt;&lt;/tt=\r\n&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;the host&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font fac=\r\ne=3D&quot;Courier New&quot;&gt;&gt; queues).&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier N=\r\new&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; This fix dras=\r\ntically reduced the number of\nopen files and for the &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;=\r\nfont face=3D&quot;Courier New&quot;&gt;most part&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Couri=\r\ner New&quot;&gt;&gt; eliminated this problem. With it out of the\nway it was possibl=\r\ne to &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;launch a&lt;/font&gt;&lt;/tt&gt;&lt;b=\r\nr&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; crawl with this many seeds. &lt;/font&gt;&lt;=\r\n/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font fac=\r\ne=3D&quot;Courier New&quot;&gt;&gt; Initial progress was quite impressive,\nrunning at ov=\r\ner 50 documents &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;per&lt;/font&gt;&lt;=\r\n/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; second initially. Eventually i=\r\nt started to\ngradually drop and now &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Cour=\r\nier New&quot;&gt;several&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; days l=\r\nater it stands at around 17 documents\nper second. I&#39;m unsure &lt;/font&gt;&lt;/tt&gt;&lt;b=\r\nr&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;of the&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;=\r\nCourier New&quot;&gt;&gt; reason for this gradual decline. It may be\nrelated to inc=\r\nreasing &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;sizes of&lt;/font&gt;&lt;/tt=\r\n&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; various data structures. &lt;/font&gt;&lt;=\r\n/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font fac=\r\ne=3D&quot;Courier New&quot;&gt;&gt; Over the course of the past 3 days the crawl\nhas dow=\r\nnloaded nearly &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;4 million&lt;/f=\r\nont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; documents totaling over 1=\r\n50 GB. In addition\nsome 11 million plus &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;=\r\nCourier New&quot;&gt;documents&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; =\r\nhave been discovered and are waiting\nprocessing. While impressive &lt;/font&gt;&lt;/=\r\ntt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;this is&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font fa=\r\nce=3D&quot;Courier New&quot;&gt;&gt; still only scratching the top of the .is\ndomain. I =\r\nestimate that &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;currently&lt;/fo=\r\nnt&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; documents 3 link hops from=\r\n the seeds are\nbeing processed.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier N=\r\new&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Memory use wa=\r\ns initially my main concern. The\ncrawl is running on a &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt=\r\n&gt;&lt;font face=3D&quot;Courier New&quot;&gt;machine&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Couri=\r\ner New&quot;&gt;&gt; with 1.5 GB RAM and the JVM max heap size was\nset to 1.25 GB. =\r\nTo &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;date the&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;=\r\n\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; JVM has only allocated itself 1 GB and=\r\n\ngarbage collection still &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;d=\r\nrops the&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; memory being u=\r\nsed to almost half that.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt=\r\n; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Disk use by the disk=\r\n bound queues however has\nbeen much greater &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=\r\n=3D&quot;Courier New&quot;&gt;then I&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt;=\r\n anticipated. With said 11 million URLs\nwaiting in the queues they &lt;/font&gt;&lt;=\r\n/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;now take&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font =\r\nface=3D&quot;Courier New&quot;&gt;&gt; up about 16 GB. This comes out at about 1.6\nKB pe=\r\nr URI. This will &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;turn out&lt;/=\r\nfont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; to be the limiting facto=\r\nr for my current\ncrawl since the disk in &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;question&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; =\r\nonly has another 3 GB free so it will be exhausted\nsoon.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;t=\r\nt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courie=\r\nr New&quot;&gt;&gt; Even with much larger disks, say 200 GB,\ncrawls will be limited=\r\n to &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;having&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n=\r\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; 120-130 million URIs waiting. This seem=\r\ns like\na huge number until &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;=\r\nyou&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; consider that the .=\r\nis domain would seem to\nhave at least that many &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font =\r\nface=3D&quot;Courier New&quot;&gt;documents&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier Ne=\r\nw&quot;&gt;&gt; based on this crawl. Doing a crawl like this\non an even larger &lt;/fo=\r\nnt&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;scale would&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt=\r\n&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; seem to merit trying to reduce the size (o=\r\nn\ndisk) of these URIs.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; =\r\n&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Of course a crawl of t=\r\nhat scope is not\npossible until the list of &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=\r\n=3D&quot;Courier New&quot;&gt;already&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt=\r\n; seen URIs can be disk backed.&nbsp; With the\ncurrent method of using 4 &lt;/=\r\nfont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;byte&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;fon=\r\nt face=3D&quot;Courier New&quot;&gt;&gt; fingerprints for each encountered URI 1 GB of\nm=\r\nemory can hold &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;around 28&lt;/f=\r\nont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; million URIs. Even with a=\r\n machine with 4 GB\nRAM would not be able &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;to scale&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; =\r\nup to even a full .is TLD crawl.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier =\r\nNew&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;=\r\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;&gt; Some of my thoughts on dealing with the\nlimits follow:&lt;/=\r\nfont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;fo=\r\nnt face=3D&quot;Courier New&quot;&gt;&gt; Moving the list of encountered URIs to disk\nse=\r\nems to be imperative. &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;But t=\r\nhat&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; will pose even grea=\r\nter demands on disk space\nso I would suggest &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font fac=\r\ne=3D&quot;Courier New&quot;&gt;that we&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&g=\r\nt; remain aware of that issue and try (whenever\npossible) to limit the &lt;/fo=\r\nnt&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;size of&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;fo=\r\nnt face=3D&quot;Courier New&quot;&gt;&gt; the data being written to disk to that which\ni=\r\ns actually needed. &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;That may=\r\n&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; have the additional be=\r\nnefit of speeding I/O\noperations (even if only&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font fa=\r\nce=3D&quot;Courier New&quot;&gt;&gt; marginally). Perhaps it should also be\npossible to =\r\nsplit the data &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;among many&lt;/=\r\nfont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; disk (of course this can=\r\n be done at the OS\nlevel to some extent).&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Mul=\r\nti machine setup might alleviate some of\nthis but for any sort &lt;/font&gt;&lt;/tt&gt;=\r\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;of large&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=\r\n=3D&quot;Courier New&quot;&gt;&gt; scale crawl (targeting 100 million +) each\nmachine wo=\r\nuld probably be&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; handlin=\r\ng tens of millions URIs at least. For\ntruly large scale &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;t=\r\nt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;crawls (like&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;&gt; the ones I know IA wants to do) the demands\ngrow even mo=\r\nre.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;t=\r\nt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Another approach which might limit the\npr=\r\noblem is the &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;implementation=\r\n of the&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; site first craw=\r\nling strategy. In my crawl all\n.is domains are &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font f=\r\nace=3D&quot;Courier New&quot;&gt;tackled in&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier Ne=\r\nw&quot;&gt;&gt; parallel. This makes the crawl a true breadth\nfirst crawl. A site &lt;=\r\n/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;first&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;f=\r\nont face=3D&quot;Courier New&quot;&gt;&gt; approach would let Heritrix focus it&#39;s\neffort=\r\ns on a limited number &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;of si=\r\ntes&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; at a time. Since th=\r\ne usual pattern for\ncrawling a site is an initial&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font=\r\n face=3D&quot;Courier New&quot;&gt;&gt; explosion of available URIs followed by a\nsteady=\r\n decline and &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;eventual&lt;/font=\r\n&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; exhaustion we would likely n=\r\not wind up having\nas huge a number of &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Co=\r\nurier New&quot;&gt;URIs&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; waiting=\r\n (at least not as quickly). This does\nnothing for the limits &lt;/font&gt;&lt;/tt&gt;&lt;b=\r\nr&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;imposed&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D=\r\n&quot;Courier New&quot;&gt;&gt; by the demands on RAM memory though.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt=\r\n&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier=\r\n New&quot;&gt;&gt; &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Kristinn Si=\r\ngur=F0sson&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; Software eng=\r\nineer&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;&gt; National and Univ=\r\nersity Library of Iceland&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/span&gt;&lt;/font&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;=\r\n\n\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n\r\n------=_NextPart_000_0014_01C4417A.69192E50--\r\n\n"}}