{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"28mNaBTL_KnUmYXLM3_vesIMY6BDxwPM0veoWrHRiQAMHBJ6QiC7GuLhAUK5HmEhj_oUkxqRHSfHsPooWLODkgiprCT-aA8","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Improving Crawling Speeds","postDate":"1194045237","msgId":4649,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3MkJBRjM1LjEwMjAyMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDhGMTE3MjJBMDU2MkJCNEY4MEE2ODBFRDRDRkVEMEQ1MEM4MTE3QEVWU0JORzAyLmFkLm9mZmljZS5hb2wuY29tPg==","referencesHeader":"PDhGMTE3MjJBMDU2MkJCNEY4MEE2ODBFRDRDRkVEMEQ1MEM4MTE3QEVWU0JORzAyLmFkLm9mZmljZS5hb2wuY29tPg=="},"prevInTopic":4648,"nextInTopic":4663,"prevInTime":4648,"nextInTime":4650,"topicId":4648,"numMessagesInTopic":4,"msgSnippet":"If you have permission/assurance that it s OK to crawl the target sites with no politeness delays, I d say try it: set the delays to 0 and see how fast things","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 54785 invoked from network); 2 Nov 2007 23:14:13 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m48.grp.scd.yahoo.com with QMQP; 2 Nov 2007 23:14:13 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta16.grp.scd.yahoo.com with SMTP; 2 Nov 2007 23:14:12 -0000\r\nX-Received: (qmail 71663 invoked from network); 2 Nov 2007 23:13:54 -0000\r\nX-Received: from unknown (HELO ?192.168.1.30?) (unknown)\n  by unknown with SMTP; 2 Nov 2007 23:13:54 -0000\r\nX-pair-Authenticated: 76.102.230.209\r\nMessage-ID: &lt;472BAF35.1020200@...&gt;\r\nDate: Fri, 02 Nov 2007 16:13:57 -0700\r\nUser-Agent: Thunderbird 2.0.0.6 (Windows/20070728)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;8F11722A0562BB4F80A680ED4CFED0D50C8117@...&gt;\r\nIn-Reply-To: &lt;8F11722A0562BB4F80A680ED4CFED0D50C8117@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Improving Crawling Speeds\r\nX-Yahoo-Group-Post: member; u=137285340; y=WjOLj98EgRmUl9EgNtlBLBjzUzpZuwEuQDWsEaQ83QX4\r\nX-Yahoo-Profile: gojomo\r\n\r\nIf you have permission/assurance that it&#39;s OK to crawl the target sites \nwith no politeness delays, I&#39;d say try it: set the delays to 0 and see \nhow fast things go.\n\nIt might be the target site rather than the crawler that is the limiting \nfactor.\n\nIf instead the remaining politeness practice (to try only one URI from a \nsite at a given time) remains a bottleneck, it is harder to disable but \n  some combination of a custom queue-assignment-policy and other tricks \nmight work.\n\nBut first, see if zero-delays (and plenty of threads, if crawling many \ndomains at once) gets the speed you want.\n\n(A single moderately powerful machine can easily do millions of URLs in \na day when there are diverse sites to collect from without hammering any \none.)\n\n- Gordon @ IA\n\nGoel, Ankur wrote:\n&gt; \n&gt; Hi Folks,\n&gt;             I am using Heritrix 1.12.1 to crawl 500,000 thousand URLs.\n&gt; The setup has 2 machines with 1 GB RAM, each running a crawler\n&gt; instance with 10 Toe Threads.\n&gt;  \n&gt; I am observing an avg crawl speed of 3 - 4 URI&#39;s per second per box\n&gt; which is ok considering the built-in politeness.\n&gt;  \n&gt; If I were to crawl a particular domain RELAXING poiliteness\n&gt; how fast can I crawl ?\n&gt;  \n&gt; Ideally I would like to see a crawl speed of 50 - 100 URIs per second\n&gt; per box.\n&gt;  \n&gt; In other words I would like to know the bottlenecks and known workarounds\n&gt; to fast crawling and if none exist then I would like to share ideas on\n&gt; future works for improving crawling speeds.\n&gt;  \n&gt;  \n&gt; -Ankur\n&gt; \n\n"}}