{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225011788,"authorName":"Karl Wright","from":"Karl Wright &lt;kwright@...&gt;","profile":"daddywri","replyTo":"LIST","senderId":"DAMVLFVJZsyZZBrT6-lr7iHY-mYNEZvXopCR3keYXS6Zcd0mXIwCIoqyFToUmEE0Wj3UG4U03rvHZTmB49PCaILXGuhhnsCt2nQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Advice needed on how to (properly) structure new Heritrix modify and delete functionality","postDate":"1153310425","msgId":3085,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkUxRUQ5LjEwNjAxMDJAbWV0YWNhcnRhLmNvbT4=","inReplyToHeader":"PDIwMDYwNzE5MDgyMDE0LjlCRDQ3MTQxNTZBOTZAbWFpbC5hcmNoaXZlLm9yZz4=","referencesHeader":"PDIwMDYwNzE5MDgyMDE0LjlCRDQ3MTQxNTZBOTZAbWFpbC5hcmNoaXZlLm9yZz4="},"prevInTopic":3084,"nextInTopic":3086,"prevInTime":3084,"nextInTime":3086,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"... If this question is for me, I need to begin my work within the next couple of weeks. ... I am curious as to why you would not try to build the second index","rawEmail":"Return-Path: &lt;kwright@...&gt;\r\nX-Sender: kwright@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 79513 invoked from network); 19 Jul 2006 12:01:28 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m40.grp.scd.yahoo.com with QMQP; 19 Jul 2006 12:01:28 -0000\r\nReceived: from unknown (HELO silene.metacarta.com) (65.77.47.18)\n  by mta5.grp.scd.yahoo.com with SMTP; 19 Jul 2006 12:01:27 -0000\r\nReceived: from localhost (silene.metacarta.com [65.77.47.18])\n\tby silene.metacarta.com (Postfix) with ESMTP id 84E5E73943\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 19 Jul 2006 07:58:51 -0400 (EDT)\r\nReceived: from silene.metacarta.com ([65.77.47.18])\n\tby localhost (silene.metacarta.com [65.77.47.18]) (amavisd-new, port 10024)\n\twith ESMTP id 02821-02 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 19 Jul 2006 07:58:50 -0400 (EDT)\r\nX-Auth-Received: from [192.168.1.101] (146-115-66-62.c3-0.lex-ubr1.sbo-lex.ma.cable.rcn.com [146.115.66.62])\n\t(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))\n\t(No client certificate requested)\n\tby silene.metacarta.com (Postfix) with ESMTP id 6807C7392B\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 19 Jul 2006 07:58:50 -0400 (EDT)\r\nMessage-ID: &lt;44BE1ED9.1060102@...&gt;\r\nDate: Wed, 19 Jul 2006 08:00:25 -0400\r\nUser-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;20060719082014.9BD4714156A96@...&gt;\r\nIn-Reply-To: &lt;20060719082014.9BD4714156A96@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-Virus-Scanned: by amavisd-new-20030616-p10 (Debian) at metacarta.com\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Karl Wright &lt;kwright@...&gt;\r\nSubject: Re: [archive-crawler] Advice needed on how to (properly) structure\n new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=225011788; y=PD_Jd0cZoJ0myjkJxKFss_zPcfv1iqTY0UpUzwtkeyJSgps\r\nX-Yahoo-Profile: daddywri\r\n\r\nKristinn Sigur�sson wrote:\n&gt;  \n&gt; \n&gt; \n&gt;&gt;-----Original Message-----\n&gt;&gt;From: archive-crawler@yahoogroups.com \n&gt;&gt;[mailto:archive-crawler@yahoogroups.com] On Behalf Of Michael Stack\n&gt;&gt;Sent: 18. j�l� 2006 19:20\n&gt;&gt;To: archive-crawler@yahoogroups.com\n&gt;&gt;Subject: Re: [archive-crawler] Advice needed on how to \n&gt;&gt;(properly) structure new Heritrix modify and delete functionality\n&gt;&gt;\n&gt;&gt;We&#39;ve been talking here at the Archive on how to broach the \n&gt;&gt;topics under discussion here: duplicate/near-duplicate \n&gt;&gt;detection, change-detection, junk-avoidance, etc. Here are a \n&gt;&gt;few notes on what we&#39;ve been thinking. \n&gt;&gt;They pertain to Karl and Kris&#39;s discussion so I&#39;ll add them \n&gt;&gt;in here under the same subject (Implementation is probably \n&gt;&gt;too far off to solve Karl&#39;s needs).\n&gt; \n&gt; \n&gt; Just out of curiosity, what kind of timeframe are you looking at?\n&gt; \n\nIf this question is for me, I need to begin my work within the next \ncouple of weeks.\n\n&gt; \n&gt;&gt;As per Kris&#39;s experience, crawl-time duplicate/change \n&gt;&gt;detection is hard. We&#39;re thinking, like Kris, that at first, \n&gt;&gt;we&#39;ll do this by post-processing crawls. The product of \n&gt;&gt;post-processing will be distilled data-structures that the \n&gt;&gt;crawler can do fast lookups against as its crawling to ask \n&gt;&gt;such questions as: &quot;Is this a duplicate?&quot;; &quot;Is this a near \n&gt;&gt;duplicate?&quot;; &quot;Is this page junk?&quot;; &quot;Is this page \n&gt;&gt;&#39;important&#39;?&quot;; and &quot;Has this page changed?&quot;. Or, more likely, \n&gt;&gt;we&#39;ll aggregate all URL evaluation &#39;plugins&#39; and a crawler \n&gt;&gt;will simply ask, &quot;Should I crawl this?&quot; and get back an \n&gt;&gt;&#39;evaluation&#39; that falls somewhere between 0 and 1 with a \n&gt;&gt;configurable threshold in the crawler saying what level of \n&gt;&gt;evaluations it&#39;ll pursue.\n&gt; \n&gt; \n&gt; Sounds like the DeDuplicator on steroids. Good stuff. Any thoughts on how to\n&gt; rule things as junk? That might be one of the most interesting applications\n&gt; of this.\n&gt; \n&gt; \n&gt;&gt;We&#39;re thinking we need to build a pluggable bulk-processing \n&gt;&gt;infrastructure to do the post-crawl analysis and we&#39;ll need \n&gt;&gt;to build a fast lookup service to field crawl-time queries.\n&gt;&gt;\n&gt;&gt;For the bulk-processing platform, hadoop -- \n&gt;&gt;lucene.apache.org/hadoop -- and parts of nutch are what we&#39;re \n&gt;&gt;considering. We&#39;ve experience using both in our nutchwax, \n&gt;&gt;building full-text indices out of ARCs. For example, \n&gt;&gt;evaluating exact duplicates, mirror-detection, and \n&gt;&gt;rate-of-change detection could be done as mapreduce jobs \n&gt;&gt;written to run on hadoop where the ingest is the Heritrix \n&gt;&gt;crawl.log. Also, Nutch has mapreduce jobs we&#39;ll want to \n&gt;&gt;model/exploit. Nutch has jobs to strip markup from HTML (or \n&gt;&gt;PDF, etc.), extract anchor text, calculate inlinks, and it \n&gt;&gt;keeps a running database of all URLs ever seen each of which \n&gt;&gt;could serve as input to mapreduce jobs that evaluate \n&gt;&gt;&#39;importance&#39;, &#39;change&#39;, or whether or not a page is &#39;junk&#39;. \n&gt;&gt;Nutch won&#39;t work for us out of the box -- for instance, we&#39;ll \n&gt;&gt;need to make the extractors used by Heritrix sync with the \n&gt;&gt;extractors used by Nutch in post-processing and we&#39;ll need to \n&gt;&gt;add notions of &#39;history&#39; to the crawl database -- but its a start.\n&gt;&gt;\n&gt;&gt;For the lookup service, we&#39;re thinking it&#39;ll have to be able \n&gt;&gt;to service a cluster of crawlers. Bdbje probably won&#39;t be \n&gt;&gt;fast enough. Kris&#39;s lucene experiment looks interesting. We \n&gt;&gt;also need to look at hashing External Memory Alogrithms \n&gt;&gt;similar to the one discussed in the Mercator paper.\n&gt; \n&gt; \n&gt; Lucene seems to scale fine to at least 10 million URLs. Above that (in my\n&gt; experience) I/O becomes an issue, especially as it starts to contend with\n&gt; the BdbFrontier&#39;s datastructures.\n&gt; \n\nI am curious as to why you would not try to build the second index (with \nLucene etc) as URIs are crawled, rather than at snapshot time.  Are you \nsimply concerned about disk contention?\n\nAt some point one really has to notice that you are in effect building \nfeatures of a true database out of berkeley db plus lucene; if you used \nsomething like postgresql with the proper indexes you may be surprised \nat its performance, esp. if you can contrive to query multiple uri&#39;s out \nof it at once.  I&#39;ve done this in (separate) crawling work by fetching \neverything that fits a given schedule slice, for instance, where if the \nslice is longer the query simply returns more uri&#39;s.  The real point is \nthat someone else spent lots of time optimizing disk access, so you may \nnot have to.\n\nJust a thought...\nKarl\n\n&gt; \n&gt;&gt;Blue-skying it, if we could agree on a bulk processing \n&gt;&gt;platform, such as hadoop, and if we could agree on how to \n&gt;&gt;package the fast-lookups -- e.g. \n&gt;&gt;all implement a Lookup Interface that takes a (shrunken?) \n&gt;&gt;CrawlURI returning an object that had a &#39;score&#39; and \n&gt;&gt;&#39;rationale&#39; -- then it seems like we could collaborate/split-the-work.\n&gt; \n&gt; \n&gt; Just how big do you expect the indexes to get? Also, I&#39;ve purposely gone\n&gt; after &#39;high yield&#39; documents (i.e. non text) that serves to limit the number\n&gt; of entries in the index and the number of lookups at crawl time. I take it\n&gt; you are looking for a more complete solution?\n&gt; \n&gt; - Kris\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}