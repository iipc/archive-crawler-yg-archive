{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":325624130,"authorName":"Noah Levitt","from":"Noah Levitt &lt;nlevitt@...&gt;","profile":"nlevitt","replyTo":"LIST","senderId":"R9J-gYD_WiCogPRGTdOdC8lmCVcFIO6o7EGg0q08BQs7MGUBC9ypRZ_rk7Rr2_RK3PZs7puVanY0mbAQteJNdbkBnw5ZCYQi","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] RE: Ignore robots.txt for specific URLs","postDate":"1371166284","msgId":8168,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBQS16NzArMWNWM3pRK3BMVnVyWjg0SEQrQmktQStBd09KNDJCNnkwaWJtREZaeV91QUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PEU0ODZEOTNFMzcyRDhDNDI4MkU4REYwNzgzOTJGMTJEOTVBNjhCQGJsaWtpLmJva2hsYWRhLmxvY2FsPg==","referencesHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MTBFNDhBMTFAdzJrMy1ic3BleDE+CTxFNDg2RDkzRTM3MkQ4QzQyODJFOERGMDc4MzkyRjEyRDk1QTY4QkBibGlraS5ib2tobGFkYS5sb2NhbD4="},"prevInTopic":8001,"nextInTopic":0,"prevInTime":8167,"nextInTime":8169,"topicId":7999,"numMessagesInTopic":4,"msgSnippet":"Hey, We happened to run into a problem (for us at least, depends on your needs) with using preconditions.calculateRobotsOnly, namely that it doesn t work for","rawEmail":"Return-Path: &lt;nlevitt@...&gt;\r\nX-Sender: nlevitt@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 10869 invoked by uid 102); 13 Jun 2013 23:31:33 -0000\r\nX-Received: from unknown (HELO mtaq4.grp.bf1.yahoo.com) (10.193.84.143)\n  by m9.grp.bf1.yahoo.com with SMTP; 13 Jun 2013 23:31:33 -0000\r\nX-Received: (qmail 31592 invoked from network); 13 Jun 2013 23:31:32 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.224.6)\n  by mtaq4.grp.bf1.yahoo.com with SMTP; 13 Jun 2013 23:31:32 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 1CAE168401B1\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 13 Jun 2013 16:31:31 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id XJkY5rfvvOjl for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 13 Jun 2013 16:31:25 -0700 (PDT)\r\nX-Received: from mail-oa0-f47.google.com (mail-oa0-f47.google.com [209.85.219.47])\n\tby mail.archive.org (Postfix) with ESMTPSA id 7044A6840153\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 13 Jun 2013 16:31:25 -0700 (PDT)\r\nX-Received: by mail-oa0-f47.google.com with SMTP id m1so9657217oag.6\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 13 Jun 2013 16:31:24 -0700 (PDT)\r\nX-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;\n        d=google.com; s=20120113;\n        h=mime-version:in-reply-to:references:date:message-id:subject:from:to\n         :content-type;\n        bh=bFig+yzGV19hufEGfUhvhucjcz+CRUU5Ia636Mbjb0M=;\n        b=SphgSM6KOoQW1u/BOo3pki6bMmkQQVfozCwSJr+Tx65CJYRITRYCJRPtOUY4xeKIgf\n         aIsf0V4QdRxu3cOz4hAulIxM9rSGIpc64rIkdztljVuiqxhVdSrg85fYXIzo2D31tAFN\n         q5z11hkiY71KBeBhObP/nvIDAD8DKJQRjeK0oWXpTSZZJDcCSrAeB/8sG4b+4zHc3kwD\n         sJ4e4s3JEoyJeR3E3K0XJcDYEofy9l+rrR51TzerlvyE1AfTh87vUMrqXpAaU+iVUXAe\n         NQ0weuRbn+hIMPGeX1KRUfCk1gi4JSB0+jfTuTBHB0P7LTOsPH2agRZJXuQy4vHI1BK5\n         LlLw==\r\nMIME-Version: 1.0\r\nX-Received: by 10.182.40.132 with SMTP id x4mr2356585obk.61.1371166284677;\n Thu, 13 Jun 2013 16:31:24 -0700 (PDT)\r\nX-Received: by 10.76.110.16 with HTTP; Thu, 13 Jun 2013 16:31:24 -0700 (PDT)\r\nIn-Reply-To: &lt;E486D93E372D8C4282E8DF078392F12D95A68B@...&gt;\r\nReferences: &lt;74C97E7DF5A7784D997217FF75D1216610E48A11@w2k3-bspex1&gt;\n\t&lt;E486D93E372D8C4282E8DF078392F12D95A68B@...&gt;\r\nDate: Thu, 13 Jun 2013 16:31:24 -0700\r\nMessage-ID: &lt;CAA-z70+1cV3zQ+pLVurZ84HD+Bi-A+AwOJ42B6y0ibmDFZy_uA@...&gt;\r\nTo: &quot;archive-crawler@yahoogroups.com&quot; &lt;archive-crawler@yahoogroups.com&gt;\r\nContent-Type: multipart/alternative; boundary=001a11c339d2538c0604df118701\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Noah Levitt &lt;nlevitt@...&gt;\r\nSubject: Re: [archive-crawler] RE: Ignore robots.txt for specific URLs\r\nX-Yahoo-Group-Post: member; u=325624130; y=er9IbWs0MSXdTCHWwbNEKgeaBlQL7fSej8KsmUdzfRxGMQ\r\nX-Yahoo-Profile: nlevitt\r\n\r\n\r\n--001a11c339d2538c0604df118701\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHey,\n\nWe happened to run into a problem (for us at least, depends on your n=\r\needs)\nwith using preconditions.calculateRobotsOnly, namely that it doesn&#39;t =\r\nwork\nfor &lt;meta name=3D&quot;robots&quot;&gt; tags.\n\nYou can get around this problem by c=\r\nreating a special CrawlMetadata bean\njust for ExtractorHTML, whose robotsPo=\r\nlicyName can be overridden in a\nsheet. The &quot;robotExcluded&quot; annotation will =\r\nbe missing for meta-robots\nexcluded urls but I think everything should work=\r\n otherwise.\n\nNoah\n\nOn Wed, Mar 27, 2013 at 6:18 AM, Kristinn Sigur=F0sson &lt;=\r\n\nkristinn@...&gt; wrote:\n\n&gt; Hi Roger,\n&gt;\n&gt; One option would be to =\r\ncreate a sheet overlay that uses\n&gt; DecideRuledSheetAssociation rather SurtP=\r\nrefixesSheetAssociation to\n&gt; determine when it should be applied.\n&gt;\n&gt; You c=\r\nould then use HopsPathMatchesRegexDecideRule to match only seeds and\n&gt; thei=\r\nr embeds.\n&gt;\n&gt; It all might look something like:\n&gt;\n&gt;   &lt;!-- Let seeds ignore=\r\n robots --&gt;\n&gt;   &lt;bean class=3D&quot;org.archive.crawler.spring.DecideRuledSheetA=\r\nssociation&quot;&gt;\n&gt;     &lt;property name=3D&quot;rules&quot;&gt;\n&gt;       &lt;bean class=3D&quot;org.arc=\r\nhive.modules.deciderules.DecideRuleSequence&quot;&gt;\n&gt;         &lt;property name=3D&quot;r=\r\nules&quot;&gt;\n&gt;           &lt;list&gt;\n&gt;             &lt;!-- Begin by REJECTing all... --&gt;\n=\r\n&gt;             &lt;bean\n&gt; class=3D&quot;org.archive.modules.deciderules.RejectDecide=\r\nRule&quot;&gt; &lt;/bean&gt;\n&gt;             &lt;!-- ...then ACCEPT those whose hoppath implie=\r\ns seed or embed\n&gt; of seed --&gt;\n&gt;             &lt;bean\n&gt; class=3D&quot;org.archive.mo=\r\ndules.deciderules.surt.HopsPathMatchesRegexDecideRule\n&gt; &quot;&gt;\n&gt;               =\r\n&lt;property name=3D&quot;decision&quot; value=3D&quot;ACCEPT&quot; /&gt; &lt;!-- Default\n&gt; behavior --&gt;=\r\n\n&gt;               &lt;property name=3D&quot;regex&quot; value=3D&quot;^.E?$&quot; /&gt;\n&gt;             =\r\n&lt;/bean&gt;\n&gt;           &lt;/list&gt;\n&gt;         &lt;/property&gt;\n&gt;       &lt;/bean&gt;\n&gt;     &lt;/p=\r\nroperty&gt;\n&gt;     &lt;property name=3D&quot;targetSheetNames&quot;&gt;\n&gt;       &lt;list&gt;\n&gt;       =\r\n  &lt;value&gt;ignoreRobots&lt;/value&gt;\n&gt;       &lt;/list&gt;\n&gt;     &lt;/property&gt;\n&gt;   &lt;/bean&gt;=\r\n\n&gt;\n&gt;   &lt;bean id=3D&quot;ignoreRobots&quot; class=3D&quot;org.archive.spring.Sheet&quot;&gt;\n&gt;     =\r\n&lt;property name=3D&quot;map&quot;&gt;\n&gt;       &lt;map&gt;\n&gt;         &lt;entry key=3D&quot;preconditions=\r\n.calculateRobotsOnly&quot; value=3D&quot;true&quot; /&gt;\n&gt;       &lt;/map&gt;\n&gt;     &lt;/property&gt;\n&gt; =\r\n  &lt;/bean&gt;\n&gt;\n&gt; The above is untested but should be roughly on the right trac=\r\nk.\n&gt;\n&gt; You can, of course, override the robots using other settings (such a=\r\ns the\n&gt; &#39;ignore&#39; policy) but I find this useful as it leaves trace in the c=\r\nrawl.log\n&gt; that the robots.txt were willfully ignored.\n&gt;\n&gt; Best,\n&gt; Kris\n&gt;\n&gt;=\r\n\n&gt; ------------------------------------------------------------------------=\r\n-\n&gt; Landsb=F3kasafn =CDslands - H=E1sk=F3lab=F3kasafn | Arngr=EDmsg=F6tu 3 =\r\n- 107 Reykjav=EDk\n&gt; S=EDmi/Tel: +354 5255600 | www.landsbokasafn.is\n&gt; -----=\r\n--------------------------------------------------------------------\n&gt; fyri=\r\nrvari/disclaimer - http://fyrirvari.landsbokasafn.is\n&gt; &gt; -----Original Mess=\r\nage-----\n&gt; &gt; From: archive-crawler@yahoogroups.com [mailto:archive-\n&gt; &gt; cra=\r\nwler@yahoogroups.com] On Behalf Of Coram, Roger\n&gt; &gt; Sent: 27. mars 2013 10:=\r\n58\n&gt; &gt; To: archive-crawler@yahoogroups.com\n&gt; &gt; Subject: [archive-crawler] I=\r\ngnore robots.txt for specific URLs\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt; We&#39;re considerin=\r\ng ignoring robots.txt for some URLs, specifically\n&gt; &gt; seeds and anything em=\r\nbedded on a seed. We&#39;ve tried using a .force\n&gt; &gt; file in the Action Directo=\r\nry but robots.txt seems to overrule this.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Any suggestions a=\r\ns to how to go about this?\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Thanks,\n&gt; &gt;\n&gt; &gt; Roger\n&gt; &gt;\n&gt; &gt;\n&gt;\n=\r\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\r\n--001a11c339d2538c0604df118701\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;&lt;div class=3D&quot;gmail_default&quot; style=3D&quot;font-family:arial,he=\r\nlvetica,sans-serif&quot;&gt;Hey,&lt;/div&gt;&lt;div class=3D&quot;gmail_default&quot; style=3D&quot;font-fa=\r\nmily:arial,helvetica,sans-serif&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_default&quot; sty=\r\nle=3D&quot;font-family:arial,helvetica,sans-serif&quot;&gt;\nWe happened to run into a pr=\r\noblem (for us at least, depends on your needs) with using &lt;span style=3D&quot;fo=\r\nnt-family:arial&quot;&gt;preconditions.&lt;/span&gt;&lt;span style=3D&quot;font-family:arial&quot;&gt;cal=\r\nculateRobotsOnly, namely that it doesn&#39;t work for &lt;meta name=3D&quot=\r\n;robots&quot;&gt; tags.=A0&lt;/span&gt;&lt;/div&gt;\n&lt;div class=3D&quot;gmail_default&quot; style=\r\n=3D&quot;font-family:arial,helvetica,sans-serif&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_d=\r\nefault&quot; style=3D&quot;font-family:arial,helvetica,sans-serif&quot;&gt;You can get around=\r\n this problem by creating a special CrawlMetadata bean just for ExtractorHT=\r\nML, whose robotsPolicyName can be overridden in a sheet. The &quot;robotExc=\r\nluded&quot; annotation will be missing for meta-robots excluded urls but I =\r\nthink everything should work otherwise.&lt;/div&gt;\n&lt;div class=3D&quot;gmail_default&quot; =\r\nstyle=3D&quot;font-family:arial,helvetica,sans-serif&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gm=\r\nail_default&quot; style=3D&quot;font-family:arial,helvetica,sans-serif&quot;&gt;Noah&lt;/div&gt;&lt;di=\r\nv class=3D&quot;gmail_default&quot; style=3D&quot;font-family:arial,helvetica,sans-serif&quot;&gt;=\r\n\n&lt;br&gt;&lt;/div&gt;&lt;div class=3D&quot;gmail_extra&quot;&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;On Wed, Ma=\r\nr 27, 2013 at 6:18 AM, Kristinn Sigur=F0sson &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=\r\n=3D&quot;mailto:kristinn@...&quot; target=3D&quot;_blank&quot;&gt;kristinn@landsbokas=\r\nafn.is&lt;/a&gt;&gt;&lt;/span&gt; wrote:&lt;br&gt;\n&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D=\r\n&quot;margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,2=\r\n04,204);border-left-style:solid;padding-left:1ex&quot;&gt;Hi Roger,&lt;br&gt;\n&lt;br&gt;\nOne op=\r\ntion would be to create a sheet overlay that uses DecideRuledSheetAssociati=\r\non rather SurtPrefixesSheetAssociation to determine when it should be appli=\r\ned.&lt;br&gt;\n&lt;br&gt;\nYou could then use HopsPathMatchesRegexDecideRule to match onl=\r\ny seeds and their embeds.&lt;br&gt;\n&lt;br&gt;\nIt all might look something like:&lt;br&gt;\n&lt;b=\r\nr&gt;\n=A0 &lt;!-- Let seeds ignore robots --&gt;&lt;br&gt;\n=A0 &lt;bean class=3D&quo=\r\nt;org.archive.crawler.spring.DecideRuledSheetAssociation&quot;&gt;&lt;br&gt;\n=A0 =\r\n=A0 &lt;property name=3D&quot;rules&quot;&gt;&lt;br&gt;\n=A0 =A0 =A0 &lt;bean clas=\r\ns=3D&quot;org.archive.modules.deciderules.DecideRuleSequence&quot;&gt;&lt;br&gt;\n=\r\n=A0 =A0 =A0 =A0 &lt;property name=3D&quot;rules&quot;&gt;&lt;br&gt;\n=A0 =A0 =A0 =\r\n=A0 =A0 &lt;list&gt;&lt;br&gt;\n=A0 =A0 =A0 =A0 =A0 =A0 &lt;!-- Begin by REJECTing=\r\n all... --&gt;&lt;br&gt;\n=A0 =A0 =A0 =A0 =A0 =A0 &lt;bean class=3D&quot;org.archi=\r\nve.modules.deciderules.RejectDecideRule&quot;&gt; &lt;/bean&gt;&lt;br&gt;\n=A0 =A0=\r\n =A0 =A0 =A0 =A0 &lt;!-- ...then ACCEPT those whose hoppath implies seed or=\r\n embed of seed --&gt;&lt;br&gt;\n=A0 =A0 =A0 =A0 =A0 =A0 &lt;bean class=3D&quot;or=\r\ng.archive.modules.deciderules.surt.HopsPathMatchesRegexDecideRule &quot;&gt=\r\n;&lt;br&gt;\n=A0 =A0 =A0 =A0 =A0 =A0 =A0 &lt;property name=3D&quot;decision&quot; =\r\nvalue=3D&quot;ACCEPT&quot; /&gt; &lt;!-- Default behavior --&gt;&lt;br&gt;\n=A0 =\r\n=A0 =A0 =A0 =A0 =A0 =A0 &lt;property name=3D&quot;regex&quot; value=3D&quot=\r\n;^.E?$&quot; /&gt;&lt;br&gt;\n=A0 =A0 =A0 =A0 =A0 =A0 &lt;/bean&gt;&lt;br&gt;\n=A0 =A0 =\r\n=A0 =A0 =A0 &lt;/list&gt;&lt;br&gt;\n=A0 =A0 =A0 =A0 &lt;/property&gt;&lt;br&gt;\n=A0 =A0=\r\n =A0 &lt;/bean&gt;&lt;br&gt;\n=A0 =A0 &lt;/property&gt;&lt;br&gt;\n=A0 =A0 &lt;property n=\r\name=3D&quot;targetSheetNames&quot;&gt;&lt;br&gt;\n=A0 =A0 =A0 &lt;list&gt;&lt;br&gt;\n=A0=\r\n =A0 =A0 =A0 &lt;value&gt;ignoreRobots&lt;/value&gt;&lt;br&gt;\n=A0 =A0 =A0 &lt;/l=\r\nist&gt;&lt;br&gt;\n=A0 =A0 &lt;/property&gt;&lt;br&gt;\n=A0 &lt;/bean&gt;&lt;br&gt;\n&lt;br&gt;\n=A0 &l=\r\nt;bean id=3D&quot;ignoreRobots&quot; class=3D&quot;org.archive.spring.Sheet=\r\n&quot;&gt;&lt;br&gt;\n=A0 =A0 &lt;property name=3D&quot;map&quot;&gt;&lt;br&gt;\n=A0 =A0 =\r\n=A0 &lt;map&gt;&lt;br&gt;\n=A0 =A0 =A0 =A0 &lt;entry key=3D&quot;preconditions.cal=\r\nculateRobotsOnly&quot; value=3D&quot;true&quot; /&gt;&lt;br&gt;\n=A0 =A0 =A0 &lt;/=\r\nmap&gt;&lt;br&gt;\n=A0 =A0 &lt;/property&gt;&lt;br&gt;\n=A0 &lt;/bean&gt;&lt;br&gt;\n&lt;br&gt;\nThe ab=\r\nove is untested but should be roughly on the right track.&lt;br&gt;\n&lt;br&gt;\nYou can,=\r\n of course, override the robots using other settings (such as the &#39;igno=\r\nre&#39; policy) but I find this useful as it leaves trace in the crawl.log =\r\nthat the robots.txt were willfully ignored.&lt;br&gt;\n&lt;br&gt;\nBest,&lt;br&gt;\nKris&lt;br&gt;\n&lt;br=\r\n&gt;\n&lt;br&gt;\n--------------------------------------------------------------------=\r\n-----&lt;br&gt;\nLandsb=F3kasafn =CDslands - H=E1sk=F3lab=F3kasafn | Arngr=EDmsg=\r\n=F6tu 3 - 107 Reykjav=EDk&lt;br&gt;\nS=EDmi/Tel: &lt;a href=3D&quot;tel:%2B354%205255600&quot; =\r\nvalue=3D&quot;+3545255600&quot;&gt;+354 5255600&lt;/a&gt; | &lt;a href=3D&quot;http://www.landsbokasaf=\r\nn.is&quot; target=3D&quot;_blank&quot;&gt;www.landsbokasafn.is&lt;/a&gt;&lt;br&gt;\n----------------------=\r\n---------------------------------------------------&lt;br&gt;\nfyrirvari/disclaime=\r\nr - &lt;a href=3D&quot;http://fyrirvari.landsbokasafn.is&quot; target=3D&quot;_blank&quot;&gt;http://=\r\nfyrirvari.landsbokasafn.is&lt;/a&gt;&lt;br&gt;\n&lt;div class=3D&quot;im&quot;&gt;&gt; -----Original Mes=\r\nsage-----&lt;br&gt;\n&gt; From: &lt;a href=3D&quot;mailto:archive-crawler@yahoogroups.com&quot;=\r\n&gt;archive-crawler@yahoogroups.com&lt;/a&gt; [mailto:&lt;a href=3D&quot;mailto:archive-&quot;&gt;ar=\r\nchive-&lt;/a&gt;&lt;br&gt;\n&gt; &lt;a href=3D&quot;mailto:crawler@yahoogroups.com&quot;&gt;crawler@yaho=\r\nogroups.com&lt;/a&gt;] On Behalf Of Coram, Roger&lt;br&gt;\n&gt; Sent: 27. mars 2013 10:=\r\n58&lt;br&gt;\n&gt; To: &lt;a href=3D&quot;mailto:archive-crawler@yahoogroups.com&quot;&gt;archive-=\r\ncrawler@yahoogroups.com&lt;/a&gt;&lt;br&gt;\n&gt; Subject: [archive-crawler] Ignore robo=\r\nts.txt for specific URLs&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; Hi,&lt;br&gt;\n&gt; W=\r\ne&#39;re considering ignoring robots.txt for some URLs, specifically&lt;br&gt;\n&g=\r\nt; seeds and anything embedded on a seed. We&#39;ve tried using a .force&lt;br=\r\n&gt;\n&gt; file in the Action Directory but robots.txt seems to overrule this.&lt;=\r\nbr&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; Any suggestions as to how to go about t=\r\nhis?&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; Thanks,&lt;br&gt;\n&gt;&lt;br&gt;\n&gt; Roger&lt;br=\r\n&gt;\n&gt;&lt;br&gt;\n&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;/div&gt;------------------------------------&lt;br=\r\n&gt;\n&lt;br&gt;\nYahoo! Groups Links&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; To visit your group on the we=\r\nb, go to:&lt;br&gt;\n=A0 =A0 &lt;a href=3D&quot;http://groups.yahoo.com/group/archive-craw=\r\nler/&quot; target=3D&quot;_blank&quot;&gt;http://groups.yahoo.com/group/archive-crawler/&lt;/a&gt;&lt;=\r\nbr&gt;\n&lt;br&gt;\n&lt;*&gt; Your email settings:&lt;br&gt;\n=A0 =A0 Individual Email | Trad=\r\nitional&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; To change settings online go to:&lt;br&gt;\n=A0 =A0 &lt;a =\r\nhref=3D&quot;http://groups.yahoo.com/group/archive-crawler/join&quot; target=3D&quot;_blan=\r\nk&quot;&gt;http://groups.yahoo.com/group/archive-crawler/join&lt;/a&gt;&lt;br&gt;\n=A0 =A0 (Yaho=\r\no! ID required)&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; To change settings via email:&lt;br&gt;\n=A0 =\r\n=A0 &lt;a href=3D&quot;mailto:archive-crawler-digest@yahoogroups.com&quot;&gt;archive-crawl=\r\ner-digest@yahoogroups.com&lt;/a&gt;&lt;br&gt;\n=A0 =A0 &lt;a href=3D&quot;mailto:archive-crawler=\r\n-fullfeatured@yahoogroups.com&quot;&gt;archive-crawler-fullfeatured@yahoogroups.com=\r\n&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; To unsubscribe from this group, send an email to:&lt;b=\r\nr&gt;\n=A0 =A0 &lt;a href=3D&quot;mailto:archive-crawler-unsubscribe@yahoogroups.com&quot;&gt;a=\r\nrchive-crawler-unsubscribe@yahoogroups.com&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;*&gt; Your use =\r\nof Yahoo! Groups is subject to:&lt;br&gt;\n=A0 =A0 &lt;a href=3D&quot;http://docs.yahoo.co=\r\nm/info/terms/&quot; target=3D&quot;_blank&quot;&gt;http://docs.yahoo.com/info/terms/&lt;/a&gt;&lt;br&gt;\n=\r\n&lt;br&gt;\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;\n\r\n--001a11c339d2538c0604df118701--\r\n\n"}}