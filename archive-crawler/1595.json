{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"lqDBpysVTymJUDNV7Ua91MA6OLIc-V3clx6p8vcOBZlWrfFHz_WGw4GpCVku5h4Fu8sFVB1RZa4YEpsE29vnGQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Newbie question -- I should be more specific","postDate":"1108990710","msgId":1595,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyMTlEQUY2LjkwNzAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGN2N3I2OCtvaGJiQGVHcm91cHMuY29tPg==","referencesHeader":"PGN2N3I2OCtvaGJiQGVHcm91cHMuY29tPg=="},"prevInTopic":1589,"nextInTopic":1596,"prevInTime":1594,"nextInTime":1596,"topicId":1588,"numMessagesInTopic":7,"msgSnippet":"... In the above, you need to explain more what you mean by ...grab a few pages based on content...  When you say grab, do you mean crawl only certain pages?","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 61382 invoked from network); 21 Feb 2005 14:00:46 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m21.grp.scd.yahoo.com with QMQP; 21 Feb 2005 14:00:46 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta4.grp.scd.yahoo.com with SMTP; 21 Feb 2005 14:00:46 -0000\r\nReceived: (qmail 24389 invoked by uid 100); 21 Feb 2005 13:44:09 -0000\r\nReceived: from host236.n061-193-185-224.pri.iprevolution.ne.jp (HELO ?61.193.185.236?) (stack@...@61.193.185.236)\n  by mail-dev.archive.org with SMTP; 21 Feb 2005 13:44:09 -0000\r\nMessage-ID: &lt;4219DAF6.9070303@...&gt;\r\nDate: Mon, 21 Feb 2005 13:58:30 +0100\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.5) Gecko/20050105 Debian/1.7.5-1\r\nX-Accept-Language: us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;cv7r68+ohbb@...&gt;\r\nIn-Reply-To: &lt;cv7r68+ohbb@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.0 required=6.5 tests=none autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: Newbie question -- I should be more specific\r\nX-Yahoo-Group-Post: member; u=168599281\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nbillo_ga wrote:\n\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, &quot;billo_ga&quot; &lt;billo@r...&gt; wrote:\n&gt; &gt; I am looking for a tool that will allow me to focus on a domain and\n&gt; &gt; only grab a few pages based on content - for example, simple text\n&gt; &gt; searching criteria.  For instance, I may be only looking for pages\n&gt; &gt; that contain the word &quot;acre&quot; or &quot;juniper,&quot; or only pages that have\n&gt; &gt; links to more than two .mpeg movies. Of course, the more useful things\n&gt; &gt; are more complex.\n\nIn the above, you need to explain more what you mean by &#39;...grab a few \npages based on content...&#39;  When you say grab, do you mean crawl only \ncertain pages?  Or, do you mean that pages with &#39;acre&#39; or &#39;juniper&#39; are \nthe pages that you&#39;ll subsequently be interested in after all has been \ncrawled?  (You probably mean the latter because you understand that the \ncrawler has to download all pages first before it can see which pages \nhave &#39;acre&#39; or &#39;juniper&#39; in them).\n\nSee more below.\n\n&gt; &gt;\n&gt;\n&gt; I should probably be more specific here.  I am a forensic pathologist,\n&gt; and am interested in searching various sites for information and\n&gt; imagery involving patterned injury of the skin.  I am, for instance,\n&gt; writing an atlas of patterned injury in order to help people determine\n&gt; what object was used in an assault (hammer, crowbar, etc.) from the\n&gt; patterns those objects leave on the skin.\n\nInteresting.  I&#39;m guessing it&#39;ll be a large volume.\n\n&gt;\n&gt; I am interested in searching hardware sites for specific kinds of\n&gt; implements, and searching medical sites for images of skin injury.\n&gt;\n&gt; I happen to have some formal training in Computer Science, and can\n&gt; write code in Java, C++, C, python, perl, and a few others (even --\n&gt; not to show my age, APL, God help me; I still have a keyboard laying\n&gt; around for it).  So, I&#39;m happy to write a parser or whatever if I can\n&gt; get a handle on how to fit it in here (or where to fit it in).\n\nHere&#39;s a sketch of one way in which I could imagine it working.\n\nRun the crawler against the sites you are interested in.  Then, after \nthe crawl has completed, feed the downloaded ARCs to a search engine so \nyou can run your &#39;acre&#39; and &#39;juniper&#39; queries.  There is quite a bit of \nwork involved here -- parsing ARC files, feeding each ARC record to a \nmimetype-particular parser (i.e. an html parser for the text/html), then \npassing the extracted text to a search engine indexer, etc. -- but the \ngood news here is that you should be able to leverage the work begun \nhere, \nhttp://cvs.sourceforge.net/viewcvs.py/*checkout*/archive-access/archive-access/projects/nutch/README.txt?rev=1, \nwhich has tools to feed ARC files to nutch (Whats there has been tried \non 40million plus text/html pages.  The quality of the searches is \nlacking but is currently being worked on).\n\nAsking nutch to return pages with two or more mpegs may work.  If it \ndoesn&#39;t, you&#39;d need to do your own purposed parse of the pages \npopulating an index/db that you can run these types of queries against \n(You could write the parse in APL).  Dependent on the type of parse you \nwant to run, this would probably be a significant undertaking (See \nportions of nutch for an example).\n\nAre the domains you are interested in large?  If they are, running a \nlarge crawl and running the downloaded crawl via an indexer is a \nsignificant adminstrative task requiring ample hardware (disk, cpu).  \nFor example, feeding the 40million plus above mentioned pages to nutch \nto index took 4 machines running multiple days (Speed is also being \nworked on in the aforementioned archive-access nutch project).\n\nYours,\nSt.Ack\n\n&gt;\n&gt; billo\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; *Yahoo! Groups Sponsor*\n&gt; &lt;http://us.ard.yahoo.com/SIG=129qjc88h/M=324658.6070095.7083352.3001176/D=groups/S=1705004924:HM/EXP=1108918675/A=2343726/R=0/SIG=12ij8ddfl/*http://clk.atdmt.com/VON/go/yhxxxvon01900091von/direct/01/&time=1108832275327474&gt; \n&gt;\n&gt;\n&gt; Get unlimited calls to\n&gt;\n&gt; U.S./Canada\n&gt;\n&gt; &lt;http://us.ard.yahoo.com/SIG=129qjc88h/M=324658.6070095.7083352.3001176/D=groups/S=1705004924:HM/EXP=1108918675/A=2343726/R=1/SIG=12ij8ddfl/*http://clk.atdmt.com/VON/go/yhxxxvon01900091von/direct/01/&time=1108832275327474&gt; \n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}