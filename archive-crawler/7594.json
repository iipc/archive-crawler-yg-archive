{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"L7mv1rpjLLtjE-5hKz8d0c2ertDQN_iNz9AtWXbHpbgP2_HKcpQUS0QQhUTUD7-tpvnw2Mube-B-Ft77_AQXhwtp_psGxnE","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] H3 - distributed crawling and memory/cpu utilization","postDate":"1328047597","msgId":7594,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMjg2NUVELjcwOTAzQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDRGMjg2MkYyLjcwNjA4MDlAY3MuY211LmVkdT4=","referencesHeader":"PGo3NHMzNCtldWVnQGVHcm91cHMuY29tPiA8NEU5OEVBN0QuMTA4MDgwNkBhcmNoaXZlLm9yZz4gPDRGMjg2MkYyLjcwNjA4MDlAY3MuY211LmVkdT4="},"prevInTopic":7593,"nextInTopic":0,"prevInTime":7593,"nextInTime":7595,"topicId":7351,"numMessagesInTopic":7,"msgSnippet":"URIs are checked/entered into the bloom filter (when using that non-default UriUniqFilter option) when they are presented to the frontier for potential","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 46732 invoked from network); 31 Jan 2012 22:06:43 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m3.grp.sp2.yahoo.com with QMQP; 31 Jan 2012 22:06:43 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta2.grp.sp2.yahoo.com with SMTP; 31 Jan 2012 22:06:42 -0000\r\nX-Received: (qmail 6643 invoked by uid 0); 31 Jan 2012 22:06:41 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay02.pair.com with SMTP; 31 Jan 2012 22:06:41 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4F2865ED.70903@...&gt;\r\nDate: Tue, 31 Jan 2012 14:06:37 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: David Pane &lt;dpane@...&gt;\r\nReferences: &lt;j74s34+eueg@...&gt; &lt;4E98EA7D.1080806@...&gt; &lt;4F2862F2.7060809@...&gt;\r\nIn-Reply-To: &lt;4F2862F2.7060809@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] H3 - distributed crawling and memory/cpu utilization\r\nX-Yahoo-Group-Post: member; u=137285340; y=cU-tTqMtku4sJKcvNXrx6X_2vb36jBNMaG6FQORGoij9\r\nX-Yahoo-Profile: gojomo\r\n\r\nURIs are checked/entered into the bloom filter (when using that \nnon-default UriUniqFilter option) when they are presented to the \nfrontier for potential enqueueing.\n\nSo, when using the &#39;CrawlMapper&#39; technique of diverting some discovered \nURIs to a log, those URIs do *not* get added to the local bloom filter.\n\nHowever, the discovered count (or downloaded+queued) does accurately \nreflect how many URIs have been locally presented to the bloom filter.\n\nTo predict the growing false-positive rate over time, you can go to \nreference material on Bloom filters with the relevant formula(s), for \nexample:\n\nhttp://en.wikipedia.org/wiki/Bloom_filter#Probability_of_false_positives\n\nThe false-positive rate is always growing; it&#39;s just that at a certain \ndesigned size threshold it has a certain round value closely related to \nthe starting parameters.\n\n- Gordon\n\nOn 1/31/12 1:53 PM, David Pane wrote:\n&gt; Noah,\n&gt;\n&gt; I noticed a warning message on a recent rebuild fromt the frontier\n&gt; recovery logs.\n&gt;\n&gt; WARNING Bloom has reached expected limit 400,000,000; false-positive\n&gt; rate will now rise above goal of 1-in-(2^30 (in thread &#39;pool-3-thread-1&#39;)\n&gt;\n&gt; The current totals (with 164,285,847 pages crawled per instance) for\n&gt; each of our 5 instances we are seeing statistics similar to this:\n&gt;\n&gt; 297,472,096 downloaded + 854,566,462 queued = 1,152,039,733 total\n&gt;\n&gt; By the time we reach 200 million pages, we will be seeing over 1.7\n&gt; billion URIs (downloaded + queued) per instance.  This would mean that\n&gt; we would need a 10GB or 11GB bloom filter.  In doing these calculations,\n&gt; I had a couple questions:\n&gt;\n&gt; 1) Does each of the instances bloom filter contain only the URIs that it\n&gt; had crawled and what is queued  or does it contain discovered URIs that\n&gt; are sent to other instances?\n&gt;\n&gt; 2) When the bloom filter reaches the expected limit, what does the rate\n&gt; of false-positive increase to?\n&gt;\n&gt; --David\n&gt;\n&gt;\n&gt; On 10/14/2011 10:05 PM, Noah Levitt wrote:\n&gt;&gt; Hello David,\n&gt;&gt;\n&gt;&gt; On 10/12/2011 01:08 PM, david_pane1 wrote:\n&gt;&gt;&gt; 1) I am using two 8 core 32GB machines for some test crawls. The\n&gt;&gt;&gt; machines are connected to the internet on a gigabit connection. The\n&gt;&gt;&gt; internal network is also a gigabit. I am writing the data from the\n&gt;&gt;&gt; crawler to a NAS which I can copy data to at an average of 70MB/sec\n&gt;&gt;&gt; transfer rate. I am trying to adjust the memory and maxToeThread\n&gt;&gt;&gt; settings to get the maximum throughput. Currently my settings are:\n&gt;&gt;&gt;\n&gt;&gt;&gt; JAVA_OPTS=&quot;-Xmx11000M\n&gt;&gt;&gt; &lt;property name=&quot;maxToeThreads&quot; value=&quot;1200&quot; /&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; With these values, I am seeing about 50% cpu utilization (about half\n&gt;&gt;&gt; of the 8 cores) and an average of around 750MB/min or (96Mbps) network\n&gt;&gt;&gt; activity. I would like to utilize more of the cpu. Is this reasonable?\n&gt;&gt;\n&gt;&gt; If the crawl has plenty of urls to keep it busy, the limiting factor is\n&gt;&gt; typically disk, not cpu.\n&gt;&gt;\n&gt;&gt;&gt; Increasing the maxToeThreads to a higher value than 1200 causes the\n&gt;&gt;&gt; java application to fall to minimal to no cpu usage and the web\n&gt;&gt;&gt; interface to be unresponsive. Does anyone know why this is happening?\n&gt;&gt;\n&gt;&gt; Don&#39;t know without looking more closely at the logs and state of the\n&gt;&gt; java process when that happens. But 1200 threads for an 11000M heap\n&gt;&gt; seems like a lot, maybe too many depending on the rest of your config.\n&gt;&gt; The other big consumers of memory are the bdb cache and the bloom filter\n&gt;&gt; if you&#39;re using that. By default the bdb cache occupies 60% of available\n&gt;&gt; heap, so 6600M in your case.\n&gt;&gt;\n&gt;&gt; It sounds like you&#39;re doing a broad crawl, so the bloom filter is\n&gt;&gt; probably appropriate for your already-seen check. The default\n&gt;&gt; BdbUriUniqFilter can be a bottleneck in a large crawl. The bloom filter\n&gt;&gt; occupies a fixed amount of memory and does not use disk, but is\n&gt;&gt; probabilistic, and accepts a small rate of false positives. With default\n&gt;&gt; settings, it uses 495M of heap, and has an expected false-positive rate\n&gt;&gt; of 1-in-4million through the first 125 million urls it sees.\n&gt;&gt;\n&gt;&gt; There&#39;s some old but generally relevant discussion of the bloom filter\n&gt;&gt; here: http://tech.groups.yahoo.com/group/archive-crawler/message/3142\n&gt;&gt;\n&gt;&gt; I&#39;m not finding existing docs on this so just for the record, this is\n&gt;&gt; how you can configure a different sized bloom filter in h3. This example\n&gt;&gt; would use\n&gt;&gt; 1.44 * 400,000,000 * 30 / 8 = 2,160,000,000 bytes of heap with an\n&gt;&gt; expected false-postive rate of 2^-30 or about 1-in-a-billion through the\n&gt;&gt; first 400 million urls seen. Hopefully I&#39;m getting this math right. :)\n&gt;&gt;\n&gt;&gt; &lt;bean id=&quot;uriUniqFilter&quot;\n&gt;&gt; class=&quot;org.archive.crawler.util.BloomUriUniqFilter&quot;&gt;\n&gt;&gt; &lt;property name=&quot;bloomFilter&quot;&gt;\n&gt;&gt; &lt;bean class=&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&gt;&gt; &lt;constructor-arg value=&quot;400000000&quot;/&gt;\n&gt;&gt; &lt;constructor-arg value=&quot;30&quot;/&gt;\n&gt;&gt; &lt;/bean&gt;\n&gt;&gt; &lt;/property&gt;\n&gt;&gt; &lt;/bean&gt;\n&gt;&gt;\n&gt;&gt;&gt; How much of the 32GB of memory should I allocate to JAVA_OPTS?\n&gt;&gt;\n&gt;&gt; The more the merrier, as long as other processes have enough room and\n&gt;&gt; the system doesn&#39;t start swapping.\n&gt;&gt;\n&gt;&gt; There&#39;s no hard limit on the amount of heap each toe thread can use, and\n&gt;&gt; heritrix can be vulnerable to websites that do weird stuff. With default\n&gt;&gt; settings though, 5-10M per toe thread is a reasonable rule of thumb.\n&gt;&gt; Most of the time each thread will use much less than that, but sometimes\n&gt;&gt; they&#39;ll use more. If you get an OutOfMemoryError then you should reduce\n&gt;&gt; the number of toe threads.\n&gt;&gt;\n&gt;&gt; So let&#39;s say you give heritrix a 25G heap, leave the bdb cache at the\n&gt;&gt; default of 60%, and use the default .5G bloom filter. That leaves you\n&gt;&gt; with 25GB * (100% - 60%) - .5G = 9.5G for toe threads. At 7.5M per toe\n&gt;&gt; thread that comes to 1266 of them.\n&gt;&gt;\n&gt;&gt; With this config and hardware I would expect your crawl rate to be\n&gt;&gt; limited elsewhere, likely in warc writing, assuming you have that\n&gt;&gt; enabled. Presumably all these threads will be competing to write to a\n&gt;&gt; small number of disks.\n&gt;&gt;\n&gt;&gt;&gt; 2) One of the 2 crawlers stopped crawling due to a congestion ratio of\n&gt;&gt;&gt; infinity. What are some ways to overcome this? What can I do to avoid\n&gt;&gt;&gt; it happening in the future?\n&gt;&gt;\n&gt;&gt; I don&#39;t know. Not clear what you mean by &quot;stopped&quot; exactly for one\n&gt;&gt; thing. If it happens again, maybe gather more information from\n&gt;&gt; heritrix_out.log, toe thread report, frontier report, top, iostat,\n&gt;&gt; jstack, etc.\n&gt;&gt;\n&gt;&gt;&gt; 3) In http://tech.groups.yahoo.com/group/archive-crawler/message/3846\n&gt;&gt;&gt; Gordon stated:\n&gt;&gt;&gt;\n&gt;&gt;&gt; &quot;...\n&gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the SURT\n&gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI is\n&gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to a\n&gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally.\n&gt;&gt;&gt; Depending on the character of your crawl, you may want to feed these\n&gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore them.\n&gt;&gt;&gt; ...\n&gt;&gt;&gt; &quot;\n&gt;&gt;&gt;\n&gt;&gt;&gt; How does one feed the diverted URIs/logs to a sibling crawler?\n&gt;&gt;\n&gt;&gt; A coworker kindly put up this page yesterday:\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Multiple+Machine+Crawling\n&gt;&gt; which says, &quot;Crawl operators must set up a process where the the URIs\n&gt;&gt; contained in .divert files are copied from each crawler to their\n&gt;&gt; assigned crawlers and queued into the active crawl (putting the .divert\n&gt;&gt; file in the actions directory as a .include should be sufficient).&quot;\n&gt;&gt;\n&gt;&gt; Noah\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt; Any help is greatly appreciated.\n&gt;&gt;&gt;\n&gt;&gt;&gt; --David\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}