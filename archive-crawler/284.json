{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163922992,"authorName":"John Erik Halse","from":"John Erik Halse &lt;johnh@...&gt;","profile":"johnerikhalse","replyTo":"LIST","senderId":"PYMZRXPZwV5-tTmPx6yNDGn2smvHYUICbEkvmRZ42Jfj7MFU8EJbyLoLjJwERrtU__cQyF-a6HLHMtwoKBgiCZBYkz55ldusj9w","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Revisiting - input needed","postDate":"1078533874","msgId":284,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEwNzg1MzM4NzQuMzE5NDYuNDIuY2FtZWxAYjExNi1keW4tMzcuYXJjaGl2ZS5vcmc+"},"prevInTopic":0,"nextInTopic":285,"prevInTime":283,"nextInTime":285,"topicId":284,"numMessagesInTopic":3,"msgSnippet":"We have begun discussing how the crawler could be able to revisit already crawled URIs. Some initial thoughts have been written on the Wiki ","rawEmail":"Return-Path: &lt;johnh@...&gt;\r\nX-Sender: johnh@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60893 invoked from network); 6 Mar 2004 00:51:21 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m7.grp.scd.yahoo.com with QMQP; 6 Mar 2004 00:51:21 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta1.grp.scd.yahoo.com with SMTP; 6 Mar 2004 00:51:21 -0000\r\nReceived: (qmail 24923 invoked by uid 100); 6 Mar 2004 00:47:20 -0000\r\nReceived: from b116-dyn-37.archive.org (johnh@...@209.237.240.37)\n  by mail-dev.archive.org with RC4-MD5 encrypted SMTP; 6 Mar 2004 00:47:20 -0000\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nContent-Type: multipart/alternative; boundary=&quot;=-QB103uocRlZWfcZ0nNK9&quot;\r\nMessage-Id: &lt;1078533874.31946.42.camel@...&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Ximian Evolution 1.4.5 \r\nDate: Fri, 05 Mar 2004 16:44:34 -0800\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.1 required=6.0 tests=AWL,HTML_MESSAGE autolearn=ham \n\tversion=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: John Erik Halse &lt;johnh@...&gt;\r\nSubject: Revisiting - input needed\r\nX-Yahoo-Group-Post: member; u=163922992\r\nX-Yahoo-Profile: johnerikhalse\r\n\r\n\r\n--=-QB103uocRlZWfcZ0nNK9\r\nContent-Type: text/plain\r\nContent-Transfer-Encoding: 7bit\r\n\r\nWe have begun discussing how the crawler could be able to revisit\nalready crawled URIs. Some initial thoughts have been written on the\nWiki &lt;url:\nhttp://crawler.archive.org/cgi-bin/wiki.pl?RevisitingScenarios &gt; (The\nwhole text of the page as writing this mail is pasted in below). What we\nneed, are possible real-world use-cases for a &quot;periodic revisit&quot;\nfeature. To give some input to the discussion, I also forward a part of\na mail that Gordon wrote.\n\nPlease feel free to write comments directly into the Wiki-page or\ncomment on this mail.\n\n- John Erik\n\n--- cut and paste from Gordon&#39;s mail ---\nFor example, assuming the crawler were not in such active\ndevelopment, weekly crawls like the UK Gov and LoC Election\njust recur within a single long-lived -- or occasionally restarted\ndue to outages/etc -- process?\n\nThe &quot;null hypothesis&quot; would be that these needs could be met by\nsimply launching a new independent crawl, manually or via a cron\njob, at the desired interval.\n\nWhat might be wrong with that approach?\n\n   - no chance to benefit from recognizing exact duplicates\n     (though this is a larger issue needing other discussion)\n   - crawls could overlap in time; causing problems on the\n     limited hardware resources or violating politeness goals\n   - minimal chance to use other info from past crawls to influence\n     future crawls (though such uses remain theoretical, like\n     prioritizing crawl n+1 based on link structure found in\n     crawl n)\n\nI suspect that it is the first item that really motivates a\nlot of the desire for an integrated/duplicate-aware periodic\nrecrawl capability. People want to save on storage; they want\nto have crawl records which give some hint as to rates of\nresource change; they want to crawl the big relatively-static\narchives once but then the volatile new material regularly.\n\n\n\n\n\n--- cut and paste from Wiki ---\n\nPossible scenarios for revisiting\n\nThe current crawler starts off with a list of URIs called seeds. Then it\nextracts new URIs from these seeds and apply certain rules (the scope)\nto these URIs and if it passes they are crawled. Every crawled URI is\nput into a &#39;already crawled&#39; table that makes sure they&#39;re not crawled\nagain if the same URI comes up again. \n\nIf you want to crawl the same URI over again to get changed pages, this\nhas to be done by running a new crawl job. We want to look into\nsolutions to alter this behavior by letting the crawler revisit URIs\nbased on different policies. But by doing this several questions comes\nup.\n\nThe first feature that one might want to have is the ability to check if\na web-page has changed since it was last crawled and only revisit it if\nit has. The downside with this is that you have to revisit every page to\ndetermine if it was actually changed. You could maybe trust the last\nchanged date in the http header, but it seems that there are many\nmisconfigured servers out there so you are most likely to download the\npage and compute a checksum for it. The other drawback is that you have\nto keep a database with all the checksums.\n\nEither you want to check for changes or not, it is preferable to not\nrevisit pages unless it is likely that it might have changed. The\nscenarios mentioned here is examples of how we could create rules for\nrevisiting so that we don&#39;t revisit a page more often than necessary.\n\nAnother question is what priority the revisits should have. Should it\njust be put at the end of the queue of discovered document, or should\nthe revisiting have a higher priority than the documents thats already\nbeeing crawled (i.e. put in the front of the queue). One other\npossibility is to have more than one queue (with, most likely, different\npriorities) which intermixes old discovered URIs with revisits. This\ncould be of particular value if the priorities during revisits change as\nwe move furhter from the seed (a little more on this further down).\n\nFor all the scenarios I assume that the site has been crawled pretty\ndeep when we first came across it.\n\nYou are encouraged to add or update possible scenarios as well as the\nassumptions for revisits mentioned here. To do so, just follow the link\n&quot;Edit text of this page&quot; at the bottom of the page.\n\n\n\nRevisit everything\n\nAfter crawling for a while, readd all the URIs in the seed list into the\ncrawler. The main difference between this and starting a new job is that\nthe crawler will continue working on the discovered URIs found in the\nfirst pass along with the revisits. \n\nThis is pretty simple to implement. The main thing to consider is how to\nmark the already included as eligible for revisiting without allowing an\narbitrary amount of revisits to occur.\n\n\n\nRevisit marked seeds\n\nThis is a variant of the above in that it only revisits seeds that is\nmarked for revisits. The mark could possibly have a specification of an\ninterval between revisits. \n\nProblem to solve: What then is the current scope? That is, the current\nscope might be domain, should then any URIs discovered during revisits\nthat are within the overall scope be accepted or only those within the\ndomains being revisited? The latter is much harder to achieve in a\nconsistent manner I suspect, but it is also more likely to be the\ndesired behavior. \n\n\n\nRevisit frontpage and x hops\n\nThis could be useful for sites that mainly updates to frontmost pages.\nAn example of such a site is a news site (for example the webpage of a\nnewspaper). \n\nA news site have a frontpage with all the latest headlines. When you\nfollow the links you get to the articles, the articles often have links\nto archived articles on the same subject.\n\nThis policy implies that we have a way of marking the seeds which should\nbe handled this way.\n\nA revisiting policy could be:\n\n\n      * Crawl the frontpage on a regular, timed basis.\n      * Follow links x levels deep\n      * Crawl all inline links\n\nOne issue: what is a frontpage? Presumably it would be the seeds, but\ncould it be just some seeds? If not the seeds, then how do we determine\nthis?\n\nInterestingly enough this policy sort of also allows for doing the same\nthing as &quot;revisit everything&quot; if we set the links level high enough.\n\nMaybe when we first start doing revisits under this the new (revisiting)\nURIs could have a very high priority, but as we move deeper into them,\ntheir priority (versus URIs discovered as a result of the initial crawl\nor even previous revisits) drops. This would ensure that the &#39;frontpage&#39;\nis revisited as scheduled, but deeper stuff less so. This would\ndefinitely make this solution more versatile as you could set high link\nlevels without worrying about starving old URIs. The main level of\nadditional complexity would be:\n\n1. &#39;Already included&#39; map fingerprint needs to include what iteration it\nwas from so that we can perform revisits.\n\n2. Need to replace existing queues with priority queues. By specifying a\nfixed number of priorities this might be most easily achieved by having\neach one in fact contain multiple queues. Also a side note on this. For\nthis we may want &#39;priority&#39; to mean more likely to be crawled rather\nthen preferred. So that a higher priority item is more likely to be\nscheduled then a lower priority but not certain to be so. This reduces\nthe likelihood of starvation and limits how many levels of priority we\nneed since it&#39;s all just an approximation anyway.\n\nFinally if we also said that some seeds could be marked as frontpage\n(i.e. revisit) seeds, then this policy in fact solves all three of the\npolicies above!\n\n\r\n--=-QB103uocRlZWfcZ0nNK9\r\nContent-Type: text/html; charset=utf-8\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 TRANSITIONAL//EN&quot;&gt;\n&lt;HTML&gt;\n&lt;HEAD&gt;\n  &lt;META HTTP-EQUIV=&quot;Content-Type&quot; CONTENT=&quot;text/html; CHARSET=UTF-8&quot;&gt;\n  &lt;META NAME=&quot;GENERATOR&quot; CONTENT=&quot;GtkHTML/3.0.9&quot;&gt;\n&lt;/HEAD&gt;\n&lt;BODY&gt;\nWe have begun discussing how the crawler could be able to revisit already crawled URIs. Some initial thoughts have been written on the Wiki &lt;url: &lt;A HREF=&quot;http://crawler.archive.org/cgi-bin/wiki.pl?RevisitingScenarios&quot;&gt;http://crawler.archive.org/cgi-bin/wiki.pl?RevisitingScenarios&lt;/A&gt; &gt; (The whole text of the page as writing this mail is pasted in below). What we need, are possible real-world use-cases for a &quot;periodic revisit&quot; feature. To give some input to the discussion, I also forward a part of a mail that Gordon wrote.&lt;BR&gt;\n&lt;BR&gt;\nPlease feel free to write comments directly into the Wiki-page or comment on this mail.&lt;BR&gt;\n&lt;BR&gt;\n- John Erik&lt;BR&gt;\n&lt;BR&gt;\n--- cut and paste from Gordon&#39;s mail ---&lt;BR&gt;\n&lt;TT&gt;For example, assuming the crawler were not in such active&lt;BR&gt;\ndevelopment, weekly crawls like the UK Gov and LoC Election&lt;BR&gt;\njust recur within a single long-lived -- or occasionally restarted&lt;BR&gt;\ndue to outages/etc -- process?&lt;BR&gt;\n&lt;BR&gt;\nThe &quot;null hypothesis&quot; would be that these needs could be met by&lt;BR&gt;\nsimply launching a new independent crawl, manually or via a cron&lt;BR&gt;\njob, at the desired interval.&lt;BR&gt;\n&lt;BR&gt;\nWhat might be wrong with that approach?&lt;BR&gt;\n&lt;BR&gt;\n&nbsp;&nbsp; - no chance to benefit from recognizing exact duplicates&lt;BR&gt;\n&nbsp;&nbsp;&nbsp;&nbsp; (though this is a larger issue needing other discussion)&lt;BR&gt;\n&nbsp;&nbsp; - crawls could overlap in time; causing problems on the&lt;BR&gt;\n&nbsp;&nbsp;&nbsp;&nbsp; limited hardware resources or violating politeness goals&lt;BR&gt;\n&nbsp;&nbsp; - minimal chance to use other info from past crawls to influence&lt;BR&gt;\n&nbsp;&nbsp;&nbsp;&nbsp; future crawls (though such uses remain theoretical, like&lt;BR&gt;\n&nbsp;&nbsp;&nbsp;&nbsp; prioritizing crawl n+1 based on link structure found in&lt;BR&gt;\n&nbsp;&nbsp;&nbsp;&nbsp; crawl n)&lt;BR&gt;\n&lt;BR&gt;\nI suspect that it is the first item that really motivates a&lt;BR&gt;\nlot of the desire for an integrated/duplicate-aware periodic&lt;BR&gt;\nrecrawl capability. People want to save on storage; they want&lt;BR&gt;\nto have crawl records which give some hint as to rates of&lt;BR&gt;\nresource change; they want to crawl the big relatively-static&lt;BR&gt;\narchives once but then the volatile new material regularly.&lt;/TT&gt;&lt;BR&gt;\n&lt;BR&gt;\n&lt;BR&gt;\n&lt;BR&gt;\n&lt;BR&gt;\n&lt;BR&gt;\n--- cut and paste from Wiki ---\n&lt;H2&gt;Possible scenarios for revisiting&lt;/H2&gt;\nThe current crawler starts off with a list of URIs called seeds. Then it extracts new URIs from these seeds and apply certain rules (the scope) to these URIs and if it passes they are crawled. Every crawled URI is put into a &#39;already crawled&#39; table that makes sure they&#39;re not crawled again if the same URI comes up again. &lt;BR&gt;\n&lt;BR&gt;\nIf you want to crawl the same URI over again to get changed pages, this has to be done by running a new crawl job. We want to look into solutions to alter this behavior by letting the crawler revisit URIs based on different policies. But by doing this several questions comes up.&lt;BR&gt;\n&lt;BR&gt;\nThe first feature that one might want to have is the ability to check if a web-page has changed since it was last crawled and only revisit it if it has. The downside with this is that you have to revisit every page to determine if it was actually changed. You could maybe trust the last changed date in the http header, but it seems that there are many misconfigured servers out there so you are most likely to download the page and compute a checksum for it. The other drawback is that you have to keep a database with all the checksums.&lt;BR&gt;\n&lt;BR&gt;\nEither you want to check for changes or not, it is preferable to not revisit pages unless it is likely that it might have changed. The scenarios mentioned here is examples of how we could create rules for revisiting so that we don&#39;t revisit a page more often than necessary.&lt;BR&gt;\n&lt;BR&gt;\nAnother question is what priority the revisits should have. Should it just be put at the end of the queue of discovered document, or should the revisiting have a higher priority than the documents thats already beeing crawled (i.e. put in the front of the queue). One other possibility is to have more than one queue (with, most likely, different priorities) which intermixes old discovered URIs with revisits. This could be of particular value if the priorities during revisits change as we move furhter from the seed (a little more on this further down).&lt;BR&gt;\n&lt;BR&gt;\nFor all the scenarios I assume that the site has been crawled pretty deep when we first came across it.&lt;BR&gt;\n&lt;BR&gt;\n&lt;B&gt;You are encouraged to add or update possible scenarios as well as the assumptions for revisits mentioned here.&lt;/B&gt; To do so, just follow the link &lt;I&gt;&quot;Edit text of this page&quot;&lt;/I&gt; at the bottom of the page.&lt;BR&gt;\n&lt;BR&gt;\n&lt;BR&gt;\n&lt;H2&gt;Revisit everything&lt;/H2&gt;\nAfter crawling for a while, readd all the URIs in the seed list into the crawler. The main difference between this and starting a new job is that the crawler will continue working on the discovered URIs found in the first pass along with the revisits. &lt;BR&gt;\n&lt;BR&gt;\nThis is pretty simple to implement. The main thing to consider is how to mark the already included as eligible for revisiting without allowing an arbitrary amount of revisits to occur.&lt;BR&gt;\n&lt;BR&gt;\n&lt;BR&gt;\n&lt;H2&gt;Revisit marked seeds&lt;/H2&gt;\nThis is a variant of the above in that it only revisits seeds that is marked for revisits. The mark could possibly have a specification of an interval between revisits. &lt;BR&gt;\n&lt;BR&gt;\nProblem to solve: What then is the current scope? That is, the current scope might be domain, should then any URIs discovered during revisits that are within the overall scope be accepted or only those within the domains being revisited? The latter is much harder to achieve in a consistent manner I suspect, but it is also more likely to be the desired behavior. &lt;BR&gt;\n&lt;BR&gt;\n&lt;BR&gt;\n&lt;H2&gt;Revisit frontpage and x hops&lt;/H2&gt;\nThis could be useful for sites that mainly updates to frontmost pages. An example of such a site is a news site (for example the webpage of a newspaper). &lt;BR&gt;\n&lt;BR&gt;\nA news site have a frontpage with all the latest headlines. When you follow the links you get to the articles, the articles often have links to archived articles on the same subject.&lt;BR&gt;\n&lt;BR&gt;\nThis policy implies that we have a way of marking the seeds which should be handled this way.&lt;BR&gt;\n&lt;BR&gt;\nA revisiting policy could be:&lt;BR&gt;\n&lt;BR&gt;\n&lt;UL&gt;\n    &lt;LI&gt;Crawl the frontpage on a regular, timed basis.\n    &lt;LI&gt;Follow links x levels deep\n    &lt;LI&gt;Crawl all inline links\n&lt;/UL&gt;\nOne issue: what is a frontpage? Presumably it would be the seeds, but could it be just some seeds? If not the seeds, then how do we determine this?&lt;BR&gt;\n&lt;BR&gt;\nInterestingly enough this policy sort of also allows for doing the same thing as &quot;revisit everything&quot; if we set the links level high enough.&lt;BR&gt;\n&lt;BR&gt;\nMaybe when we first start doing revisits under this the new (revisiting) URIs could have a very high priority, but as we move deeper into them, their priority (versus URIs discovered as a result of the initial crawl or even previous revisits) drops. This would ensure that the &#39;frontpage&#39; is revisited as scheduled, but deeper stuff less so. This would definitely make this solution more versatile as you could set high link levels without worrying about starving old URIs. The main level of additional complexity would be:&lt;BR&gt;\n&lt;BR&gt;\n1. &#39;Already included&#39; map fingerprint needs to include what iteration it was from so that we can perform revisits.&lt;BR&gt;\n&lt;BR&gt;\n2. Need to replace existing queues with priority queues. By specifying a fixed number of priorities this might be most easily achieved by having each one in fact contain multiple queues. Also a side note on this. For this we may want &#39;priority&#39; to mean more likely to be crawled rather then preferred. So that a higher priority item is more likely to be scheduled then a lower priority but not certain to be so. This reduces the likelihood of starvation and limits how many levels of priority we need since it&#39;s all just an approximation anyway.&lt;BR&gt;\n&lt;BR&gt;\nFinally if we also said that some seeds could be marked as frontpage (i.e. revisit) seeds, then this policy in fact solves all three of the policies above!&lt;BR&gt;\n&lt;BR&gt;\n&lt;/BODY&gt;\n&lt;/HTML&gt;\n\r\n--=-QB103uocRlZWfcZ0nNK9--\r\n\n"}}