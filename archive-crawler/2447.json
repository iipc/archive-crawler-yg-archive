{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"Tr1Ew9p4sJ2Bc_wMkkt607A4ynnhq7lDXOohUhf8eJwL0N69QiXnQEwlu1lOQZURyZwjG7cuvCyOnNmldAHiRTV10n9jD-K7sooWvq7E","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Large crawl experience (like, 500M links)","postDate":"1135034710","msgId":2447,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRvN2ZnbStxbWRAZUdyb3Vwcy5jb20+","inReplyToHeader":"PDQzOTg5RUMzLjEwNjA2MDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":2405,"nextInTopic":2450,"prevInTime":2446,"nextInTime":2448,"topicId":2391,"numMessagesInTopic":12,"msgSnippet":"I did a proof crawling using BroadScope and 22K seeds. I got OOME within a day. I then checkpoint it, restart the crawler, start another crawl from the","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 98051 invoked from network); 19 Dec 2005 23:27:26 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m25.grp.scd.yahoo.com with QMQP; 19 Dec 2005 23:27:26 -0000\r\nReceived: from unknown (HELO n9a.bullet.scd.yahoo.com) (66.94.237.43)\n  by mta3.grp.scd.yahoo.com with SMTP; 19 Dec 2005 23:27:26 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.6] by n9.bullet.scd.yahoo.com with NNFMP; 19 Dec 2005 23:25:11 -0000\r\nReceived: from [66.218.66.68] by mailer6.bullet.scd.yahoo.com with NNFMP; 19 Dec 2005 23:25:11 -0000\r\nDate: Mon, 19 Dec 2005 23:25:10 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;do7fgm+qmd@...&gt;\r\nIn-Reply-To: &lt;43989EC3.1060602@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: Large crawl experience (like, 500M links)\r\nX-Yahoo-Group-Post: member; u=132996324; y=fxTq6E6RqYk38EomEaCZ7-n2yQQZCpiz2KvzN_odETsrV_k__g\r\nX-Yahoo-Profile: joehung302\r\n\r\nI did a proof crawling using BroadScope and 22K seeds. I got OOME \nwithin a=\r\n day. I then checkpoint it, restart the crawler, start \nanother crawl from =\r\nthe checkpoint, OOME within a day.\n\nI then changed to use 5K seeds and Broa=\r\ndScope, OOME within a day. \nRestart with the checkpoint and still OOME with=\r\nin a day.\n\nI then run 5K seeds with DomainScope (kind of given up on \nbroad=\r\nscope). OOME within a day.\n\nI have my JVM set to -Xmx1500m. BTW, I&#39;m using =\r\n64 bit JDK1.5.\n\nOne thing that I observed is, broad scope runs much faster =\r\nthan \ndomain scope under roughly the same condition. In both broadscope \nru=\r\nns I was able to top 1000KB/s bandwidth limit with around 50% cpu \nusage. I=\r\nn the domain scope run I can only get to 500KB/s throughput \nwith 100% cpu =\r\nbusy. \n\nI used to be able to run 1.0.4 for a week with &lt;1K seeds and get \na=\r\nround 1M links per day. I thought the bdb improvement should be \nable to ta=\r\nke more seeds and run longer. I really want the crawler to \nrun with a big =\r\nseed list because we&#39;re going to seed my big crawl \nwith links from ODP. \n\n=\r\nAny suggestions that I can try?\n\ncheers,\n-joe\n\n--- In archive-crawler@yahoo=\r\ngroups.com, stack &lt;stack@a...&gt; wrote:\n&gt;\n&gt; joehung302 wrote:\n&gt; \n&gt; &gt;\n&gt; &gt; &gt; Us=\r\ne the bloom filter option for the already-seen in \nBdbFrontier. \n&gt; &gt; Seems\n=\r\n&gt; &gt; &gt; to work better when a machine goes above 30-50million.  Bloom\n&gt; &gt; bec=\r\nomes\n&gt; &gt; &gt; saturated at 125million so thats about the upperbound per \nmachi=\r\nne at\n&gt; &gt; the\n&gt; &gt; &gt; moment unless you up the bloom filter size  (but its al=\r\nready \nbig and\n&gt; &gt; &gt; you&#39;ll start eating into heap the crawler is using goi=\r\nng about \nits\n&gt; &gt; other\n&gt; &gt; &gt; business).  Thereafter the rate of false posi=\r\ntives -- reports \nthat\n&gt; &gt; we&#39;ve\n&gt; &gt; &gt; seen an URL when in fact we haven&#39;t =\r\n-- starts to increase \n(Read the\n&gt; &gt; &gt; BloomFilter javadoc for more on its =\r\nworkings).\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt; How confident do you guys feel that if I use broad=\r\n-scope I can go\n&gt; &gt; above 50M links (or even 100M links) without OOME on a =\r\nsingle \nmachine?\n&gt; \n&gt; \n&gt; I&#39;d suggest you startup a proofing test crawl with=\r\n BroadScope and \nsee it \n&gt; does.\n&gt; \n&gt; On machines with specs like those lis=\r\nted below we&#39;ve pulled down \n&gt;  &gt;50Million documents per instance with &gt;125=\r\nmillion discovered.  \nScope \n&gt; was not BroadScope.  Once or twice we OOME&#39;d=\r\n but thought is that \n&gt; probable cause has been addressed in 1.6 release (I=\r\nf there is an \nOOME, \n&gt; you can checkpoint, restart and recover the crawl. =\r\n Often it will \n&gt; continue the crawl as it avoids an exact replay of the \nc=\r\nircumstances \n&gt; that brought on the OOME).\n&gt; \n&gt; One thing I forgot to add t=\r\no yesterday&#39;s list is regular \ncheckpointing \n&gt; -- every 4 hours or so.\n&gt; \n=\r\n&gt; St.Ack\n&gt; \n&gt; \n&gt; -bash-3.00$ uname -a\n&gt; Linux crawling015.archive.org 2.6.1=\r\n1-1.27_FC3smp #1 SMP Tue May 17 \n&gt; 20:43:11 EDT 2005 i686 athlon i386 GNU/L=\r\ninux\n&gt; \n&gt; -bash-3.00$ more /etc/issue\n&gt; Fedora Core release 3 (Heidelberg)\n=\r\n&gt; Kernel &#92;r on an &#92;m\n&gt; \n&gt; Dual AMD Opteron(tm) Processor 246  w/ cpu MHz   =\r\n      : 2009.374 \nand \n&gt; cache size      : 1024 KB\n&gt; \n&gt; [crawling013 5] ~ &gt;=\r\n /lib/libc.so.6\n&gt; GNU C Library stable release version 2.3.4 (20050218), by=\r\n Roland \nMcGrath \n&gt; et al.\n&gt; Copyright (C) 2005 Free Software Foundation, I=\r\nnc.\n&gt; This is free software; see the source for copying conditions.\n&gt; There=\r\n is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A\n&gt; PARTICULAR=\r\n PURPOSE.\n&gt; Configured for i586-suse-linux.\n&gt; Compiled by GNU CC version 3.=\r\n3.5 20050117 (prerelease) (SUSE \nLinux).\n&gt; Compiled on a Linux 2.6.9 system=\r\n on 2005-06-10.\n&gt; Available extensions:\n&gt;       GNU libio by Per Bothner\n&gt; =\r\n      crypt add-on version 2.1 by Michael Glad and others\n&gt;       linuxthre=\r\nads-0.10 by Xavier Leroy\n&gt;       GNU Libidn by Simon Josefsson\n&gt;       NoVe=\r\nrsion patch for broken glibc 2.0 binaries\n&gt;       BIND-8.2.3-T5B\n&gt;       li=\r\nbthread_db work sponsored by Alpha Processor Inc\n&gt;       NIS(YP)/NIS+ NSS m=\r\nodules 0.19 by Thorsten Kukuk\n&gt; Thread-local storage support included.\n&gt; Fo=\r\nr bug reporting instructions, please see:\n&gt; &lt;http://www.gnu.org/software/li=\r\nbc/bugs.html&gt;.\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; We used sun 1.5.0:\n&gt; \n&gt; -bash-3.00$ /usr/local=\r\n/jdk1.5.0_03/bin/java -version\n&gt; java version &quot;1.5.0_03&quot;\n&gt; Java(TM) 2 Runti=\r\nme Environment, Standard Edition (build 1.5.0_03-\nb07)\n&gt; Java HotSpot(TM) S=\r\nerver VM (build 1.5.0_03-b07, mixed mode)\n&gt; \n&gt; \n&gt; \n&gt; &gt; That to me that seem=\r\ns to be the deciding factor on whether we \nshould\n&gt; &gt; start with 5 beefy ma=\r\nchines and hope each one can go up to 100M \nlinks,\n&gt; &gt; or with 10 less beef=\r\ny machines and each one can go up to 50M \nlinks\n&gt; &gt; without OOME.\n&gt; &gt;\n&gt; &gt; I=\r\n know I&#39;m shooting darts in the dark now...I have to start the\n&gt; &gt; project =\r\nplanning soon so I&#39;d like to take my best guess with all \nthe\n&gt; &gt; advices I=\r\n can get.\n&gt; &gt;\n&gt; &gt; cheers,\n&gt; &gt; -joe\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ----------------=\r\n-------------------------------------------------\n-------\n&gt; &gt; YAHOO! GROUPS=\r\n LINKS\n&gt; &gt;\n&gt; &gt;     *  Visit your group &quot;archive-crawler\n&gt; &gt;       &lt;http://g=\r\nroups.yahoo.com/group/archive-crawler&gt;&quot; on the \nweb.\n&gt; &gt;        \n&gt; &gt;     * =\r\n To unsubscribe from this group, send an email to:\n&gt; &gt;        archive-crawl=\r\ner-unsubscribe@yahoogroups.com\n&gt; &gt;       &lt;mailto:archive-crawler-unsubscrib=\r\ne@yahoogroups.com?\nsubject=3DUnsubscribe&gt;\n&gt; &gt;        \n&gt; &gt;     *  Your use o=\r\nf Yahoo! Groups is subject to the Yahoo! Terms \nof\n&gt; &gt;       Service &lt;http:=\r\n//docs.yahoo.com/info/terms/&gt;.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --------------------------------=\r\n---------------------------------\n-------\n&gt; &gt;\n&gt;\n\n\n\n\n\n"}}