{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225011788,"authorName":"Karl Wright","from":"Karl Wright &lt;kwright@...&gt;","profile":"daddywri","replyTo":"LIST","senderId":"ddqq02CGs7zHnNDiuyWFVCZERln9_XFpjuhf9vI5bwiLhxNleijuhRsfKufn2H7AI8v4hJ_d3ltIiKbjMbSHuBOWj_3SBDm8oys","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Politeness proposal, and CrawlHost class","postDate":"1138055188","msgId":2582,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzRDU1ODE0LjEwNzA1MDJAbWV0YWNhcnRhLmNvbT4=","inReplyToHeader":"PDQzRDU0QTVFLjQwNDA5MDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PDQzRDEzRDU5LjgwOTA3MDBAbWV0YWNhcnRhLmNvbT4gPDQzRDUyRTdCLjIwOTA1MDVAYXJjaGl2ZS5vcmc+IDw0M0Q1MzY4OC40MDUwMDA1QG1ldGFjYXJ0YS5jb20+IDw0M0Q1NEE1RS40MDQwOTA3QGFyY2hpdmUub3JnPg=="},"prevInTopic":2581,"nextInTopic":0,"prevInTime":2581,"nextInTime":2583,"topicId":2574,"numMessagesInTopic":5,"msgSnippet":"... Yes, and we Strongly Agree.  However - this is an unusual situation in a number of respects.  First, this site has a robots.txt, indicating that crawling","rawEmail":"Return-Path: &lt;kwright@...&gt;\r\nX-Sender: kwright@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 24723 invoked from network); 23 Jan 2006 22:25:40 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m35.grp.scd.yahoo.com with QMQP; 23 Jan 2006 22:25:40 -0000\r\nReceived: from unknown (HELO metacarta.com) (65.77.47.18)\n  by mta2.grp.scd.yahoo.com with SMTP; 23 Jan 2006 22:25:39 -0000\r\nReceived: from localhost (silene.metacarta.com [65.77.47.24])\n\tby metacarta.com (Postfix) with ESMTP id 68F96518203\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 23 Jan 2006 17:25:14 -0500 (EST)\r\nReceived: from metacarta.com ([65.77.47.18])\n\tby localhost (silene.metacarta.com [65.77.47.24]) (amavisd-new, port 10024)\n\twith ESMTP id 25037-04 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 23 Jan 2006 17:25:11 -0500 (EST)\r\nReceived: from [192.168.1.104] (146-115-66-62.c3-0.lex-ubr1.sbo-lex.ma.cable.rcn.com [146.115.66.62])\n\tby metacarta.com (Postfix) with ESMTP id ECB29518091\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 23 Jan 2006 17:25:10 -0500 (EST)\r\nMessage-ID: &lt;43D55814.1070502@...&gt;\r\nDate: Mon, 23 Jan 2006 17:26:28 -0500\r\nUser-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;43D13D59.8090700@...&gt; &lt;43D52E7B.2090505@...&gt; &lt;43D53688.4050005@...&gt; &lt;43D54A5E.4040907@...&gt;\r\nIn-Reply-To: &lt;43D54A5E.4040907@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: by amavisd-new-20030616-p10 (Debian) at metacarta.com\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Karl Wright &lt;kwright@...&gt;\r\nSubject: Re: [archive-crawler] Politeness proposal, and CrawlHost class\r\nX-Yahoo-Group-Post: member; u=225011788; y=Ddna5RPUCsQk4_gj5nHhH64G_kPQuX1a7m_T42o-bZjAokI\r\nX-Yahoo-Profile: daddywri\r\n\r\nGordon Mohr (archive.org) wrote:\n&gt; Karl Wright wrote:\n&gt; \n&gt;&gt;&gt;(I presume that it&#39;s generally better to space requests out evenly, rather\n&gt;&gt;&gt;than pulse on and off. That is, if you&#39;re going to hit a site 4K times\n&gt;&gt;&gt;over a four-day period, it&#39;s be better to do 1K a day, roughly 40 every\n&gt;&gt;&gt;hour, than to do 2K every other day, and 0 on the off days.)\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;Actually, that&#39;s the assumption that seems to be false.\n&gt;&gt;We&#39;ve had complaints from webmasters who were being crawled slowly and \n&gt;&gt;steadily over a 4 day period.  Although the rate of crawl did not exceed \n&gt;&gt;one document per 38 seconds, and the maximum bandwidth used did not \n&gt;&gt;exceed 133 bytes per second, we still got complained at, because we did \n&gt;&gt;not LOOK like a user - we looked like a crawler.  We concluded that we \n&gt;&gt;needed to look more &quot;bursty&quot;, with a random amount between bursts, to \n&gt;&gt;seem like a real user.\n&gt; \n&gt; \n&gt; Interesting. If the sites don&#39;t want to be crawled at all, and you&#39;re\n&gt; trying to look like a human browser, there are lots of other issues you&#39;d\n&gt; have to address, too -- like User-Agent and total traffic coming from a\n&gt; single IP address (not to mention issues of legal authority to collect\n&gt; without implied target site permission). We at IA Strongly Discourage\n&gt; crawling against a site owner&#39;s wishes.\n&gt; \n\nYes, and we Strongly Agree.  However - this is an unusual situation in a \nnumber of respects.  First, this site has a robots.txt, indicating that \ncrawling is allowed.  Second, the site in question is basically a \nlibrary index, and we only appeared at all on their radar because we \ncrawled slowly, consistently, and predictably, for 4 solid days.  It&#39;s \nhard to see what the harm is, but it&#39;s also true that there is no \nimmediate benefit to the site owner (which is a governmental \norganization) to allow the site to be crawled.  We spent a considerable \namount of time trying to figure out why we&#39;d gotten dinged, and actually \ntalked to the site owner, who said &quot;I saw this consistent access \npattern, which is something I didn&#39;t like&quot;.  Nothing more than that.\n\n&gt; If random pauses are preferred by target sites, it seems that the\n&gt; politeness delays could be drawn from a random distribution around the\n&gt; desired average delays for the intended effect.\n&gt; \n\nHmm - perhaps two independent access decay patterns would do the trick. \n  One short term (emulating an active user), and another long term \n(emulating a user who has stepped away for a while).\n\n&gt; \n&gt;&gt;&gt;The key method for including alternate information about politeness\n&gt;&gt;&gt;delays is AbstractFrontier.politenessDelayFor(). Considering your\n&gt;&gt;&gt;need, this method could be changed to look inside the CrawlURI to\n&gt;&gt;&gt;see if an earlier step/Processor had already calculated a delay, and\n&gt;&gt;&gt;use that rather than the default calculation. That would make it\n&gt;&gt;&gt;easier for custom delays to be effected without editting/overriding\n&gt;&gt;&gt;the core Frontier code. If you need this change, let us know and I&#39;ll\n&gt;&gt;&gt;look into the possibility further.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;I&#39;d need to figure out the delay based on some time window into the past \n&gt;&gt;(e.g. 2 days, settable), and the number of fetches over that time \n&gt;&gt;period.  I don&#39;t know of any current data structure in Heritrix where \n&gt;&gt;this kind of information can be tracked or attached - any ideas?\n&gt; \n&gt; \n&gt; Having thought about this in similar contexts in the past, I believe\n&gt; keeping an accurate sliding window of past traffic can be complicated\n&gt; and require a largish memory footprint.\n&gt; \n\nYup.\n\n&gt; Some sort of decaying average would be one alternative.\n&gt; \n&gt; But, I think you could effectively get the desired result with a\n&gt; probabilistic approach as mentioned above. The average delay would\n&gt; ensure the overall rate is correct over the long, but the variance in\n&gt; delays would occasionally give pauses of many hours or even days.\n&gt; \n&gt; Something like a 1/(2^n) chance of multiplying the standard delay by\n&gt; n might do the trick. If the math is right, including any clipping of\n&gt; extreme values you apply, keeping any record of the overall visit rate\n&gt; could be rendered superfluous -- and only interesting as a fallback\n&gt; double-check.\n&gt;\n\nSo it becomes a Poisson distribution.  I&#39;ll think about that.  Thanks \nfor the advice.\n\nKarl\n\n\n&gt; - Gordon @ IA\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}