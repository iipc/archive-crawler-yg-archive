{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"KKC6MGsYuDbo4QQDyBQFZxMBcGZiFbfV-SBLztPPp7f7bVkaSgZsdSs3G7sM1AoHXyDeBaYN97jtI6BxtuvWVZV3U-QaXp_A","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Crawl top level domains only","postDate":"1178973869","msgId":4239,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2NDVCNkFELjgwMzAyMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGYyNGE4aCtmaXIyQGVHcm91cHMuY29tPg==","referencesHeader":"PGYyNGE4aCtmaXIyQGVHcm91cHMuY29tPg=="},"prevInTopic":4238,"nextInTopic":4253,"prevInTime":4238,"nextInTime":4240,"topicId":4238,"numMessagesInTopic":4,"msgSnippet":"You can set Archiver decide-rules to reject storing of all unwanted URIs. For example: If you want to save only slash pages of second level domains of the com","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 47480 invoked from network); 12 May 2007 12:45:24 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m42.grp.scd.yahoo.com with QMQP; 12 May 2007 12:45:24 -0000\r\nReceived: from unknown (HELO smtp5-g19.free.fr) (212.27.42.35)\n  by mta5.grp.scd.yahoo.com with SMTP; 12 May 2007 12:45:24 -0000\r\nReceived: from [127.0.0.1] (nor75-24-88-170-99-175.fbx.proxad.net [88.170.99.175])\n\tby smtp5-g19.free.fr (Postfix) with ESMTP id EDCF443F4E\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Sat, 12 May 2007 14:45:01 +0200 (CEST)\r\nMessage-ID: &lt;4645B6AD.8030201@...&gt;\r\nDate: Sat, 12 May 2007 05:44:29 -0700\r\nUser-Agent: Thunderbird 1.5.0.10 (Windows/20070221)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f24a8h+fir2@...&gt;\r\nIn-Reply-To: &lt;f24a8h+fir2@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Crawl top level domains only\r\nX-Yahoo-Group-Post: member; u=137477665; y=CJmHkW7MUrf19GJXwqgBwy2kLro3sr7iZtc0y5FIBWmB8yKrXw\r\nX-Yahoo-Profile: iranitovic\r\n\r\nYou can set Archiver decide-rules to reject storing of all unwanted URIs.\n\nFor example: If you want to save only slash pages of second level \ndomains of the com domain, you can add\nREJECT NoMatchesRegExpDecideRule ^https?://(www&#92;.)?[^&#92;.]+&#92;.com/$\n\nThis rule will save the following URIs (I did not test the regex):\n\nhttp://a.com/\nhttp://www.a.com/\nhttps://www.a.com/\n.\n.\n\nbut not\n\nhttp://a.com/index.html\nhttp://a.com:8080/\nhttp://a.gov/\n.\n.\n\n\nThis is just a trivial example but decide-rules should allow you to \ntailor more complicated rules that will deal with index pages, \nredirects, frames and etc.\n\nI hope this helps.\ni.\n\n\nvpdn81 wrote:\n&gt; Hi,\n&gt; I was looking for a way to only crawl top-level domains i.e. using\n&gt; subdomains and subfolders only to search for more links, but purging\n&gt; them after a temporary storage again. Since I only need the toplevel\n&gt; domains, the goal is to save storage and speed up the crawling process.\n&gt; \n&gt; Does anyone know how to do this using heritrix? A hint where I should\n&gt; look at is also greatly appreciated!\n&gt; \n&gt; Cheers,\n&gt; Frank\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n\n"}}