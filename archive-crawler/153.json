{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"XwrTPbRQoFx6CMW4K6sk2qU4w_o8Oz2NVX2F71bxxA5WWuVfor243IYZ3wQYwNfmwxKTp4JcdEtwqsbEaJR7pgG1IpnFImo","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Impressions sought: crawler configuration file","postDate":"1065641345","msgId":153,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNGODQ2NTgxLjQwNTA5MDBAYXJjaGl2ZS5vcmc+"},"prevInTopic":0,"nextInTopic":154,"prevInTime":152,"nextInTime":154,"topicId":153,"numMessagesInTopic":3,"msgSnippet":"Appended below is an example crawler configuration ( crawl-order ) file, as currently drives my working-dev-branch crawler in desktop test crawls. I would","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 34286 invoked from network); 8 Oct 2003 19:29:22 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m18.grp.scd.yahoo.com with QMQP; 8 Oct 2003 19:29:22 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 8 Oct 2003 19:29:22 -0000\r\nReceived: (qmail 19974 invoked by uid 100); 8 Oct 2003 19:24:04 -0000\r\nReceived: from b116-dyn-72.archive.org (HELO archive.org) (gojomo@...@209.237.240.72)\n  by mail-dev.archive.org with SMTP; 8 Oct 2003 19:24:04 -0000\r\nMessage-ID: &lt;3F846581.4050900@...&gt;\r\nDate: Wed, 08 Oct 2003 12:29:05 -0700\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.5b) Gecko/20030901 Thunderbird/0.2\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nSubject: Impressions sought: crawler configuration file\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-4.2 required=6.0\n\ttests=AWL,BAYES_01,HTML_10_20,USER_AGENT_MOZILLA_UA\n\tversion=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nAppended below is an example crawler configuration (&quot;crawl-order&quot;)\nfile, as currently drives my working-dev-branch crawler in desktop\ntest crawls.\n\nI would appreciate if people were to look it over and -- based solely\non its hoped-for &quot;self-documenting&quot; qualities -- give their impressions\nabout the suitability of this approach.\n\nAll comments/questions appreciated.\n\nThanks,\n\n- Gordon\n\n========================================================================\n\n&lt;crawl-order\n   name=&quot;dev-crawl&quot;\n   comment=&quot;A simple crawl for development/example purposes.&quot;\n  &gt;\n\n  &lt;scope\n    class=&quot;org.archive.crawler.basic.Scope&quot;\n    mode=&quot;broad&quot;\n    max-link-hops=&quot;10&quot;\n    max-trans-hops=&quot;3&quot;&gt;\n\n   &lt;seeds src=&quot;seeds.txt&quot;&gt;\n  #  http://www.archive.org/movies/movies.php\n  #  http://www.drudgereport.com\n     http://www.yahoo.com\n     http://www.google.com/dirhp\n   &lt;/seeds&gt;\n\n  &lt;/scope&gt;\n\n  &lt;behavior\n    disk-path=&quot;disk&quot;\n    max-toe-threads=&quot;50&quot;\n    &gt;\n\n   &lt;http-headers\n     User-Agent=&quot;heritrix-aoc/alpha (+http://crawler.archive.org)&quot;\n     From=&quot;archive-crawler-agent@...&quot;\n    /&gt;\n\n   &lt;frontier\n     class=&quot;org.archive.crawler.basic.Frontier&quot;\n     delay-factor=&quot;0&quot;\n     minimum-delay=&quot;0&quot;\n     maximum=delay=&quot;1000&quot;\n    /&gt;\n\n    &lt;processors&gt;\n\n     &lt;processor\n       name=&quot;Preselector&quot;\n       class=&quot;org.archive.crawler.basic.Preselector&quot;\n       next=&quot;Preprocessor&quot;\n\n       scope=&quot;yes&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;Preprocessor&quot;\n       class=&quot;org.archive.crawler.basic.PreconditionEnforcer&quot;\n       next=&quot;DNS&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;DNS&quot;\n       class=&quot;org.archive.crawler.fetcher.FetcherDNS&quot;\n       next=&quot;HTTP&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;HTTP&quot;\n       class=&quot;org.archive.crawler.fetcher.FetcherHTTPSimple&quot;\n       next=&quot;ExtractorHTTP&quot;\n\n       timeout-seconds=&quot;10&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;ExtractorHTTP&quot;\n       class=&quot;org.archive.crawler.extractor.ExtractorHTTP&quot;\n       next=&quot;ExtractorHTML&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;ExtractorHTML&quot;\n       class=&quot;org.archive.crawler.extractor.ExtractorHTML&quot;\n       next=&quot;ExtractorDOC&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;ExtractorDOC&quot;\n   \t  class=&quot;org.archive.crawler.extractor.ExtractorDOC&quot;\n   \t  next=&quot;ExtractorSWF&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;ExtractorSWF&quot;\n       class=&quot;org.archive.crawler.extractor.ExtractorSWF&quot;\n       next=&quot;ExtractorPDF&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;ExtractorPDF&quot;\n       class=&quot;org.archive.crawler.extractor.ExtractorPDF&quot;\n       next=&quot;Archiver&quot;\n      /&gt;\n\n     &lt;processor\n       name=&quot;Archiver&quot;\n       class=&quot;org.archive.crawler.basic.ARCWriter&quot;\n       next=&quot;Updater&quot;\n\n       path=&quot;arcs&quot;\n       compress=&quot;yes&quot;\n       max-arc-size=&quot;20000000&quot;&gt;\n\t   &lt;!--\n\t   &lt;filter\n\t     name=&quot;http-only&quot;\n          class=&quot;org.archive.crawler.filter.URIRegExpFilter&quot;\n          regexp=&quot;^http://.*&quot;\n\t   /&gt;\n\t   --&gt;\n      &lt;/processor&gt;\n\n      &lt;processor\n        name=&quot;Updater&quot;\n        class=&quot;org.archive.crawler.basic.CrawlStateUpdater&quot;\n        postprocessor=&quot;defined&quot;\n        next=&quot;Postselector&quot;\n       /&gt;\n\n     &lt;processor\n       name=&quot;Postselector&quot;\n       class=&quot;org.archive.crawler.basic.Postselector&quot;\n      /&gt;\n\n   &lt;/processors&gt;\n\n   &lt;loggers&gt;\n    &lt;crawl-statistics\n      interval=&quot;10&quot;\n     /&gt;\n   &lt;/loggers&gt;\n\n  &lt;/behavior&gt;\n\n&lt;/crawl-order&gt;\n\n\n"}}