{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":308001930,"authorName":"Cetin Sert","from":"&quot;Cetin Sert&quot; &lt;cetin.sert@...&gt;","profile":"cetinsert","replyTo":"LIST","senderId":"O51FaqL4POZh0KOGz_EPKHg697cRaQR6Dijjobbe3PrI-2VDWQTTmWpG9Az6ug7DjvFUvOSTbNtfJKgUIBPsmhkIMA6021xdZWU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: New duplication-reduction features in 1.12 and later versions","postDate":"1178277569","msgId":4218,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxZjRzMStyZ2llQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYwbjF1bCt1a2s1QGVHcm91cHMuY29tPg=="},"prevInTopic":4190,"nextInTopic":0,"prevInTime":4217,"nextInTime":4219,"topicId":4166,"numMessagesInTopic":4,"msgSnippet":"Could you just attach a working order.xml in your reply? Although your explanations are really helpful, I seem to manage to run into some exceptions like the","rawEmail":"Return-Path: &lt;cetin.sert@...&gt;\r\nX-Sender: cetin.sert@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 75726 invoked from network); 4 May 2007 11:21:56 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m43.grp.scd.yahoo.com with QMQP; 4 May 2007 11:21:56 -0000\r\nReceived: from unknown (HELO n13a.bullet.sp1.yahoo.com) (69.147.64.112)\n  by mta10.grp.scd.yahoo.com with SMTP; 4 May 2007 11:21:56 -0000\r\nReceived: from [216.252.122.219] by n13.bullet.sp1.yahoo.com with NNFMP; 04 May 2007 11:19:30 -0000\r\nReceived: from [66.218.69.4] by t4.bullet.sp1.yahoo.com with NNFMP; 04 May 2007 11:19:30 -0000\r\nReceived: from [66.218.66.73] by t4.bullet.scd.yahoo.com with NNFMP; 04 May 2007 11:19:30 -0000\r\nDate: Fri, 04 May 2007 11:19:29 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;f1f4s1+rgie@...&gt;\r\nIn-Reply-To: &lt;f0n1ul+ukk5@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;1-2223917424-2156564459=:4&quot;\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Cetin Sert&quot; &lt;cetin.sert@...&gt;\r\nSubject: Re: New duplication-reduction features in 1.12 and later versions\r\nX-Yahoo-Group-Post: member; u=308001930; y=WINp029GvmoHb7yfna4mQqxv0nL8IEkeKUYxeAB0UJXHg7Up\r\nX-Yahoo-Profile: cetinsert\r\n\r\n\r\n--1-2223917424-2156564459=:4\r\nContent-Type: text/plain; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nCould you just attach a working order.xml in your reply?\n\nAlthough your exp=\r\nlanations are really helpful, I seem to manage to run\ninto some exceptions =\r\nlike the below (1st and 2nd crawls run ok, 3rd one\ncrawl throws this except=\r\nion)\n\nTime:                                            Mai. 4, 2007 10:55:1=\r\n1\nGMT                                                             Level:\nSE=\r\nVERE                                                            \nMessage:  =\r\n                                          On crawl:\ndefault-mirror-writer-3=\r\n Unable to setup crawl modules\nException:                                  =\r\n         \norg.archive.crawler.framework.exceptions.FatalConfigurationExcept=\r\nion:\n(JE 3.2.23) Lock expired. Locker -1_StartNextJob_BasicLocker: waited f=\r\nor\nlock on database=3D_jeNameMap node=3D44 type=3DWRITE grant=3DWAIT_PROMOT=\r\nION\ntimeoutMillis=3D5000 startTime=3D1178276106686 endTime=3D1178276111695\n=\r\nOwners: [, ]\nWaiters: []\nTransaction -1_StartNextJob_BasicLocker owns 44\nTr=\r\nansaction -1_StartNextJob_BasicLocker waits for  node 44\n\nCause: java.io.IO=\r\nException: (JE 3.2.23) Lock expired. Locker\n-1_StartNextJob_BasicLocker: wa=\r\nited for lock on database=3D_jeNameMap\nnode=3D44 type=3DWRITE grant=3DWAIT_=\r\nPROMOTION timeoutMillis=3D5000\nstartTime=3D1178276106686 endTime=3D11782761=\r\n11695\nOwners: [, ]\nWaiters: []\nTransaction -1_StartNextJob_BasicLocker owns=\r\n 44\nTransaction -1_StartNextJob_BasicLocker waits for  node 44\n\nat org.arch=\r\nive.crawler.util.BdbUriUniqFilter.(BdbUriUniqFilter.java:102)\nat\norg.archiv=\r\ne.crawler.frontier.BdbFrontier.createAlreadyIncluded(BdbFronti&#92;\ner.java:154=\r\n)\nat\norg.archive.crawler.frontier.WorkQueueFrontier.initialize(WorkQueueFro=\r\nnt&#92;\nier.java:320)\nat\norg.archive.crawler.frontier.BdbFrontier.initialize(Bd=\r\nbFrontier.java:282&#92;\n)\nat\norg.archive.crawler.framework.CrawlController.setu=\r\npCrawlModules(CrawlCon&#92;\ntroller.java:656)\nat\norg.archive.crawler.framework.=\r\nCrawlController.initialize(CrawlController&#92;\n.java:377)\nat\norg.archive.crawl=\r\ner.admin.CrawlJob.setupForCrawlStart(CrawlJob.java:846)\nat\norg.archive.craw=\r\nler.admin.CrawlJobHandler.startNextJobInternal(CrawlJobH&#92;\nandler.java:1142)=\r\n\nat\norg.archive.crawler.admin.CrawlJobHandler$3.run(CrawlJobHandler.java:11=\r\n2&#92;\n5)\nat java.lang.Thread.run(Thread.java:619)\n\nStacktrace:\norg.archive.cra=\r\nwler.framework.exceptions.FatalConfigurationException:\n(JE 3.2.23) Lock exp=\r\nired. Locker -1_StartNextJob_BasicLocker: waited for\nlock on database=3D_je=\r\nNameMap node=3D44 type=3DWRITE grant=3DWAIT_PROMOTION\ntimeoutMillis=3D5000 =\r\nstartTime=3D1178276106686 endTime=3D1178276111695\nOwners: [, ]\nWaiters: []\n=\r\nTransaction -1_StartNextJob_BasicLocker owns 44\nTransaction -1_StartNextJob=\r\n_BasicLocker waits for  node 44\n\nat\norg.archive.crawler.frontier.WorkQueueF=\r\nrontier.initialize(WorkQueueFront&#92;\nier.java:324)\nat\norg.archive.crawler.fro=\r\nntier.BdbFrontier.initialize(BdbFrontier.java:282&#92;\n)\nat\norg.archive.crawler=\r\n.framework.CrawlController.setupCrawlModules(CrawlCon&#92;\ntroller.java:656)\nat=\r\n\norg.archive.crawler.framework.CrawlController.initialize(CrawlController&#92;\n=\r\n.java:377)\nat\norg.archive.crawler.admin.CrawlJob.setupForCrawlStart(CrawlJo=\r\nb.java:846)\nat\norg.archive.crawler.admin.CrawlJobHandler.startNextJobIntern=\r\nal(CrawlJobH&#92;\nandler.java:1142)\nat\norg.archive.crawler.admin.CrawlJobHandle=\r\nr$3.run(CrawlJobHandler.java:112&#92;\n5)\nat java.lang.Thread.run(Thread.java:61=\r\n9)\nCaused by: java.io.IOException: (JE 3.2.23) Lock expired. Locker\n-1_Star=\r\ntNextJob_BasicLocker: waited for lock on database=3D_jeNameMap\nnode=3D44 ty=\r\npe=3DWRITE grant=3DWAIT_PROMOTION timeoutMillis=3D5000\nstartTime=3D11782761=\r\n06686 endTime=3D1178276111695\nOwners: [, ]\nWaiters: []\nTransaction -1_Start=\r\nNextJob_BasicLocker owns 44\nTransaction -1_StartNextJob_BasicLocker waits f=\r\nor  node 44\n\nat org.archive.crawler.util.BdbUriUniqFilter.(BdbUriUniqFilter=\r\n.java:102)\nat\norg.archive.crawler.frontier.BdbFrontier.createAlreadyInclude=\r\nd(BdbFronti&#92;\ner.java:154)\nat\norg.archive.crawler.frontier.WorkQueueFrontier=\r\n.initialize(WorkQueueFront&#92;\nier.java:320)\n... 7 more\n\n\n--- In archive-crawl=\r\ner@yahoogroups.com, &quot;ivlcic&quot; &lt;ivlcic@...&gt; wrote:\n&gt;\n&gt; I&#39;ve managed to make s=\r\nome tests and here is how I see it.\n&gt; Please note that I&#39;m just a Heritrix =\r\nbeginner and I had the same wiki\n&gt; page as reference.\n&gt;\n&gt; 1.) Setting up th=\r\ne processors\n&gt; 1.1.) I&#39;ve put on top of the fetch-processors list the:\n&gt; or=\r\ng.archive.crawler.processor.recrawl.PersistLoadProcessor\n&gt; This processor l=\r\noads all previous CrawlURL&#39;s pressistent AList data\nfrom\n&gt; the crawl state =\r\ndirectory. The most important is the content digest\nfrom\n&gt; previous crawls =\r\nassociated with CrawlURL\n&gt;\n&gt; 1.2.) On the bottom of the fetch-processors I&#39;=\r\nve put the:\n&gt; org.archive.crawler.processor.recrawl.FetchHistoryProcessor\n&gt;=\r\n This processor is responsible form maintaining the history list of\n&gt; Crawl=\r\nURLs from previous crawls including the current CrawlURL.\n&gt; Thats why we ar=\r\ne supposed to have the minimum of two. At least one\n&gt; slot for previous and=\r\n one slot for current.\n&gt;\n&gt; 1.3.) At the end of whole chain (post-processors=\r\n) I&#39;ve put the:\n&gt; org.archive.crawler.processor.recrawl.PersistStoreProcess=\r\nor\n&gt; This processor actually stores the CrawlURLs to Berkely DB in to state=\r\n\n&gt; directory of the crawl.\n&gt;\n&gt;\n&gt; 2.) Using the processors\n&gt;\n&gt; 2.1.) In orde=\r\nr to use the current setup (As far as I get it) we have\nto\n&gt; maintain the s=\r\name state directory for each crawl since the BDB files\nin\n&gt; it have also a =\r\nhistory of previous crawls.\n&gt; Enable the view of expert settings in web UI =\r\nand set up the crawl\norder\n&gt; state-path attribute to always point\n&gt; to the =\r\nsame directory for all the jobs.\n&gt;\n&gt; 2.2.) To disable writing the duplicate=\r\n pages I&#39;ve set the attribute\n&gt; skip-identical-digests=3Dtrue on the\n&gt; org.=\r\narchive.crawler.writer.ARCWriterProcessor\n&gt;\n&gt; To achive the same thing on p=\r\nrocessors that do not have such attribute\n&gt; I&#39;ve used the\n&gt; org.archive.cra=\r\nwler.deciderules.recrawl.IdenticalDigestDecideRule\nfilter\n&gt; setting it to R=\r\nEJECT.\n&gt; (org.archive.crawler.writer.MirrorWriterProcessor is such processo=\r\nr)\n&gt;\n&gt; You should also maintain the same &quot;mirror&quot; or &quot;arcs&quot; store path\nbetw=\r\neen\n&gt; crawls.\n&gt; If you don&#39;t you will get only the &quot;changed pages&quot;\n&gt; (chang=\r\ned pages are those with different content digests and/or etag,\n&gt; modified s=\r\nince headers)\n&gt;\n&gt; 2.3.) To prevent the fetching of the same CrawlURLs for t=\r\nhe following\n&gt; crawls I&#39;ve set up\n&gt; send-if-modified-since=3Dtrue\n&gt; send-if=\r\n-none-match=3Dtrue\n&gt; on the org.archive.crawler.fetcher.FetchHTTP.\n&gt; This o=\r\nnly works for static pages / files (send-if-modified-since) or\n&gt; with dynam=\r\nic pages that support the\n&gt; etag HTTP response header (send-if-none-match).=\r\n Those are the most\n&gt; common options that you can use to decide\n&gt; prior to =\r\nfetching the whole content.\n&gt;\n&gt; 2.4.) To minimize the content processing an=\r\nd link extraction you can\nuse\n&gt; TrapSuppressExtractor.\n&gt; If you crawled URL=\r\n [A] with content digest of [123] and just fetched\nthe\n&gt; URL [B] with conte=\r\nnt digest of [123],\n&gt; then this processor disables link extraction so no fu=\r\nrther processing\nis\n&gt; done. (the links from URL [B] are never promoted back=\r\n to frontier)\n&gt; (I haven&#39;t tested this one yet)\n&gt;\n&gt;\n&gt; 3.) alternatives\n&gt; In=\r\nstead of org.archive.crawler.processor.recrawl.PersistLoadProcessor\n&gt; you c=\r\nan use the\n&gt; org.archive.crawler.processor.recrawl.PersistLogProcessor that=\r\n uses\ntxt\n&gt; file for history\n&gt; (don&#39;t use both)\n&gt;\n&gt; If you want to minimize=\r\n the fetch duplication between crawls the only\n&gt; thing you can do is mimic =\r\nthe\n&gt; the org.archive.crawler.fetcher.FetchHTTP.send-if-none-match and\nhand=\r\nle\n&gt; some specific\n&gt; HTTP headers that target hosts return.\n&gt; You can store=\r\n specific data to CrawlURLs persistent AList.\n&gt; (I guess Heritrix people di=\r\ndn&#39;t do it because there is no standard. I\n&gt; haven&#39;t tested this one yet al=\r\nso)\n&gt;\n&gt; Nikola\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, &quot;Cetin Sert&quot; cet=\r\nin.sert@\n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; I would love to know how to u=\r\nse the new duplicate-reduction\nfeatures.\n&gt; The\n&gt; &gt; wiki page about them is =\r\ntoo short to understand and there are many\nnew\n&gt; &gt; processors etc., which m=\r\nakes it difficult for me to understand how\n&gt; best to\n&gt; &gt; combine them.\n&gt; &gt;\n=\r\n&gt; &gt; B. Abbrieviate retrieval of unchanged content.\n&gt; &gt;\n&gt; &gt; Based on headers=\r\n of previous retrieval of same URI, adjust current\n&gt; retrieval\n&gt; &gt; (via con=\r\nditional-GET requests) so that if content is unchanged,\n&gt; retrieval\n&gt; &gt; end=\r\ns quickly and cheaply and no duplicate content is stored.\nMotivated\n&gt; by\n&gt; =\r\n&gt; KB-Denmark study (Clausen 2004); reduces bandwidth used as well as\n&gt; stor=\r\nage.\n&gt; &gt;\n&gt; &gt; This is accomplished in Heritrix 1.12.0 by:\n&gt; &gt;\n&gt; &gt; * Using th=\r\ne FetchHistoryProcessor and either PersistLogProcessor or\n&gt; &gt; PersistStoreP=\r\nrocessor on an initial crawl1\n&gt; &gt; * Carrying forward the initial crawl info=\r\n in a form accessible to a\n&gt; &gt; later crawl, perhaps by using utility functi=\r\nonality on the\n&gt; PersistProcessor\n&gt; &gt; class2\n&gt; &gt; * Using the PersistLoadPro=\r\ncessor and FetchHistoryProcessor in a\n&gt; &gt; following crawl3 so that relevant=\r\n history information is available\nat\n&gt; &gt; store-decision time3\n&gt; &gt; * Using n=\r\new capabilities of the FetchHTTP processor to issue\n&gt; &gt; conditional-GETs wh=\r\nere appropriate\n&gt; &gt; * Using new options on any writer processors to skip or=\r\n abbrieviate\n&gt; &gt; storage as desired4\n&gt; &gt;\n&gt; &gt; C. Note and discard duplicate =\r\n&#39;trap&#39; content from same crawl.\n&gt; &gt;\n&gt; &gt; For one simple kind of crawler-trap=\r\n, where followup URIs return\n&gt; identical\n&gt; &gt; content at different (extended=\r\n) URIs, disable link-extraction.\n&gt; &gt;\n&gt; &gt; The TrapSuppressExtractor in 1.12.=\r\n0 demonstrates this strategy, and\n&gt; could be\n&gt; &gt; the basis for other simila=\r\nr trap/duplicate suppression processors.5\n&gt; &gt;\n&gt; &gt; 1: where exactly? Is this=\r\n a preprocessor, a postprocessor or a\nwriter\n&gt; &gt; module?\n&gt; &gt;\n&gt; &gt; 2: where a=\r\nnd how?\n&gt; &gt;\n&gt; &gt; 3: where and how?\n&gt; &gt;\n&gt; &gt; 4: are these options readily impl=\r\nemented or should we implement them\n&gt; &gt; ourselves? If they are already ther=\r\ne, then how do we use them?\n&gt; &gt;\n&gt; &gt; 5: how?\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; A configuration=\r\n xml file (order.xml) with all the settings enabled,\nor\n&gt; a few\n&gt; &gt; screens=\r\nhots of the web interface with the processors at place might\nbe\n&gt; of\n&gt; &gt; gr=\r\neat help.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; I have had experience with implementing custom de=\r\ncision rules and\n&gt; writers\n&gt; &gt; but this seems to require some explanation.\n=\r\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Thanks in advance to anyone who takes time to answer my que=\r\nstion.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yours Sincerely,\n&gt; &gt;\n&gt; &gt; Cetin Sert\n&gt; &gt;\n&gt;\n\n\r\n--1-2223917424-2156564459=:4\r\nContent-Type: text/html; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nCould you just attach a working order.xml in your reply?&lt;br&gt;&lt;br&gt;Although yo=\r\nur explanations are really helpful, I seem to manage to run into some excep=\r\ntions like the below (1st and 2nd crawls run ok, 3rd one crawl throws this =\r\nexception)&lt;br&gt;&lt;br&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;b&gt;Time:&lt;/b&gt;&nbsp;\n            &lt;/td=\r\n&gt;\n            &lt;td&gt;\n                Mai. 4, 2007 10:55:11 GMT\n            &lt;/=\r\ntd&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;\n                &lt;b&gt;Level:&lt;/=\r\nb&gt;&nbsp;\n            &lt;/td&gt;\n            &lt;td&gt;\n                SEVERE\n        =\r\n    &lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td valign=3D&quot;top&quot;&gt;\n      =\r\n          &lt;b&gt;Message:&lt;/b&gt;&nbsp;\n            &lt;/td&gt;\n            &lt;td&gt;\n        =\r\n        &lt;pre&gt;On crawl: default-mirror-writer-3 Unable to setup crawl module=\r\ns&lt;/pre&gt;\n            &lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td valign=\r\n=3D&quot;top&quot;&gt;\n                &lt;b&gt;Exception:&lt;/b&gt;&nbsp;\n            &lt;/td&gt;\n       =\r\n     &lt;td&gt;\n                &lt;pre&gt;org.archive.crawler.framework.exceptions.Fat=\r\nalConfigurationException: (JE 3.2.23) Lock expired. Locker -1_StartNextJob_=\r\nBasicLocker: waited for lock on database=3D_jeNameMap node=3D44 type=3DWRIT=\r\nE grant=3DWAIT_PROMOTION timeoutMillis=3D5000 startTime=3D1178276106686 end=\r\nTime=3D1178276111695&lt;br&gt;Owners: [&lt;lockinfo locker=3D&quot;-1_StartNextJob_BasicL=\r\nocker&quot; type=3D&quot;READ&quot;&gt;, &lt;lockinfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; typ=\r\ne=3D&quot;READ&quot;&gt;]&lt;br&gt;Waiters: []&lt;br&gt;Transaction -1_StartNextJob_BasicLocker owns=\r\n 44 &lt;lockinfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;&lt;br&gt;Tran=\r\nsaction -1_StartNextJob_BasicLocker waits for  node 44&lt;br&gt;&lt;br&gt;Cause: java.i=\r\no.IOException: (JE 3.2.23) Lock expired. Locker -1_StartNextJob_BasicLocker=\r\n: waited for lock on database=3D_jeNameMap node=3D44 type=3DWRITE grant=3DW=\r\nAIT_PROMOTION timeoutMillis=3D5000 startTime=3D1178276106686 endTime=3D1178=\r\n276111695&lt;br&gt;Owners: [&lt;lockinfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; type=\r\n=3D&quot;READ&quot;&gt;, &lt;lockinfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;=\r\n]&lt;br&gt;Waiters: []&lt;br&gt;Transaction -1_StartNextJob_BasicLocker owns 44 &lt;lockin=\r\nfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;&lt;br&gt;Transaction -1_=\r\nStartNextJob_BasicLocker waits for  node 44&lt;br&gt;&lt;br&gt;\tat org.archive.crawler.=\r\nutil.BdbUriUniqFilter.&lt;init&gt;(BdbUriUniqFilter.java:102)&lt;br&gt;\tat org.archive.=\r\ncrawler.frontier.BdbFrontier.createAlreadyIncluded(BdbFrontier.java:154)&lt;br=\r\n&gt;\tat org.archive.crawler.frontier.WorkQueueFrontier.initialize(WorkQueueFro=\r\nntier.java:320)&lt;br&gt;\tat org.archive.crawler.frontier.BdbFrontier.initialize(=\r\nBdbFrontier.java:282)&lt;br&gt;\tat org.archive.crawler.framework.CrawlController.=\r\nsetupCrawlModules(CrawlController.java:656)&lt;br&gt;\tat org.archive.crawler.fram=\r\nework.CrawlController.initialize(CrawlController.java:377)&lt;br&gt;\tat org.archi=\r\nve.crawler.admin.CrawlJob.setupForCrawlStart(CrawlJob.java:846)&lt;br&gt;\tat org.=\r\narchive.crawler.admin.CrawlJobHandler.startNextJobInternal(CrawlJobHandler.=\r\njava:1142)&lt;br&gt;\tat org.archive.crawler.admin.CrawlJobHandler$3.run(CrawlJobH=\r\nandler.java:1125)&lt;br&gt;\tat java.lang.Thread.run(Thread.java:619)&lt;br&gt;&lt;br&gt;Stack=\r\ntrace: org.archive.crawler.framework.exceptions.FatalConfigurationException=\r\n: (JE 3.2.23) Lock expired. Locker -1_StartNextJob_BasicLocker: waited for =\r\nlock on database=3D_jeNameMap node=3D44 type=3DWRITE grant=3DWAIT_PROMOTION=\r\n timeoutMillis=3D5000 startTime=3D1178276106686 endTime=3D1178276111695&lt;br&gt;=\r\nOwners: [&lt;lockinfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;, &lt;=\r\nlockinfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;]&lt;br&gt;Waiters:=\r\n []&lt;br&gt;Transaction -1_StartNextJob_BasicLocker owns 44 &lt;lockinfo locker=3D&quot;=\r\n-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;&lt;br&gt;Transaction -1_StartNextJob_=\r\nBasicLocker waits for  node 44&lt;br&gt;&lt;br&gt;\tat org.archive.crawler.frontier.Work=\r\nQueueFrontier.initialize(WorkQueueFrontier.java:324)&lt;br&gt;\tat org.archive.cra=\r\nwler.frontier.BdbFrontier.initialize(BdbFrontier.java:282)&lt;br&gt;\tat org.archi=\r\nve.crawler.framework.CrawlController.setupCrawlModules(CrawlController.java=\r\n:656)&lt;br&gt;\tat org.archive.crawler.framework.CrawlController.initialize(Crawl=\r\nController.java:377)&lt;br&gt;\tat org.archive.crawler.admin.CrawlJob.setupForCraw=\r\nlStart(CrawlJob.java:846)&lt;br&gt;\tat org.archive.crawler.admin.CrawlJobHandler.=\r\nstartNextJobInternal(CrawlJobHandler.java:1142)&lt;br&gt;\tat org.archive.crawler.=\r\nadmin.CrawlJobHandler$3.run(CrawlJobHandler.java:1125)&lt;br&gt;\tat java.lang.Thr=\r\nead.run(Thread.java:619)&lt;br&gt;Caused by: java.io.IOException: (JE 3.2.23) Loc=\r\nk expired. Locker -1_StartNextJob_BasicLocker: waited for lock on database=\r\n=3D_jeNameMap node=3D44 type=3DWRITE grant=3DWAIT_PROMOTION timeoutMillis=\r\n=3D5000 startTime=3D1178276106686 endTime=3D1178276111695&lt;br&gt;Owners: [&lt;lock=\r\ninfo locker=3D&quot;-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;, &lt;lockinfo locke=\r\nr=3D&quot;-1_StartNextJob_BasicLocker&quot; type=3D&quot;READ&quot;&gt;]&lt;br&gt;Waiters: []&lt;br&gt;Transac=\r\ntion -1_StartNextJob_BasicLocker owns 44 &lt;lockinfo locker=3D&quot;-1_StartNextJo=\r\nb_BasicLocker&quot; type=3D&quot;READ&quot;&gt;&lt;br&gt;Transaction -1_StartNextJob_BasicLocker wa=\r\nits for  node 44&lt;br&gt;&lt;br&gt;\tat org.archive.crawler.util.BdbUriUniqFilter.&lt;init=\r\n&gt;(BdbUriUniqFilter.java:102)&lt;br&gt;\tat org.archive.crawler.frontier.BdbFrontie=\r\nr.createAlreadyIncluded(BdbFrontier.java:154)&lt;br&gt;\tat org.archive.crawler.fr=\r\nontier.WorkQueueFrontier.initialize(WorkQueueFrontier.java:320)&lt;br&gt;\t... 7 m=\r\nore&lt;br&gt;&lt;/init&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lock=\r\ninfo&gt;&lt;/init&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lockinfo&gt;&lt;/lockin=\r\nfo&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;br&gt;--- In archive-crawler@yahoogrou=\r\nps.com, &quot;ivlcic&quot; &lt;ivlcic@...&gt; wrote:&lt;br&gt;&gt;&lt;br&gt;&gt; I&#39;ve managed to =\r\nmake some tests and here is how I see it.&lt;br&gt;&gt; Please note that I&#39;m just=\r\n a Heritrix beginner and I had the same wiki&lt;br&gt;&gt; page as reference.&lt;br&gt;=\r\n&gt; &lt;br&gt;&gt; 1.) Setting up the processors&lt;br&gt;&gt; 1.1.) I&#39;ve put on top o=\r\nf the fetch-processors list the:&lt;br&gt;&gt; org.archive.crawler.processor.recr=\r\nawl.PersistLoadProcessor&lt;br&gt;&gt; This processor loads all previous CrawlURL=\r\n&#39;s pressistent AList data from&lt;br&gt;&gt; the crawl state directory. The most =\r\nimportant is the content digest from&lt;br&gt;&gt; previous crawls associated wit=\r\nh CrawlURL&lt;br&gt;&gt; &lt;br&gt;&gt; 1.2.) On the bottom of the fetch-processors I&#39;v=\r\ne put the:&lt;br&gt;&gt; org.archive.crawler.processor.recrawl.FetchHistoryProces=\r\nsor&lt;br&gt;&gt; This processor is responsible form maintaining the history list=\r\n of&lt;br&gt;&gt; CrawlURLs from previous crawls including the current CrawlURL.&lt;=\r\nbr&gt;&gt; Thats why we are supposed to have the minimum of two. At least one&lt;=\r\nbr&gt;&gt; slot for previous and one slot for current.&lt;br&gt;&gt; &lt;br&gt;&gt; 1.3.) =\r\nAt the end of whole chain (post-processors) I&#39;ve put the:&lt;br&gt;&gt; org.archi=\r\nve.crawler.processor.recrawl.PersistStoreProcessor&lt;br&gt;&gt; This processor a=\r\nctually stores the CrawlURLs to Berkely DB in to state&lt;br&gt;&gt; directory of=\r\n the crawl.&lt;br&gt;&gt; &lt;br&gt;&gt; &lt;br&gt;&gt; 2.) Using the processors&lt;br&gt;&gt; &lt;br&gt;=\r\n&gt; 2.1.) In order to use the current setup (As far as I get it) we have t=\r\no&lt;br&gt;&gt; maintain the same state directory for each crawl since the BDB fi=\r\nles in&lt;br&gt;&gt; it have also a history of previous crawls.&lt;br&gt;&gt; Enable th=\r\ne view of expert settings in web UI and set up the crawl order&lt;br&gt;&gt; stat=\r\ne-path attribute to always point&lt;br&gt;&gt; to the same directory for all the =\r\njobs.&lt;br&gt;&gt; &lt;br&gt;&gt; 2.2.) To disable writing the duplicate pages I&#39;ve se=\r\nt the attribute&lt;br&gt;&gt; skip-identical-digests=3Dtrue on the&lt;br&gt;&gt; org.ar=\r\nchive.crawler.writer.ARCWriterProcessor&lt;br&gt;&gt; &lt;br&gt;&gt; To achive the same=\r\n thing on processors that do not have such attribute&lt;br&gt;&gt; I&#39;ve used the&lt;=\r\nbr&gt;&gt; org.archive.crawler.deciderules.recrawl.IdenticalDigestDecideRule f=\r\nilter&lt;br&gt;&gt; setting it to REJECT.&lt;br&gt;&gt; (org.archive.crawler.writer.Mir=\r\nrorWriterProcessor is such processor)&lt;br&gt;&gt; &lt;br&gt;&gt; You should also main=\r\ntain the same &quot;mirror&quot; or &quot;arcs&quot; store path between&lt;br&gt;&gt; crawls.&lt;br&gt;&gt;=\r\n If you don&#39;t you will get only the &quot;changed pages&quot;&lt;br&gt;&gt; (changed pages =\r\nare those with different content digests and/or etag,&lt;br&gt;&gt; modified sinc=\r\ne headers)&lt;br&gt;&gt; &lt;br&gt;&gt; 2.3.) To prevent the fetching of the same Crawl=\r\nURLs for the following&lt;br&gt;&gt; crawls I&#39;ve set up&lt;br&gt;&gt; send-if-modified-=\r\nsince=3Dtrue&lt;br&gt;&gt; send-if-none-match=3Dtrue&lt;br&gt;&gt; on the org.archive.c=\r\nrawler.fetcher.FetchHTTP.&lt;br&gt;&gt; This only works for static pages / files =\r\n(send-if-modified-since) or&lt;br&gt;&gt; with dynamic pages that support the&lt;br&gt;=\r\n&gt; etag HTTP response header (send-if-none-match). Those are the most&lt;br&gt;=\r\n&gt; common options that you can use to decide&lt;br&gt;&gt; prior to fetching th=\r\ne whole content.&lt;br&gt;&gt; &lt;br&gt;&gt; 2.4.) To minimize the content processing =\r\nand link extraction you can use&lt;br&gt;&gt; TrapSuppressExtractor.&lt;br&gt;&gt; If y=\r\nou crawled URL [A] with content digest of [123] and just fetched the&lt;br&gt;&gt=\r\n; URL [B] with content digest of [123],&lt;br&gt;&gt; then this processor disable=\r\ns link extraction so no further processing is&lt;br&gt;&gt; done. (the links from=\r\n URL [B] are never promoted back to frontier)&lt;br&gt;&gt; (I haven&#39;t tested thi=\r\ns one yet)&lt;br&gt;&gt; &lt;br&gt;&gt; &lt;br&gt;&gt; 3.) alternatives&lt;br&gt;&gt; Instead of or=\r\ng.archive.crawler.processor.recrawl.PersistLoadProcessor&lt;br&gt;&gt; you can us=\r\ne the&lt;br&gt;&gt; org.archive.crawler.processor.recrawl.PersistLogProcessor tha=\r\nt uses txt&lt;br&gt;&gt; file for history&lt;br&gt;&gt; (don&#39;t use both)&lt;br&gt;&gt; &lt;br&gt;&g=\r\nt; If you want to minimize the fetch duplication between crawls the only&lt;br=\r\n&gt;&gt; thing you can do is mimic the&lt;br&gt;&gt; the org.archive.crawler.fetcher=\r\n.FetchHTTP.send-if-none-match and handle&lt;br&gt;&gt; some specific&lt;br&gt;&gt; HTTP=\r\n headers that target hosts return.&lt;br&gt;&gt; You can store specific data to C=\r\nrawlURLs persistent AList.&lt;br&gt;&gt; (I guess Heritrix people didn&#39;t do it be=\r\ncause there is no standard. I&lt;br&gt;&gt; haven&#39;t tested this one yet also)&lt;br&gt;=\r\n&gt; &lt;br&gt;&gt; Nikola&lt;br&gt;&gt; &lt;br&gt;&gt; --- In archive-crawler@...=\r\nm, &quot;Cetin Sert&quot; cetin.sert@&lt;br&gt;&gt; wrote:&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; Hi,&lt;br=\r\n&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; I would love to know how =\r\nto use the new duplicate-reduction features.&lt;br&gt;&gt; The&lt;br&gt;&gt; &gt; wiki =\r\npage about them is too short to understand and there are many new&lt;br&gt;&gt; &=\r\ngt; processors etc., which makes it difficult for me to understand how&lt;br&gt;&=\r\ngt; best to&lt;br&gt;&gt; &gt; combine them.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; B. Abbriev=\r\niate retrieval of unchanged content.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; Based on hea=\r\nders of previous retrieval of same URI, adjust current&lt;br&gt;&gt; retrieval&lt;br=\r\n&gt;&gt; &gt; (via conditional-GET requests) so that if content is unchanged,&lt;=\r\nbr&gt;&gt; retrieval&lt;br&gt;&gt; &gt; ends quickly and cheaply and no duplicate co=\r\nntent is stored. Motivated&lt;br&gt;&gt; by&lt;br&gt;&gt; &gt; KB-Denmark study (Clause=\r\nn 2004); reduces bandwidth used as well as&lt;br&gt;&gt; storage.&lt;br&gt;&gt; &gt;&lt;br=\r\n&gt;&gt; &gt; This is accomplished in Heritrix 1.12.0 by:&lt;br&gt;&gt; &gt;&lt;br&gt;&gt;=\r\n &gt; * Using the FetchHistoryProcessor and either PersistLogProcessor or&lt;b=\r\nr&gt;&gt; &gt; PersistStoreProcessor on an initial crawl1&lt;br&gt;&gt; &gt; * Carry=\r\ning forward the initial crawl info in a form accessible to a&lt;br&gt;&gt; &gt; l=\r\nater crawl, perhaps by using utility functionality on the&lt;br&gt;&gt; PersistPr=\r\nocessor&lt;br&gt;&gt; &gt; class2&lt;br&gt;&gt; &gt; * Using the PersistLoadProcessor a=\r\nnd FetchHistoryProcessor in a&lt;br&gt;&gt; &gt; following crawl3 so that relevan=\r\nt history information is available at&lt;br&gt;&gt; &gt; store-decision time3&lt;br&gt;=\r\n&gt; &gt; * Using new capabilities of the FetchHTTP processor to issue&lt;br&gt;&=\r\ngt; &gt; conditional-GETs where appropriate&lt;br&gt;&gt; &gt; * Using new option=\r\ns on any writer processors to skip or abbrieviate&lt;br&gt;&gt; &gt; storage as d=\r\nesired4&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; C. Note and discard duplicate &#39;trap&#39; cont=\r\nent from same crawl.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; For one simple kind of crawl=\r\ner-trap, where followup URIs return&lt;br&gt;&gt; identical&lt;br&gt;&gt; &gt; content =\r\nat different (extended) URIs, disable link-extraction.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt;=\r\n &gt; The TrapSuppressExtractor in 1.12.0 demonstrates this strategy, and&lt;b=\r\nr&gt;&gt; could be&lt;br&gt;&gt; &gt; the basis for other similar trap/duplicate sup=\r\npression processors.5&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; 1: where exactly? Is this a=\r\n preprocessor, a postprocessor or a writer&lt;br&gt;&gt; &gt; module?&lt;br&gt;&gt; &gt=\r\n;&lt;br&gt;&gt; &gt; 2: where and how?&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; 3: where and how=\r\n?&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; 4: are these options readily implemented or sho=\r\nuld we implement them&lt;br&gt;&gt; &gt; ourselves? If they are already there, th=\r\nen how do we use them?&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; 5: how?&lt;br&gt;&gt; &gt;&lt;br&gt;&g=\r\nt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; A configuration xml file (order.xml) with=\r\n all the settings enabled, or&lt;br&gt;&gt; a few&lt;br&gt;&gt; &gt; screenshots of the=\r\n web interface with the processors at place might be&lt;br&gt;&gt; of&lt;br&gt;&gt; &gt=\r\n; great help.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; I have ha=\r\nd experience with implementing custom decision rules and&lt;br&gt;&gt; writers&lt;br=\r\n&gt;&gt; &gt; but this seems to require some explanation.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt;=\r\n &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; Thanks in advance to anyone who takes time =\r\nto answer my question.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; =\r\nYours Sincerely,&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; Cetin Sert&lt;br&gt;&gt; &gt;&lt;br&gt;&gt;&lt;=\r\nbr&gt;\n\n\r\n--1-2223917424-2156564459=:4--\r\n\n"}}