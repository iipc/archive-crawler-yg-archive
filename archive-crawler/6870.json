{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"kJTvj4cLM_i5bEhFYyzcKcojiJ-9VEr2RLr8cvtqufQ3lOxaFt_Wsz6O_RGQG3ToLG1n5nWBXJndcXwwCHENJlGbeUfYxqw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Max Suggested Seeds?","postDate":"1291757986","msgId":6870,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDRkVBOUEyLjcwNDAwMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEEyMUU0Mzk4MzFGRDQ4NzJBNkU5RDE1QTMzOEZCREJEQGRhdGFjbGlwLmNvbT4=","referencesHeader":"PEEyMUU0Mzk4MzFGRDQ4NzJBNkU5RDE1QTMzOEZCREJEQGRhdGFjbGlwLmNvbT4="},"prevInTopic":6867,"nextInTopic":0,"prevInTime":6869,"nextInTime":6871,"topicId":6867,"numMessagesInTopic":2,"msgSnippet":"Once the crawl is underway, seeds aren t really any different than other URIs, so even what rules-of-thumb we have aren t expressed in terms of seeds. (In","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 18932 invoked from network); 7 Dec 2010 21:39:50 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m4.grp.sp2.yahoo.com with QMQP; 7 Dec 2010 21:39:50 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.sp2.yahoo.com with SMTP; 7 Dec 2010 21:39:49 -0000\r\nX-Received: (qmail 69231 invoked by uid 0); 7 Dec 2010 21:39:48 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 7 Dec 2010 21:39:48 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4CFEA9A2.7040004@...&gt;\r\nDate: Tue, 07 Dec 2010 13:39:46 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.12) Gecko/20101027 Thunderbird/3.1.6\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Zach Bailey &lt;zach.bailey@...&gt;\r\nReferences: &lt;A21E439831FD4872A6E9D15A338FBDBD@...&gt;\r\nIn-Reply-To: &lt;A21E439831FD4872A6E9D15A338FBDBD@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Max Suggested Seeds?\r\nX-Yahoo-Group-Post: member; u=137285340; y=p_S-hgdcFbHhX2KSQVBuAvojtk5NX6u7hMGovQLaTeXJ\r\nX-Yahoo-Profile: gojomo\r\n\r\nOnce the crawl is underway, seeds aren&#39;t really any different than other \nURIs, so even what rules-of-thumb we have aren&#39;t expressed in terms of \nseeds. (In particular, once a seed or its related site is &#39;finished&#39;, \nit&#39;s only the redundant rediscovery of its URLs which has it involved in \nthe continuing crawl at all.)\n\nOur goal for single-machine operation is that &#39;any&#39; size crawl is \npossible, as long as you have sufficient disk space -- just that as the \ncrawl gets further along the (IO-intensive) task of reconciling current \ndiscoveries with the already-seen material becomes slower.\n\nIn our larger H1 crawls on machines of ~4GB RAM, the BdbUriUniqFilter \n(set of already-seen URIs) has often become an obvious bottleneck \nsomewhere in the 10 million - 30 million URI range, so crawls expected \nto grow beyond that, on a single machine, are usually started with the \nBloomUriUniqFilter. (Such crawls have then been run, over the course of \nmany weeks, up through 125-million-plus on a single machine.)\n\nWe&#39;re still working to understand and minimize the bottlenecks on crawls \nsplit over many machines, reaching billion of URLs over many weeks. \nWe&#39;ve done a few such crawls, but with little dedicated \nanalysis/optimization so far.\n\nCrawls that are limited to a fixed set of hostnames (no matter how \ndeep), versus those against large (like country-based) TLDs, versus \nthose that go &#39;everywhere&#39; are all somewhat different in the way they \nstrain as they get larger. And, tinkering with the settings can modify \nthe &#39;shape&#39; of a crawl. (For example you could bias a crawl towards \ngetting a little from every hostname -- so that at every moment the \ncrawl is a broad sample of content -- or so that it only starts one \nhostname when another seems finished.)\n\n- Gordon @ IA\n\nOn 12/7/10 11:24 AM, Zach Bailey wrote:\n&gt; I realize there technically should be no hard limit to the number of\n&gt; seeds I could provide to Heritrix, but I was wondering if any of the\n&gt; more experienced members on the group could give me some guidance on a\n&gt; practical limit dependent on hardware sizing.\n&gt;\n&gt; I would imagine a practical limit on the number of seeds might be\n&gt; related to the performance of the BDB engine that backs the frontier?\n&gt; What&#39;s the largest seeded crawl anyone has performed? Was it on the\n&gt; order of millions of domains? Tens of millions?\n&gt;\n&gt; For the record, the largest crawl I have done was seeded with about 350k\n&gt; domains and resulted in about 3-4 million crawled URIs. My crawl machine\n&gt; has 7G of RA M, and heritrix is the only thing running and I give it 5G\n&gt; of heap. The machine has 8 CPUs and I run the crawl with 150 toe threads\n&gt; which does a pretty good job of keeping CPU utilization pegged.\n&gt;\n&gt; After a while on larger crawls, the crawler seems to become increasingly\n&gt; I/O bound when dealing with the BDB store. I have not yet investigated\n&gt; the BloomUriUniqFilter to replace the BDB-based one, but I definitely\n&gt; will be investigating that as well.\n&gt;\n&gt; Thanks,\n&gt; Zach\n&gt;\n&gt;\n&gt; \n\n"}}