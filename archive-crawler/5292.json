{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":171884130,"authorName":"hijbul_bd","from":"&quot;hijbul_bd&quot; &lt;hijbul_bd@...&gt;","profile":"hijbul_bd","replyTo":"LIST","senderId":"NLPiGru2R62nLyhmujiDsYtaeWr97FPJRY75dfaCxfalLitSapLPUdgJWcBpyZqa2Z5q91mEsfBK3XysrlXyMA-MLAapQUVS","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Crawling 100 million pages","postDate":"1212766009","msgId":5292,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PGcyYmt2cCtiMmsxQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":5293,"prevInTime":5291,"nextInTime":5293,"topicId":5292,"numMessagesInTopic":11,"msgSnippet":"Dear All I would like to crawl 100 million pages with in a month for crawling research. As far i know some research crawler(IRLbot(6 billion pages in 41 days),","rawEmail":"Return-Path: &lt;hijbul_bd@...&gt;\r\nReceived: (qmail 18858 invoked from network); 6 Jun 2008 18:29:11 -0000\r\nReceived: from unknown (66.218.67.97)\n  by m47.grp.scd.yahoo.com with QMQP; 6 Jun 2008 18:29:11 -0000\r\nReceived: from unknown (HELO n18a.bullet.scd.yahoo.com) (66.94.237.47)\n  by mta18.grp.scd.yahoo.com with SMTP; 6 Jun 2008 18:29:11 -0000\r\nReceived: from [66.218.69.1] by n18.bullet.scd.yahoo.com with NNFMP; 06 Jun 2008 18:29:11 -0000\r\nReceived: from [66.218.66.86] by t1.bullet.scd.yahoo.com with NNFMP; 06 Jun 2008 18:29:11 -0000\r\nX-Sender: hijbul_bd@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 76976 invoked from network); 6 Jun 2008 15:26:50 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m45.grp.scd.yahoo.com with QMQP; 6 Jun 2008 15:26:50 -0000\r\nX-Received: from unknown (HELO n46d.bullet.mail.sp1.yahoo.com) (66.163.169.172)\n  by mta17.grp.scd.yahoo.com with SMTP; 6 Jun 2008 15:26:50 -0000\r\nX-Received: from [216.252.122.217] by n46.bullet.mail.sp1.yahoo.com with NNFMP; 06 Jun 2008 15:26:50 -0000\r\nX-Received: from [66.218.69.1] by t2.bullet.sp1.yahoo.com with NNFMP; 06 Jun 2008 15:26:50 -0000\r\nX-Received: from [66.218.66.92] by t1.bullet.scd.yahoo.com with NNFMP; 06 Jun 2008 15:26:50 -0000\r\nDate: Fri, 06 Jun 2008 15:26:49 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g2bkvp+b2k1@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;hijbul_bd&quot; &lt;hijbul_bd@...&gt;\r\nSubject: Crawling 100 million pages\r\nX-Yahoo-Group-Post: member; u=171884130; y=GLva2EUyE8UMlCszw8D4KV0qWidIkuxSGQSPOeZH7yyqELL3\r\nX-Yahoo-Profile: hijbul_bd\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 06 Jun 2008 18:29:09 -0000\r\n\r\nDear All\n\nI would like to crawl 100 million pages with in a month for crawl=\r\ning \nresearch. As far i know some research crawler(IRLbot(6 billion pages i=\r\nn \n41 days), polybot(120 millions pages in 19 days)) can download huge \npag=\r\nes in short amount of time wich is not open source. In 2005 \naccording to s=\r\nome blog site Heritrix can download about 20 miilion \npages in a month. Wha=\r\nt is the speed of current Heritrix version and How \ncan I  speed up heritri=\r\nx to download 100 million or at least 50 million \npages with in a month. Ar=\r\ne there any ohter open source crawler which \ncan do this?\n\nThanks in Advanc=\r\ne\nHijbul Alam\n\n\n"}}