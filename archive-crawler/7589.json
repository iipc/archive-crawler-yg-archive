{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"david_pane1","from":"&quot;david_pane1&quot; &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"ECAOVP1h5XLpbi7HeonS9rmuNXzql6Pg8rDexKou-7ZfkGu3CdIcYSs3LcyXd5xCG60OkiqMPW8CmlMR-r0AXh4EPfcHftc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: questions before we restart the crawl","postDate":"1327954780","msgId":7589,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpnNnUwcytsbjdoQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRGMjZGNUJDLjgwNDAxQGFyY2hpdmUub3JnPg=="},"prevInTopic":7588,"nextInTopic":7595,"prevInTime":7588,"nextInTime":7590,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"Thanks Gordon, I will try this. --David","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 1656 invoked from network); 30 Jan 2012 20:19:42 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m14.grp.sp2.yahoo.com with QMQP; 30 Jan 2012 20:19:42 -0000\r\nX-Received: from unknown (HELO ng20-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.252)\n  by mta4.grp.sp2.yahoo.com with SMTP; 30 Jan 2012 20:19:42 -0000\r\nX-Received: from [98.137.0.80] by ng20.bullet.mail.gq1.yahoo.com with NNFMP; 30 Jan 2012 20:19:42 -0000\r\nX-Received: from [69.147.65.149] by tg1.bullet.mail.gq1.yahoo.com with NNFMP; 30 Jan 2012 20:19:41 -0000\r\nX-Received: from [98.137.34.155] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 30 Jan 2012 20:19:40 -0000\r\nDate: Mon, 30 Jan 2012 20:19:40 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jg6u0s+ln7h@...&gt;\r\nIn-Reply-To: &lt;4F26F5BC.80401@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;david_pane1&quot; &lt;dpane@...&gt;\r\nSubject: Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=500983475; y=_w_1kFPPfk1AzFRnU55Kpc4_qgte4S8nIPPFzBt7O1DOfWkIOSjcnQ\r\nX-Yahoo-Profile: david_pane1\r\n\r\nThanks Gordon,\n\nI will try this.\n\n--David\n\n--- In archive-crawler@yahoogrou=\r\nps.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; Given the evident bottleneck Ke=\r\nnji has pointed out in your threads \n&gt; report, I would definitely try a lar=\r\nger &#39;poolMaxActive&#39; value. That can \n&gt; especially help when one large (even=\r\n multi-GB) resource is taking a \n&gt; significant amount of time to store to W=\r\nARC, allowing smaller resources \n&gt; to be saved to other WARCs in the meanti=\r\nme.\n&gt; \n&gt; However, it could also be general IO contention on whatever disks =\r\nare \n&gt; the target of your WARC-writing. Also, since large resources spill o=\r\nver \n&gt; into temporary files in the &#39;scratch&#39; directory before being permane=\r\nntly \n&gt; stored in WARCs, having the &#39;scratch&#39; and &#39;warc&#39; storage paths on \n=\r\n&gt; separate disks can often be helpful.\n&gt; \n&gt; The number of active writers in=\r\n the pool may be dynamically changed in a \n&gt; paused crawl, by the Scripting=\r\n Console or Bean Browse interface, finding \n&gt; the &#39;WarcWriterPool&#39; instance=\r\n and changing its &#39;maxActive&#39; value. It can \n&gt; also be changed in the CXML =\r\nbetween restarts/resumes.\n&gt; \n&gt; Having many WARCs open in parallel at once m=\r\neans:\n&gt; \n&gt; (1) whenever there is a break (by error or requested checkpoint)=\r\n in \n&gt; writing, many files will all be in-progress, and thus of \n&gt; ragged/p=\r\notentially-small sizes -- rather than just 1\n&gt; \n&gt; (2) unless they&#39;re spread=\r\n over multiple disk volumes (by using multiple \n&gt; &#39;storePaths&#39;), they may s=\r\ntill be fighting over the same limited amount \n&gt; of disk capacity (seeks/th=\r\nroughput)\n&gt; \n&gt; - Gordon\n&gt; \n&gt; \n&gt; On 1/30/12 7:27 AM, david_pane1 wrote:\n&gt; &gt; =\r\nKenji,\n&gt; &gt;\n&gt; &gt; I didn&#39;t change any of the values in WARCWriterProcessor.  I=\r\nt is the default.\n&gt; &gt;\n&gt; &gt;   &lt;bean id=3D&quot;warcWriter&quot; class=3D&quot;org.archive.mo=\r\ndules.writer.WARCWriterProcessor&quot;&gt;\n&gt; &gt;    &lt;!--&lt;property name=3D&quot;compress&quot; v=\r\nalue=3D&quot;true&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;property name=3D&quot;prefix&quot; value=3D&quot;IAH&quot; /&gt;=\r\n  --&gt;\n&gt; &gt;    &lt;!--&lt;property name=3D&quot;suffix&quot; value=3D&quot;${HOSTNAME}&quot; /&gt;  --&gt;\n&gt; =\r\n&gt;    &lt;!--&lt;property name=3D&quot;maxFileSizeBytes&quot; value=3D&quot;1000000000&quot; /&gt;  --&gt;\n&gt;=\r\n &gt;    &lt;!--&lt;property name=3D&quot;poolMaxActive&quot; value=3D&quot;1&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;=\r\nproperty name=3D&quot;MaxWaitForIdleMs&quot; value=3D&quot;500&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;proper=\r\nty name=3D&quot;skipIdenticalDigests&quot; value=3D&quot;false&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;proper=\r\nty name=3D&quot;maxTotalBytesToWrite&quot; value=3D&quot;0&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;property n=\r\name=3D&quot;directory&quot; value=3D&quot;${launchId}&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;property name=\r\n=3D&quot;storePaths&quot;&gt;\n&gt; &gt;          &lt;list&gt;\n&gt; &gt;           &lt;value&gt;warcs&lt;/value&gt;\n&gt; &gt;=\r\n          &lt;/list&gt;\n&gt; &gt;         &lt;/property&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;property name=3D=\r\n&quot;writeRequests&quot; value=3D&quot;true&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;property name=3D&quot;writeMe=\r\ntadata&quot; value=3D&quot;true&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;property name=3D&quot;writeRevisitFor=\r\nIdenticalDigests&quot; value=3D&quot;true&quot; /&gt;  --&gt;\n&gt; &gt;    &lt;!--&lt;property name=3D&quot;write=\r\nRevisitForNotModified&quot; value=3D&quot;true&quot; /&gt;  --&gt;\n&gt; &gt;   &lt;/bean&gt;\n&gt; &gt;\n&gt; &gt; --David=\r\n\n&gt; &gt;\n&gt; &gt; --- In archive-crawler@yahoogroups.com, Kenji Nagahashi&lt;knagahashi=\r\n@&gt;  wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt; David,\n&gt; &gt;&gt;\n&gt; &gt;&gt; It is fairly common H3 goes 2x~3x fas=\r\nter at the beginning, where URI/s\n&gt; &gt;&gt; figure includes lots of DNS queries,=\r\n many queues are ready, state\n&gt; &gt;&gt; database is smaller, etc. I often see my=\r\n crawlers running at&gt;200URI/s,\n&gt; &gt;&gt; too. (Also URI/s shown on the web UI is=\r\n not really reliable.)\n&gt; &gt;&gt;\n&gt; &gt;&gt; It is difficult to tell how many queues ar=\r\ne enough to keep all threads\n&gt; &gt;&gt; busy, as there are many factors affecting=\r\n crawl speed. Assuming 0.5 sec\n&gt; &gt;&gt; processing time per URI and 100% concur=\r\nrency, 1200 thread could do 2400\n&gt; &gt;&gt; URI/s. On the other hand, assuming co=\r\nnstant crawl delay of 3sec, 4000\n&gt; &gt;&gt; active queues can only emit 1333 URI/=\r\ns. In this case, active queue size\n&gt; &gt;&gt; becomes the limiting factor. If pro=\r\ncessing time per URI becomes 1 sec,\n&gt; &gt;&gt; then processing time becomes a bot=\r\ntleneck. Some queues get snoozed much\n&gt; &gt;&gt; longer than 3 sec and it makes U=\r\nRI emit rate lower. I tried drawing a\n&gt; &gt;&gt; active-queue to crawl-speed grap=\r\nh, but what I can say is that there is a\n&gt; &gt;&gt; strong correlation between th=\r\nem (roughly, 5000 queues -&gt;  40URI/s, 8000\n&gt; &gt;&gt; queue -&gt;  70URI/s ; varianc=\r\ne is really big).\n&gt; &gt;&gt;\n&gt; &gt;&gt; Looking at your thread and frontier report, I n=\r\noticed:\n&gt; &gt;&gt;\n&gt; &gt;&gt; - 994 queues are &quot;ready&quot;\n&gt; &gt;&gt; - 911 threads are in warcWr=\r\niter\n&gt; &gt;&gt;\n&gt; &gt;&gt; that is, you have enough active queues to keep threads busy,=\r\n but threads\n&gt; &gt;&gt; are spending too much time on writing WARCs to process UR=\r\nIs on time.\n&gt; &gt;&gt; WarcWriter is the bottleneck in this case, probably becaus=\r\ne of small\n&gt; &gt;&gt; number of concurrent writers? What is your warcWriter.poolM=\r\naxActive?\n&gt; &gt;&gt;\n&gt; &gt;&gt; --Kenji\n&gt; &gt;&gt;\n&gt; &gt;&gt; (1/27/12 7:32 AM), david_pane1 wrote:=\r\n\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Kenji,\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; You understood correctly 25M pages/da=\r\ny for 5 machines. Right now, with\n&gt; &gt;&gt;&gt; three instances running, we are see=\r\ning around 17M pages per day. This\n&gt; &gt;&gt;&gt; makes me think that we may be satu=\r\nrating our network throughput.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; But, we have seen, at the beginn=\r\ning of the crawl, 250-300 URIs/second\n&gt; &gt;&gt;&gt; (an average of 52M pages/day ov=\r\ner the first 3 days of the crawl. Once we\n&gt; &gt;&gt;&gt; get past the first few days=\r\n, the crawl slows. During mid crawl, we found\n&gt; &gt;&gt;&gt; that stopping the crawl=\r\n and restarting it improves the throughput but\n&gt; &gt;&gt;&gt; never back to 50M page=\r\ns/day.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; How many queues do you need active per thread? Wouldn&#39;t =\r\nHeritrix\n&gt; &gt;&gt;&gt; activate more queues if there are enough threads to handle t=\r\nhem? We\n&gt; &gt;&gt;&gt; certainly have a lot of queues.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; As an example, on=\r\n one instance we are seeing a rate of 93.25 URIs/sec\n&gt; &gt;&gt;&gt; average.\n&gt; &gt;&gt;&gt;\n&gt;=\r\n &gt;&gt;&gt; Load:\n&gt; &gt;&gt;&gt; 1176 active of 1176 threads; 3,456.5 congestion ration 136=\r\n88365 deepest\n&gt; &gt;&gt;&gt; queue; 62 average depth\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Threads:\n&gt; &gt;&gt;&gt; 1176=\r\n threads: 1176 ABOUT_TO_BEGIN_PROCESSOR; 911 warcWriter, 262\n&gt; &gt;&gt;&gt; fetchHtt=\r\np, 2 candidates, 1 extractorHtml\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Frontier:\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; RUN 1295=\r\n5728 URI queues: 4302 active (1200 in-process; 994 ready; 2108\n&gt; &gt;&gt;&gt; snooze=\r\nd); 11429799 inactive; 0 ineligible; 0 retired; 1521627 exhausted.\n&gt; &gt;&gt;&gt;\n&gt; =\r\n&gt;&gt;&gt; --David\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; --- In archive-crawler@yahoogroups.com\n&gt; &gt;&gt;&gt; &lt;mailt=\r\no:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt; &gt;&gt;&gt; &lt;knagahashi@&gt;  =\r\nwrote:\n&gt; &gt;&gt;&gt;   &gt;\n&gt; &gt;&gt;&gt;   &gt;  David,\n&gt; &gt;&gt;&gt;   &gt;\n&gt; &gt;&gt;&gt;   &gt;  I understood &quot;25M +=\r\n page/day&quot; was total for 5 machines, as you wrote\n&gt; &gt;&gt;&gt;   &gt;  &quot;(these number=\r\ns are totals of all 5 instances combined)&quot;. Did you mean\n&gt; &gt;&gt;&gt;   &gt;  25M+ pa=\r\nge/day/instance? If so, my &quot;100 threads&quot; comment is pointless.\n&gt; &gt;&gt;&gt;   &gt;  P=\r\nlease disregard it.\n&gt; &gt;&gt;&gt;   &gt;\n&gt; &gt;&gt;&gt;   &gt;  I&#39;m running broad crawl that captu=\r\nres everything linked: images, script,\n&gt; &gt;&gt;&gt;   &gt;  CSS, PDF, Excel, ... even=\r\n mpeg4 videos. Our Heritrix 3 runs on 8GB\n&gt; &gt;&gt;&gt;   &gt;  memory + 4 core virtua=\r\nl machine (KVM), with 100 threads. it goes\n&gt; &gt;&gt;&gt;   &gt;  ~60URI/s on average (=\r\nper instance). Probably we could go as high as 150\n&gt; &gt;&gt;&gt;   &gt;  threads to ge=\r\nt higher crawl speed, but it comes with higher risk of\n&gt; &gt;&gt;&gt;   &gt;  dying of =\r\nOutOfMemoryError, empirically. Crawl speed is also limited by\n&gt; &gt;&gt;&gt;   &gt;  lo=\r\nwer disk I/O performance of VMs. 100 seems to be a good number for us.\n&gt; &gt;&gt;=\r\n&gt;   &gt;\n&gt; &gt;&gt;&gt;   &gt;  Yes, increasing threads brings significant increase in cra=\r\nwl speed, to\n&gt; &gt;&gt;&gt;   &gt;  certain extent. If you don&#39;t have enough &quot;active qu=\r\neues,&quot; threads are\n&gt; &gt;&gt;&gt;   &gt;  just wasted. There are other bottleneck, too,=\r\n and it can change over\n&gt; &gt;&gt;&gt; time.\n&gt; &gt;&gt;&gt;   &gt;\n&gt; &gt;&gt;&gt;   &gt;  At least we&#39;re get=\r\nting sustained 60URI/s level of speed with 100\n&gt; &gt;&gt;&gt;   &gt;  threads. With 1,2=\r\n00 threads and enough active queues, you should be\n&gt; &gt;&gt;&gt;   &gt;  getting crawl=\r\n speed much much higher than that (I&#39;ve never been able to\n&gt; &gt;&gt;&gt;   &gt;  run m=\r\ny crawler with 1200 threads, though!)\n&gt; &gt;&gt;&gt;   &gt;\n&gt; &gt;&gt;&gt;   &gt;  --Kenji\n&gt; &gt;&gt;&gt;   =\r\n&gt;\n&gt; &gt;&gt;&gt;   &gt;  (1/26/12 10:31 AM), david_pane1 wrote:\n&gt; &gt;&gt;&gt;   &gt;  &gt;  Kenji,\n&gt; =\r\n&gt;&gt;&gt;   &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  Are you saying that you can get 25M pages per day =\r\non 100 threads and 1\n&gt; &gt;&gt;&gt;   &gt;  &gt;  instance or 25M URIs/day? Do you capture=\r\n all images, pdfs, and\n&gt; &gt;&gt;&gt;   &gt;  &gt;  supporting page documents or are you j=\r\nust capturing html pages?\n&gt; &gt;&gt;&gt;   &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  My test crawls before =\r\nrunning our large crawl showed significant\n&gt; &gt;&gt;&gt;   &gt;  &gt;  increase in the nu=\r\nmber of pages captured when we increased the\n&gt; &gt;&gt;&gt; number of\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\nthreads.\n&gt; &gt;&gt;&gt;   &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  --David\n&gt; &gt;&gt;&gt;   &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  --- =\r\nIn archive-crawler@yahoogroups.com\n&gt; &gt;&gt;&gt; &lt;mailto:archive-crawler%40yahoogro=\r\nups.com&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Na=\r\ngahashi\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &lt;knagahashi@&gt;  wrote:\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;=\r\n  Hi,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  May be a bit off-topic, but 25M/day =\r\nwith 5 machine is average\n&gt; &gt;&gt;&gt; 58/s per\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  machine. Since I =\r\nknow Heritrix-3 can crawl at this speed with\n&gt; &gt;&gt;&gt; just 100\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;=\r\n  ToeThreads, I wonder if most of your 1200 ToeThreads are idle.\n&gt; &gt;&gt;&gt;   &gt; =\r\n &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  While why you don&#39;t get much higher speed with 1200 =\r\nthreads is a big\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  question, it may make sense to cut down t=\r\nhe number of ToeThreads if\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  you&#39;re okay with current crawl =\r\nspeed. Less threads will make H3 less\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  susceptible to memor=\r\ny problems... Just a thought.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  --Kenji\n&gt; &gt;&gt;=\r\n&gt;   &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  (1/20/12 9:54 PM), David Pane wrote:\n&gt; &gt;&gt;&gt;   &gt;=\r\n  &gt;  &gt;  &gt;  Gordon,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  Thank you for you=\r\nr response. And I am sorry for the\n&gt; &gt;&gt;&gt; overwhelming amount\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  of information...I think I am a little overwhelmed.... and\n&gt; &gt;&gt;&gt; feel=\r\ning the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  pressure.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;=\r\n  1) Our Bloom filter configuration:\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt; =\r\n &lt;bean id=3D&quot;uriUniqFilter&quot;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  class=3D&quot;org.archive.crawle=\r\nr.util.BloomUriUniqFilter&quot;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &lt;property name=3D&quot;bloomFilt=\r\ner&quot;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &lt;bean class=3D&quot;org.archive.util.BloomFilter64bit&quot;&gt;=\r\n\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &lt;constructor-arg value=3D&quot;400000000&quot;/&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;=\r\n  &gt;  &lt;constructor-arg value=3D&quot;30&quot;/&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &lt;/bean&gt;\n&gt; &gt;&gt;&gt;   &gt; =\r\n &gt;  &gt;  &gt;  &lt;/property&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &lt;/bean&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt; =\r\n  &gt;  &gt;  &gt;  &gt;  2) We are writing the crawl data to a NAS configured with RAI=\r\nD 6.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  We did\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  see some problems with disk e=\r\nrrors on the NAS earlier in the crawl\n&gt; &gt;&gt;&gt;   &gt;  &gt;  (late\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  =\r\n&gt;  Dec ). I recently found this out. We were/are running in a degraded\n&gt; &gt;&gt;=\r\n&gt;   &gt;  &gt;  &gt;  &gt;  raid state - a few of the disks have been replaced and the =\r\nRAID is\n&gt; &gt;&gt;&gt;   &gt;  &gt;  being\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  rebuilt. We didn&#39;t see any =\r\nblock device errors in the logs on\n&gt; &gt;&gt;&gt; the NAS\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  so the=\r\n write failures we saw are probably not related to the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  rebuil=\r\nd. We\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  did see some network hiccups (no outright failure=\r\ns) in the logs.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  So, this may be the culprit for some of=\r\n the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  3) Yes, we have been cross-feed=\r\ning URIs.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  --David\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;=\r\n\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  On 1/21/12 12:22 AM, Gordon Mohr wrote:\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  &gt;  You&#39;ve provided an overwhelming amount of information and we\n&gt; &gt;&gt;&gt;=\r\n may be\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  dealing with multiple issues, some of which =\r\nhave roots going back\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  earlier than the diagnostic da=\r\nta we now have available.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  A fe=\r\nw key points of emphasis:\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  - we=\r\n&#39;ve not run crawls with 1200 threads before, or on hardware\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;=\r\n  &gt;  &gt;  similar to yours, so our experience is only vaguely suggestive\n&gt; &gt;&gt;=\r\n&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  - it&#39;s not the lower thread counts=\r\n that are the real source of\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  concern; you can even a=\r\ndjust the number of threads mid-crawl.\n&gt; &gt;&gt;&gt; It&#39;s\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  th=\r\nat the error that killed the threads almost certainly left\n&gt; &gt;&gt;&gt; a queue\n&gt; =\r\n&gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  in a &#39;phantom&#39; state where no progress would be made c=\r\nrawling its\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  URIs, each time it happened, on each res=\r\nume leading to the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  current state.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;=\r\n   &gt;  &gt;  &gt;  &gt;  &gt;  - without having understood and fixed whatever software o=\r\nr system\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  problems caused the earliest/most-foundatio=\r\nnal errors in your\n&gt; &gt;&gt;&gt; crawl,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  it&#39;s impossible to s=\r\nay how likely they are to recur.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  =\r\n&gt;  With that in mind, I&#39;ll try to provide quick answers to your\n&gt; &gt;&gt;&gt; other=\r\n\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  questions...\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt; =\r\n &gt;  &gt;  On 1/20/12 4:20 PM, David Pane wrote:\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;  =\r\n &gt;  &gt;  &gt;  &gt;  &gt;&gt;  We have collected about 550 million pages along with the\n&gt;=\r\n &gt;&gt;&gt; images and\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  supporting documents on our 5 insta=\r\nnce crawl that was started\n&gt; &gt;&gt;&gt;   &gt;  &gt;  Dec. 23rd.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt; =\r\n Although we are please with the amount of data we captured to\n&gt; &gt;&gt;&gt;   &gt;  &gt;=\r\n  date, we\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  are very concerned about the state of th=\r\ne Heritrix instances. If\n&gt; &gt;&gt;&gt;   &gt;  &gt;  fact,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  we are=\r\nn&#39;t very confident that the instances will last until the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  end=\r\n of\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  February. We are now running on a total of over=\r\n 500 less threads\n&gt; &gt;&gt;&gt;   &gt;  &gt;  than\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  the configured=\r\n 1200 threads/instance.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  0 - =\r\nnot running right now.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  1 - running on 1198 ( 2 less=\r\n)\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  2 - running on 931 (269 less)\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt; =\r\n &gt;&gt;  3 - running on 987 (213 less)\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  4 - running on 1=\r\n170 (30 less)\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  Since we are s=\r\neriously considering throwing away this past\n&gt; &gt;&gt;&gt;   &gt;  &gt;  month&#39;s work\n&gt; &gt;=\r\n&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  and starting over, we would like to pick your brain on=\r\n some\n&gt; &gt;&gt;&gt;   &gt;  &gt;  strategies\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  that will help us av=\r\noid getting into this situation again.\n&gt; &gt;&gt;&gt; We were\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;=\r\n  hoping to be done crawling by the end of February so this\n&gt; &gt;&gt;&gt;   &gt;  &gt;  r=\r\nestart will\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  put us behind schedule.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;=\r\n  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  1) Can we continue from here but with &quot;clea=\r\nn&quot; Heritrix\n&gt; &gt;&gt;&gt; instances?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt; =\r\n Is there a way that we can continue from the this point\n&gt; &gt;&gt;&gt; forward, but=\r\n\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  start with Heritrix instances that will not be cor=\r\nrupt due\n&gt; &gt;&gt;&gt; to sever\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  error? (e.g. using the\n&gt; &gt;&gt;=\r\n&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/=\r\nCrawl+Recovery\n&gt; &gt;&gt;&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Cra=\r\nwl+Recovery&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &lt;https://webarchive.jira.com/wiki/display/Heritr=\r\nix/Crawl+Recovery\n&gt; &gt;&gt;&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/=\r\nCrawl+Recovery&gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt; &lt;https://webarchive.jira.com/wiki=\r\n/display/Heritrix/Crawl+Recovery\n&gt; &gt;&gt;&gt; &lt;https://webarchive.jira.com/wiki/di=\r\nsplay/Heritrix/Crawl+Recovery&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &lt;https://webarchive.jira.com/w=\r\niki/display/Heritrix/Crawl+Recovery\n&gt; &gt;&gt;&gt; &lt;https://webarchive.jira.com/wiki=\r\n/display/Heritrix/Crawl+Recovery&gt;&gt;&gt;  ) If\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  so, would=\r\n you recommend doing this? You mentioned that this\n&gt; &gt;&gt;&gt; could be\n&gt; &gt;&gt;&gt;   &gt;=\r\n  &gt;  &gt;  &gt;  &gt;&gt;  time consuming. Each of our instances has downloaded around =\r\n170M\n&gt; &gt;&gt;&gt;   &gt;  &gt;  URIs,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  they have over 700M queued=\r\n URIs, what is your time estimate for\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  something thi=\r\ns large?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  We are willing to s=\r\nacrifice a few days to get our crawler to\n&gt; &gt;&gt;&gt; a clean\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt; =\r\n &gt;&gt;  state again so we can crawl for another 30 days at the pace we\n&gt; &gt;&gt;&gt;  =\r\n &gt;  &gt;  have been\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  crawling.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; =\r\n&gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  You can do a big &#39;frontier-recover&#39; log replay to avoi=\r\nd\n&gt; &gt;&gt;&gt;   &gt;  &gt;  recrawling the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  same URIs, and approx=\r\nimate the earlier queue state.\n&gt; &gt;&gt;&gt; Splitting/filters\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  =\r\n&gt;  the logs manually beforehand as alluded to in the wiki page\n&gt; &gt;&gt;&gt; can sp=\r\need\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  this process somewhat... but given the size of a=\r\nll your\n&gt; &gt;&gt;&gt; log-segments\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  that log grooming beforeh=\r\nand is itself likely to be a lengthy\n&gt; &gt;&gt;&gt;   &gt;  &gt;  process.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;=\r\n  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  I don&#39;t think we&#39;ve ever done it with logs of=\r\n 170M crawled / 870M\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  discovered before, nor on any h=\r\nardware comparable to yours.\n&gt; &gt;&gt;&gt; So it&#39;s\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  impossibl=\r\ne to project its duration in your environment. It&#39;s\n&gt; &gt;&gt;&gt;   &gt;  &gt;  taken 2-3=\r\n\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  days for us on smaller crawls, slower hardware.\n&gt; &gt;=\r\n&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  An added complication is that thi=\r\ns older frontier-recover-log\n&gt; &gt;&gt;&gt; replay\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  technique =\r\nhappens in its own thread separate from the\n&gt; &gt;&gt;&gt; checkpointing\n&gt; &gt;&gt;&gt;   &gt;  =\r\n&gt;  &gt;  &gt;  &gt;  process, so it is not, itself, accurately checkpointed during t=\r\nhe\n&gt; &gt;&gt;&gt;   &gt;  &gt;  long\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  reload process.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  At nearly 1B discovered URIs per node, even =\r\nif you are using the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  alternate BloomUriUniqFilter, i=\r\nf you are using it at its\n&gt; &gt;&gt;&gt; default size\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  (~500MB=\r\n) it will now be heavily saturated and thus returning many\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt; =\r\n &gt;  &gt;  false-positives causing truly unique URIs to be rejected as\n&gt; &gt;&gt;&gt;   =\r\n&gt;  &gt;  &gt;  &gt;  &gt;  duplicates. (If you&#39;re using a significantly larger filter,\n=\r\n&gt; &gt;&gt;&gt; you may\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  not yet be at a high false-positive ra=\r\nte: you&#39;d have to do\n&gt; &gt;&gt;&gt; the bloom\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  filter math. If=\r\n you&#39;re still using BdbUriUniqFilter, you&#39;re\n&gt; &gt;&gt;&gt; way way\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt; =\r\n &gt;  &gt;  past the point where its disk seeks have usually made it too\n&gt; &gt;&gt;&gt; s=\r\nlow for\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  our purposes.)\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;  =\r\n &gt;  &gt;  &gt;  &gt;  &gt;&gt;  2) What can be done to avoid corrupting the Heritrix insta=\r\nnces?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  - What kind of strateg=\r\nies might we take to keep the crawl error\n&gt; &gt;&gt;&gt;   &gt;  &gt;  free?\n&gt; &gt;&gt;&gt;   &gt;  &gt; =\r\n &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  - Do you think the SEVER errors that we h=\r\nave seen are\n&gt; &gt;&gt;&gt;   &gt;  &gt;  deterministic or\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  random =\r\n(e.g., triggered by occasional flaky network conditions,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  disk=\r\ns,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  race conditions, or whatever)?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  =\r\n&gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  Hard to say. The main thing I could suggest is =\r\nwatch very\n&gt; &gt;&gt;&gt; closely and\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  when a SEVERE error occ=\r\nurs, prioritize diagnosing and\n&gt; &gt;&gt;&gt; resolving the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  c=\r\nause while the info is fresh.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt; =\r\n - Do you believe that we can reliably backup to the previous\n&gt; &gt;&gt;&gt;   &gt;  &gt; =\r\n checkpoint\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  if we watch the logs and stop as soon a=\r\ns we see the first SEVER\n&gt; &gt;&gt;&gt;   &gt;  &gt;  error?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  If we=\r\n do this, do you speculate that the same SEVER will occur\n&gt; &gt;&gt;&gt;   &gt;  &gt;  aga=\r\nin?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  Resuming from the latest c=\r\nheckpoint before an error believed to\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  corrupt the on=\r\n-disk state will be the best strategy.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  &gt;  If we never figure out the real cause, but run the same\n&gt; &gt;&gt;&gt; soft=\r\nware on\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  the same machine, yes, I expect the same pro=\r\nblem will recur!\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  - Is there a=\r\nny reason why a Heritrix instance that is run while\n&gt; &gt;&gt;&gt;   &gt;  &gt;  binded\n&gt; =\r\n&gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  to one ip address can&#39;t be resumed binded to a differ=\r\nent ip\n&gt; &gt;&gt;&gt; address?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  Only the=\r\n web UI to my knowledge binds to a chosen address,\n&gt; &gt;&gt;&gt; and it is\n&gt; &gt;&gt;&gt;   =\r\n&gt;  &gt;  &gt;  &gt;  &gt;  common to have it bind to all. I don&#39;t expect the outbound\n&gt;=\r\n &gt;&gt;&gt; requests\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  would be hurt by a machine changing it=\r\ns IP address while the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  crawl was\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  runni=\r\nng, but I would run a test to be sure if that was an\n&gt; &gt;&gt;&gt; important,\n&gt; &gt;&gt;&gt;=\r\n   &gt;  &gt;  &gt;  &gt;  &gt;  expected transition.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  &gt;&gt;  3) Should we configure the crawler with more instances and\n&gt; &gt;&gt;&gt; =\r\nswitch\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  between them?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;  =\r\n &gt;  &gt;  &gt;  &gt;  &gt;&gt;  We have seen that we can run a single instance to 100M pag=\r\nes +\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  supporting images and documents. Perhaps this =\r\nmeans that we need\n&gt; &gt;&gt;&gt;   &gt;  &gt;  10 or\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  more instanc=\r\nes instead of 5. That raises the possibility of\n&gt; &gt;&gt;&gt;   &gt;  &gt;  running 2\n&gt; &gt;=\r\n&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  instances per machine. If we could run 2, or even 4,\n&gt;=\r\n &gt;&gt;&gt; instances on a\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  single machine, they would each=\r\n run half as long.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  I don&#39;t thi=\r\nnk the problems as reported are specifically due\n&gt; &gt;&gt;&gt; to one\n&gt; &gt;&gt;&gt;   &gt;  &gt; =\r\n &gt;  &gt;  &gt;  node&#39;s progress growing beyond a certain size, but it might\n&gt; &gt;&gt;&gt;=\r\n be the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  case that giant instances are more likely to=\r\n suffer from, and\n&gt; &gt;&gt;&gt; harder\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  to recover from, sing=\r\nle glitches (eg a single disk error). On the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  other h=\r\nand, many instances introduce more redundant overhead\n&gt; &gt;&gt;&gt; costs\n&gt; &gt;&gt;&gt;   &gt;=\r\n  &gt;  &gt;  &gt;  &gt;  (certain data structures, cross-feeding URIs if you&#39;re doing\n=\r\n&gt; &gt;&gt;&gt;   &gt;  &gt;  that, etc.).\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  - =\r\nCan you suggest a way to start/stop instances from a script so\n&gt; &gt;&gt;&gt;   &gt;  &gt;=\r\n  we can\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  change between instances automatically?\n&gt; =\r\n&gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  Not a mode I&#39;ve thought much abo=\r\nut.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  - Have you seen frequent =\r\nstarting / stopping of instances\n&gt; &gt;&gt;&gt; introduce\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  in=\r\nstability?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  No... but it might =\r\nmake you notice latent issues sooner.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;=\r\n  &gt;  &gt;&gt;  4) Crawl slows but restarting seems to improve the speed again.\n&gt; =\r\n&gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  We noticed that the all of our=\r\n instances would initially run at\n&gt; &gt;&gt;&gt;   &gt;  &gt;  a fast\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  =\r\n&gt;&gt;  pace. We would collect an average of 25M + pages/day for 2-3\n&gt; &gt;&gt;&gt;   &gt; =\r\n &gt;  days and\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  then the crawl would slow down to 10M =\r\npages/day over the next\n&gt; &gt;&gt;&gt;   &gt;  &gt;  few days.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  (th=\r\nese numbers are totals of all 5 instances combined). When we\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  &gt;&gt;  restarted the instances, the average pages would improve back to\n=\r\n&gt; &gt;&gt;&gt;   &gt;  &gt;  25M +\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  pages/day. The total crawled nu=\r\nmbers (TiB) also reflected the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  slow down.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt; =\r\n &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  - Is this something that others have experience=\r\nd as well?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  I don&#39;t recall hear=\r\ning other reports of speed boosts after\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  checkpoint-r=\r\nesumes but others may have more experience.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;=\r\n  &gt;  &gt;  &gt;  &gt;&gt;  5) We are capturing tweets from twitter, harvesting the urls=\r\n and\n&gt; &gt;&gt;&gt;   &gt;  &gt;  want to\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  crawl those urls within =\r\n1 day of receiving the tweet. Can you\n&gt; &gt;&gt;&gt;   &gt;  &gt;  recommend\n&gt; &gt;&gt;&gt;   &gt;  &gt; =\r\n &gt;  &gt;  &gt;&gt;  a strategy for doing this with the 5 instances we are running?\n&gt;=\r\n &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  - Do we need to run a separat=\r\ne crawler dedicated to this? If so,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  can you\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  =\r\n&gt;  &gt;&gt;  suggest a way to crawl out from the tweeted urls but when we get\n&gt; &gt;=\r\n&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  additional urls from the tweets, quickly change focus =\r\nto\n&gt; &gt;&gt;&gt; these urls\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  instead of the ones branching o=\r\nut. When adding urls as seeds,\n&gt; &gt;&gt;&gt;   &gt;  &gt;  can you\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;=\r\n  set a high priority to crawl those before the discovered urls?\n&gt; &gt;&gt;&gt;   &gt; =\r\n &gt;  Do you\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  recommend maybe setting up a specific cr=\r\nawl for these urls and\n&gt; &gt;&gt;&gt;   &gt;  &gt;  then only\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  craw=\r\nl a few hopes from the seeds - injecting the urls from the\n&gt; &gt;&gt;&gt;   &gt;  &gt;  tw=\r\neets as\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  seeds?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  &gt;  Dedicating a special script or crawler to URIs that come from\n&gt; &gt;&gt;=\r\n&gt; such a\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  constrained source (Twitter feeds), or that=\r\n need to be\n&gt; &gt;&gt;&gt; crawled in a\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  special timeframe, or=\r\n according to other special limits\n&gt; &gt;&gt;&gt; (fewer hops),\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  =\r\n&gt;  could make sense.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  It would =\r\ntake some customization of the queueing-policy or\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  &#39;p=\r\nrecedence&#39; features of Heritrix to allow URIs added\n&gt; &gt;&gt;&gt; mid-crawl to be\n&gt;=\r\n &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  prioritized above those already discovered and queued=\r\n. The most\n&gt; &gt;&gt;&gt;   &gt;  &gt;  simple\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  possible customizati=\r\non might be a UriPrecedencePolicy that\n&gt; &gt;&gt;&gt; takes all\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  =\r\n&gt;  zero-hop URIs (which all seeds and most direct-fed URIs would\n&gt; &gt;&gt;&gt; be) =\r\nand\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  gives them a higher precedence (lower precedence=\r\n number) than all\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  other URIs.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n=\r\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  6) I think the answer is no for this question, but =\r\nI will ask it\n&gt; &gt;&gt;&gt;   &gt;  &gt;  anyway.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  If you have a H=\r\neritrix instance that is configured for 1200\n&gt; &gt;&gt;&gt;   &gt;  &gt;  threads on\n&gt; &gt;&gt;&gt;=\r\n   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  one machine, can you recover from a checkpoint from that=\r\n\n&gt; &gt;&gt;&gt; 1200 thread\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;  configuration on a different mac=\r\nhine with an Heritrix instance\n&gt; &gt;&gt;&gt;   &gt;  &gt;  that is\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;&gt;=\r\n  configured for less threads (e.g. the default 25 threads)?\n&gt; &gt;&gt;&gt;   &gt;  &gt;  =\r\n&gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  Yes - there&#39;s no need to keep the thread cou=\r\nnt the same after a\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  resume. None of the checkpoint s=\r\ntructures (or usual disk\n&gt; &gt;&gt;&gt; structures)\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  are based=\r\n on the number of worker threads (&#39;ToeThreads&#39;)... as\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;=\r\n  mentioned above you can even vary the number of threads in a\n&gt; &gt;&gt;&gt; runnin=\r\ng\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;  crawl.\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;=\r\n  - Gordon\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;  &gt;  &gt;\n&gt; &gt;&gt;=\r\n&gt;   &gt;  &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;  &gt;\n&gt; &gt;&gt;&gt;   &gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; =\r\n&gt;\n&gt; &gt;\n&gt; &gt; ------------------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n=\r\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}