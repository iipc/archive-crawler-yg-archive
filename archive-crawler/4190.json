{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":308001930,"authorName":"Cetin Sert","from":"&quot;Cetin Sert&quot; &lt;cetin.sert@...&gt;","profile":"cetinsert","replyTo":"LIST","senderId":"Qk_BNjfnz1OByOqS9r1QXSVNLeK1fvZIyBE5T-K3Ht5DC5j7Aaihoaad4gjBpeXlquWF1BrV3knzdKEsXboJZ6Z4ffzae6uCbnI","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: New duplication-reduction features in 1.12 and later versions","postDate":"1177601126","msgId":4190,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYwcWc5NitxbXFkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGYwbjF1bCt1a2s1QGVHcm91cHMuY29tPg=="},"prevInTopic":4177,"nextInTopic":4218,"prevInTime":4189,"nextInTime":4191,"topicId":4166,"numMessagesInTopic":4,"msgSnippet":"... use ... the ... is ... Have you been able to figure out where TrapSuppressExtractor is to be used? I have read and followed your instructions, which were","rawEmail":"Return-Path: &lt;cetin.sert@...&gt;\r\nX-Sender: cetin.sert@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 68667 invoked from network); 26 Apr 2007 15:26:57 -0000\r\nReceived: from unknown (66.218.66.72)\n  by m44.grp.scd.yahoo.com with QMQP; 26 Apr 2007 15:26:57 -0000\r\nReceived: from unknown (HELO n27.bullet.scd.yahoo.com) (66.94.237.56)\n  by mta14.grp.scd.yahoo.com with SMTP; 26 Apr 2007 15:26:56 -0000\r\nReceived: from [66.218.69.2] by n27.bullet.scd.yahoo.com with NNFMP; 26 Apr 2007 15:25:26 -0000\r\nReceived: from [66.218.66.84] by t2.bullet.scd.yahoo.com with NNFMP; 26 Apr 2007 15:25:26 -0000\r\nDate: Thu, 26 Apr 2007 15:25:26 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;f0qg96+qmqd@...&gt;\r\nIn-Reply-To: &lt;f0n1ul+ukk5@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Cetin Sert&quot; &lt;cetin.sert@...&gt;\r\nSubject: Re: New duplication-reduction features in 1.12 and later versions\r\nX-Yahoo-Group-Post: member; u=308001930; y=J8FSDFGkGOd8okYrvuWoaz0cVbD1uk3hB4Nn68mgff-oHavR\r\nX-Yahoo-Profile: cetinsert\r\n\r\n&gt; 2.4.) To minimize the content processing and link extraction you can\nuse\n=\r\n&gt; TrapSuppressExtractor.\n&gt; If you crawled URL [A] with content digest of [1=\r\n23] and just fetched\nthe\n&gt; URL [B] with content digest of [123],\n&gt; then thi=\r\ns processor disables link extraction so no further processing\nis\n&gt; done. (t=\r\nhe links from URL [B] are never promoted back to frontier)\n&gt; (I haven&#39;t tes=\r\nted this one yet)\n\nHave you been able to figure out where TrapSuppressExtra=\r\nctor is to be\nused?\n\nI have read and followed your instructions, which were=\r\n really much\nclearer\nthan the wiki page. It is surprising to have someone o=\r\nffer such a\nguidance. I\nthank you for your e-mail.\n\nI was wondering in what=\r\n context you might be using Heritrix and how come\nyou are so knowledgeable =\r\nabout it.\n\nI was commissioned by a research institute to crawl the web to b=\r\nuild a\ncorpus\nand that&#39;s how I came in contact with Heritrix.\n\nBest Regards=\r\n,\nCetin Sert\n\n--- In archive-crawler@yahoogroups.com, &quot;ivlcic&quot; &lt;ivlcic@...&gt;=\r\n wrote:\n&gt;\n&gt; I&#39;ve managed to make some tests and here is how I see it.\n&gt; Ple=\r\nase note that I&#39;m just a Heritrix beginner and I had the same wiki\n&gt; page a=\r\ns reference.\n&gt;\n&gt; 1.) Setting up the processors\n&gt; 1.1.) I&#39;ve put on top of t=\r\nhe fetch-processors list the:\n&gt; org.archive.crawler.processor.recrawl.Persi=\r\nstLoadProcessor\n&gt; This processor loads all previous CrawlURL&#39;s pressistent =\r\nAList data\nfrom\n&gt; the crawl state directory. The most important is the cont=\r\nent digest\nfrom\n&gt; previous crawls associated with CrawlURL\n&gt;\n&gt; 1.2.) On the=\r\n bottom of the fetch-processors I&#39;ve put the:\n&gt; org.archive.crawler.process=\r\nor.recrawl.FetchHistoryProcessor\n&gt; This processor is responsible form maint=\r\naining the history list of\n&gt; CrawlURLs from previous crawls including the c=\r\nurrent CrawlURL.\n&gt; Thats why we are supposed to have the minimum of two. At=\r\n least one\n&gt; slot for previous and one slot for current.\n&gt;\n&gt; 1.3.) At the e=\r\nnd of whole chain (post-processors) I&#39;ve put the:\n&gt; org.archive.crawler.pro=\r\ncessor.recrawl.PersistStoreProcessor\n&gt; This processor actually stores the C=\r\nrawlURLs to Berkely DB in to state\n&gt; directory of the crawl.\n&gt;\n&gt;\n&gt; 2.) Usin=\r\ng the processors\n&gt;\n&gt; 2.1.) In order to use the current setup (As far as I g=\r\net it) we have\nto\n&gt; maintain the same state directory for each crawl since =\r\nthe BDB files\nin\n&gt; it have also a history of previous crawls.\n&gt; Enable the =\r\nview of expert settings in web UI and set up the crawl\norder\n&gt; state-path a=\r\nttribute to always point\n&gt; to the same directory for all the jobs.\n&gt;\n&gt; 2.2.=\r\n) To disable writing the duplicate pages I&#39;ve set the attribute\n&gt; skip-iden=\r\ntical-digests=3Dtrue on the\n&gt; org.archive.crawler.writer.ARCWriterProcessor=\r\n\n&gt;\n&gt; To achive the same thing on processors that do not have such attribute=\r\n\n&gt; I&#39;ve used the\n&gt; org.archive.crawler.deciderules.recrawl.IdenticalDigestD=\r\necideRule\nfilter\n&gt; setting it to REJECT.\n&gt; (org.archive.crawler.writer.Mirr=\r\norWriterProcessor is such processor)\n&gt;\n&gt; You should also maintain the same =\r\n&quot;mirror&quot; or &quot;arcs&quot; store path\nbetween\n&gt; crawls.\n&gt; If you don&#39;t you will get=\r\n only the &quot;changed pages&quot;\n&gt; (changed pages are those with different content=\r\n digests and/or etag,\n&gt; modified since headers)\n&gt;\n&gt; 2.3.) To prevent the fe=\r\ntching of the same CrawlURLs for the following\n&gt; crawls I&#39;ve set up\n&gt; send-=\r\nif-modified-since=3Dtrue\n&gt; send-if-none-match=3Dtrue\n&gt; on the org.archive.c=\r\nrawler.fetcher.FetchHTTP.\n&gt; This only works for static pages / files (send-=\r\nif-modified-since) or\n&gt; with dynamic pages that support the\n&gt; etag HTTP res=\r\nponse header (send-if-none-match). Those are the most\n&gt; common options that=\r\n you can use to decide\n&gt; prior to fetching the whole content.\n&gt;\n&gt; 2.4.) To =\r\nminimize the content processing and link extraction you can\nuse\n&gt; TrapSuppr=\r\nessExtractor.\n&gt; If you crawled URL [A] with content digest of [123] and jus=\r\nt fetched\nthe\n&gt; URL [B] with content digest of [123],\n&gt; then this processor=\r\n disables link extraction so no further processing\nis\n&gt; done. (the links fr=\r\nom URL [B] are never promoted back to frontier)\n&gt; (I haven&#39;t tested this on=\r\ne yet)\n&gt;\n&gt;\n&gt; 3.) alternatives\n&gt; Instead of org.archive.crawler.processor.re=\r\ncrawl.PersistLoadProcessor\n&gt; you can use the\n&gt; org.archive.crawler.processo=\r\nr.recrawl.PersistLogProcessor that uses\ntxt\n&gt; file for history\n&gt; (don&#39;t use=\r\n both)\n&gt;\n&gt; If you want to minimize the fetch duplication between crawls the=\r\n only\n&gt; thing you can do is mimic the\n&gt; the org.archive.crawler.fetcher.Fet=\r\nchHTTP.send-if-none-match and\nhandle\n&gt; some specific\n&gt; HTTP headers that ta=\r\nrget hosts return.\n&gt; You can store specific data to CrawlURLs persistent AL=\r\nist.\n&gt; (I guess Heritrix people didn&#39;t do it because there is no standard. =\r\nI\n&gt; haven&#39;t tested this one yet also)\n&gt;\n&gt; Nikola\n&gt;\n&gt; --- In archive-crawler=\r\n@yahoogroups.com, &quot;Cetin Sert&quot; cetin.sert@\n&gt; wrote:\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;=\r\n\n&gt; &gt; I would love to know how to use the new duplicate-reduction\nfeatures.\n=\r\n&gt; The\n&gt; &gt; wiki page about them is too short to understand and there are man=\r\ny\nnew\n&gt; &gt; processors etc., which makes it difficult for me to understand ho=\r\nw\n&gt; best to\n&gt; &gt; combine them.\n&gt; &gt;\n&gt; &gt; B. Abbrieviate retrieval of unchanged=\r\n content.\n&gt; &gt;\n&gt; &gt; Based on headers of previous retrieval of same URI, adjus=\r\nt current\n&gt; retrieval\n&gt; &gt; (via conditional-GET requests) so that if content=\r\n is unchanged,\n&gt; retrieval\n&gt; &gt; ends quickly and cheaply and no duplicate co=\r\nntent is stored.\nMotivated\n&gt; by\n&gt; &gt; KB-Denmark study (Clausen 2004); reduce=\r\ns bandwidth used as well as\n&gt; storage.\n&gt; &gt;\n&gt; &gt; This is accomplished in Heri=\r\ntrix 1.12.0 by:\n&gt; &gt;\n&gt; &gt; * Using the FetchHistoryProcessor and either Persis=\r\ntLogProcessor or\n&gt; &gt; PersistStoreProcessor on an initial crawl1\n&gt; &gt; * Carry=\r\ning forward the initial crawl info in a form accessible to a\n&gt; &gt; later craw=\r\nl, perhaps by using utility functionality on the\n&gt; PersistProcessor\n&gt; &gt; cla=\r\nss2\n&gt; &gt; * Using the PersistLoadProcessor and FetchHistoryProcessor in a\n&gt; &gt;=\r\n following crawl3 so that relevant history information is available\nat\n&gt; &gt; =\r\nstore-decision time3\n&gt; &gt; * Using new capabilities of the FetchHTTP processo=\r\nr to issue\n&gt; &gt; conditional-GETs where appropriate\n&gt; &gt; * Using new options o=\r\nn any writer processors to skip or abbrieviate\n&gt; &gt; storage as desired4\n&gt; &gt;\n=\r\n&gt; &gt; C. Note and discard duplicate &#39;trap&#39; content from same crawl.\n&gt; &gt;\n&gt; &gt; F=\r\nor one simple kind of crawler-trap, where followup URIs return\n&gt; identical\n=\r\n&gt; &gt; content at different (extended) URIs, disable link-extraction.\n&gt; &gt;\n&gt; &gt; =\r\nThe TrapSuppressExtractor in 1.12.0 demonstrates this strategy, and\n&gt; could=\r\n be\n&gt; &gt; the basis for other similar trap/duplicate suppression processors.5=\r\n\n&gt; &gt;\n&gt; &gt; 1: where exactly? Is this a preprocessor, a postprocessor or a\nwri=\r\nter\n&gt; &gt; module?\n&gt; &gt;\n&gt; &gt; 2: where and how?\n&gt; &gt;\n&gt; &gt; 3: where and how?\n&gt; &gt;\n&gt; &gt;=\r\n 4: are these options readily implemented or should we implement them\n&gt; &gt; o=\r\nurselves? If they are already there, then how do we use them?\n&gt; &gt;\n&gt; &gt; 5: ho=\r\nw?\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; A configuration xml file (order.xml) with all the settin=\r\ngs enabled,\nor\n&gt; a few\n&gt; &gt; screenshots of the web interface with the proces=\r\nsors at place might\nbe\n&gt; of\n&gt; &gt; great help.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; I have had expe=\r\nrience with implementing custom decision rules and\n&gt; writers\n&gt; &gt; but this s=\r\neems to require some explanation.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Thanks in advance to anyo=\r\nne who takes time to answer my question.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yours Sincerely,\n&gt;=\r\n &gt;\n&gt; &gt; Cetin Sert\n&gt; &gt;\n&gt;\n\n\n\n"}}