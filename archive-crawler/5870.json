{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"fYbczju-ek_bSPUemyJoqJ5M3x4g49ItHQ7uf0sKuZPMbXbUqlaHi-QluzdFvjgxtvHqAN4BWKo-CJkAojzTZbkYcq7vELQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Limitations on the number of Jobs per instance of Heretrix ?","postDate":"1243640161","msgId":5870,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBMjA3MTYxLjEwMzA2MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEyNDM1OTQxMTAuNjg3NS40OC5jYW1lbEBib2hyPg==","referencesHeader":"PDEyNDM1OTQxMTAuNjg3NS40OC5jYW1lbEBib2hyPg=="},"prevInTopic":5866,"nextInTopic":5872,"prevInTime":5869,"nextInTime":5871,"topicId":5866,"numMessagesInTopic":6,"msgSnippet":"At the Internet Archive, we don t usually split up crawls to one-site-per-job. We prefer fewer jobs, with more seeds/sites. We then rely on postprocessing and","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 72386 invoked from network); 29 May 2009 23:37:04 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m4.grp.re1.yahoo.com with QMQP; 29 May 2009 23:37:04 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.sp2.yahoo.com with SMTP; 29 May 2009 23:37:03 -0000\r\nX-Received: (qmail 85497 invoked from network); 29 May 2009 23:36:02 -0000\r\nX-Received: from 67.170.223.242 (HELO ?192.168.1.84?) (67.170.223.242)\n  by relay03.pair.com with SMTP; 29 May 2009 23:36:02 -0000\r\nX-pair-Authenticated: 67.170.223.242\r\nMessage-ID: &lt;4A207161.1030605@...&gt;\r\nDate: Fri, 29 May 2009 16:36:01 -0700\r\nUser-Agent: Thunderbird 2.0.0.21 (Windows/20090302)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;1243594110.6875.48.camel@bohr&gt;\r\nIn-Reply-To: &lt;1243594110.6875.48.camel@bohr&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Limitations on the number of Jobs per instance\n of Heretrix ?\r\nX-Yahoo-Group-Post: member; u=137285340; y=hpwwrYRmvyTvvqDSvEHThLXhxW7FpKspjo6lRg5RrgTx\r\nX-Yahoo-Profile: gojomo\r\n\r\nAt the Internet Archive, we don&#39;t usually split up crawls to \none-site-per-job. We prefer fewer jobs, with more seeds/sites. We then \nrely on postprocessing and indexing to extract individual sites, or site \nstatistics, later.\n\nSome other groups do a job-per-site, with Heritrix, but it&#39;s not very \neasy to run many jobs simultaneously in the same running instance. (It&#39;s \npossible, but a little tricky -- unless you shrink some of the usual job \nparameters, most importantly in 1.14.x the bdb-cache percentage, \nsimultaneous jobs can exhaust all allocated JVM memory. And I think \nthose who have done so have done more like dozens per instance, rather \nthan hundreds.)\n\nSome potential weaknesses of the one-job-per-site approach:\n\n- offsite inline resources, which our default configuration gets \n(because we want as accurate a rendition of the page as possible), may \nbe duplicated between the jobs -- each has no idea another may have \nalready retrieved that URI\n\n- if there are &#39;deep&#39; areas of a site only discoverable from other \nsites, but not from the home site&#39;s root page, these may not be \ncollected in separate crawls. (In the job they&#39;re discovered, they&#39;re \nout-of-scope; in the job for their own site, they&#39;re not discovered.)\n\n- because of trying to schedule/distribute the jobs, within the \nconstraints of a certain amount of per-job overhead, hardware may often \nbe underutilized\n\nUnless the sites are especially large, a single job on a single powerful \nmachine may be plenty to collect 200K sites. A crawler with (for \nexample) 200 crawling &#39;toe&#39; threads is actually making maximal progress \non far more than 200 sites at once, because while some sites are in \n&#39;politeness wait&#39;, threads continue on other sites with pending URIs. \nAnd since many sites are not large, many crawls finish most of their \ntarget sites in the first hours/days of crawling, with later days/weeks \nonly working on the very deep or very slow sites.\n\nWhat are your main goals in splitting the jobs up?\n\nCan those goals be met by postprocessing, or by creating alternate views \nof a single in-progress job&#39;s logs and reports? (For example, you can \nforce dump larger per-host-summary and frontier-queue reports from a \nrunning crawl.)\n\n- Gordon @ IA\n\nJoel Halbert wrote:\n&gt; Hi,\n&gt; \n&gt; I&#39;m using Heretrix, over Nutch, the documentation seems more robust and\n&gt; I&#39;m not interested in Nutch&#39;s tight integration with Lucene - I want to\n&gt; do quite a lot or pre-processing with the crawl data before indexing it\n&gt; in our own format. So Heretrix seems a good flexible fit in this regard.\n&gt; \n&gt; I want to crawl about 200k unique domains, using DomainScope. I&#39;d like\n&gt; to distribute this over n machines, with each machine running say 100\n&gt; threads - one thread per domain. \n&gt; \n&gt; I&#39;d also like to be able to track & manage the progress of each domain\n&gt; individually. \n&gt; \n&gt; I&#39;d like to do all of this by assigning one job per domain and have a\n&gt; central management process that tracks each heretrix instance, and\n&gt; assigns new jobs as appropriate e.g. as a job on one machine completes,\n&gt; create a new one for another domain - keeping the total number of\n&gt; running jobs in each instance at say 200.\n&gt; \n&gt; The docs generally talk of creating a single job with a seed list. Will\n&gt; I run into any issues within heretrix with having a seedlist of size 1 -\n&gt; and instead having multiple jobs e.g. up to 200?\n&gt; \n&gt; I&#39;m guessing this should all be fine, but just want to make sure there\n&gt; are no hidden gotchas.\n&gt; \n&gt; Thx,\n&gt; \n&gt; Joel\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}