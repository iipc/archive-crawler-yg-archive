{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"lekash","from":"lekash &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"vKd4nVBVVgT0lrGJGYrgsngBQYvBMe7R1ht4jgyV9z40nomw1yZ0IS0I34qY9yKxxTBA3TyuCoTGZ6OPOXrpcUD2","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] how to exclude a domain from a crawl","postDate":"1220900599","msgId":5452,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4QzU3NkY3LjQwNTA3MDlAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PGdhM2Y3NCs0YWVxQGVHcm91cHMuY29tPg==","referencesHeader":"PGdhM2Y3NCs0YWVxQGVHcm91cHMuY29tPg=="},"prevInTopic":5451,"nextInTopic":5836,"prevInTime":5451,"nextInTime":5453,"topicId":5451,"numMessagesInTopic":3,"msgSnippet":"Here you go.  The XML object to not crawl these guys. Pretty straightforward.  Use it, early and often. Where nocrawl-all.surt is the list of do not crawl. ","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 90759 invoked from network); 8 Sep 2008 19:05:56 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m56.grp.scd.yahoo.com with QMQP; 8 Sep 2008 19:05:56 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta16.grp.scd.yahoo.com with SMTP; 8 Sep 2008 19:05:56 -0000\r\nX-Received: from [192.168.0.12] (72.20.109.026.bayarea.net [72.20.109.26])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id m88J5pHb075488\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 8 Sep 2008 12:05:56 -0700 (PDT)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;48C576F7.4050709@...&gt;\r\nDate: Mon, 08 Sep 2008 12:03:19 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.11) Gecko/20050727\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ga3f74+4aeq@...&gt;\r\nIn-Reply-To: &lt;ga3f74+4aeq@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: lekash &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] how to exclude a domain from a crawl\r\nX-Yahoo-Group-Post: member; u=90724651; y=al29cUj8CVYuBbF_cakMSvw2Szj0eLgjscylVAfP5b7V\r\nX-Yahoo-Profile: lekash\r\n\r\n   \nHere you go.  The XML object to not crawl these guys.\nPretty straightforward.  Use it, early and often.\n\nWhere nocrawl-all.surt is the list of do not crawl.\n\ne.g.:\n+http://(com,ebay,\n+http://(com,meetup,\n\n         &lt;newObject name=&quot;nocrawl&quot; \nclass=&quot;org.archive.crawler.deciderules.SurtPrefixedDecideRule&quot;&gt;\n            &lt;string name=&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n            &lt;string \nname=&quot;surts-source-file&quot;&gt;/heritrix/nocrawl-all.surt&lt;/string&gt;\n            &lt;boolean name=&quot;seeds-as-surt-prefixes&quot;&gt;false&lt;/boolean&gt;\n            &lt;string name=&quot;surts-dump-file&quot;&gt;&lt;/string&gt;\n            &lt;boolean name=&quot;also-check-via&quot;&gt;false&lt;/boolean&gt;\n            &lt;boolean name=&quot;rebuild-on-reconfig&quot;&gt;true&lt;/boolean&gt;\n          &lt;/newObject&gt;\n    \n\nrhispm wrote:\n\n&gt; Hello!\n&gt;\n&gt; I tried to find this in the docs but it seems a rather uncommon use\n&gt; case to explicitly exclude some domain (including all subdomains) from\n&gt; a crawl. Any hint on how one could do so?\n&gt;\n&gt; cheers\n&gt; --pm\n&gt;\n&gt; p.s. the FAQ entry #9\n&gt; (http://crawler.archive.org/faq.html#crawllogstatuscodes \n&gt; &lt;http://crawler.archive.org/faq.html#crawllogstatuscodes&gt;) seems to\n&gt; link to a blank page?\n&gt; (http://crawler.archive.org/articles/user_manual.html#statuscodes \n&gt; &lt;http://crawler.archive.org/articles/user_manual.html#statuscodes&gt;)\n&gt;\n&gt;  \n\n\n"}}