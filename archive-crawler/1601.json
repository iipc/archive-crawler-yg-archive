{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":7083574,"authorName":"billo_ga","from":"&quot;billo_ga&quot; &lt;billo@...&gt;","profile":"billo_ga","replyTo":"LIST","senderId":"bQoG1X5dmC2AMv1FTFhGyugOegwRu2oAHtKroViWF2-1D_vvjS8avgKCYkqzjbXHaV3eAcCnJllxvU48SvaJLo1rxA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Newbie question -- I should be more specific","postDate":"1109077502","msgId":1601,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGN2ZmFsdStwYmhnQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQyMTlEQUY2LjkwNzAzMDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":1596,"nextInTopic":1602,"prevInTime":1600,"nextInTime":1602,"topicId":1588,"numMessagesInTopic":7,"msgSnippet":"... (You probably mean the latter because you understand that the ... You re right.  It s the latter I m interested in. ... Not as large as you might think.","rawEmail":"Return-Path: &lt;billo@...&gt;\r\nX-Sender: billo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 71083 invoked from network); 22 Feb 2005 13:05:55 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m25.grp.scd.yahoo.com with QMQP; 22 Feb 2005 13:05:55 -0000\r\nReceived: from unknown (HELO n14a.bulk.scd.yahoo.com) (66.94.237.28)\n  by mta1.grp.scd.yahoo.com with SMTP; 22 Feb 2005 13:05:54 -0000\r\nReceived: from [66.218.69.5] by n14.bulk.scd.yahoo.com with NNFMP; 22 Feb 2005 13:05:03 -0000\r\nReceived: from [66.218.66.70] by mailer5.bulk.scd.yahoo.com with NNFMP; 22 Feb 2005 13:05:03 -0000\r\nDate: Tue, 22 Feb 2005 13:05:02 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;cvfalu+pbhg@...&gt;\r\nIn-Reply-To: &lt;4219DAF6.9070303@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 3455\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.28\r\nFrom: &quot;billo_ga&quot; &lt;billo@...&gt;\r\nSubject: Re: Newbie question -- I should be more specific\r\nX-Yahoo-Group-Post: member; u=7083574\r\nX-Yahoo-Profile: billo_ga\r\n\r\n\n--- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n  (You probably mean the latter because you understand that the \n&gt; crawler has to download all pages first before it can see which pages \n&gt; have &#39;acre&#39; or &#39;juniper&#39; in them).\n\n\n\nYou&#39;re right.  It&#39;s the latter I&#39;m interested in.\n\n&gt; \n&gt; See more below.\n&gt; \n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt; I should probably be more specific here.  I am a forensic pathologist,\n&gt; &gt; and am interested in searching various sites for information and\n&gt; &gt; imagery involving patterned injury of the skin.  I am, for instance,\n&gt; &gt; writing an atlas of patterned injury in order to help people determine\n&gt; &gt; what object was used in an assault (hammer, crowbar, etc.) from the\n&gt; &gt; patterns those objects leave on the skin.\n&gt; \n&gt; Interesting.  I&#39;m guessing it&#39;ll be a large volume.\n&gt; \n\n\nNot as large as you might think.  In fact, there&#39;s a fair amount out\nthere, but it is sparsely distributed -- most general pathology\narchives, for instance, will have one or two images of my forensic\ninterest out of a hundred or thousand images.  More often, I will find\none image in an article about a particular subject.  I will search 100\nweb pages dealing with acid burns, for instance, of which 30 will have\nimages, 5 will have *good* images, and 1 or 2 will have images that\nare either unencumbered with copyright issues or have authors\ninterested in contributing to the interactive atlas.\n\n\n\n&gt; \n&gt; Here&#39;s a sketch of one way in which I could imagine it working.\n&gt; \n&gt; Run the crawler against the sites you are interested in.  Then, after \n&gt; the crawl has completed, feed the downloaded ARCs to a search engine so \n&gt; you can run your &#39;acre&#39; and &#39;juniper&#39; queries.  There is quite a bit of \n&gt; work involved here -- parsing ARC files, feeding each ARC record to a \n&gt; mimetype-particular parser (i.e. an html parser for the text/html),\nthen \n&gt; passing the extracted text to a search engine indexer, etc. -- but the \n&gt; good news here is that you should be able to leverage the work begun \n&gt; here, \n&gt;\nhttp://cvs.sourceforge.net/viewcvs.py/*checkout*/archive-access/archive-access/projects/nutch/README.txt?rev=1,\n\n&gt; which has tools to feed ARC files to nutch (Whats there has been tried \n&gt; on 40million plus text/html pages.  The quality of the searches is \n&gt; lacking but is currently being worked on).\n\n\nThanks for the pointer!  My problem is not as much in evaluating a\nsite once I find it -- though for large image libraries that *can* be\na problem. My problem is finding sites that discuss it in the first\nplace.  I&#39;m hoping for a tool that will follow links from site to site\nlooking for images/keywords/spoor that suggests a site fo interest.\n\n\n\n&gt; Are the domains you are interested in large?  If they are, running a \n&gt; large crawl and running the downloaded crawl via an indexer is a \n&gt; significant adminstrative task requiring ample hardware (disk, cpu).  \n&gt; For example, feeding the 40million plus above mentioned pages to nutch \n&gt; to index took 4 machines running multiple days (Speed is also being \n&gt; worked on in the aforementioned archive-access nutch project).\n&gt; \n\nWell, I have a bunch of older machines sitting around and I am playing\nwith setting up a small grid (meaning I&#39;ve read a couple of books and\nam futzing around, but only have a few tutorial apps working).  But\nwhether I get the grid running right or not, I&#39;m quite willing to\ndevote four or five boxes to this for a long time -- I&#39;m not up \nagainst a hard deadline yet.\n\nbillo\n\n\n\n\n"}}