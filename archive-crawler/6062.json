{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"5Ff_pbBXoX76fDS_5g4sTQRwHA47i5eGBwmlkl-TKwxalcAUQRVSdSPMESpro2o_dswTYgiD0PVGNYKTWqilSVamkVKFqUM","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Re: Basic question: How to limit the crawling scope within a host?","postDate":"1254158595","msgId":6062,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBQzBGMTAzLjgwNjA3MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGg5cG5yaStzdHJAZUdyb3Vwcy5jb20+","referencesHeader":"PGg5cG5yaStzdHJAZUdyb3Vwcy5jb20+"},"prevInTopic":6059,"nextInTopic":0,"prevInTime":6061,"nextInTime":6063,"topicId":6051,"numMessagesInTopic":6,"msgSnippet":"... http://www.cs.cmu.edu was only in seeds because that was the way you started your crawl setup. As I d mentioned, it might make more sense to start","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 33320 invoked from network); 28 Sep 2009 17:23:50 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m1.grp.sp2.yahoo.com with QMQP; 28 Sep 2009 17:23:50 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta1.grp.re1.yahoo.com with SMTP; 28 Sep 2009 17:23:50 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 1E2013D07C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 28 Sep 2009 10:24:27 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id wGSN9aGDvcAW for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 28 Sep 2009 10:24:25 -0700 (PDT)\r\nX-Received: from [10.0.13.17] (unknown [70.137.138.250])\n\tby mail.archive.org (Postfix) with ESMTPSA id 3E66B356FB\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 28 Sep 2009 10:24:25 -0700 (PDT)\r\nMessage-ID: &lt;4AC0F103.8060702@...&gt;\r\nDate: Mon, 28 Sep 2009 10:23:15 -0700\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;h9pnri+str@...&gt;\r\nIn-Reply-To: &lt;h9pnri+str@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Basic question: How to limit the crawling\n scope within a host?\r\nX-Yahoo-Group-Post: member; u=137285340; y=KGa0CMGLjtAMX29tYe_-SSQrq_UI6NUhyo4-e8LoOEpJ\r\nX-Yahoo-Profile: gojomo\r\n\r\nshichuanwuhan@... wrote:\n&gt; Thank you Gordon. I follow your instruction and some homepages can be crawled now. But why we put &#39;http://www.cs.cmu.edu&#39; in seeds and &#39;http://people.cs.cmu.edu/faculty/index.html&#39; in surts-source-file? What if I change their position?\n\n&#39;http://www.cs.cmu.edu&#39; was only in seeds because that was the way you \nstarted your crawl setup. As I&#39;d mentioned, it might make more sense to \nstart crawling with the &#39;http://people.cs.cmu.edu/faculty/index.html&#39; \nURI as a seed.\n\nIf that were your seed, and &#39;http://www.cs.cmu.edu&#39; your SURT-prefix \nextension, then the crawl would start on the seed, but follow outlinks \nto the &#39;www.cs.cmu.edu&#39; host.\n\nUltimately, because I suspect the &#39;www.cs.cmu.edu&#39; host is highly \ninternally-linked, the end result might be all of &#39;www.cs.cmu.edu&#39; is \ncrawled in any case. (All it takes is one personal homepage to link to \nthe root page.)\n\nIf you only want to crawl pages fitting some homepage-like pattern, you \ncould add additional restrictive rules, or use a SURT-prefix that does \nnot allow for the entire &#39;www.cs.cmu.edu&#39; site to be crawled.\n\nFor example, adding this exact SURT-prefix (instead of the full \n&#39;www.cs.cmu.edu&#39; prefix):\n\n+http://(edu,cmu,cs,www,)/~\n\n...will only add allowance for URIs that begin exactly \n&quot;http://www.cs.cmu.edu/~&quot;.\n\n- Gordon @ IA\n\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; shichuanwuhan@... wrote:\n&gt;&gt;&gt; Dear Gordon,\n&gt;&gt;&gt;\n&gt;&gt;&gt;     Thanks a lot. Your answer gives me a deeper understanding of Heritrix. \n&gt;&gt;&gt;       \n&gt;&gt;&gt;     But I still can&#39;t configure it correctly following your instruction so I turn to you for help. Sorry about that :)\n&gt;&gt;&gt;\n&gt;&gt;&gt;     1.Is it optional to use &#39;surts-source-file&#39;? I add the URL:&#39;+http://people.cs.cmu.edu/faculty/index.html&#39; in the &#39;seeds&#39; table. Is it enough?\n&gt;&gt; It is optional, if you want to specify a lot of SURT prefixes.\n&gt;&gt;\n&gt;&gt; The one &#39;+&#39; directive, added to the seeds, is enough for a crawl that \n&gt;&gt; starts on &#39;www.cs.cmu.edu&#39; to follow discovered links to \n&gt;&gt; &#39;people.cs.cmu.edu&#39;.\n&gt;&gt;\n&gt;&gt;&gt;     2. When I want to use &#39;surts-source-file&#39;, I don&#39;t know where to put it on my disk. I am using Windows XP. I try to put it in the same directory with &#39;seeds.txt&#39;, but it doesn&#39;t work.\n&gt;&gt; The path entered here is interpreted relative to the job directory, \n&gt;&gt; where seeds.txt is, so that should work. How do you know it&#39;s not \n&gt;&gt; finding the file? Is there an error?\n&gt;&gt;\n&gt;&gt;&gt;     3.Would you please give me an example of writing a correct SURT to\n&gt;&gt;&gt; crawl all html files under &#39;http://people.cs.cmu.edu/faculty/&#39;?\n&gt;&gt; I tried a crawl based on the bundled &#39;deciding-default&#39;, with the seed \n&gt;&gt; &#39;http://www.cs.cmu.ed&#39; and the added directive \n&gt;&gt; &#39;+http://people.cs.cmu.edu/faculty/index.html&#39;. It found the \n&gt;&gt; &#39;http://people.cs.cmu.edu/faculty/index.html&#39; URI from the main site.\n&gt;&gt;\n&gt;&gt; The very first faculty URI listed on that page on &#39;www.cs.cmu.edu&#39; (as \n&gt;&gt; opposed to some other host like &#39;www-2&#39;) is \n&gt;&gt; &#39;http://www.cs.cmu.edu/~dga/&#39;. After a short while, my test crawl \n&gt;&gt; fetched this URI.\n&gt;&gt;\n&gt;&gt; So if this is not working for you, it&#39;s due to some other change you&#39;ve \n&gt;&gt; made to the default configuration. Also, if you want to get pages on \n&gt;&gt; other hostnames, like &#39;www-2.cs.cmu.edu&#39;, you will have to add \n&gt;&gt; additional directives.\n&gt;&gt;\n&gt;&gt;&gt; Here is my crawl order:\n&gt;&gt;&gt;\n&gt;&gt;&gt; 17 Admin 20090927015119 settings logs checkpoints state scratch 0 0 0 100 4096 65536 0 true seeds.txt true ACCEPT true result.txt false true Mozilla/5.0 (compatible; heritrix/@1.14.3@ +http://192.168.0.1) test@... ignore false 5.0 30000 3000 300 30 900 1 0 0 org.archive.crawler.frontier.HostnameQueueAssignmentPolicy false false false true true 3000 100 -1 org.archive.crawler.frontier.UnitCostAssignmentPolicy 300000 50 org.archive.crawler.util.BdbUriUniqFilter false true false false false true 21600 86400 false true false true sha1 true 1200 20000 0 0 false true open ISO-8859-1 true sha1 true true true true false true true true true false true true true true true index.html %2E . true mirror 1023 255 false true LONG true true false true -1 true true false true true\n&gt;&gt; An actual crawl order would include XML that helps interpret these \n&gt;&gt; values. You must have copied this out of some view that hides that XML. \n&gt;&gt; If you need to share an order.xml, you should probably open it directly \n&gt;&gt; from the filesystem in a text editor.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt; Thank you in advance.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Chuan\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@&gt; wrote:\n&gt;&gt;&gt;&gt; shichuanwuhan@ wrote:\n&gt;&gt;&gt;&gt;&gt; Hi all,\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; I am using version 1.14.3. My final goal is to get all the URLs of professors&#39; mainpages on one host i.e www.cs.cmu.edu. So firstly, I plan to fetch all pages that are within the host. However, I fail to do that.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; I tried both the traditional &#39;hostscope&#39; and recommended &#39;decidingscope&#39; but I still cannot achieve my goal. As English is not my native language, maybe I misunderstand something in &#39;user manual&#39;. Would someone kindly answer several questions?\n&gt;&gt;&gt;&gt; The included &#39;deciding-default&#39; profile should work for the purpose of \n&gt;&gt;&gt;&gt; getting all pages on a single web host.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; 1.My seed is simply: &#39;http://www.cs.cmu.edu/&#39;. Is it right?\n&gt;&gt;&gt;&gt; Supplying that as a seed, with the deciding-default settings, should \n&gt;&gt;&gt;&gt; cause the crawler to:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; (1) start by visiting &quot;http://www.cs.cmu.edu/&quot;, examining the outlinks \n&gt;&gt;&gt;&gt; of that page for &quot;in-scope&quot; URIs\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; (2) evaluate any URIs that begin &quot;http://www.cs.cmu.edu/&quot; as being \n&gt;&gt;&gt;&gt; &quot;in-scope&quot; (along with some other rules), and thus eligible for \n&gt;&gt;&gt;&gt; recursive fetching\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; The default configuration, with only the single seed, will only wander \n&gt;&gt;&gt;&gt; off &quot;www.cs.cmu.edu&quot; to fetch URIs that appears necessary to render \n&gt;&gt;&gt;&gt; another page (like inline references to scripts, images, frames, etc., \n&gt;&gt;&gt;&gt; or URLs found in Javascript that may auto-load).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; I see that the &#39;faculty&#39; link from &#39;www.cs.cmu.edu&#39; goes to another \n&gt;&gt;&gt;&gt; host, &#39;people.cs.cmu.edu. Your crawl will not in general visit that \n&gt;&gt;&gt;&gt; other host without additional scope customization to say those URLs are \n&gt;&gt;&gt;&gt; of interest. Also, it appears that faculty web pages are on a variety of \n&gt;&gt;&gt;&gt; hosts (including &#39;www-2&#39; and other departmental servers).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; 2.Is it possible that I simply use &#39;hostscope&#39; to achieve my goal? \n&gt;&gt;&gt;&gt; It might be possible, but it is not recommended -- HostScope is \n&gt;&gt;&gt;&gt; deprecated, less efficient and flexible than the DecidingScope + \n&gt;&gt;&gt;&gt; SurtPrefixedDecideRule mechanism.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; 3.If not, how to configure &#39;decidingscope&#39;?\n&gt;&gt;&gt;&gt; You probably want to tell your crawl that begins at www.cs.cmu.edu that \n&gt;&gt;&gt;&gt; it may accept URIs on other hosts, like &#39;people.cs.cmu.edu&#39; and others, \n&gt;&gt;&gt;&gt; as &#39;in-scope&#39;. This involves giving the SurtPrefixedDecideRule more \n&gt;&gt;&gt;&gt; acceptable &#39;SURT&#39; (URI-like) prefixes.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; This can be done either by specifying a file with such prefixes as the \n&gt;&gt;&gt;&gt; SurtPrefixedDecideRule&#39;s &#39;surt-source-file&#39;, or by adding lines to your \n&gt;&gt;&gt;&gt; seeds list that begin &#39;+&#39;.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; You can read more about SURTs as a means of scoping at:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; http://crawler.archive.org/articles/user_manual/config.html#surtprefixscope\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Another strategy might be to start your crawling at the faculty directory:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; http://people.cs.cmu.edu/faculty/index.html\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Supplying that URI as a seed will cause the &quot;implied scope&quot; to be all \n&gt;&gt;&gt;&gt; URLs beginning &quot;http://people.cs.cmu.edu/faculty/&quot; -- which should get \n&gt;&gt;&gt;&gt; all the other pages of the directory, as well. *If* you are confident \n&gt;&gt;&gt;&gt; all homepages always contain the &#39;~&#39; character, you could also add a new \n&gt;&gt;&gt;&gt; rule to the list of rules, such as a MatchesRegExpDecideRule, that \n&gt;&gt;&gt;&gt; always ACCEPTs any URI with a &#39;~&#39; character. That would get the \n&gt;&gt;&gt;&gt; directory, and all the linked pages with &#39;~&#39; anywhere in their URI (and \n&gt;&gt;&gt;&gt; quite probably other pages, at other hosts and universities, when their \n&gt;&gt;&gt;&gt; URIs with &#39;~&#39; are discovered).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Hope this helps,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - Gordon @ IA\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}