{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"BNHBOOe2xGys17DFr1-QE8hDHxMPSoM7KdHxxwtKD5C8sRzJUDRCNxiQnZq6peDIjTexpn931stm85lsYxeRjbgnLD1MvJQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"&quot;Garden&quot; Tests Wanted","postDate":"1067283419","msgId":157,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNGOUQ3M0RCLjQwNzAwMDRAYXJjaGl2ZS5vcmc+"},"prevInTopic":0,"nextInTopic":158,"prevInTime":156,"nextInTime":158,"topicId":157,"numMessagesInTopic":3,"msgSnippet":"In our meeting Friday, we resolved to each create 3 new test cases for our crawler test garden . This is an expandable collection of web-server content","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 57699 invoked from network); 27 Oct 2003 19:37:05 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m20.grp.scd.yahoo.com with QMQP; 27 Oct 2003 19:37:05 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta3.grp.scd.yahoo.com with SMTP; 27 Oct 2003 19:37:05 -0000\r\nReceived: (qmail 18028 invoked by uid 100); 27 Oct 2003 19:31:17 -0000\r\nReceived: from b116-dyn-42.archive.org (HELO archive.org) (gojomo@...@209.237.240.42)\n  by mail-dev.archive.org with SMTP; 27 Oct 2003 19:31:17 -0000\r\nMessage-ID: &lt;3F9D73DB.4070004@...&gt;\r\nDate: Mon, 27 Oct 2003 11:36:59 -0800\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.5) Gecko/20031013 Thunderbird/0.3\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nSubject: &quot;Garden&quot; Tests Wanted \r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-5.4 required=6.0\n\ttests=AWL,BAYES_01,HTML_60_70,HTML_JAVASCRIPT,\n\t      USER_AGENT_MOZILLA_UA\n\tversion=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nIn our meeting Friday, we resolved to each create 3 new test\ncases for our crawler &quot;test garden&quot;. This is an expandable\ncollection of web-server content against which any crawler&#39;s\nability to retrieve certain documents, while avoiding others,\ncan be tested. For now, the results are evaluated by\npostprocessing the crawler logs, though we may move to a system\nwhere the server uses its own logs to evaluate crawler\nbehavior.\n\nBy creating a batch of new standalone tests, we hope to...\n\n   (1) expand the number of available tests\n   (2) gain insight into the ease with which\n       new tests can be contributed\n   (3) probe any limitations of the expressiveness\n       of the current approach (are there things we\n       want to test, but cannot?)\n   (4) probe how well the approach scales as the\n       number/variety of tests grows\n\nFor reference, Parker&#39;s original note about the garden\nsetup and practices is appended below.\n\nSo that we don&#39;t overlap, and as an example of the kinds\nof tests that would be useful, I intend my tests to be:\n\n  (1) Are absolute URIs extracted properly in\n      included (&quot;&lt;script src=&quot;) Javascript files?\n\n      Expected result: Heritrix does not yet parse\n      Javscript in standalone JS files -- only in HTML --\n      and so would fail this test right now. But the\n      our current best-effort search for URI-like\n      strings in Javascript code could easily be placed\n      in a new Javascript Extractor prtocessor, so\n      we expect Heritrix to pass this test soon.\n\n  (2) Are relative URIs extracted properly in\n      included (&quot;&lt;script src=&quot;) Javascript files?\n\n      Expected result: Need to do more research;\n      I&#39;m not sure if they should be interpreted\n      relative to the JS file URI or the page that\n      included it. As above, Heritrix would fail\n      currently but should pass soon.\n\n  (3) Are URIs that are created via concatenation\n      of Javascript string variables or string variables\n      and string literals extracted properly?\n\n      Expected result: No crawler I know of yet handles\n      this. Someday, perhaps.\n\nSo as should be clear, these tests don&#39;t have to be\nthings we believe current crawlers do (or must) achieve.\nAny open or closed bugs in our Sourceforge Bug Tracker\nmight also be good candidates for new test cases.\n\nAnyone else following the Heritrix project is also\nwelcome to submit tests in the appropriate form, or\ncomments about what should be testable.\n\nI&#39;d like to integrate any new tests onto crawl08 by the\nend of this week.\n\n- Gordon\n\n\n================================================================\nParker Thompson (parkert@...) wrote:\n &gt; Today I checked into cvs a new test garden that should make automated\n &gt; testing a whole lot easier.  I&#39;ve attached a readme.txt that explains how\n &gt; the tools work and what&#39;s available.  You should be able to check out this\n &gt; code from anywhere and use it witout modification (tests will let you know\n &gt; if/when this is not the case).\n &gt;\n &gt; To see it in action check out:\n &gt;\n &gt;   http://crawl08.archive.org/\n &gt;\n &gt; For a file manifest:\n &gt;\n &gt;   http://crawl08.archive.org/?links=1\n &gt;\n &gt; Note: You may run any/all of the reporting scripts locally (results.cgi\n &gt; and testreport.pl) even if the garden is remote, though they fetch\n &gt; configuration information real-time based on the log files being parsed,\n &gt; so you will need an internet connection when evaluating test results.\n &gt;\n &gt; Let me know if you have any questions,\n &gt;\n &gt; pt.\n &gt;\n &gt;\n &gt; ------------------------------------------------------------------------\n &gt;\n &gt; Heritrix Web-based Test Suite\n &gt; -----------------------------\n &gt;\n &gt; I. Introduction\n &gt;\n &gt; The following is a description of the functionaly provided by the Heritrix\n &gt; web-based test suite.  This suite is meant to be a set of &quot;unit tests&quot;,\n &gt; that can exist individually or as part of &quot;test suites&quot;.\n &gt;\n &gt; II. Installing\n &gt;\n &gt; To install these tests, just put the directory which came with this file\n &gt; somewhere in the apache document tree, so it&#39;s viewable from the web, make\n &gt; sure the path to perl is correct in each cgi (default is /usr/bin/perl),\n &gt; change it if it is not, make sure you can execute cgis (see httpd.conf),\n &gt; make sure index.cgi is a valid index (see httpd.conf), and you should be\n &gt; set.\n &gt;\n &gt; Addionally, if you want the infinite breadth test to work you must make\n &gt; sure the host on which these tests reside has a dns entry in the form of:\n &gt;\n &gt;   *.host.org\n &gt;\n &gt; so that infinite-content.cgi can create arbitrarily-named aliases.\n &gt;\n &gt; II. Running All Test\n &gt;\n &gt; To run all tests just point the crawler at the test directory.  It will be\n &gt; presented with links to each test.\n &gt;\n &gt; III. Single Tests\n &gt;\n &gt; To run a single test, or test suite, point the crawler at the test\n &gt; directory with the argument &#39;?test=name&#39; where name is the directory name\n &gt; of the test or suite.  By convention this is &#39;testX&#39; where X is the test\n &gt; number (e.g. http://test.archive.org/?test=test23).\n &gt;\n &gt; This will display a page with links to only the test specified, and any\n &gt; tests on which the specified test is dependent.\n &gt;\n &gt; IV. Test Suites\n &gt;\n &gt; Test suites are simply tests that contain dependencies that are themselves\n &gt; tests.  This terminology is just used to make it easier with people\n &gt; already familiar with unit testing in other contexts.  To execute a single\n &gt; test suite see the directions above for &#39;Single Tests&#39;.\n &gt;\n &gt; V. Viewing Results\n &gt;\n &gt; When you run the crawler several log files are created.  These contain\n &gt; information about what pages were visited by the crawler.  You will want\n &gt; to locate the file &#39;uri-processing.log&#39; in your ouput directory, which\n &gt; will be used to determine what succeded and what failed.\n &gt;\n &gt; Once you have located this file you have two ways to view the results.\n &gt;\n &gt; A. Web-based Interface\n &gt;\n &gt; Included in this package is a script called &#39;report.cgi&#39; that will allow\n &gt; you to upload a log file, which it will then parse, reporting on the tests\n &gt; seen during your crawl.\n &gt;\n &gt; This should be most convienent when your garden is on a remote host and\n &gt; you don&#39;t want to move log files around manually, or can&#39;t run the\n &gt; command-line tool locally.\n &gt;\n &gt; B. Command-line Interface\n &gt;\n &gt; There is also a command-line tool that can be used to parse uri-processing\n &gt; logs.  This tool is located in the root garden directory (where this\n &gt; readme is) and is called &#39;testreport.pl&#39;.  It can be used to generate\n &gt; reports similar to those produced by the web-based interface, which can\n &gt; either be dumped to standard out, or emailed to a recipient list.\n &gt;\n &gt; Usage is as follows:\n &gt;\n &gt;   testreport.pl [-mail &lt;mailto list comma-delimited&gt;] &lt;heritrix log file&gt;\n &gt;\n &gt; Examples:\n &gt;   ./testreport.pl uri-processing.log\n &gt;   ./testreport.pl -mail a@...,b@... uri-processing.log\n &gt;\n &gt; VI. Test Configuration Files\n &gt;\n &gt; Tests are simply collections of files to be crawled, and a configuration\n &gt; file that can e used to evaluate the results.  Tests have the following\n &gt; properties:\n &gt;\n &gt; - The top-level test directory must be placed in the garden&#39;s root.\n &gt;\n &gt; - All files required by a test are contained within the tests&#39; directory\n &gt; tree (though this tree may be arbitrarily complex).\n &gt;\n &gt; - The top-level directory must be named testXXX where XXX is a positive\n &gt; integer and is unique within the garden.\n &gt;\n &gt; - The top-level test directory must contain a file called test.conf that\n &gt; defines the tests&#39; properties.\n &gt;\n &gt; Test configuration files are defined using a simple key:value pair syntax.\n &gt; Valid tags are:\n &gt;\n &gt; name - human readable name for the test\n &gt; find - require a uri be found.\n &gt; omit - require a uri be omitted.\n &gt; test - require another test as a prerequisite.\n &gt; info - a comment to be presented in the report.\n &gt;\n &gt; All URIs within the configuration file, including tests, are specified as\n &gt; relative paths.\n &gt;\n &gt; A. Best Practices for Defining Tests\n &gt;\n &gt; While the &#39;name&#39; tag may contain any human readable string, it is\n &gt; considered a best practice to prefix the test name with &#39;Test X&#39;, and use\n &gt; a descriptive name.\n &gt;\n &gt; B. Example Configuration File\n &gt;\n &gt; name: Test 101\n &gt; find: subdirectory/hardtofindlink.html\n &gt; find: anothersub/sneakylink.html\n &gt; omit: excluded/by/robots/page.html\n &gt; omit: commented/out/link/page.html\n &gt; test: ../test5\n &gt; test: ../test46\n &gt; test: ../test99\n &gt; info: Note: Make sure you copied test101/robots.txt to your document root.\n &gt;\n &gt; VII. Special Arguments to Test Index (advanced)\n &gt;\n &gt; A. Generating Infinite Content\n &gt;\n &gt; If you wish to do crawler performance testing you may use the test index\n &gt; to generate an infinite (more or less) amount of content by calling\n &gt; index.cgi with the argument &#39;infinite=1&#39;, as in:\n &gt;\n &gt;   http://host.org/path/to/test/index/?infinite=1\n &gt;\n &gt; This will generate the appropriate links.\n &gt;\n &gt; Note:  To have this work the host &#39;host&#39; must have a dns entry that\n &gt; resolves *.host.org to the host on which the files live, and apache must\n &gt; be set up with the appropriate VirtualHost entries.\n &gt;\n &gt; B. Show Links to Test Requirements/Exclusions\n &gt;\n &gt; In general you will not want the test index to provide hyperlinks to a\n &gt; test&#39;s requirements/omissions, since the crawler should be forced to\n &gt; &quot;discover them&quot;.  However, if you&#39;d like to view an index with all URIs\n &gt; hyperlinked for ease of browsing, you may pass the test index the argument\n &gt; &#39;links=1&#39; as in:\n &gt;\n &gt;   http://host.org/path/to/test/index/?links=1\n &gt;\n &gt;\n\n\n\n\n\n\n\n\n"}}