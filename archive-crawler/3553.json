{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"Dh6f3lW2eDfgZOvRk44vLsb5eJBCGP5q51ZAmByZN6Yh_RdeYTnGd60WZK3S7LK2ehKEMyL6TQpx4lWanwLdZcbCSFebC53V","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Feature suggestion for the frontier","postDate":"1164221279","msgId":3553,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1NjQ5QjVGLjUwOTA0MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ1NjMzREUwLjcwNzAzMDFAYmF5YXJlYS5uZXQ+","referencesHeader":"PGVqdGZycSs4cnRwQGVHcm91cHMuY29tPiA8NDU2MzM3NEMuNjA5MDMwM0BhcmNoaXZlLm9yZz4gPDQ1NjMzREUwLjcwNzAzMDFAYmF5YXJlYS5uZXQ+"},"prevInTopic":3552,"nextInTopic":3554,"prevInTime":3552,"nextInTime":3554,"topicId":3547,"numMessagesInTopic":6,"msgSnippet":"lekash wrote: Hey John; ... You mean, disabled FrontierScheduler? ... Is this a good or a bad thing? ... Is the above phenomenon same as that described in the","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 48984 invoked from network); 22 Nov 2006 18:49:48 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m34.grp.scd.yahoo.com with QMQP; 22 Nov 2006 18:49:48 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.118)\n  by mta10.grp.scd.yahoo.com with SMTP; 22 Nov 2006 18:49:48 -0000\r\nReceived: by dns.duboce.net (Postfix, from userid 1008)\n\tid 671BCC51D; Wed, 22 Nov 2006 09:29:22 -0800 (PST)\r\nX-Spam-Checker-Version: SpamAssassin 3.1.4 (2006-07-26) on dns.duboce.net\r\nX-Spam-Level: \r\nX-Spam-Status: No, score=-4.4 required=5.0 tests=ALL_TRUSTED,AWL,BAYES_00 \n\tautolearn=ham version=3.1.4\r\nReceived: from [192.168.1.10] (debord.duboce.net [192.168.1.10])\n\tby dns.duboce.net (Postfix) with ESMTP id CC46EC256;\n\tWed, 22 Nov 2006 09:29:15 -0800 (PST)\r\nMessage-ID: &lt;45649B5F.5090405@...&gt;\r\nDate: Wed, 22 Nov 2006 10:47:59 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.7) Gecko/20060910 SeaMonkey/1.0.5\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ejtfrq+8rtp@...&gt; &lt;4563374C.6090303@...&gt; &lt;45633DE0.7070301@...&gt;\r\nIn-Reply-To: &lt;45633DE0.7070301@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Feature suggestion for the frontier\r\nX-Yahoo-Group-Post: member; u=168599281; y=XN_5O80LWTyCgd8AKnZPFNfjRdFxiB8y43XRacqK_xjY9ZvsQkYRRYRn\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nlekash wrote:\n\nHey John;\n\n&gt; Hi Michael,\n&gt; Yes, I have turned off the frontier.\n&gt;\n\n\n\nYou mean, disabled FrontierScheduler?\n\n\n\n&gt; It kind of acts like house cleaning.\n&gt; (Turn off the frontier for a day, it munches through the system)\n&gt;\n\n\n\nIs this a good or a bad thing?\n\n&gt;\n&gt; The patch I will be pestering Joe (yeah, and you, too)\n&gt; more to do is some more intelligence in the database.\n&gt; For example, it appears to sort of keep entries for diversions,\n&gt; and includes this in the count. Doesn&#39;t sound like much,\n&gt; but the numbers get huge, and clog the system. With the frontier\n&gt; off, they fall off the count. Its fun to watch the count of\n&gt; downloaded + queued drop fast.\n&gt;\n\n\n\n\n\n\n\n\nIs the above phenomenon same as that described in the below issue John?\n\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1562566&group_id=73833&atid=539099\n\n\n\n&gt; Throughput falls on its face though, something to do with not\n&gt; following up on a current web site in the frontier. Cause, duh,\n&gt; new stuff isn&#39;t appearing on the frontier.\n&gt;\n\n\n\n\nTell us more about the above.  One of the crawling lads here may \nrecognize this condition but I&#39;m not familiar with it.\n\nGood stuff,\nSt.Ack\n\n\n\n\n\n&gt; John\n&gt;\n&gt; Michael Stack wrote:\n&gt;\n&gt; &gt; Have you tried disabling the FrontierScheduler once the crawler goes\n&gt; &gt; past 120M fetched? You might even be able to set the disabled flag from\n&gt; &gt; JMX. If so, since you can get counts of crawled and queued via JMX, you\n&gt; &gt; could automate switching off the processor.\n&gt; &gt;\n&gt; &gt; Otherwise, do you have a patch for the below for us to apply Joe?\n&gt; &gt;\n&gt; &gt; St.Ack\n&gt; &gt;\n&gt; &gt; joehung302 wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi,\n&gt; &gt; &gt;\n&gt; &gt; &gt; We&#39;ve been using heritrix with crawl map for crawling &gt; 1B urls. We\n&gt; &gt; &gt; current have 8 crawl machines at one time.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Under this configuration, the crawl can only be successful if the\n&gt; &gt; &gt; crawlers don&#39;t fail. Hence we&#39;re really focused on how to keep the\n&gt; &gt; &gt; crawler running as long as possible.\n&gt; &gt; &gt;\n&gt; &gt; &gt; We thought it might be a good idea to *limit* bdb growth after\n&gt; &gt; &gt; certain size. And we also know that bdb size has a lot to do with\n&gt; &gt; &gt; the to-be-crawled URL list. Hence the question: Does it make sense\n&gt; &gt; &gt; to set a upper limit on the Frontier so, when the to-be-cralwed URL\n&gt; &gt; &gt; reached a certain number (say, 120M urls), stop collecting to-be-\n&gt; &gt; &gt; crawled and throw any new links away? The idea is, in order to\n&gt; &gt; &gt; download 120M URLs, we probably would have a to-be-crawled list of\n&gt; &gt; &gt; 400M, and bdb probably won&#39;t survive under that size.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Does it make sense?\n&gt; &gt; &gt;\n&gt; &gt; &gt; cheers,\n&gt; &gt; &gt; -Joe\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt;  \n\n\n"}}