{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225011788,"authorName":"Karl Wright","from":"Karl Wright &lt;kwright@...&gt;","profile":"daddywri","replyTo":"LIST","senderId":"PSmhlQppFiNVZXyiM-VcMVGH-EtUsuU6e1kIscLuzA-GrQWl7fjt_mnzazkarEVilw5I3Yy7mEKL506m6bgYT53N9KnHQl_uLIU","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Advice needed on how to (properly) structure      new Heritrix modify and delete functionality","postDate":"1153156386","msgId":3068,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkJDNTIyLjUwMzAzMDVAbWV0YWNhcnRhLmNvbT4=","inReplyToHeader":"PDM4OTQuMTMwLjIwOC4xNTIuODAuMTE1MzE1MTAwMC5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg==","referencesHeader":"PDM4OTQuMTMwLjIwOC4xNTIuODAuMTE1MzE1MTAwMC5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg=="},"prevInTopic":3064,"nextInTopic":3074,"prevInTime":3067,"nextInTime":3069,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"... Ok.  We build appliances here that index content for specialized searches.  We use a number of tools for ingesting content into the system and removing it","rawEmail":"Return-Path: &lt;kwright@...&gt;\r\nX-Sender: kwright@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 13901 invoked from network); 17 Jul 2006 17:11:43 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m34.grp.scd.yahoo.com with QMQP; 17 Jul 2006 17:11:43 -0000\r\nReceived: from unknown (HELO silene.metacarta.com) (65.77.47.18)\n  by mta9.grp.scd.yahoo.com with SMTP; 17 Jul 2006 17:11:42 -0000\r\nReceived: from localhost (silene.metacarta.com [65.77.47.18])\n\tby silene.metacarta.com (Postfix) with ESMTP id 87113739A7\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 17 Jul 2006 13:11:31 -0400 (EDT)\r\nReceived: from silene.metacarta.com ([65.77.47.18])\n\tby localhost (silene.metacarta.com [65.77.47.18]) (amavisd-new, port 10024)\n\twith ESMTP id 06606-02 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 17 Jul 2006 13:11:30 -0400 (EDT)\r\nX-Auth-Received: from [65.77.47.197] (dhcp-65-77-47-197.metacarta.com [65.77.47.197])\n\t(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))\n\t(No client certificate requested)\n\tby silene.metacarta.com (Postfix) with ESMTP id 549FF739A3\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 17 Jul 2006 13:11:30 -0400 (EDT)\r\nMessage-ID: &lt;44BBC522.5030305@...&gt;\r\nDate: Mon, 17 Jul 2006 13:13:06 -0400\r\nUser-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;3894.130.208.152.80.1153151000.squirrel@...&gt;\r\nIn-Reply-To: &lt;3894.130.208.152.80.1153151000.squirrel@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-Virus-Scanned: by amavisd-new-20030616-p10 (Debian) at metacarta.com\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Karl Wright &lt;kwright@...&gt;\r\nSubject: Re: [archive-crawler] Advice needed on how to (properly) structure\n      new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=225011788; y=LOHtIxNTMp5YvnOUI8TX61_8bP1pIwHcd6GR8rxX7laCU_s\r\nX-Yahoo-Profile: daddywri\r\n\r\nkris@... wrote:\n&gt; Hey Karl,\n&gt; \n&gt; That is a very ambitious project. It would be very interesting if you\n&gt; could share why you want this functionality.\n&gt; \n\nOk.  We build appliances here that index content for specialized \nsearches.  We use a number of tools for ingesting content into the \nsystem and removing it when it goes away.  Our other tools all work \nincrementally; they only ingest stuff into the index that is new or has \nchanged, and they delete content from the index when the content has \ngone away.  We use Heritrix right now, but it effectively a one-shot for \nus; effectively we cannot detect changes without rerunning a job from \nscratch, and also we cannot detect deletions at all.  Several of our \nclients have attempted to solve these problems by building their own \nre-fetcher, which looks at the Heritrix crawl logs to determine what to \ndo, but basically this runs afoul of politeness issues and all that \nstuff.  So we concluded that we really did want to build a version of \nHeritrix that fully understands how to work incrementally.\n\nThe adaptive frontier seemed like a possible way to go, except that I \nwas told (by stack) that it did not use a database, and therefore would \nrun out of memory readily.  We already have problems even with the \nbdbfrontier on memory consumption, so I did not think that would be a \nviable way to proceed.\n\n\n&gt; \n&gt;&gt;-----Original Message-----\n&gt;&gt;From: archive-crawler@yahoogroups.com\n&gt;&gt;[mailto:archive-crawler@yahoogroups.com] On Behalf Of Karl Wright\n&gt;&gt;Sent: 17. j�l� 2006 14:10\n&gt;&gt;To: archive-crawler@yahoogroups.com\n&gt;&gt;Subject: [archive-crawler] Advice needed on how to (properly)\n&gt;&gt;structure new Heritrix modify and delete functionality\n&gt;&gt;\n&gt;&gt;Hi folks,\n&gt;&gt;\n&gt;&gt;I&#39;m about to go deep on a project to add support for processing\n&gt;&gt;modifications and deletions for Heritrix, and I&#39;m looking for input as\n&gt;&gt;to how best to proceed on this.\n&gt; \n&gt; \n&gt; It occurs to me that it _might_ be best to implement this is a separate\n&gt; program that is &quot;fed&quot; by Heritrix.\n&gt; \n&gt; I&#39;m unclear if you want this as a method of improving crawling or you\n&gt; desire to synchronize some local database with what is one the web? If the\n&gt; latter, then using a separate program that is fed Heritrix&#39;s snapshots\n&gt; would be better. If the former then I&#39;d really like to hear what the goal\n&gt; is.\n&gt; \n\nHopefully I answered this above.\n\n&gt; \n&gt;&gt;The project basically will involve two stages\n&gt;&gt;\n&gt;&gt;First, I would be building into the Heritrix infrastructure the means\n&gt;&gt;of detecting changes to pages, and also detecting deletions of pages.\n&gt; \n&gt; \n&gt; This is largely in place thanks to the AdaptiveRevisitFrontier (which is\n&gt; actually a bit of a misnomer since the adaptive part is handled in the\n&gt; processing chain) and the processor associated with it. Basically the\n&gt; ARFrontier reissues URIs (with knowledge of their pasts) and the\n&gt; ChangeEvaluator uses that past knowledge to determine changes (a sudden\n&gt; 404 would then signal deletion).\n&gt; \n&gt; One of the most difficult issue I ran into (and the one that ultimately\n&gt; convinced me that the incremental crawl approach using adaptive tuning\n&gt; doesn&#39;t work) was that detecting changes is bloody hard. At least if you\n&gt; care to differentiate between changes in content and trivial changes in\n&gt; layout (such as random images, irrelevant sections - stock ticker, clock\n&gt; etc. - and so forth. I&#39;d love to hear your thoughts on that issue.\n&gt; \n\nWe don&#39;t need to be too precise about detecting changes.  If content \nchanges only in formatting, say, the worst that can happen is we will \nreingest needlessly.  It&#39;s still a lot better than reingesting \n*everything*, which would be the alternative.\n\n&gt; \n&gt;&gt;These conditions would have to be signaled in some way to all Writers.\n&gt; \n&gt; \n&gt; URIs always pass through all processor unless their processing is\n&gt; interrupted so once the determination has been made it is simply a matter\n&gt; of recording the meta-data in the CrawlURI and having the writers pick up\n&gt; on that. You&#39;d clearly need your own writers but that was a given in any\n&gt; case.\n\nYes, we have our own writer.  What I didn&#39;t know (or didn&#39;t think of) \nwas that the writer would be called for a 404 error.  If that&#39;s the \ncase, the delete path issue is &quot;solved&quot;.\n\n&gt; \n&gt; \n&gt;&gt;Alternatively, since backwards compatibility might be a problem, it&#39;s\n&gt;&gt;potentially possible that a new kind of Writer-like module would need\n&gt;&gt;to be created. I was going to look at the adaptive frontier to see how\n&gt;&gt;exactly it handled changes, if indeed it does anything special at all.\n&gt; \n&gt; \n&gt; It issues URIs, the URIs pass through the processing chain where stuff\n&gt; happens and then they return to the frontier with updated state and are\n&gt; reinserted. The ARFrontier doesn&#39;t worry to much about &#39;change&#39; as such.\n&gt; It relies on the fact that change can only occur when a URI is being\n&gt; processed. There are no external &#39;events&#39; that can trigger a change.\n&gt; \n\nI understand that the change detecting comes only as a result of \nrefetching a url.  What I don&#39;t get is how I could detect a change \nwithout saving some indication somewhere of what the data looked like \nlast time through.\n\nOne thing that would help in this regard is simply keeping a checksum \naround of the contents of the url as it was fetched last time round, and \nthe checksum of the data for the current fetch.  Then, if they differ, \nwe&#39;d presume that the data had changed.\n\n\n&gt; \n&gt;&gt;As far as I know, though, there&#39;s no signaling pathway in Heritrix at\n&gt;&gt;all for handling deletions, so that would have to be added, hopefully\n&gt;&gt;in an approved manner.\n&gt; \n&gt; \n&gt; Not as such. The CrawlURIs provide a means of carrying information from\n&gt; one processor to the next and (eventually) to the Frontier. Adding another\n&gt; layer of communications doesn&#39;t seem necessary or wise.\n&gt; \n\nRight, I got that.\n\n&gt; \n&gt;&gt;The second phase would involve marrying the BdbFrontier with the\n&gt;&gt;Adaptive frontier. The goal is to set the crawler up so that it\n&gt;&gt;dynamically and cost-effectively performs a synchronization over an\n&gt;&gt;extended time.\n&gt; \n&gt; \n&gt; Marrying the BdbFrontier and the ARFrontier isn&#39;t going to happen. You\n&gt; could write something that encapsulated the functionality of both, but it\n&gt; would be an entirely new beast. The BdbFrontier has been written with one\n&gt; objective in mind, efficient snapshot crawling. The ARFrontier was built\n&gt; around the idea of incremental crawling, using adaptive strategies to\n&gt; reduce workload as you would only crawl URLs when they were likely to have\n&gt; changed. As a result it compromises heavily on efficiency. They queuing\n&gt; structure of the two are similar in concept but their implementations are\n&gt; worlds apart as the ARFrontier relies on queues with much richer state due\n&gt; to its more complicated crawling strategy.\n&gt; \n\nI would definitely plan on doing this as an entirely new frontier, based \non both the Bdb guy and the adaptive guy.\n\n&gt; \n&gt;&gt;I looked at the\n&gt;&gt;BdbFrontier and discovered that there was actually not much going on\n&gt;&gt;in this module, per-se. In fact, I found that the frontier interface\n&gt;&gt;itself is not at all what I expected. Is there any design documents or\n&gt;&gt;anything you can throw my way that would help me understand how the\n&gt;&gt;BdbFrontier works, and what I might need to do to accomplish my goal?\n&gt; \n&gt; \n&gt; The Developers manual has a section on this and is a good place to start\n&gt; (even if it is a little light on details). You may also want to give my\n&gt; masters thesis a read (http://vefsofnun.bok.hi.is/upload/3/thesis.pdf). It\n&gt; dealt with the ARFrontier. It has a lot of non-relevant stuff and is\n&gt; somewhat out of date but chapter 3.5 talks about Frontiers in general and\n&gt; 6.2 talks about the ARFrontier.\n&gt; \n&gt; The Frontier is the most complicated part of Heritrix. As far as I know,\n&gt; only Gordon and I have attempted to write one of the beasts from scratch.\n&gt; There are some arguments that the Frontier should no be a plug-able module\n&gt; at all and in fact the Frontier interface has been anything but stable. I\n&gt; suggest starting by looking at the ARFrontier. Not because it is the\n&gt; better frontier, but because it has a much cleaner linage. The BdbFrontier\n&gt; is broken into several levels (AbstractFrontier extended by\n&gt; WorkQueueFrontier extended by the BdbFrontier). The ARFrontier is a\n&gt; cleaner implementations simply because it hasn&#39;t been rewritten and\n&gt; tweaked multiple times and as such should be easier to understand. Its\n&gt; separation of logic in regards to the work queues is also much clearer\n&gt; even if it is less efficient.\n\nI hear you.  I will look further at both frontiers.  In the meantime, \nhave you considered how you might have proceeded if you needed to make \nthe adaptive frontier store its queue on disk?  That&#39;s basically what I \nam trying to build.\n\nThanks,\nKarl\n\n&gt; \n&gt; - Kris\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}