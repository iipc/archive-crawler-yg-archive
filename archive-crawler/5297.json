{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"lekash","from":"lekash &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"bnksWQ1yi6MxSSuOqaJpeUnCUygv9G8b45fZqalf4aHL3Q_cg39QPkt1hXy_pKYEAhlk8Ua0E5mSxzfx72lePiou","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Crawling 100 million pages","postDate":"1212783665","msgId":5297,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NDk5QzMxLjkwNTA3MDJAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PGcyYmt2cCtiMmsxQGVHcm91cHMuY29tPg==","referencesHeader":"PGcyYmt2cCtiMmsxQGVHcm91cHMuY29tPg=="},"prevInTopic":5296,"nextInTopic":5298,"prevInTime":5296,"nextInTime":5298,"topicId":5292,"numMessagesInTopic":11,"msgSnippet":"Hi there, I m like to very much encourage being careful about speeding up heritrix . Politeness is really important on the net.  I keep doubling my inter ","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 66418 invoked from network); 6 Jun 2008 20:21:40 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m47.grp.scd.yahoo.com with QMQP; 6 Jun 2008 20:21:40 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta18.grp.scd.yahoo.com with SMTP; 6 Jun 2008 20:21:40 -0000\r\nX-Received: from [192.168.0.12] (72.20.109.026.bayarea.net [72.20.109.26])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id m56KLebS044497\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 6 Jun 2008 13:21:40 -0700 (PDT)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;48499C31.9050702@...&gt;\r\nDate: Fri, 06 Jun 2008 13:21:05 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.11) Gecko/20050727\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g2bkvp+b2k1@...&gt;\r\nIn-Reply-To: &lt;g2bkvp+b2k1@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: lekash &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Crawling 100 million pages\r\nX-Yahoo-Group-Post: member; u=90724651; y=KoMFV2eVTprxKC3VkrgInrg-cCO1unq4wBacEdQ9OvjS\r\nX-Yahoo-Profile: lekash\r\n\r\nHi there,\n\nI&#39;m like to very much encourage being careful about &#39;speeding up heritrix&#39;.\nPoliteness is really important on the net.  I keep doubling my inter \ncrawl delay every\nyear, and still people have problems. \n\nIts not a heretrix limit you will be looking at.  Its a hardware \ncapacity limit.\n(Though the most I&#39;ve gotten a single crawler to do is\n385M over a three month period.)\n  A crawler group, I&#39;ve gotten to 6B /year.\nSo, 100 M /month is well within the operating range.\n\nThe 1.x crawler seems to have a limit around 680M for a single crawler,\nlike the queued numbers stop going up then.  Never got there, so don&#39;t\nknow what would happen.  I haven&#39;t run a multi-billion one on the 2.x line,\nmaybe next year.\n\nThe things that run out of space are memory for java heap space, and\nsomething funny with the database.  Re-writing scratch files in that\nseems to be what slows it down later, despite a still wide frontier.\n\nAnd of course, urls are not a consistent measure, as size varies widely.\ne.g. .gov sites are 10 times as dense in text/pdf as .com sites,\nand .com sites have all the visuals and movies.  Depends on what\nyou are crawling.\n\nJohn\n\n\nhijbul_bd wrote:\n\n&gt; Dear All\n&gt;\n&gt; I would like to crawl 100 million pages with in a month for crawling\n&gt; research. As far i know some research crawler(IRLbot(6 billion pages in\n&gt; 41 days), polybot(120 millions pages in 19 days)) can download huge\n&gt; pages in short amount of time wich is not open source. In 2005\n&gt; according to some blog site Heritrix can download about 20 miilion\n&gt; pages in a month. What is the speed of current Heritrix version and How\n&gt; can I speed up heritrix to download 100 million or at least 50 million\n&gt; pages with in a month. Are there any ohter open source crawler which\n&gt; can do this?\n&gt;\n&gt; Thanks in Advance\n&gt; Hijbul Alam\n&gt;\n&gt;  \n\n\n"}}