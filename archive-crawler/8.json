{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"&quot;Gordon Mohr&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"6LBDSn4Bhasb4fqXdCE-MPU6lwop8sva2oJh42-IKClD1K5K9P6FXTz6JpQxZI2HZqXdGECJqqLJFqdAOyZBN6Z7-VWqGmUt2g","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Crawler engineering review kickoff - 10am Friday","postDate":"1045811758","msgId":8,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAwZmMwMWMyZDk3OSQxNWU2NzQ1MCQ2NTBhMDAwYUBnb2xkZW4+","referencesHeader":"PDUuMi4wLjkuMC4yMDAzMDIyMDIwNTAyMi4wMGI3ZmMyOEBtYWlsLmFyY2hpdmUub3JnPg=="},"prevInTopic":0,"nextInTopic":9,"prevInTime":7,"nextInTime":9,"topicId":8,"numMessagesInTopic":4,"msgSnippet":"[cc d to the archive-crawler@yahoogroups.com discussion list] These are all important matters to address -- and for most of these issues, I think there will be","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (EGP: mail-8_2_3_4); 21 Feb 2003 07:16:01 -0000\r\nReceived: (qmail 35254 invoked from network); 21 Feb 2003 07:16:01 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m10.grp.scd.yahoo.com with QMQP; 21 Feb 2003 07:16:01 -0000\r\nReceived: from unknown (HELO mail.archive.org) (209.237.232.3)\n  by mta2.grp.scd.yahoo.com with SMTP; 21 Feb 2003 07:16:01 -0000\r\nReceived: from golden (adsl-67-119-27-15.dsl.snfc21.pacbell.net [67.119.27.15])\n\tby mail.archive.org (8.10.2/8.10.2) with SMTP id h1L6cim29738;\n\tThu, 20 Feb 2003 22:38:44 -0800\r\nMessage-ID: &lt;00fc01c2d979$15e67450$650a000a@golden&gt;\r\nTo: &quot;Brad Tofel&quot; &lt;brad@...&gt;, &quot;Igor Ranitovic&quot; &lt;igor@...&gt;,\n   &lt;michele@...&gt;, &quot;Brewster Kahle&quot; &lt;brewster@...&gt;\r\nCc: &quot;Raymie Stata&quot; &lt;raymie@...&gt;, &lt;archive-crawler@yahoogroups.com&gt;\r\nReferences: &lt;5.2.0.9.0.20030220205022.00b7fc28@...&gt;\r\nSubject: Re: Crawler engineering review kickoff - 10am Friday\r\nDate: Thu, 20 Feb 2003 23:15:58 -0800\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2800.1106\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1106\r\nFrom: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\n[cc&#39;d to the archive-crawler@yahoogroups.com discussion list]\n\nThese are all important matters to address -- and for most of these issues,\nI think there will be three overlapping answers:\n\n  (1) what range of choices are enabled by the design;\n  (2) what choice we make in a first, simple, proof-of-design\n      crawler;\n...and ultimately...\n  (3) what choice we make in the eventual high-performance,\n      multi-machine, whole-web, max-any-downlink crawler\n\nConsidering a few of the specifics:\n&gt;    * how to prioritize the urls to crawl.   maybe we could list the different ways current crawlers work as a guideline for some\nof the options.\n&gt;         we know xyleme and alexa and mercator.\n&gt;       gordon, maybe you could write up a short summary as a way to understand the options.\n\nCommon choices have included:\n\n  - a breadth-first traversal of the seed URI set and discovered URIs\n  - a depth-first traversal (within politeness and site-saturation constraints)\n  - descending estimated value of unvisited URIs, where estimated value is...\n    - number of inlinks\n    - classic Google PageRank (where inlinks from high-inlink pages are worth more)\n    - apparent popularity by user visits (eg Alexa traffic data)\n    - weighted by URI contents or content-analysis of the inlink sources\n\nInto these can be mixed other measures of a page&#39;s importance or\nimplied volativility -- to influence how often something is recrawled.\n\nBecause of the wide range of anticipated users, the crawler design must\nbe capable of having any of these policies -- or weighted combinations of\nthese policies -- plugged in, either by configuration file or code\nmodules. (&quot;Answer #1&quot;)\n\nFor our first instantiation, simpleminded swappable breadth-first and\ndepth-first policies will suffice for verifying the design and initial\ntesting. (&quot;Answer #2&quot;)\n\nFor the eventual high-performance, mega-scale crawler, a more\nsophisticated ranking for ensuring &quot;more important&quot; pages are\nfetched before (or more often than) other pages will eventually\nbe devised. Some research indicates breadth-first is pretty good\nat approximating the (much harder to compute) inlink and PageRank\napproaches. We&#39;ll have to tinker over time. (&quot;Answer #3&quot;)\n\n&gt;    * how tasks will be broken up on different machines.  for instance:\n&gt;        1.  different machines for different stages of processing, or\n&gt;        2.  a host does all stages, but urls are broken up onto different machines some way\n\nThe second approach -- distribution to relatively full-fledged hosts\nby URI ranges -- seems to dominate at first (it&#39;s straightforward), but\nthen at  really large scales there may be opportunities for specialized\nmachines to take over tough subproblems.\n\nA nice part of the stages-and-queues model is that every stage\ntransition, through a queue, can potentially travel to another machine\nin a clean fashion. So breaking up the process &quot;horizontally&quot; (a URI\nlifecycle per machine) or &quot;vertically&quot; (a URI lifecycle spans many\nmachines) or a mixture of the two are all possible.\n\n&gt;    *  will we use an RAM based system or disk based (for the url list)\n&gt;        (xyleme and mercator used RAM,    (xyleme said this was a big limitation on their system)\n&gt;          alexa does not)\n&gt;        I dont know if you can buy cost-effective large ram machines.\n&gt;        talking with Jad at alexa, he said 2GB is cheap otherwise very expensive (this would be\n&gt;        based on the HP line).\n\nAgain, swappable strategies will be enabled, and I think our choice will\nchange over time, starting with a simple RAM-based approach to get\nthe crawler testable for small crawls, transitioning to a disk-based\nsystem when we get to web-wide crawls.\n\n&gt;    *  how can remote coordination be done between crawlers that are geographically far apart\n\nThis is an interesting question, but right now seems pretty far down\nthe priority scale. I recall Paula at Alexa saying they were capable of\ndoing this -- but didn&#39;t really use that capability much. This also\nmay not be much different from the multiple-machines-in-same-location\nsituation, if the method by which proximate machines coordinate their\nactions isn&#39;t especially bandwidth and low-latency intensive.\n\n&gt;    * what the thread/process model should be for processing the urls.   separate processes or threads\n&gt;       in a single process.\n\nUnsure exactly what you mean; but with the assumption that we&#39;re using Java,\nit will make sense to keep all related activities in the same running\nJVM, and split tasks among Java threads.\n\nEven with a relatively simple/traditional structure -- one thread works\nits way through the whole process of handling one URI, blocking socket\nI/O -- using a lot of Java threads can achieve pretty good throughput.\nWe&#39;re likely to make use of the new nonblocking socket support, and\ndecompose the process into bite-sized nonblocking stages, so that a\nsmaller number of threads can scream through the workload with minimal\nscheduling overhead.\n\n&gt;    * duplicate site detection techniques\n\nI want to enable hash-based content-body duplication detection to the\ngreatest extent possible. Detecting when whole sites are exactly or\nnearly alike is a challenge for further down the road; at least with\ncontent-body duplicate detection, the storage cost of revisiting entire\nexact-mirror sites remains very small.\n\n&gt;    * session-id detection and handling\n\nThis is an area where there seems to be a lot of folk wisdom and\nhand-tuning (eg the Alexa URL purification regexp rules) but not\nmuch documentation.\n\nI&#39;ve started a tracking database at our Sourceforge project for\ncapturing exactly these kind of real-web tricky/nonobvious challenges\nfor crawlers and retrieval tools -- so that issues and potential\nfixes don&#39;t just get hidden in the code (and mind) of the first\nperson to encounter them.\n\n&gt;    * what user-agent we use.\n\nI think we need to pick a name for the software package better than\n&quot;archive-crawler&quot; -- and that name should be featured in the User-Agent\nstring during official Archive crawls.\n\nSome crawl applications/customers will need to change User-Agent\narbitrarily, so that will also be supported.\n\n- Gordon\n\n\n----- Original Message -----\nFrom: &quot;Brewster Kahle&quot; &lt;brewster@...&gt;\nTo: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;; &quot;Brad Tofel&quot; &lt;brad@...&gt;; &quot;Igor Ranitovic&quot; &lt;igor@...&gt;; &lt;michele@...&gt;\nCc: &quot;Raymie Stata&quot; &lt;raymie@...&gt;; &lt;gojomo@...&gt;\nSent: Thursday, February 20, 2003 9:08 PM\nSubject: Re: Crawler engineering review kickoff - 10am Friday\n\n\n&gt;\n&gt; thank you for pulling this together.\n&gt;\n&gt; A few other design decisions that seem to be important at this stage:\n&gt;\n&gt;    * how to prioritize the urls to crawl.   maybe we could list the different ways current crawlers work as a guideline for some\nof the options.\n&gt;         we know xyleme and alexa and mercator.\n&gt;       gordon, maybe you could write up a short summary as a way to understand the options.\n&gt;\n&gt;    * how tasks will be broken up on different machines.  for instance:\n&gt;        1.  different machines for different stages of processing, or\n&gt;        2.  a host does all stages, but urls are broken up onto different machines some way\n&gt;\n&gt;    *  will we use an RAM based system or disk based (for the url list)\n&gt;        (xyleme and mercator used RAM,    (xyleme said this was a big limitation on their system)\n&gt;          alexa does not)\n&gt;        I dont know if you can buy cost-effective large ram machines.\n&gt;        talking with Jad at alexa, he said 2GB is cheap otherwise very expensive (this would be\n&gt;        based on the HP line).\n&gt;\n&gt;    *  how can remote coordination be done between crawlers that are geographically far apart\n&gt;\n&gt;    * what the thread/process model should be for processing the urls.   separate processes or threads\n&gt;       in a single process.\n&gt;\n&gt;    * duplicate site detection techniques\n&gt;\n&gt;    * session-id detection and handling\n&gt;\n&gt;    * what user-agent we use.\n&gt;\n&gt;\n&gt; -brewster\n&gt;\n&gt;\n&gt; At 06:13 PM 2/20/2003 -0800, Gordon Mohr wrote:\n&gt; &gt;Brad, Brewster, Igor:\n&gt; &gt;\n&gt; &gt;Key points to cover in our crawler engineering review meeting\n&gt; &gt;(tomorrow morning at 10am) are listed below.\n&gt; &gt;\n&gt; &gt;Most important will be going over the general requirements,\n&gt; &gt;so if you have a chance to look over the first document referenced\n&gt; &gt;below before the meeting, that&#39;d be ideal.\n&gt; &gt;\n&gt; &gt;We lose Brewster to another meeting at 11am -- but it&#39;s early in\n&gt; &gt;the project and we should try to keep discussion to broad issues\n&gt; &gt;rather than details, so I&#39;m hopeful we&#39;ll get through what we\n&gt; &gt;need to in an hour.\n&gt; &gt;\n&gt; &gt;Agenda:\n&gt; &gt;\n&gt; &gt;(1) General crawler requirements\n&gt; &gt;\n&gt; &gt;  This document is a starting point:\n&gt; &gt;\n&gt; &gt;  http://homeserver.archive.org/webprojects/crawler-requirements.htm\n&gt; &gt;\n&gt; &gt;  Does this breakdown cover everything it should? What are\n&gt; &gt;  the relative priorities of all the requirements -- in\n&gt; &gt;  both urgency and long-term importance?\n&gt; &gt;\n&gt; &gt;(2) Key early design decisions\n&gt; &gt;\n&gt; &gt;  Using Java\n&gt; &gt;\n&gt; &gt;  Crawler modeled as loosely-linked program stages connected\n&gt; &gt;  by queues.\n&gt; &gt;\n&gt; &gt;  Top-level stages:\n&gt; &gt;   -&gt; URIChoosing    (what URI next?)\n&gt; &gt;   &#92;- Preprocessing  (robots/politeness/etc.)\n&gt; &gt;   &#92;_ Fetching       (HTTP)\n&gt; &gt;   &#92;_ Postprocessing (analysis/archival)\n&gt; &gt;   &#92;_ URIStoring     (return URI(s) to pool for revisiting later)\n&gt; &gt;\n&gt; &gt;  First priority: dumb, bad single-machine crawler -- then\n&gt; &gt;  incremental extension to better, multi-machine crawler.\n&gt; &gt;\n&gt; &gt;(3) Chosen tools and processes\n&gt; &gt;\n&gt; &gt;  &quot;archive-crawler&quot; Yahoo Group for team discussion and file sharing:\n&gt; &gt;\n&gt; &gt;    http://groups.yahoo.com/group/archive-crawler/\n&gt; &gt;\n&gt; &gt;  &quot;archive-crawler&quot; Sourceforge project for source code/CVS hosting\n&gt; &gt;  and engineering-issue tracking:\n&gt; &gt;\n&gt; &gt;    http://sourceforge.net/projects/archive-crawler\n&gt; &gt;\n&gt; &gt;  Overview design documents, plus Javadoc extracted from skeleton/\n&gt; &gt;  actual code files.\n&gt; &gt;\n&gt; &gt;  Engineering review meetings (like this one) every few weeks to\n&gt; &gt;  get feedback and report progress.\n&gt; &gt;\n&gt; &gt;  Others needed or recommended?\n&gt; &gt;\n&gt; &gt;(4) Test plans\n&gt; &gt;\n&gt; &gt;  Unit/stage conformance tests alongside coding\n&gt; &gt;\n&gt; &gt;  Acceptable output tests using existing ARC processing tools\n&gt; &gt;  and compared on &quot;test crawls&quot; against historic data or data\n&gt; &gt;  obtained by other crawlers.\n&gt; &gt;\n&gt; &gt;  Coordination with acceptance rules/processes established\n&gt; &gt;  by the national libraries and project agreement.\n&gt; &gt;\n&gt; &gt;(5) Everything else (?)\n&gt; &gt;\n&gt; &gt;- Gordon\n&gt;\n&gt;\n\n\n"}}