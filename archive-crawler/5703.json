{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":370021973,"authorName":"takeru sasaki","from":"takeru sasaki &lt;sasaki.takeru@...&gt;","replyTo":"LIST","senderId":"F9VESgotcySt5UODqFBH6zESmdkCnarTsQMK_liKwEeWEVofkKJReQC9KYrLBzZxtijZXys_TOlGfS5dciPXwkVQ0qfqdLeJlt7YsuIK","spamInfo":{"isSpam":false,"reason":"12"},"subject":"I wrote re-crawl helper script","postDate":"1235736901","msgId":5703,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM1MDczNjZmMDkwMjI3MDQxNXkzOTMyZWIyN2wxMDdlOGMxNDQ1ODVlYzcxQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":5702,"nextInTime":5704,"topicId":5703,"numMessagesInTopic":1,"msgSnippet":"Hi, everyone, If you interested, please help me. Our project is: need to crawl about 1000-10000 sites, or more. need to check updates, various period.","rawEmail":"Return-Path: &lt;sasaki.takeru@...&gt;\r\nX-Sender: sasaki.takeru@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 65345 invoked from network); 27 Feb 2009 12:15:02 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m52.grp.scd.yahoo.com with QMQP; 27 Feb 2009 12:15:02 -0000\r\nX-Received: from unknown (HELO wf-out-1314.google.com) (209.85.200.171)\n  by mta16.grp.scd.yahoo.com with SMTP; 27 Feb 2009 12:15:02 -0000\r\nX-Received: by wf-out-1314.google.com with SMTP id 24so1157580wfg.5\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 27 Feb 2009 04:15:02 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.142.113.17 with SMTP id l17mr1237467wfc.293.1235736901973; \n\tFri, 27 Feb 2009 04:15:01 -0800 (PST)\r\nDate: Fri, 27 Feb 2009 21:15:01 +0900\r\nMessage-ID: &lt;c507366f0902270415y3932eb27l107e8c144585ec71@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: takeru sasaki &lt;sasaki.takeru@...&gt;\r\nSubject: I wrote re-crawl helper script\r\nX-Yahoo-Group-Post: member; u=370021973\r\n\r\nHi, everyone,\n\nIf you interested, please help me.\n\nOur project is:\n  need to crawl about 1000-10000 sites, or more.\n  need to check updates, various period. day/week/month... I want to\ndefine site SURT or REGEXP.\n  all content is not needed. I want to create filter SURT or REGEXP.\n\nI think our workflow is :\n 1. pre-crawl\n       small crawl for new site. about 10-30 minutes. Collect site\nstructure overview.\n 2. define URL filter for new site. from pre-crawl logs.\n 3. daily crawl\n     from seeds and defined URL filters, check sites.\n 4. daily DB update.\n     parse fetched contents in ARCorWARC, update my Database.\n\nAt first, for &#39;3&#39;, I wrote re-crawl helper script.\nhttp://dl.getdropbox.com/u/76825/ruby/setup_recrawl_job_20090227.rb\nhttp://dl.getdropbox.com/u/76825/ruby/recrawlable_order_20090227.xml\n\nIt can create job name = Daily_00001,Daily_00002,Daily_00003 .....\norder.xml and seeds.txt is copied, state DB is migrated.\nIt is very usefull.\n\nPlease comment me about it.\n\n\nAnd TODOs:\n# TODO (help me!!)\n#   - I need to re-fed urls from last job&#39;s Frontier?? &quot;Ended by\noperator&quot; or Clashed job.\n#   - How to controll re-visit period(everyday/everyweek/everymonth/as\nsoon as possible/never...)\n#   - Is it better to run without webui?? but I want to progress...\n#   - How to stop WebUI from console. PID-FILE and KILL???\n\nThank you.\nand I want good solution for &#39;1&#39; or &#39;2&#39;, too.\n\ntakeru.\n\n##########################################################\n# setup_recawl_job.rb    2009/2/27 sasaki.takeru@...\n#   Heritrix helper script for re-crawling task.\n#\n#   ALL COMMENT OR MESSAGE IS WELCOME!!!\n#   PLEASE HELP ME!!!\n#\n# step-by-step:\n#   1. Download and setup Heritrix (version=1.14.2)\n#\n#   2. Create re-crawl supported profile(=Recrawlable) with WebUI.\n#       Read last of this script.\n#       Edit modules, settings, add urls to seeds.\n#\n#   3. Stop WebUI process.\n#\n#   4. Create first crawl job (=00001)\n#      run this script. --base is job or profile directory.\n#       ruby setup_recawl_job.rb --profile TestABC --state SKIP &#92;\n#                                --base ./conf/profiles/Recrawlable &#92;\n#                                --run --admin xxxx:yyyy\n#      New job &#39;TestABC_00001&#39; is created based on profile &#39;Recrawlable&#39;.\n#      order.xml and seeds.txt is copied.\n#\n#   5. Crawl will automaticaly start(by --run).\n#      run few minutes. And stop job and stop WebUI process.\n#\n#   6. Create second crawl job (=00002)\n#      run this script.\n#       ruby setup_recawl_job.rb --run --admin xxxx:yyyy --profile TestABC\n#      Latest job(=00001) is found and copied order.xml and seeds.txt\nto new job &#39;TestABC_00002&#39;,\n#      a &#39;state&#39; DB for re-crawl is migtarted.\n#\n#   7. Crawl will automaticaly start(by --run).\n#       Check WebUI and logs re-crawl functions are working.\n#       This is my case:\n#       WebUI Console: 334 MB crawled (279 MB novel, 25 MB\ndup-by-hash, 30 MB not-modified)\n#       crawl.log    : appear &#39;unwritten:identicalDigest&#39;,\n&#39;warcRevisit:notModified&#39;\n#         2009-02-27T09:36:22.474Z   200        372\nhttp://itpro.nikkeibp.co.jp/robots.txt EP\nhttp://itpro.nikkeibp.co.jp/article/OPINION/20090226/325527/ text/html\n#047 20090227093622360+109 sha1:OZUREG6MOW4CDQU6CGEQ4543MNPQMLSA -\nunwritten:identicalDigest\n#         2009-02-27T09:40:22.873Z   304          0\nhttp://codezine.jp/static/common/images/btn-bn.gif LEEE\nhttp://codezine.jp/article/detail/3626 no-type #032\n20090227094022746+76 sha1:3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ -\nwarcRevisit:notModified\n#\n# TODO (help me!!)\n#   - I need to re-fed urls from last job&#39;s Frontier?? &quot;Ended by\noperator&quot; or Clashed job.\n#   - How to controll re-visit period(everyday/everyweek/everymonth/as\nsoon as possible/never...)\n#   - Is it better to run without webui?? but I want to progress...\n#   - How to stop WebUI from console. PID-FILE and KILL???\n##########################################################\n\n"}}