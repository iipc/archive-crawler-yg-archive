{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"dAw6-tEAzMu1FAHKCa_E6DgSKncydBqpwRfDvSmgG4ccv5gAXemmJ4NgutqMWJa-xXSAi9rzzzhn-8gM5voRRHtrIcyJfFY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: failed to get char replay sequence","postDate":"1149205579","msgId":2902,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0N0Y3QzRCLjkwODAzMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGU1bnJxaSs5M3QzQGVHcm91cHMuY29tPg==","referencesHeader":"PGU1bnJxaSs5M3QzQGVHcm91cHMuY29tPg=="},"prevInTopic":2901,"nextInTopic":2903,"prevInTime":2901,"nextInTime":2903,"topicId":2891,"numMessagesInTopic":8,"msgSnippet":"... One thing to keep in mind is that the % completion shown in the web UI is a very rough/flawed estimate -- almost certain to be a massive underestimate in","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 1931 invoked from network); 1 Jun 2006 23:47:40 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m24.grp.scd.yahoo.com with QMQP; 1 Jun 2006 23:47:40 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta2.grp.scd.yahoo.com with SMTP; 1 Jun 2006 23:47:40 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 0782514156C8D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  1 Jun 2006 16:46:01 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 31877-03-49 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 1 Jun 2006 16:46:00 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 0853F14156C88\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  1 Jun 2006 16:46:00 -0700 (PDT)\r\nMessage-ID: &lt;447F7C4B.9080304@...&gt;\r\nDate: Thu, 01 Jun 2006 16:46:19 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e5nrqi+93t3@...&gt;\r\nIn-Reply-To: &lt;e5nrqi+93t3@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: failed to get char replay sequence\r\nX-Yahoo-Group-Post: member; u=137285340; y=0rHWU_r9k8s3Pdt1StmAeRj3l2_vr55r3w1yXYHvHjZD\r\nX-Yahoo-Profile: gojomo\r\n\r\njonathansiddharth wrote:\n&gt; Yes that is all I&#39;m crawling(wikipedia pages..and archiving just the\n&gt; history pages of featured articles. Im starting the crawl from the\n&gt; featured aticles home page).\n&gt; \n&gt; I don&#39;t think the crawl hangs. Its just that the FRontier queue is so\n&gt; long that progress in % appears slow. Looking at the crawl log in real\n&gt; time shows that it is crawling. \n&gt; \n&gt; But it seems to stop abruptly after crawling for say 7 to 8 hrs. The\n&gt; estimated time was 4 days. With only say 5% of the job done.\n&gt; The log says the job is &quot;finished&quot; ,no exception or alert.\n&gt; \n&gt; I dont understand one thing. The log says the job is finished. Yet\n&gt; each time I look at its crawl report it is updated. The URIs\n&gt; discovered keeps increasing. Does it mean that when it says crawl job\n&gt; done it means you are done crawling. But the % progress that is shown\n&gt; tells you how many more URLs are left to explore? In other words, your\n&gt; crawl can end when you&#39;re progress on the console page is just 5%. But\n&gt; the URLs already in the queue keep getting downloaded?\n&gt; \n&gt; I&#39;m sorry but this really confused me.\n\nOne thing to keep in mind is that the % completion shown in the web UI \nis a very rough/flawed estimate -- almost certain to be a massive \nunderestimate in usual situations. It would only be accurate if no \nadditional URIs are discovered, and future progress is at the same rate \nas previous progress. To the contrary, URIs are always being discovered, \nand typically the end of crawls are dominated by large/slow hosts where \npoliteness dominates, making progress slower at the end.\n\n(I thought there was an existing RFE for better estimates, but couldn&#39;t \nfind it, so I made one: \nhttp://sourceforge.net/support/tracker.php?aid=1499205 .)\n\nThat said, the crawl should only show as &#39;finished&#39; if it has in fact \nstopped crawling -- so nothing new should be appearing in the crawl.log, \nnor changes occurring to the console/report totals. In what log do you \nsee &#39;finished&#39;?\n\nAre you sure that a followup job hasn&#39;t started, after the first \nfinishes? Have you set any URL/time limits on your crawl that could \nexplain a finish while URLs remained in queues?\n\n- Gordon @ IA\n\n\n&gt; thanks,\n&gt; Jonathan\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@...&gt; wrote:\n&gt;&gt; For sure its hung?  You seem to be crawling wikipedia this time and in \n&gt;&gt; your last mail.  Is this all you are crawling?  If so, retries because \n&gt;&gt; of the below connection timeouts with the Heritrix increasing interval \n&gt;&gt; between retries may look like the crawler is hung.  Check the frontier \n&gt;&gt; report.  \n&gt;&gt;\n&gt;&gt; Is there nothing in heritrix_out.log?  You could send the process a \n&gt;&gt; SIGQUIT signal -- $ kill -3 HERITRIX_PROCESS_ID -- and it&#39;ll send a \n&gt;&gt; thread dump to heritrix_out.log.  Study the output.  Wait a while.  \n&gt;&gt; Redo. Compare the stack traces.  Any progress?  If none or you can&#39;t \n&gt;&gt; make sense of it, send it on over. Maybe we can see something in thread \n&gt;&gt; dump tea leaves.\n&gt;&gt;\n&gt;&gt; St.Ack\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; jonathansiddharth wrote:\n&gt;&gt;&gt; Thanks for the reply St.Ack.\n&gt;&gt;&gt; I think you&#39;re right. Although I got these alerts, I dont think the\n&gt;&gt;&gt; crawl terminated because of that.\n&gt;&gt;&gt; Because on re-running the crawl it again mysteriously stopped after a\n&gt;&gt;&gt; (long) time even though I&#39;m sure the crawl was not completed. It was\n&gt;&gt;&gt; only some 5% done. And this time there were no alerts.\n&gt;&gt;&gt; However I saw some errors in the local-errors.log of this sort\n&gt;&gt;&gt;\n&gt;&gt;&gt; 2006-06-01T04:42:01.909Z    -2          -\n&gt;&gt;&gt; http://en.wikipedia.org/wiki/J%C3%BCrgen_Habermas LL\n&gt;&gt;&gt; http://en.wikipedia.org/wiki/Bernard_Williams no-type #032 - - -\n&gt;&gt;&gt; le:SocketTimeoutException@HTTP\n&gt;&gt;&gt; java.net.SocketTimeoutException: connect timed out: timeout set at\n&gt;&gt;&gt; 20000ms.\n&gt;&gt;&gt;       at\n&gt;&gt;&gt;\n&gt; org.archive.crawler.fetcher.HeritrixProtocolSocketFactory.createSocket(HeritrixProtocolSocketFactory.java:142)\n&gt;&gt;&gt;       at\n&gt;&gt;&gt;\n&gt; org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)\n&gt;&gt;&gt;       at\n&gt;&gt;&gt;\n&gt; org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:382)\n&gt;&gt;&gt;       at\n&gt;&gt;&gt;\n&gt; org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:168)\n&gt;&gt;&gt;       at\n&gt;&gt;&gt;\n&gt; org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)\n&gt;&gt;&gt;       at\n&gt;&gt;&gt;\n&gt; org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)\n&gt;&gt;&gt;       at \n&gt;&gt;&gt; org.archive.crawler.fetcher.FetchHTTP.innerProcess(FetchHTTP.java:408)\n&gt;&gt;&gt;       at \n&gt;&gt;&gt; org.archive.crawler.framework.Processor.process(Processor.java:103)\n&gt;&gt;&gt;       at\n&gt;&gt;&gt;\n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:306)\n&gt;&gt;&gt;       at\n&gt; org.archive.crawler.framework.ToeThread.run(ToeThread.java:153)\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; There were 2 errors like this.\n&gt;&gt;&gt; I doubt if this caused the crawl to end though. Since there were two\n&gt;&gt;&gt; of these and if it was really terminal the first one should have\n&gt;&gt;&gt; brought the crawl to an end.\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; is there anyother condition for which the crawler gracefully gives up.\n&gt;&gt;&gt; Like if there are too many URLs in the frontier queue.\n&gt;&gt;&gt; I&#39;m assuming a heap overflow would have an explicit error message\n&gt;&gt;&gt; indicating that.\n&gt;&gt;&gt;\n&gt;&gt;&gt; (My estimated crawl job time was 4 days)\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; thanks,\n&gt;&gt;&gt; Jonathan\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@&gt; wrote:\n&gt;&gt;&gt;&gt; jonathansiddharth wrote:\n&gt;&gt;&gt;&gt;&gt; Hi,\n&gt;&gt;&gt;&gt;&gt;         This alert/exception abruptly terminated my crawl job.\n&gt; Could\n&gt;&gt;&gt;&gt;&gt; someone tell me what it is and how it can be avoided?\n&gt;&gt;&gt;&gt; The below is an old faithful.  See\n&gt;&gt;&gt;&gt;\n&gt; http://sourceforge.net/tracker/index.php?func=detail&aid=1218961&group_id=73833&atid=539099.\n&gt; \n&gt; &lt;http://sourceforge.net/tracker/index.php?func=detail&aid=1218961&group_id=73833&atid=539099.&gt;\n&gt;&gt;&gt;&gt; It rears its head from time to time but we&#39;ve not been able to\n&gt; figure\n&gt;&gt;&gt;&gt; why it happens nor how to reliably reproduce.  I&#39;m guessing if you\n&gt;&gt;&gt; crawl\n&gt;&gt;&gt;&gt; that same page again in wikipedia, you&#39;ll succeed (least it did just\n&gt;&gt;&gt; now\n&gt;&gt;&gt;&gt; for me when I tried it).  I&#39;m surprised it terminated your crawl. \n&gt;&gt;&gt;&gt; Usually we just fail extraction on a particular page and just\n&gt; move to\n&gt;&gt;&gt;&gt; the next in the queue.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; St.Ack\n&gt;&gt;&gt;&gt; P.S. I&#39;ve just added more logging around this exception.   Perhaps\n&gt;&gt;&gt; it&#39;ll\n&gt;&gt;&gt;&gt; turn up the needed clue.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; SPONSORED LINKS\n&gt;&gt;&gt; Computer security \n&gt;&gt;&gt;\n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+security&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=BHmcxBg5sKfN9-gcWnJWDg&gt;\n&gt; \n&gt;&gt;&gt; \tComputer training \n&gt;&gt;&gt;\n&gt; &lt;http://groups.yahoo.com/gads?t=ms&k=Computer+training&w1=Computer+security&w2=Computer+training&c=2&s=46&.sig=v0JjJWA4s7mLnWQWdFxuTQ&gt;\n&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt; ------------------------------------------------------------------------\n&gt;&gt;&gt; YAHOO! GROUPS LINKS\n&gt;&gt;&gt;\n&gt;&gt;&gt;     *  Visit your group &quot;archive-crawler\n&gt;&gt;&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;&gt;&gt;        \n&gt;&gt;&gt;     *  To unsubscribe from this group, send an email to:\n&gt;&gt;&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;&gt;      \n&gt; &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;&gt;&gt;        \n&gt;&gt;&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt;&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt; ------------------------------------------------------------------------\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n\n\n"}}