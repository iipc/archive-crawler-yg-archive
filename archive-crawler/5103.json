{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225553165,"authorName":"Svein Yngvar Willassen","from":"&quot;Svein Yngvar Willassen&quot; &lt;svein@...&gt;","replyTo":"LIST","senderId":"sItp4T7IPCcSAAiBV8LBK55Yl4LdGuVlj6TpwAdYSCVDJyVtodQI_cOnP_9BqO7xkGHCk7lsqwHwDqRq-QyIfWAhhQ953fKXr8wdv6alZlJIywFr","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Nutch or Heritrix?","postDate":"1207585192","msgId":5103,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDJlNjdmNWIwMDgwNDA3MDkxOWw4ZGQ2ZTQyaDc0OTBhZWRhMzFmY2JhNTdAbWFpbC5nbWFpbC5jb20+","inReplyToHeader":"PGZ0ZGU2ZytobGkzQGVHcm91cHMuY29tPg==","referencesHeader":"PDJlNjdmNWIwMDgwNDA1MDYzNnU5NDZmMTdjeDIwZDVmZDQ3NDE5M2RiMzdAbWFpbC5nbWFpbC5jb20+CSA8ZnRkZTZnK2hsaTNAZUdyb3Vwcy5jb20+"},"prevInTopic":5101,"nextInTopic":5104,"prevInTime":5102,"nextInTime":5104,"topicId":5099,"numMessagesInTopic":6,"msgSnippet":"... No, we just want to use Hadoop to store and process the data from the crawlers. ... Heritrix Cluster Controller (hcc)?  I ve had a look at it, but must ","rawEmail":"Return-Path: &lt;svein@...&gt;\r\nX-Sender: svein@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 14715 invoked from network); 7 Apr 2008 16:19:54 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m36.grp.scd.yahoo.com with QMQP; 7 Apr 2008 16:19:54 -0000\r\nX-Received: from unknown (HELO gv-out-0910.google.com) (216.239.58.190)\n  by mta18.grp.scd.yahoo.com with SMTP; 7 Apr 2008 16:19:54 -0000\r\nX-Received: by gv-out-0910.google.com with SMTP id e6so358199gvc.18\n        for &lt;archive-crawler@yahoogroups.com&gt;; Mon, 07 Apr 2008 09:19:53 -0700 (PDT)\r\nX-Received: by 10.150.200.8 with SMTP id x8mr2801979ybf.149.1207585192930;\n        Mon, 07 Apr 2008 09:19:52 -0700 (PDT)\r\nX-Received: by 10.150.134.6 with HTTP; Mon, 7 Apr 2008 09:19:52 -0700 (PDT)\r\nMessage-ID: &lt;2e67f5b00804070919l8dd6e42h7490aeda31fcba57@...&gt;\r\nDate: Mon, 7 Apr 2008 18:19:52 +0200\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;ftde6g+hli3@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;2e67f5b00804050636u946f17cx20d5fd474193db37@...&gt;\n\t &lt;ftde6g+hli3@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Svein Yngvar Willassen&quot; &lt;svein@...&gt;\r\nSubject: Re: [archive-crawler] Re: Nutch or Heritrix?\r\nX-Yahoo-Group-Post: member; u=225553165; y=NlX8CBwJ4m9z0W4ZRrQolDUkC5koU6S_tRnsRIVjrekHWQ\r\n\r\n&gt; &gt; Basically, we&#39;re going to set up Hadoop and crawl the web for images.\n&gt;\n&gt; You intent to control your crawlers using Hadoop?\n\nNo, we just want to use Hadoop to store and process the data from the crawlers.\n\n&gt; &gt; - Which crawler will best adapt to a distributed crawling system, in\n&gt; which we\n&gt; &gt; use many servers conducting crawling together?\n&gt;\n&gt; There is an open source project that does distributed control of\n&gt; Heritrix 1.12. I don&#39;t remember the name of it.\n\nHeritrix Cluster Controller (hcc)?  I&#39;ve had a look at it, but must\nadmit I didn&#39;t understand how it works from the documentation. Does it\nallow controlling a cluster of Heritrix instances in such as way that\nan URL is fetched only once by one computer in the cluster? That seems\nto be the most important property of clustered crawling. Perhaps you\nor someone else can fill me in here.\n\n&gt; &gt; - Which crawler is/will be under most active development?\n&gt;\n&gt; This is the Heritrix mailing list, and so therefore....\n\n;)\n\nI posted the same question to the nutch mailing list and go the\nopposite answer of course...  ;)\n\n-- \nBest Regards,\n\nSvein Y. Willassen\nhttp://willassen.blogspot.com/\n\n"}}