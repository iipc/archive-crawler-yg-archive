{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"BdZzlQ3zP6eiccmeyqac3fegEZvBcj1Q4r2quz7hTKuiePh83ILpGrCuT_jYXJiFESWslKx4lFJob2BstrD-JKCJ4O0F7J8","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] snooze at the end","postDate":"1376357773","msgId":8301,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUyMDk4RDhELjEwNzA4MDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGt1NnA1cSs4OHZwQGVHcm91cHMuY29tPg==","referencesHeader":"PGt1NnA1cSs4OHZwQGVHcm91cHMuY29tPg=="},"prevInTopic":8300,"nextInTopic":0,"prevInTime":8300,"nextInTime":8302,"topicId":8300,"numMessagesInTopic":2,"msgSnippet":"... My main suggestion: don t feel that the crawl needs to finish by emptying all the queues. If you ve made a manual determination the remaining queues aren t","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 82400 invoked by uid 102); 13 Aug 2013 01:36:15 -0000\r\nX-Received: from unknown (HELO mtaq2.grp.bf1.yahoo.com) (10.193.84.33)\n  by m11.grp.bf1.yahoo.com with SMTP; 13 Aug 2013 01:36:15 -0000\r\nX-Received: (qmail 21553 invoked from network); 13 Aug 2013 01:36:14 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mtaq2.grp.bf1.yahoo.com with SMTP; 13 Aug 2013 01:36:14 -0000\r\nX-Received: (qmail 50825 invoked by uid 0); 13 Aug 2013 01:36:14 -0000\r\nX-Received: from 67.169.10.13 (HELO silverbook.local) (67.169.10.13)\n  by relay00.pair.com with SMTP; 13 Aug 2013 01:36:14 -0000\r\nX-pair-Authenticated: 67.169.10.13\r\nMessage-ID: &lt;52098D8D.1070803@...&gt;\r\nDate: Mon, 12 Aug 2013 18:36:13 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:17.0) Gecko/20130620 Thunderbird/17.0.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ku6p5q+88vp@...&gt;\r\nIn-Reply-To: &lt;ku6p5q+88vp@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] snooze at the end\r\nX-Yahoo-Group-Post: member; u=137285340; y=3FTxnfUGWGin2sCuXmRi10C4Jx5-qYVOb2gff78411pG\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 8/10/13 6:25 PM, bobbyledingo wrote:\n&gt; At the end of the crawl I always have a lot URI queues snoozed and\n&gt; it  takes years to finish\n&gt;\n&gt; I don&#39;t understand because I have the following settings in my\n&gt; BdbFrontier :\n&gt;\n&gt;   &lt;bean id=&quot;frontier&quot;\n&gt;     class=&quot;org.archive.crawler.frontier.BdbFrontier&quot;&gt;\n&gt;\n&gt;          &lt;property name=&quot;snoozeLongMs&quot; value=&quot;300000&quot; /&gt;\n&gt;          &lt;property name=&quot;retryDelaySeconds&quot; value=&quot;1&quot; /&gt;\n&gt;          &lt;property name=&quot;maxRetries&quot; value=&quot;3&quot; /&gt;\n&gt;\n&gt; So it should give up really quickly but that&#39;s not the case, I always\n&gt; have one/two hundreds of URI queues who takes years to finish (after\n&gt; having crawled approximatly 60 753 URI queues)...\n\nMy main suggestion: don&#39;t feel that the crawl needs to finish by \nemptying all the queues.\n\nIf you&#39;ve made a manual determination the remaining queues aren&#39;t \nimportant enough to wait for, manually end the crawl.\n\nIf you need to enforce a fixed time budget, automatically end it (with \nthe CrawlLimitEnforcer or an external process calling into its web \ncontrol console) at your time limit.\n\nIf you have some other heuristic -- no 200-OKs in the logs for the last \nN minutes -- implement that.\n\nYou can set the frontier&#39;s &#39;dumpPendingAtClose&#39; property to &#39;true&#39; if \nyou&#39;d like your crawl.log to include a dump of all \ndiscovered-but-untried URIs when the crawl is force-ended.\n\nRegarding the settings and their effect on your expected beahvior:\n\nIt turns out &#39;snoozeLongMs&#39; has no current role - it&#39;s left over from a \nprior internal detail so can be ignored.\n\nThe &#39;retryDelaySeconds&#39; should generally be longer than 1 second. It&#39;s \nused when there&#39;s the kind of transient error -- like a failure to \nresolve a DNS name, or unconnectable host -- that if it resolves itself, \ntakes a while (minutes to hours). So most crawls don&#39;t want to mark the \nURI as permanently-failed after just 3 tries over 3 seconds.\n\nSimilarly, usually you&#39;ll want more than 3 retries (even if not the \ndefault of 30). Some normal temporary put-aside-and-retry operations (a \nnew DNS resolution, a (re-)fetched robots.txt, or \nauthentication-form-submission) use up a retry or two... so a \n&#39;maxRetries&#39; of 3 means even a momentary network problem could cause a \nURI to be finished-as-failed entirely.\n\nStill, if your goal is to rush through URIs (and you&#39;re OK with just \nmissing content when temporary network hiccups occur), then with your \nsettings, each URI (not queue) will be tried up to 3 times, with a 1 \nsecond wait (&#39;snooze&#39;) between each try. Some tries (connects to hosts \nthat exist but aren&#39;t aren&#39;t promptly accepting connections, or DNS \nlookups) may take a while to time-out. (These will be shown as URIs in \nprocess rather than snoozed queues.) Also, there&#39;s a sort of &#39;retry \nmultiplication&#39; that can happen with the DNS/robots prerequisite \nattempts: each normal URI will be tried 3 times, but when it&#39;s \nconsidered and noticed that DNS/robots info isn&#39;t available, those may \nthemselves be tried 3 times (for each normal URI).\n\nAltogether, even with your settings, a queue with thousands (or tens of \nthousands) of URIs, all on an unresolvable/unconnectable host, will \nstill take 3* (or 3*3*) thousands (or tens of thousands) of seconds to \ntry-and-retry-and-fail each URI.\n\nAlternatively, if the host is responding but slow, the snooze durations \nare set by the politeness settings -- delayFactor, minDelayMs, \nmaxDelayMs, and respectCrawlDelayUpToSeconds -- rather than the \nretryDelaySeconds. You can tighten those politeness values a little but \nif the site notices you as a nuisance you may just trigger throttling or \nblocking entirely.\n\nIf any of these politeness/retry delays mean you ultimately would go \nover your time budget for the crawl, it&#39;s fine to just end the crawl \nwhen time runs out with URIs still in queues. By instead forcing the \nURIs out via few/fast retries, you&#39;ll turn short network outages into \nsituations where many more URIs are skipped entirely, or risk \nretaliatory blocking.\n\n- Gordon\n\n"}}