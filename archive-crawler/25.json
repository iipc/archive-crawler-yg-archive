{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":95697582,"authorName":"G.B.Reddy","from":"&quot;G.B.Reddy&quot; &lt;reddy@...&gt;","profile":"gbreddysoft","replyTo":"LIST","senderId":"UP8Fx0vvIVTIPeEIJvkx47Uy1L8VVAkkZ1iyhPhAQwewudAfnWgWM3_aDlMWgtWMxXCFnU7o03Lea8R10ej2D0eTbFtSA60OnylL7gc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: DNS and HTTP staging (was Re: [archive-crawler] Re: Web crawler work ??","postDate":"1047485649","msgId":25,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAyNDEwMWMyZThiMiQ2YTUxZTU5MCRkNTAwYThjMEB0aWRlbHBhcmsuaXNvZnR0ZWNoaW5kaWEuY29tPg==","referencesHeader":"PDM3ZWQwMWMyZDczNCQ0ZjQ0NjliMCRkNTAwYThjMEBSZWRkeUdCPiA8MDM4MDAxYzJkN2I2JDJjMTIxZjAwJDNhZWJlZGQxQGdvam92YWlvPiA8M2ZlMDAxYzJkOWM3JGU1OGE0YTgwJGQ1MDBhOGMwQFJlZGR5R0I+IDwwMGRjMDFjMmRhMDckNjZlZTEzNjAkM2FlYmVkZDFAZ29qb3ZhaW8+IDwwNzY5MDFjMmRlODEkMTNmY2ZlOTAkZDUwMGE4YzBAdGlkZWxwYXJrLmlzb2Z0dGVjaGluZGlhLmNvbT4gPDAwOGUwMWMyZGY2ZSQzZDQ4NGJlMCQ0ZGViZWRkMUBXT1JLU1RBVElPTjIxPiA8MGVjNjAxYzJlNDA5JDBiYzYwMmMwJGQ1MDBhOGMwQHRpZGVscGFyay5pc29mdHRlY2hpbmRpYS5jb20+IDwxMTY0MDFjMmU0YzYkZmNhMjU1ZjAkZDUwMGE4YzBAdGlkZWxwYXJrLmlzb2Z0dGVjaGluZGlhLmNvbT4gPDAxYTQwMWMyZTUwMSQ5MjI3NDNkMCQ0ZGViZWRkMUBXT1JLU1RBVElPTjIxPg=="},"prevInTopic":24,"nextInTopic":26,"prevInTime":24,"nextInTime":26,"topicId":24,"numMessagesInTopic":5,"msgSnippet":"Gordon, I am done with the asynchronous DNS code. I shall test it more tomorrow and checkin. I may start using the caching mechanism present in the dnsjava ","rawEmail":"Return-Path: &lt;reddy@...&gt;\r\nX-Sender: reddy@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (EGP: mail-8_2_6_1); 12 Mar 2003 16:15:00 -0000\r\nReceived: (qmail 81934 invoked from network); 12 Mar 2003 16:14:57 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m9.grp.scd.yahoo.com with QMQP; 12 Mar 2003 16:14:57 -0000\r\nReceived: from unknown (HELO pub.isofttechindia.com) (203.199.202.17)\n  by mta3.grp.scd.yahoo.com with SMTP; 12 Mar 2003 16:14:56 -0000\r\nReceived: from ReddyGB (ReddyGB.isofttechindia.com [192.168.0.213])\n\tby pub.isofttechindia.com (8.11.0/8.11.0) with SMTP id h2CGBri13005;\n\tWed, 12 Mar 2003 21:41:54 +0530\r\nMessage-ID: &lt;024101c2e8b2$6a51e590$d500a8c0@...&gt;\r\nTo: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;, &lt;archive-crawler@yahoogroups.com&gt;\r\nCc: &lt;wcr-team@...&gt;\r\nReferences: &lt;37ed01c2d734$4f4469b0$d500a8c0@ReddyGB&gt; &lt;038001c2d7b6$2c121f00$3aebedd1@gojovaio&gt; &lt;3fe001c2d9c7$e58a4a80$d500a8c0@ReddyGB&gt; &lt;00dc01c2da07$66ee1360$3aebedd1@gojovaio&gt; &lt;076901c2de81$13fcfe90$d500a8c0@...&gt; &lt;008e01c2df6e$3d484be0$4debedd1@WORKSTATION21&gt; &lt;0ec601c2e409$0bc602c0$d500a8c0@...&gt; &lt;116401c2e4c6$fca255f0$d500a8c0@...&gt; &lt;01a401c2e501$922743d0$4debedd1@WORKSTATION21&gt;\r\nSubject: Re: DNS and HTTP staging (was Re: [archive-crawler] Re: Web crawler work ??\r\nDate: Wed, 12 Mar 2003 21:44:09 +0530\r\nOrganization: ISoftTech\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 5.50.4920.2300\r\nX-MimeOLE: Produced By Microsoft MimeOLE V5.50.4920.2300\r\nX-eGroups-From: &quot;G.B.Reddy&quot; &lt;reddy@...&gt;\r\nFrom: &quot;G.B.Reddy&quot; &lt;reddy@...&gt;\r\nReply-To: &quot;G.B.Reddy&quot; &lt;reddy@...&gt;\r\nX-Yahoo-Group-Post: member; u=95697582\r\nX-Yahoo-Profile: gbreddysoft\r\n\r\nGordon,\n\nI am done with the asynchronous DNS code. I shall test it more tomorrow and\ncheckin. I may start using the caching mechanism present in the dnsjava\nlibraries. They seem to be a bit tightly bound with the response processing\ncode which I am using.\n\nThe CrawlURI object as of now contains only the uri string. I would need\nvariables for setting the ipaddress and other state to indicate lookup\nfailures. Please let me know if you have any versions over it.\n\nThanks,\nReddy\n\n\n----- Original Message -----\nFrom: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\nTo: &lt;archive-crawler@yahoogroups.com&gt;\nCc: &lt;wcr-team@...&gt;\nSent: Saturday, March 08, 2003 5:00 AM\nSubject: DNS and HTTP staging (was Re: [archive-crawler] Re: Web crawler\nwork ??\n\n\n&gt; These are good decompositions of the steps involved, and the LGPL dnsjava\n&gt; library looks very useful for our needs.\n&gt;\n&gt; My tendency would be to think fewer stages are better -- and when\n&gt; communicating between our stages, rather than using a shared RequestMap,\n&gt; include on the requesting event whatever context will be needed to deal\n&gt; with the response when it arrives.\n&gt;\n&gt; You can see this technique used in the ostore.network.ADns class I\nreferenced\n&gt; yesterday -- the user_data object lets any client of the ADns stage\nrecover\n&gt; the state it needs to deal with ADns&#39;s eventual response. (We may still\nneed\n&gt; the equivalent of a RequestMap to deal with mapping UDP response packets\nto\n&gt; the lookups that triggered them -- but that&#39;d then be a map that can be\nprivate\n&gt; to a single stage.)\n&gt;\n&gt; Nothing in the DNS procedure needs to block -- once you assume\nasynchronous\n&gt; UDP replies and an in-RAM response cache -- and thus one thread should be\n&gt; as good as (in fact better than) N threads.\n&gt;\n&gt; Similarly, I don&#39;t think more than 1 UDP socket will be necessary for\n&gt; all DNS lookups, because as a practical matter, 1 open UDP socket will\n&gt; never be &quot;busy&quot; in a way that additional UDP sockets would help.\n&gt;\n&gt; --\n&gt; Regarding 404 and other explicit HTTP app errors: these should be recorded\n&gt; into the CrawlURI, and forwarded to the next processing stage, just as\n&gt; with successes.\n&gt;\n&gt; Redirects are a special case we haven&#39;t discussed much yet; they would\nseem\n&gt; candidates for some sort of expedited fetching. I think, though, such\n&gt; expedited activity must pass through the full cycle of stages for\n&gt; proper logging and policy application. (In contrast, retries in the face\n&gt; of transient errors, which may, at least in some cases, be fed immediately\n&gt; back to the Fetching or Preprocessing stages.)\n&gt;\n&gt; --\n&gt; The issue of timeouts (esp. in HTTP) is thorny. From what I hear about\n&gt; crawler traps, we need to not only to be able to deal with hangs, but\n&gt; also various infinite-length trickles.\n&gt;\n&gt; If any of the HTTP-transactions-in-progress is not making sufficient\n&gt; progress, according to some set of progress-per-time-unit thresholds,\n&gt; we have to be ready to give up on it. We should mark it as an error,\n&gt; but probably retain any partial info we did get for later analysis.\n&gt;\n&gt; Separating this out into its own stage increases synchronization issues.\n&gt; If timeout analysis can instead be part of the same single-threaded stage\n&gt; where HTTP some-progress-made events (from sockets) are handled, I think\n&gt; it can be handled very efficiently:\n&gt;\n&gt;    - keep all outstanding transactions on a linked-list\n&gt;    - whenever progress is reported (from the socket),\n&gt;      and that progress keeps the transaction above\n&gt;      acceptable thresholds, move that transaction to\n&gt;      the head of the list\n&gt;    - check the tail of the list occasionally to see\n&gt;      if the worst transactions need to be cut\n&gt;\n&gt; That &quot;occasional&quot; check could mostly occur as part of normal\n&gt; progress-packet processing; an extremely infrequent timer could\n&gt; trigger reevaluation in the rare case where all progress on all\n&gt; sockets has stopped...\n&gt;\n&gt; - Gordon\n&gt;\n&gt; ----- Original Message -----\n&gt; From: G.B.Reddy\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Cc: wcr-team@...\n&gt; Sent: Friday, March 07, 2003 8:31 AM\n&gt; Subject: Re: [archive-crawler] Re: Web crawler work ??\n&gt;\n&gt;\n&gt;\n&gt; More insight on the DNS stages.\n&gt;\n&gt; As stated in the design earlier, &quot;DNS Querying Stage&quot;, &quot;DNS Response\nProcessing Stage&quot; and &quot;Timeout and Retry Handling Stage&quot;\n&gt; access/update the shared RequestMap. So, they need to be synchronized. We\nwere just thinking whether these need to be three separate\n&gt; stages or could it be one stage multiplexing each of those incoming event\ntypes. The pros and cons of them are as below.\n&gt;\n&gt; Single stage benefit / Multi Stage Issue :\n&gt; - If it is single stage, then synchronization is not done across stages.\nEven though, synchronization anyway would be needed\n&gt; internally in case of single stage, conceptually it looks neater not to\nsynchronize across stages.\n&gt;\n&gt; Single stage issue / Multi Stage benefit :\n&gt; - In case of single stage, the event queue would be one and this fact\npulls our legs when we want to handle overload conditions.\n&gt; Since there is no clear count of the distinct elements in the queue, it\nbecomes difficult to analyse and condition the system\n&gt; accordingly. Whereas, in multi stage, each one having its own queue,\nconditioning/prioritizating becomes easy.\n&gt;\n&gt; I think we would end up going in for multi stage, unless SEDA could take\ncare of it by itself.\n&gt;\n&gt; And in case of parallel stages, like the ones in post processing, I think\nmost often, synchronization across them may be\n&gt; unavoidable.\n&gt;\n&gt; Thanks,\n&gt; Reddy\n&gt;\n&gt;\n&gt; ----- Original Message -----\n&gt; From: G.B.Reddy\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Cc: wcr-team@...\n&gt; Sent: Thursday, March 06, 2003 11:21 PM\n&gt; Subject: Re: [archive-crawler] Re: Web crawler work ??\n&gt;\n&gt;\n&gt; Gordon and Raymie,\n&gt;\n&gt; Below are the various stages and their design with the issues involved in\nthe DNS Resolver and HTTP Client implementation.\n&gt;\n&gt; DNS History/Cache Handling Stage :\n&gt;\n&gt; Overview:\n&gt; - Maintains successful lookups in cache.\n&gt; - Does negative caching.\n&gt; - Times itself to clean the expired entries based on TTL. (Would use the\nssTimer SEDA APIs to schedule itself periodically)\n&gt; - This stage would be dummy or could be skipped as of now since we want to\ndo caching later.\n&gt;\n&gt; Events:\n&gt; - Two types of events : DNSCacheLookupEvent, DNSCacheUpdateEvent.\n&gt; - DNSCacheLookupEvent : If entry is found in cache, the ipaddress is set\nin the CrawlURI object and is enqueued into the &quot;Page\n&gt; Requesting Stage&quot;. Else, it is enqueued into the &quot;DNS Querying Stage&quot;.\n&gt; - DNSCacheUpdateEvent : This event is published by the &quot;DNS Response\nProcessing Stage&quot; every after successful/failed lookup inorder\n&gt; to update the cache.\n&gt;\n&gt; Other notes:\n&gt; - This stage could be single threaded else lot of synchronization might be\nneeded.\n&gt; - Resubmitting events on queue full exceptions while enqueuing into this\nstage&#39;s queue should be handled by the caller by scheduling\n&gt; it in future.\n&gt;\n&gt;\n&gt; DNS Querying Stage :\n&gt;\n&gt; Overview:\n&gt; - Sends the actual DNS ARecord query packets to the DNS Server. (The\nresponse packets are processed in a later stage)\n&gt; - Maintains a pool of DatagramSocket objects.\n&gt;\n&gt; Events:\n&gt; - SendDNSQueryEvent : This is published by the &quot;DNS History/Cache Handling\nStage&quot; when cache miss happens. The DNS query is formed\n&gt; and sent out. The response handling sink is set as the &quot;DNS Response\nProcessing Stage&quot;.\n&gt;\n&gt; Implementation:\n&gt; - A pool of DatagramSockets of a configurable maximum size is maintained.\nIt will be filled incrementally. All these datagram\n&gt; sockets will be registered to the selector maintained by the SEDA\ninternals. It would be ideal if this pool gets shrunk or expanded\n&gt; based on the requirement. If it is not shrunk back, then it is an\nunnecessary overhead on the selector. The reason behind having a\n&gt; pool is to restrict the number of ports the selector has to listen upon\nand also not to create individual DatagramSocket objects for\n&gt; every query. Can this logic of bounded pool, be implemented as a\nController in the SEDA framework (just like the\n&gt; ThreadPoolController) is an open question.\n&gt; - When an event comes in, a free datagram socket in the pool will be\nutilized for sending the message. If all sockets are engaged,\n&gt; the incoming event should be postponed to reenter again after a period of\ntime.\n&gt; - This stage additionally has also to maintain the list of messages sent\nout, their IDs and the request timestamps. Let us call this\n&gt; &quot;RequestMap&quot; for future reference. The ID is the integer, described in the\nRFC DNS message format, used to map the\n&gt; request-responses. The request timestamp will be made use of in query\ntimeout handling (discussed later).\n&gt;\n&gt; Parameters to this stage :\n&gt; - The DNS server hostname/ipaddress. If this is not given, then the\n/etc/resolve.conf will be parsed to get the name server (only\n&gt; the primary would be taken as of now.). As a next step we will have to\nbuild a round-robin way of querying the various name servers\n&gt; in resolve.conf, inorder to be polite with them.\n&gt; - If resolve.conf is not present, the local host will be assumed as the\nname server.\n&gt;\n&gt;\n&gt; DNS Response Processing Stage :\n&gt;\n&gt; Overview:\n&gt; - Processes DNS responses.\n&gt;\n&gt; Implementation:\n&gt; - When the DNS datagram packets are received, the ID field in the header\nshould be used to match the corresponding request packet.\n&gt; - Check for timeouts, and discard it if it had timed out; else, set the\nipaddress/canonical name in the CrawlURI object and enqueue\n&gt; it to the &quot;Page Requesting Stage&quot;. In addition, enqueue an event into the\n&quot;DNS Cache Handling Stage&quot; for it to update its cache. Do\n&gt; the same, even on DNS Errors like &quot;Name not found authoritative error&quot;.\n&gt; - The request entry in the RequestMap (maintained for timeout handling)\nshould be removed. This map, being shared across stages,\n&gt; should be synchronized.\n&gt;\n&gt;\n&gt; HTTP Page Requesting Stage :\n&gt;\n&gt; Overview:\n&gt; - Connects to host and sends GET requests for pages.\n&gt;\n&gt; Events:\n&gt; - Handles two types of events - StartFetchEvent and\nConnectionCompleteEvent.\n&gt; - The StartFetchEvent will make a TCP connect request to the host. While\ndoing so, we will register the current stage itself to\n&gt; receive back the ConnectionComplete events. Once we receive this\nConnectionCompleteEvent, we should send a HTTP GET request to the\n&gt; page. The response handling sink is set as the &quot;HTTP Response Processing\nStage&quot;. Write failures should be handled.\n&gt;\n&gt;\n&gt; HTTP Response Processing Stage :\n&gt;\n&gt; Overview:\n&gt; - Processes downloaded pages.\n&gt;\n&gt; Implementation:\n&gt; - Check for timeouts, and discard it if it had timed out; else, read the\npackets.\n&gt; - Once the response is completely read, the request entry in the\nRequestMap (maintained for timeout handling) should be removed.\n&gt; - One issue here is when we are reading lengthy HTML pages, we might\nreceive half of the page and it might stop after that. So,\n&gt; essentially the timeout should be applied between chunks of reception.\n&gt; - Where should the errors like &quot;404 Not Found&quot;, etc be propogated ???\n&gt;\n&gt;\n&gt; Timeout and Retry Handling Stage :\n&gt;\n&gt; Overview:\n&gt; This is a single threaded stage which enumerates through the RequestMap\nand checks for timeouts. The timed out CrawlURIs will be\n&gt; retried until retry count exhausts. This stage will be self-timed\nperiodically using the SEDA ssTimer APIs.\n&gt;\n&gt; Other Notes:\n&gt; This timeout handling is a common stuff between the DNS requests and the\nHTTP requests.\n&gt;\n&gt; Parameters to this stage:\n&gt; - DNS timeout value.\n&gt; - HTTP timeout value.\n&gt; - DNS retry count.\n&gt; - HTTP retry count. ( This would be 1 ).\n&gt;\n&gt;\n&gt; One other thing that could be done is that, the events by themselves will\ncontain information as to which next stage the output has\n&gt; to traverse. This will be flexible and no hardcoding is needed.\nEspecially, in making this non-blocking DNS library an open-source,\n&gt; it would come handy. Moreover, many users might not want it to be over\nSEDA. So, we will have to give other interfaces as well.\n&gt;\n&gt; I am presently using the library classes given by dnsjava-1.3.2. (\nhttp://sourceforge.net/projects/dnsjava/ ). This is an LGPL java\n&gt; based synchronous implementation of DNS Resolver. I only make use of the\nclasses which encapsulate the formation of request packets,\n&gt; parses response packets and the various ResourceRecord classes. This\nlibrary is being used by Java Apache Mail Enterprise Server (\n&gt; http://james.apache.org/ ). So, it should be pretty reliable and tested.\nMoreover it has support for IPv6, compression and security\n&gt; which we can make use of later.\n&gt;\n&gt; Thanks,\n&gt; Reddy\n&gt;\n&gt;\n&gt;\n&gt; ----- Original Message -----\n&gt; From: Gordon Mohr\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Cc: Raymie Stata ; wcr-team@...\n&gt; Sent: Saturday, March 01, 2003 2:43 AM\n&gt; Subject: Re: [archive-crawler] Re: Web crawler work ??\n&gt;\n&gt;\n&gt; Sounds like a reasonable plan.\n&gt;\n&gt; By &quot;local name server&quot; do you mean something *very* local -- for example,\n&gt; a standard nameserver we run on the same machine?\n&gt;\n&gt; That would seem to offer other benefits -- such as minimizing the modes\n&gt; of DNS lookup we have to do and offloading caching to another piece of\n&gt; software (at least at first).\n&gt;\n&gt; - Gordon\n&gt;\n&gt; ----- Original Message -----\n&gt; From: G.B.Reddy\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Cc: Raymie Stata ; wcr-team@...\n&gt; Sent: Thursday, February 27, 2003 8:55 AM\n&gt; Subject: Re: [archive-crawler] Re: Web crawler work ??\n&gt;\n&gt;\n&gt; Gordon and Raymie,\n&gt;\n&gt; Here goes the proposal for the asynchronous DNS lookup API implementation.\n&gt;\n&gt; We shall implement a minimal resolver which is capable of sending DNS\nrequest packets and processing response packets in an\n&gt; asynchrounous nio fashion. This resolver class will contact a local name\nserver and rely on it to do the actual lookup. The local\n&gt; name server will be configured to support recursion and better would be to\nuse a name server which does lookup asynchronously. (\n&gt; SQUID has asynchronous DNS lookup facilities ).  Even if the local name\nserver is not asynchrounous, our java resolver being\n&gt; asynchronous will be good enough since our primary goal is that we do not\nwant any blocking code in our crawler implementation. This\n&gt; idea even sounds good considering the fact we would only reinvent the same\nwheel if we opt to implement a complete full-fledged\n&gt; resolver implementation which complies with the RFC 1035 and 1034. We can\ndefinitely implement this full-fledged resolver but the\n&gt; real concern is that this would require a lot of testing and the efforts\nto make it rock solid in terms of robustness would be huge.\n&gt;\n&gt; So, the various jobs that we would have to do to build our Simple\nAsynchronous DNS lookup API would be\n&gt;     -- Request packet formation and reply packet parsing in the exact RFC\nformat.\n&gt;     -- Use non-blocking IO APIs and do UDP. (We might not need TCP since\nthe name server is only in the local network.)\n&gt;     -- Do canonical name queries and A record queries.\n&gt;     -- Implement timeouts.\n&gt;     -- Implement caching based on TTL. ( This may have to be deferred as\npointed by Raymie earlier. )\n&gt;     -- Integrate with SEDA.\n&gt;\n&gt; Thanks,\n&gt; Reddy\n&gt;\n&gt; ----- Original Message -----\n&gt; From: Gordon Mohr\n&gt; To: G.B.Reddy\n&gt; Cc: Raymie Stata ; wcr-team@... ;\narchive-crawler@yahoogroups.com\n&gt; Sent: Saturday, February 22, 2003 5:37 AM\n&gt; Subject: [archive-crawler] Re: Web crawler work ??\n&gt;\n&gt;\n&gt; [CC&#39;ing to archive-crawler@yahoogroups.com]\n&gt;\n&gt; Reddy writes:\n&gt;\n&gt; &gt; On the first cut do we need to look at implementing an asynchronous DNS\n&gt; &gt; lookup mechanism. If we are not, then it is going to be two stages, viz.\n&gt; &gt; DNSCacheHandlingStage and ResolvingStage, that can be employed using the\n&gt; &gt; blocking DNS lookup calls in Java. The first stage,\nDNSCacheHandlingStage,\n&gt; &gt; would check if the entry is available in the cache. If available, he\nwould\n&gt; &gt; set the resolved address in the CrawlURI object and enqueue it to the\n&gt; &gt; appropriate next stage. If the cache doesn&#39;t contain the entry, then he\n&gt; &gt; would pass the request to the Resolving stage which would call the\n&gt; &gt; InetAddress.getByName blocking method to resolve it. The getByName\nresult\n&gt; &gt; would be set in the CrawlURI object as before and enqueued into the\n&gt; &gt; appropriate next stage. In addition to this, the Resolving stage will\n&gt; &gt; enqueue another event into the DNSCacheHandlingStage to enable him\nupdate\n&gt; &gt; his cache. So, the DNSCacheHandlingStage would be handling two types of\n&gt; &gt; events, one is the lookup events and the other is the update cache\nevents.\n&gt; &gt;\n&gt; &gt; One problem here is that the InetAddress class does not expose its cache\n&gt; &gt; variables to its users. Even we cannot check if the cache has an entry\n&gt; &gt; before calling the getByName method. So, we should be disabling the java\n&gt; &gt; cache ( using the policy file ) and implementing our own caching\nmechanism.\n&gt; &gt; ( The DNSCacheHandlingStage would have to additionally do the job of\n&gt; &gt; throwing away the expired entries in the cache also.)\n&gt; &gt;\n&gt; &gt; Let me know your comments on this.\n&gt;\n&gt; This looks like a good first cut. I&#39;m still working to improve my\n&gt; understanding of the best way to use the staged style, mostly by\n&gt; looking at their HTTP and HTTP Server (Haboob) code.\n&gt;\n&gt; It seems that they&#39;ve tended to use a single Stage object to do\n&gt; many different steps/aspects of one process, by switching on the\n&gt; type of QueueElement received.\n&gt;\n&gt; So for example their seda.sandStorm.seda.apps.Haboob.http.HttpRecv\n&gt; accepts events of types....\n&gt;\n&gt;   - httpConnection\n&gt;   - httpRequest\n&gt;   - SinkClosedEvent\n&gt;   - timerEvent\n&gt;\n&gt; And their seda.sandStorm.lib.http.httpServer accepts events of\n&gt; types...\n&gt;\n&gt;   - ATcpInPacket\n&gt;   - ATcpConnection\n&gt;   - aSocketErrorEvent\n&gt;   - SinkDrainedEvent\n&gt;   - SinkCloggedEvent\n&gt;   - SinkClosedEvent\n&gt;   - ATcpListenSuccessEvent\n&gt;\n&gt; They also use Sinks that are not associated with stages; rather,\n&gt; they interface to unstaged components which nonetheless result in\n&gt; an eventual event to some supplied answer Sink. See for example\n&gt; seda.sandStorm.lib.http.httpConnection.\n&gt;\n&gt; So perhaps as a matter of grouping related tasks, the same Stage object\n&gt; should be re-entered over the course of a lookup, with different\ntriggering\n&gt; events. For example, you might want to reenter a single DNSResolvingStage\n&gt; over the course of cache lookup, lookup-initiation, result-receiving (or\n&gt; timeout), etc. I&#39;m not sure; use your judgement as to how many stages are\n&gt; really needed.\n&gt;\n&gt; &gt; P.S : We found some openly available async dns client APIs in C\nlanguage.\n&gt;\n&gt; That could be useful as a model. (I doubt we&#39;d want to call out to C\n&gt; for this simple step, though -- and if we nailed down a truly async Java\n&gt; DNS facility, a lot of open source projects would probably be quite\nhappy.)\n&gt;\n&gt; Also: I heard back from Patrick Eaton about SEDA-style async HTTP client\n&gt; code... he has a rough implementation for simple usage, and he knows of\n&gt; another one at Berkeley which goes deeper into HTTP/1.1 conformance and\n&gt; optimal performance. I&#39;ve asked him to forward whatever additional code\n&gt; or details he can.\n&gt;\n&gt; - Gordon\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; To unsubscribe from this group, send an email to:\n&gt; archive-crawler-unsubscribe@yahoogroups.com\n&gt;\n&gt;\n&gt;\n&gt; Your use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n&gt;\n&gt;\n&gt; Yahoo! Groups Sponsor\n&gt; ADVERTISEMENT\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; To unsubscribe from this group, send an email to:\n&gt; archive-crawler-unsubscribe@yahoogroups.com\n&gt;\n&gt;\n&gt;\n&gt; Your use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n&gt;\n&gt;\n&gt;\n&gt; To unsubscribe from this group, send an email to:\n&gt; archive-crawler-unsubscribe@yahoogroups.com\n&gt;\n&gt;\n&gt;\n&gt; Your use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n&gt;\n&gt;\n&gt; Yahoo! Groups Sponsor\n&gt; ADVERTISEMENT\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; To unsubscribe from this group, send an email to:\n&gt; archive-crawler-unsubscribe@yahoogroups.com\n&gt;\n&gt;\n&gt;\n&gt; Your use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n\n\n"}}