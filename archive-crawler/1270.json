{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":191507321,"authorName":"robeger","from":"&quot;robeger&quot; &lt;reger@...&gt;","profile":"robeger","replyTo":"LIST","senderId":"kFTUb1ZBRmM9Dvb9JZV41AntaLjhkmrlXQijseiB_Fqj051h8KlFF8LXn9fLlXECYjPZtRG65EkX9tcEt4Ws4V1n","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: New issue with DomainSensitiveFrontier","postDate":"1102703753","msgId":1270,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNwY3FhOStydTk2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGNwY3E2cCtjZjliQGVHcm91cHMuY29tPg=="},"prevInTopic":1269,"nextInTopic":1271,"prevInTime":1269,"nextInTime":1271,"topicId":1235,"numMessagesInTopic":20,"msgSnippet":"Forgot to say this was using the 1.2.0 release code, rebuilt with Oskar s fix put in. Rob.","rawEmail":"Return-Path: &lt;reger@...&gt;\r\nX-Sender: reger@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 13514 invoked from network); 10 Dec 2004 18:36:30 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m24.grp.scd.yahoo.com with QMQP; 10 Dec 2004 18:36:30 -0000\r\nReceived: from unknown (HELO n8a.bulk.scd.yahoo.com) (66.94.237.42)\n  by mta5.grp.scd.yahoo.com with SMTP; 10 Dec 2004 18:36:30 -0000\r\nReceived: from [66.218.69.6] by n8.bulk.scd.yahoo.com with NNFMP; 10 Dec 2004 18:35:53 -0000\r\nReceived: from [66.218.67.172] by mailer6.bulk.scd.yahoo.com with NNFMP; 10 Dec 2004 18:35:53 -0000\r\nDate: Fri, 10 Dec 2004 18:35:53 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;cpcqa9+ru96@...&gt;\r\nIn-Reply-To: &lt;cpcq6p+cf9b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 3472\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.42\r\nFrom: &quot;robeger&quot; &lt;reger@...&gt;\r\nSubject: Re: New issue with DomainSensitiveFrontier\r\nX-Yahoo-Group-Post: member; u=191507321\r\nX-Yahoo-Profile: robeger\r\n\r\n\nForgot to say this was using the 1.2.0 release code, rebuilt with\nOskar&#39;s fix put in.\n\nRob.\n\n--- In archive-crawler@yahoogroups.com, &quot;robeger&quot; &lt;reger@a...&gt; wrote:\n&gt; \n&gt; Found another problem with the DomainSensitiveFrontier.  The max docs\n&gt; per site setting seems to actually act as a max docs for the crawl\n&gt; setting.  If I crawl a single site with max docs per site set to 10, I\n&gt; get 10 pages crawled, but if I do the same with 2 sites, I end up\n&gt; getting 5 per site (10 total, not sure if the ratio would work out\n&gt; evenly all the time).\n&gt; \n&gt; Rob.\n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, &quot;ogrenholm&quot;\n&gt; &lt;oskar.grenholm@k...&gt; wrote:\n&gt; &gt; \n&gt; &gt; Hey, I did reply to this just shortly before I left for the weekend, \n&gt; &gt; but it seems that the reply didn&#39;t make it (hope this one do).\n&gt; &gt; \n&gt; &gt; And yeah, it doesn&#39;t work now, but it sure did when I tested it back \n&gt; &gt; on 1.0. But since I set up the job I wanted it to do (downloading \n&gt; &gt; the frontpage of all Swedish online newspapers) I haven&#39;t looked att \n&gt; &gt; in anymore.\n&gt; &gt; \n&gt; &gt; But now I had a look at this at Friday and I think the problem is \n&gt; &gt; that the method deleteMatchedItems() in TieredQueue.java doesn&#39;t \n&gt; &gt; update the length of the queue and therefore when asked if it&#39;s \n&gt; &gt; empty answers no. One solution might be to add a call to \n&gt; &gt; recalculateLength() in deleteMatchedItems(), just before it returns, \n&gt; &gt; i.e. \n&gt; &gt; public long deleteMatchedItems(Predicate matcher) {\n&gt; &gt;         long count = 0;\n&gt; &gt;         for (int i = 0; i &lt;= lastQueue; i++) {\n&gt; &gt;             count += innerQueues[i].deleteMatchedItems(matcher);\n&gt; &gt;         }\n&gt; &gt;         recalculateLength();  //ADDED\n&gt; &gt;         return count;\n&gt; &gt;     }\n&gt; &gt;  \n&gt; &gt; I did this and it seems to work (only basic testing, then weekend). \n&gt; &gt; If someone think this is a bad idea (yes, I mean you Stack) or has a \n&gt; &gt; better solution, please let me know.\n&gt; &gt; \n&gt; &gt; /Oskar\n&gt; &gt; \n&gt; &gt; Ps. I have a better look at this on Monday. Ds.\n&gt; &gt; \n&gt; &gt; --- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; &gt; &gt; robeger wrote:\n&gt; &gt; &gt; \n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Just tried this out.  It seems that the crawl never &quot;finishes&quot;.  \n&gt; &gt; Tried\n&gt; &gt; &gt; &gt; it on a single URL, limiting it to 10 docs for that site.  Once \n&gt; &gt; it\n&gt; &gt; &gt; &gt; reaches 10, it starts giving -6000 as the status code for \n&gt; &gt; subsequent\n&gt; &gt; &gt; &gt; pages, but the crawl never finishes, console just sits at &quot;13 of \n&gt; &gt; 39\n&gt; &gt; &gt; &gt; DOWNLOADED/QUEUED RATIO&quot;.  It also seems to count docs from\n&gt; &gt; &gt; &gt; macromedia.com as part of the 10.  If I switch back to the \n&gt; &gt; default\n&gt; &gt; &gt; &gt; frontier, it crawls the site and finishes.\n&gt; &gt; &gt; \n&gt; &gt; &gt; \n&gt; &gt; &gt; &lt;&gt;The way that the DomainSensitiveFrontier works is that when it \n&gt; &gt; hits the\n&gt; &gt; &gt; max-docs-for-this-domain threshold, it marks all remaining queued \n&gt; &gt; URLs as\n&gt; &gt; &gt; &#39;deleted&#39;. But Frontier continues to run. As it comes across these\n&gt; &gt; &gt; deleted URLs that are still in the queue, it discards them logging \n&gt; &gt; as\n&gt; &gt; &gt; crawl.log -- the -6000s (&#39;Deleted from Frontier by user&#39;).\n&gt; &gt; &gt; \n&gt; &gt; &gt; That the crawl never finishes is a bug (I just tried it and had \n&gt; &gt; same issue.\n&gt; &gt; &gt; Is your heritrix_out.log filled with exceptions?). I made an issue \n&gt; &gt; to cover\n&gt; &gt; &gt; this: &#39;[ 1078581 ] DomainSensitiveFrontier never finishes&#39;.  Let \n&gt; &gt; me see if\n&gt; &gt; &gt; we can get the fella that donated this Frontier to fix it.\n&gt; &gt; &gt; \n&gt; &gt; &gt; (The macromedias being counted has to do with your scope settings.\n&gt; &gt; &gt; Try changing max-embed-hops)\n&gt; &gt; &gt; \n&gt; &gt; &gt; St.Ack\n&gt; &gt; &gt; \n&gt; &gt; &gt;\n\n\n\n\n"}}