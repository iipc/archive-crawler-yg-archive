{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"fTbbkIf2J1W2vXxSaNdj21iZOlRirUfu1dReeRbC90MzFs9aox7Rn-JdjN7mAB0OMqSJ2ebJah1kQJN-nyKN4lx2OeWdd8DN","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] JVM crash while running a large crawl","postDate":"1195579499","msgId":4729,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3NDMxODZCLjUwNjA2MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ3NDMxMjFCLjIwOTA5QGFyY2hpdmUub3JnPg==","referencesHeader":"PDhGMTE3MjJBMDU2MkJCNEY4MEE2ODBFRDRDRkVEMEQ1MEM4NUM5QEVWU0JORzAyLmFkLm9mZmljZS5hb2wuY29tPiA8NDc0MkJFQzEuNzAyMDQwM0BhcmNoaXZlLm9yZz4gPDQ3NDMxMjFCLjIwOTA5QGFyY2hpdmUub3JnPg=="},"prevInTopic":4727,"nextInTopic":4730,"prevInTime":4728,"nextInTime":4730,"topicId":4724,"numMessagesInTopic":5,"msgSnippet":"Hi Gordon, We have seen two crawls crash recently due the limit of the open file handles. I dismissed the problem as an operator errors since we did not start","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 36192 invoked from network); 20 Nov 2007 17:27:04 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m47.grp.scd.yahoo.com with QMQP; 20 Nov 2007 17:27:04 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta16.grp.scd.yahoo.com with SMTP; 20 Nov 2007 17:27:03 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id A33ED4193A\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 20 Nov 2007 09:28:22 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id ayywlE-1cHO7 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 20 Nov 2007 09:28:21 -0800 (PST)\r\nX-Received: from [127.0.0.1] (nor75-24-88-170-99-175.fbx.proxad.net [88.170.99.175])\n\tby mail.archive.org (Postfix) with ESMTP id 385372E4A5\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 20 Nov 2007 09:28:21 -0800 (PST)\r\nMessage-ID: &lt;4743186B.5060607@...&gt;\r\nDate: Tue, 20 Nov 2007 09:24:59 -0800\r\nUser-Agent: Thunderbird 2.0.0.9 (Windows/20071031)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;8F11722A0562BB4F80A680ED4CFED0D50C85C9@...&gt; &lt;4742BEC1.7020403@...&gt; &lt;4743121B.20909@...&gt;\r\nIn-Reply-To: &lt;4743121B.20909@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] JVM crash while running a large crawl\r\nX-Yahoo-Group-Post: member; u=137477665; y=wZ-vXgnh6JNAPdY_9SExjigjOLVI082KSc8Q0c7Z40HrFeyPkw\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Gordon,\n\nWe have seen two crawls crash recently due the limit of the open file \nhandles. I dismissed the problem as an operator errors since we did not \nstart the crawls within our production environment.\nHowever, by looking back at the logs, I see that I missed the fact that \nthe errors were related to the bdb environment and not java.io.\n\nMachine setup:\n--------------\njava version &quot;1.6.0_01&quot;\nJava(TM) SE Runtime Environment (build 1.6.0_01-b06)\nJava HotSpot(TM) Server VM (build 1.6.0_01-b06, mixed mode)\nJAVA_OPTS=-Xmx2300m -Xms2300m -XX:+HeapDumpOnOutOfMemoryError \n-XX:+PrintClassHistogram \n-XX:OnOutOfMemoryError=/home/webcrawl/tools/sigquitpp\nid.sh\ncore file size        (blocks, -c) 0\ndata seg size         (kbytes, -d) unlimited\nfile size             (blocks, -f) unlimited\nmax locked memory     (kbytes, -l) unlimited\nmax memory size       (kbytes, -m) unlimited\nopen files                    (-n) 1024\npipe size          (512 bytes, -p) 8\nstack size            (kbytes, -s) 8192\ncpu time             (seconds, -t) unlimited\nmax user processes            (-u) unlimited\nvirtual memory        (kbytes, -v) unlimited\n\n\nError:\n-------\n09/07/2007 15:25:32 +0000 SEVERE \norg.archive.crawler.frontier.BdbWorkQueue peekItem peekItem failure; \nretrying\ncom.sleepycat.je.DatabaseException: (JE 3.2.23) fetchTarget of \n0x55a/0x67face parent IN=60667353 lastFullVersion=0x108a/0x703bd9 \nparent.getD\nirty()=true state=0 com.sleepycat.je.log.LogFileNotFoundException: (JE \n3.2.23) 0x55a/0x67face (JE 3.2.23) Couldn&#39;t open file /2/crawldata/NL\nA-AU-CRAWL-002-08-28-2007/state/0000055a.jdb: \n/2/crawldata/NLA-AU-CRAWL-002-08-28-2007/state/0000055a.jdb (Too many \nopen files)\n         at com.sleepycat.je.tree.IN.fetchTarget(IN.java:963)\n         at \ncom.sleepycat.je.dbi.CursorImpl.fetchCurrent(CursorImpl.java:2160)\n         at \ncom.sleepycat.je.dbi.CursorImpl.getCurrentAlreadyLatched(CursorImpl.java:1276)\n         at \ncom.sleepycat.je.dbi.CursorImpl.getNextWithKeyChangeStatus(CursorImpl.java:1422)\n         at com.sleepycat.je.dbi.CursorImpl.getNext(CursorImpl.java:1348)\n         at \ncom.sleepycat.je.Cursor.retrieveNextAllowPhantoms(Cursor.java:1577)\n         at com.sleepycat.je.Cursor.retrieveNext(Cursor.java:1387)\n         at com.sleepycat.je.Cursor.getNext(Cursor.java:446)\n         at \norg.archive.crawler.frontier.BdbMultipleWorkQueues.getNextNearestItem(BdbMultipleWorkQueues.java:293)\n         at \norg.archive.crawler.frontier.BdbMultipleWorkQueues.get(BdbMultipleWorkQueues.java:254)\n         at \norg.archive.crawler.frontier.BdbWorkQueue.peekItem(BdbWorkQueue.java:107)\n         at org.archive.crawler.frontier.WorkQueue.peek(WorkQueue.java:140)\n         at \norg.archive.crawler.frontier.WorkQueueFrontier.next(WorkQueueFrontier.java:656)\n         at org.archive.crawler.framework.ToeThread.run(ToeThread.java:144)\n\nTake care,\ni.\n\n\n\n&gt; This message is also indicative of a Sun JVM bug that Sun reports fixed \n&gt; in Java 7.0. It might be fixed in latest 6.0 releases; we haven&#39;t seen \n&gt; it recently at IA. See this JIRA issue for details:\n&gt; \n&gt;    http://webteam.archive.org/jira/browse/HER-1126\n&gt; \n&gt; Given Sun&#39;s opaqueness on this issue, it might be worth trying the \n&gt; latest 5.0 JVM (u14) -- maybe they&#39;ve fixed it there too.\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; Igor Ranitovic wrote:\n&gt;&gt; Hi Ankur,\n&gt;&gt;\n&gt;&gt; Please see this FAQ:\n&gt;&gt; http://crawler.archive.org/faq.html#toomanyopenfiles\n&gt;&gt;\n&gt;&gt; Take care,\n&gt;&gt; i.\n&gt;&gt;\n&gt;&gt;&gt; Hi,\n&gt;&gt;&gt;       I am running a large crawl with Max-Toe threads being 200.\n&gt;&gt;&gt; The crawl does fairly well for about 40-45 min before crashing\n&gt;&gt;&gt; with an following error message in the logs\n&gt;&gt;&gt;\n&gt;&gt;&gt; INFO: GET http://home.iprimus.com.au/welling/main.html 200 4834\n&gt;&gt;&gt; text/html\n&gt;&gt;&gt; - I/O exception (java.io.FileNotFoundException) caught when processing\n&gt;&gt;&gt; request: /home/agoel/dist/run/3/scratch/tt44http.ris (Too many open\n&gt;&gt;&gt; files)\n&gt;&gt;&gt; - Retrying request\n&gt;&gt;&gt; - I/O exception (java.io.IOException) caught when processing request: \n&gt;&gt;&gt; RIS already open for ToeThread #44: http://ja.wikipedia.org/robots.txt\n&gt;&gt;&gt; - Retrying request\n&gt;&gt;&gt; - I/O exception (java.io.IOException) caught when processing request: \n&gt;&gt;&gt; RIS already open for ToeThread #44: http://ja.wikipedia.org/robots.txt\n&gt;&gt;&gt; - Retrying request\n&gt;&gt;&gt; - I/O exception (java.io.IOException) caught when processing request: \n&gt;&gt;&gt; RIS already open for ToeThread #44: http://ja.wikipedia.org/robots.txt\n&gt;&gt;&gt; - Retrying request\n&gt;&gt;&gt; .\n&gt;&gt;&gt; .\n&gt;&gt;&gt; .\n&gt;&gt;&gt; #\n&gt;&gt;&gt; # An unexpected error has been detected by HotSpot Virtual Machine:\n&gt;&gt;&gt; #\n&gt;&gt;&gt; #  SIGSEGV (0xb) at pc=0xf15f27dc, pid=3054, tid=2359815072 # # \n&gt;&gt;&gt; # Java VM: Java HotSpot(TM) Server VM (1.5.0_13-b05 mixed mode) #\n&gt;&gt;&gt; Problematic frame:\n&gt;&gt;&gt; # v  ~BufferBlob::jni_fast_GetIntField\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; I think the problem is with unclosed/too many HttpRecorder I/O streams.\n&gt;&gt;&gt; If this is the real issue then is it a known one ?\n&gt;&gt;&gt; If not then any suggestions on why this would be happen?\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks\n&gt;&gt;&gt; -Ankur\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;  \n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  \n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}