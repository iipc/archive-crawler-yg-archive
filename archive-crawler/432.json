{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"t5VYXGYcVoJqPATYuZuMM8gW5yMcXn7UiZtS-LduoILKU7hd0IGVR1JnJYj3Kvc9D5omORfeOnwiiC_bXcuIF5HHmZISIyE9cF5oFXahmg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Large scale crawling with Heritrix","postDate":"1085149804","msgId":432,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBDNzFAbG9raS5ib2suaGkuaXM+"},"prevInTopic":0,"nextInTopic":433,"prevInTime":431,"nextInTime":433,"topicId":432,"numMessagesInTopic":9,"msgSnippet":"Hi all, This week I ve been experimenting with running a crawl over the entire .is TLD. As expected I ve encountered several problems and limitations that I ","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 46881 invoked from network); 21 May 2004 14:30:04 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m23.grp.scd.yahoo.com with QMQP; 21 May 2004 14:30:04 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 21 May 2004 14:30:04 -0000\r\nReceived: (qmail 20955 invoked by uid 100); 21 May 2004 14:23:12 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (130.208.152.83)\n  by mail-dev.archive.org with SMTP; 21 May 2004 14:23:12 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Fri, 21 May 2004 14:30:04 -0000\r\nMessage-ID: &lt;D958110B273CD511ACC100B0D079AA4E01960C71@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/mixed;\n\tboundary=&quot;----=_NextPart_000_0010_01C43F40.1CA50490&quot;\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nImportance: Normal\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1409\r\nX-MS-TNEF-Correlator: &lt;D958110B273CD511ACC100B0D079AA4E01960C71@...&gt;\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: **\r\nX-Spam-Status: No, hits=2.2 required=6.0 tests=AWL,DOMAIN_BODY,\n\tDRASTIC_REDUCED autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: Large scale crawling with Heritrix\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n------=_NextPart_000_0010_01C43F40.1CA50490\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi all,\n\nThis week I&#39;ve been experimenting with running a crawl over the en=\r\ntire .is\nTLD. As expected I&#39;ve encountered several problems and limitations=\r\n that I\nthought might of interest to this community as this is (as far as I=\r\n know)\nthe first attempt to run Heritrix on such a scale. Crawling has been=\r\n done\nwith very recent development builds.\n\nMany of the limits I&#39;ve encount=\r\nered are already known but I&#39;m reiterating\nthem here for the sake of comple=\r\nteness.\n\nThe .is TLD has a little over 10000 registered second level domain=\r\ns. A list\nof them with a www prefix was used as a seed list. Instead of one=\r\n of the\nscopes shipped with Heritrix I wrote a custom scope that limited th=\r\ne crawl\nto hosts with the .is suffix (plus the usual transitive includes).\n=\r\n\nThe first problem I encountered was Heritrix&#39;s excessive use of file\nhandl=\r\ners. In addition to numerous other files used by Heritrix each one of\nthe 1=\r\n0000+ domains immediately needed its own host queue with two file\nhandlers =\r\neach. This led me to (with the help of Gordon and Michael) to\nredesign the =\r\nso called DiskBackedQueues so that they only have open files\nwhen they are =\r\nlarge enough to warrant it. I.e. when items in them are too\nmany to fit int=\r\no the memory &#39;head&#39; that they have (200 files for the host\nqueues).\n\nThis f=\r\nix drastically reduced the number of open files and for the most part\nelimi=\r\nnated this problem. With it out of the way it was possible to launch a\ncraw=\r\nl with this many seeds. \n\nInitial progress was quite impressive, running at=\r\n over 50 documents per\nsecond initially. Eventually it started to gradually=\r\n drop and now several\ndays later it stands at around 17 documents per secon=\r\nd. I&#39;m unsure of the\nreason for this gradual decline. It may be related to =\r\nincreasing sizes of\nvarious data structures. \n\nOver the course of the past =\r\n3 days the crawl has downloaded nearly 4 million\ndocuments totaling over 15=\r\n0 GB. In addition some 11 million plus documents\nhave been discovered and a=\r\nre waiting processing. While impressive this is\nstill only scratching the t=\r\nop of the .is domain. I estimate that currently\ndocuments 3 link hops from =\r\nthe seeds are being processed.\n\nMemory use was initially my main concern. T=\r\nhe crawl is running on a machine\nwith 1.5 GB RAM and the JVM max heap size =\r\nwas set to 1.25 GB. To date the\nJVM has only allocated itself 1 GB and garb=\r\nage collection still drops the\nmemory being used to almost half that.\n\nDisk=\r\n use by the disk bound queues however has been much greater then I\nanticipa=\r\nted. With said 11 million URLs waiting in the queues they now take\nup about=\r\n 16 GB. This comes out at about 1.6 KB per URI. This will turn out\nto be th=\r\ne limiting factor for my current crawl since the disk in question\nonly has =\r\nanother 3 GB free so it will be exhausted soon.\n\nEven with much larger disk=\r\ns, say 200 GB, crawls will be limited to having\n120-130 million URIs waitin=\r\ng. This seems like a huge number until you\nconsider that the .is domain wou=\r\nld seem to have at least that many documents\nbased on this crawl. Doing a c=\r\nrawl like this on an even larger scale would\nseem to merit trying to reduce=\r\n the size (on disk) of these URIs.\n\nOf course a crawl of that scope is not =\r\npossible until the list of already\nseen URIs can be disk backed.  With the =\r\ncurrent method of using 4 byte\nfingerprints for each encountered URI 1 GB o=\r\nf memory can hold around 28\nmillion URIs. Even with a machine with 4 GB RAM=\r\n would not be able to scale\nup to even a full .is TLD crawl.\n\n\n\nSome of my =\r\nthoughts on dealing with the limits follow:\n\nMoving the list of encountered=\r\n URIs to disk seems to be imperative. But that\nwill pose even greater deman=\r\nds on disk space so I would suggest that we\nremain aware of that issue and =\r\ntry (whenever possible) to limit the size of\nthe data being written to disk=\r\n to that which is actually needed. That may\nhave the additional benefit of =\r\nspeeding I/O operations (even if only\nmarginally). Perhaps it should also b=\r\ne possible to split the data among many\ndisk (of course this can be done at=\r\n the OS level to some extent).\n\nMulti machine setup might alleviate some of=\r\n this but for any sort of large\nscale crawl (targeting 100 million +) each =\r\nmachine would probably be\nhandling tens of millions URIs at least. For trul=\r\ny large scale crawls (like\nthe ones I know IA wants to do) the demands grow=\r\n even more.\n\nAnother approach which might limit the problem is the implemen=\r\ntation of the\nsite first crawling strategy. In my crawl all .is domains are=\r\n tackled in\nparallel. This makes the crawl a true breadth first crawl. A si=\r\nte first\napproach would let Heritrix focus it&#39;s efforts on a limited number=\r\n of sites\nat a time. Since the usual pattern for crawling a site is an init=\r\nial\nexplosion of available URIs followed by a steady decline and eventual\ne=\r\nxhaustion we would likely not wind up having as huge a number of URIs\nwaiti=\r\nng (at least not as quickly). This does nothing for the limits imposed\nby t=\r\nhe demands on RAM memory though.\n\n\nKristinn Sigur=F0sson\nSoftware engineer\n=\r\nNational and University Library of Iceland\n\r\n------=_NextPart_000_0010_01C43F40.1CA50490\r\nContent-Type: application/ms-tnef;\n\tname=&quot;winmail.dat&quot;\r\nContent-Disposition: attachment;\n\tfilename=&quot;winmail.dat&quot;\r\n\r\n[ Attachment content not displayed ]\r\n------=_NextPart_000_0010_01C43F40.1CA50490--\r\n\n"}}