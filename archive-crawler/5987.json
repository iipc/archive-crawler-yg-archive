{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"igor@archive.org","from":"igor@...","profile":"iranitovic","replyTo":"LIST","senderId":"xtaP8SY2SGhXz8H3WlBmyxk2JBEXBvIIRi4DtUbaP-OTwNVlS_0Zked3s9ej0MMX2PMC_WjRCw","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Re: Can I split seeds for a HashCrawlMapper      crawl?","postDate":"1250313946","msgId":5987,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwMzU2Ljk4LjIzNC44Ni4xNjcuMTI1MDMxMzk0Ni5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg==","inReplyToHeader":"PGg2MWphYys3MjJlQGVHcm91cHMuY29tPg==","referencesHeader":"PGg2MWphYys3MjJlQGVHcm91cHMuY29tPg=="},"prevInTopic":5983,"nextInTopic":5988,"prevInTime":5986,"nextInTime":5988,"topicId":5971,"numMessagesInTopic":8,"msgSnippet":"The seed lists that I used were ~25MM and had no problems. However, the seeds were coming from only a few million domains. Did your seed list go from 0.5MM","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 1626 invoked from network); 15 Aug 2009 05:26:18 -0000\r\nX-Received: from unknown (69.147.108.201)\n  by m2.grp.re1.yahoo.com with QMQP; 15 Aug 2009 05:26:18 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta2.grp.re1.yahoo.com with SMTP; 15 Aug 2009 05:26:18 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 66B652810F4\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 14 Aug 2009 22:25:47 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id ty9u358bkJaN for &lt;archive-crawler@yahoogroups.com&gt;;\n\tFri, 14 Aug 2009 22:25:46 -0700 (PDT)\r\nX-Received: from mail.archive.org (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id DA04026A878\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 14 Aug 2009 22:25:46 -0700 (PDT)\r\nX-Received: from 98.234.86.167\n        (SquirrelMail authenticated user igor@...)\n        by mail.archive.org with HTTP;\n        Fri, 14 Aug 2009 22:25:46 -0700 (PDT)\r\nMessage-ID: &lt;50356.98.234.86.167.1250313946.squirrel@...&gt;\r\nIn-Reply-To: &lt;h61jac+722e@...&gt;\r\nReferences: &lt;h61jac+722e@...&gt;\r\nDate: Fri, 14 Aug 2009 22:25:46 -0700 (PDT)\r\nTo: archive-crawler@yahoogroups.com\r\nUser-Agent: SquirrelMail/1.4.10a\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nX-Priority: 3 (Normal)\r\nImportance: Normal\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: igor@...\r\nSubject: Re: [archive-crawler] Re: Can I split seeds for a HashCrawlMapper \n     crawl?\r\nX-Yahoo-Group-Post: member; u=137477665; y=2U8zEcsiZLWYf5uYOoYic45lWL_y3iS4tnphnV1zZhWSF_wjvQ\r\nX-Yahoo-Profile: iranitovic\r\n\r\nThe seed lists that I used were ~25MM and had no problems. However, the\nseeds were coming from only a few million domains.\n\nDid your seed list go from 0.5MM domains to 20MM domains?\n\ni.\n\n\n\n&gt; We used to do it. I also use 2 HashCrawlMapper processors as you\n&gt; described.\n&gt;\n&gt; We were able to run that with about 0.5MM seeds. With the new crawling\n&gt; strategy we need to support up to 20MM seeds and I did a proof run with\n&gt; one instance: the crawl rate slows down to 2/3 after 3 days. That would\n&gt; not work because we usually keep the crawler running for at least 3 weeks.\n&gt;\n&gt; I was guessing that the 20MM seed list caused the problem. Now I want to\n&gt; experiment with split-seeds but are concerned that we might be missing the\n&gt; divert URLs if the seed lists are not the same among crawlers.\n&gt;\n&gt; When a crawler gets the divert URLs, will they reject the URL if it comes\n&gt; from a seed that is not in the cralwer&#39;s seed file?\n&gt;\n&gt; Cheers,\n&gt; -Joe\n&gt;\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, igor@... wrote:\n&gt;&gt;\n&gt;&gt; Hi Joe,\n&gt;&gt;\n&gt;&gt; In the past I used to this without splitting the seeds. I used two\n&gt;&gt; identical HashCrawlMapper processors: one before the preselector and one\n&gt;&gt; after the link scoper.\n&gt;&gt;\n&gt;&gt; This way, each crawling node schedules all of the seeds but crawls only\n&gt;&gt; ones defined by the HashCrawlMapper. What I liked about this is that all\n&gt;&gt; of the nodes will have the same scope (if based on seeds) which can\n&gt;&gt; handy.\n&gt;&gt;\n&gt;&gt; Hope this helps.\n&gt;&gt; i.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; &gt; I&#39;m using Heritrix 1.14.3.\n&gt;&gt; &gt;\n&gt;&gt; &gt; Let&#39;s say I have\n&gt;&gt; &gt; 1. one big seed list consisting of 1MM seeds.\n&gt;&gt; &gt; 2. 2 crawler instances to implement HashCrawlMapper.\n&gt;&gt; &gt; 3. The crawl scope is domain + 1 (implemented through\n&gt;&gt; OnDomainDecideRule\n&gt;&gt; &gt; with &quot;seeds-as-surt-prefixes&quot;==true and &quot;also-check-via&quot;==true).\n&gt;&gt; &gt;\n&gt;&gt; &gt; Can I split the seeds using the same HashCrawlMapper rule so that each\n&gt;&gt; &gt; crawler would only get seeds that are within its scope? Would there be\n&gt;&gt; any\n&gt;&gt; &gt; difference if I use the same 1MM seeds for both crawlers?\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; The reason why I want to do this is, I have 20MM seeds among 12\n&gt;&gt; crawlers.\n&gt;&gt; &gt; I&#39;ve tested with one instance handling 20MM seeds and it doesn&#39;t seem\n&gt;&gt; to\n&gt;&gt; &gt; work. If I can split the seeds so that each cralwer starts with URLs\n&gt;&gt; that\n&gt;&gt; &gt; belong to themselves it should make the crawl process easier....\n&gt;&gt; &gt;\n&gt;&gt; &gt; Thanks,\n&gt;&gt; &gt; -Joe\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; ------------------------------------\n&gt;&gt; &gt;\n&gt;&gt; &gt; Yahoo! Groups Links\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt;\n\n\n\n"}}