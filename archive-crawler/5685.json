{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264783887,"authorName":"pbaclace","from":"&quot;pbaclace&quot; &lt;pbaclace@...&gt;","profile":"pbaclace","replyTo":"LIST","senderId":"ph3Ik9_vYGvagOoCl17FAEJigtj-9iMfuK17YoL2cX-ryAAJjEJQRPyscJgFhiKuOuDiGuBJOJsUrD1354sy3t7ujlP__g","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: lock contention in ServerCache.getServerFor()","postDate":"1234924230","msgId":5685,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGduZnJzNitodGMxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ5OTlFODFFLjkwMDA2MDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":5680,"nextInTopic":5687,"prevInTime":5684,"nextInTime":5686,"topicId":5665,"numMessagesInTopic":8,"msgSnippet":"I will use the following bug report for the proposed patch: http://webteam.archive.org/jira/browse/HER-1609 The patch is not yet ready; I will post a message","rawEmail":"Return-Path: &lt;pbaclace@...&gt;\r\nX-Sender: pbaclace@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 41993 invoked from network); 18 Feb 2009 02:30:31 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m45.grp.scd.yahoo.com with QMQP; 18 Feb 2009 02:30:31 -0000\r\nX-Received: from unknown (HELO n36b.bullet.mail.sp1.yahoo.com) (66.163.168.150)\n  by mta18.grp.scd.yahoo.com with SMTP; 18 Feb 2009 02:30:31 -0000\r\nX-Received: from [69.147.65.171] by n36.bullet.mail.sp1.yahoo.com with NNFMP; 18 Feb 2009 02:30:30 -0000\r\nX-Received: from [98.137.34.35] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 18 Feb 2009 02:30:30 -0000\r\nDate: Wed, 18 Feb 2009 02:30:30 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gnfrs6+htc1@...&gt;\r\nIn-Reply-To: &lt;4999E81E.9000605@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;pbaclace&quot; &lt;pbaclace@...&gt;\r\nSubject: Re: lock contention in ServerCache.getServerFor()\r\nX-Yahoo-Group-Post: member; u=264783887; y=tD2g-nkNvGhUDna-SdM1mAfDntXJiMlo-L_3pO8cDDMxAo4\r\nX-Yahoo-Profile: pbaclace\r\n\r\nI will use the following bug report for the proposed patch:\n\n  http://webte=\r\nam.archive.org/jira/browse/HER-1609\n\nThe patch is not yet ready; I will pos=\r\nt a message here when it is.\n\n\nPaul\n\n\n\n--- In archive-crawler@yahoogroups.c=\r\nom, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; ServerCache has been noted as a bot=\r\ntleneck before, so this is a very \n&gt; welcome result. Can you post a patch e=\r\nither here or to a JIRA issue for \n&gt; others to review and test?\n&gt; \n&gt; As I&#39;d=\r\n mentioned earlier in our offlist discussion, I didn&#39;t think the \n&gt; simple =\r\ncaching approach would help much, because there are already two \n&gt; levels o=\r\nf caching (CachedBDBMap&#39;s soft-reference object-identity cache, \n&gt; and BDB&#39;=\r\ns byte-array cache) that should minimize the IO/lock-time when \n&gt; reading t=\r\nhe same key multiple times in sequence.\n&gt; \n&gt; It&#39;s good to see that the deep=\r\ner lock-untangling offers such a big \n&gt; speedup for your crawl.\n&gt; \n&gt; - Gord=\r\non @ IA\n&gt; \n&gt; pbaclace wrote:\n&gt; &gt; The &quot;un-knotting&quot; performance change worke=\r\nd.  I see a 2X speedup in\n&gt; &gt; heritrix v1.14.2:\n&gt; &gt; \n&gt; &gt; * 460KB/sec (from =\r\n230KB/sec) network usage\n&gt; &gt; * 100% cpu with load between 15-19 (as reporte=\r\nd by &quot;w&quot; in linux)\n&gt; &gt; * disk usage at 600KB/sec (from 300KB/sec) \n&gt; &gt; * nu=\r\nmber of established HTTP sockets:  25 (an average from 13\nnetstats)\n&gt; &gt; \n&gt; =\r\n&gt; Basically, the same run took half the time.  As long as logging is not\n&gt; =\r\n&gt; verbose, the TOE worker threads are blocked on writing the WARC files.\n&gt; =\r\n&gt;  (Actually, this job writes out both WARC and ARC, so it could be\n&gt; &gt; imp=\r\nroved.) \n&gt; &gt; \n&gt; &gt; The high load number might seem scary to some people, but=\r\n it just\n&gt; &gt; means the cpu is fully utilized and more cores could help.\n&gt; &gt;=\r\n \n&gt; &gt; It requires edits to 3 files plus 3 other files need trivial changes.=\r\n\n&gt; &gt;  The un-knotting has not yet been tested against: multi-core,\n&gt; &gt; mult=\r\ni-processor, checkpointing, and recovery.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Paul\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; =\r\n--- In archive-crawler@yahoogroups.com, &quot;pbaclace&quot; &lt;pbaclace@&gt; wrote:\n&gt; &gt;&gt;\n=\r\n&gt; &gt;&gt; A test run of:\n&gt; &gt;&gt;   * Heritrix 1.14.2 on an AWS/EC2, small instance,=\r\n with 100 worker\n&gt; &gt;&gt; threads, 1.3M seeds, 900MB heap\n&gt; &gt;&gt;\n&gt; &gt;&gt; Has the fol=\r\nlowing resource utilization stats:\n&gt; &gt;&gt;\n&gt; &gt;&gt;   *  230KB/sec of the network\n=\r\n&gt; &gt;&gt;   * 100% cpu with load between 7 and 13\n&gt; &gt;&gt;   * disk starts out at 30=\r\n0KB/sec, and 24 hours later is at 1MB/sec\n&gt; &gt;&gt;   * number of established HT=\r\nTP sockets:  ranges from 1 to 7,\n&gt; &gt;&gt; occasional spiking to 14\n&gt; &gt;&gt;   * Ful=\r\nl GC every 10 minutes\n&gt; &gt;&gt;\n&gt; &gt;&gt; The limiting resource is the cpu.  A one co=\r\nre machine should\n&gt; &gt;&gt; theoretically be able to saturate either the network=\r\n or the disk\n&gt; &gt;&gt; bandwidth before the cpu hits the wall, unless it has hea=\r\nvy lock\n&gt; &gt;&gt; contention. \n&gt; &gt;&gt;\n&gt; &gt;&gt; See how many and where threads are wait=\r\ning in some jstack thread\ndumps::\n&gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritr=\r\nix.9.threaddump  |sort |uniq -c\n&gt; &gt;&gt;      25         - waiting to lock &lt;0x5=\r\nc357d90&gt; (a\n&gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt; &gt;&gt; =\r\n     61         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt;&gt; org.archive.crawler.=\r\ndatamodel.ServerCache)\n&gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.8.thread=\r\ndump  |sort |uniq -c\n&gt; &gt;&gt;       7         - waiting to lock &lt;0x5c357d90&gt; (a=\r\n\n&gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt; &gt;&gt;      56    =\r\n     - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt;&gt; org.archive.crawler.datamodel.S=\r\nerverCache)\n&gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.7.threaddump  |sort=\r\n |uniq -c\n&gt; &gt;&gt;      31         - waiting to lock &lt;0x5c357d90&gt; (a\n&gt; &gt;&gt; org.a=\r\nrchive.crawler.postprocessor.FrontierScheduler)\n&gt; &gt;&gt;      62         - wait=\r\ning to lock &lt;0x5c382828&gt; (a\n&gt; &gt;&gt; org.archive.crawler.datamodel.ServerCache)=\r\n\n&gt; &gt;&gt;\n&gt; &gt;&gt; Examination of the FrontierScheduler lock shows that it is held =\r\nin\n&gt; &gt;&gt; threaddumps 7 and 9 by a thread waiting for ServerCache.\n&gt; &gt;&gt;\n&gt; &gt;&gt; =\r\nMost threads (about 90) are waiting for a lock on ServerCache in the\n&gt; &gt;&gt; m=\r\nethod:\n&gt; &gt;&gt;\n&gt; &gt;&gt;   public synchronized CrawlServer getServerFor(String serv=\r\nerKey)\n&gt; &gt;&gt;\n&gt; &gt;&gt; Presumably, a simple name to host/server would be fast, bu=\r\nt one\nthread\n&gt; &gt;&gt; holds the lock while doing a relatively long BDB read ope=\r\nration. \n&gt; &gt;&gt; Obviously, having disk io block all cache lookups is not opti=\r\nmal,\n&gt; &gt;&gt; especially when BDB has a lock per file (FileManager).  In my tes=\r\nt\n&gt; &gt;&gt; case, the bdb data is 3.6GB and there are about 360 *.jdb files\nin t=\r\nhe\n&gt; &gt;&gt; job state directory.  If requests to getServerFor(String) were not\n=\r\n&gt; &gt;&gt; synchronized, then BDB should be able to read from multiple *.jdb\nfile=\r\n\n&gt; &gt;&gt; at the same time and threads requesting entries cached in memory by\n&gt;=\r\n &gt;&gt; CachedBDBMap would not need to wait.  \n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; I think the follo=\r\nwing high gain, small code footprint improvements\n&gt; &gt;&gt; would help:\n&gt; &gt;&gt;\n&gt; &gt;=\r\n&gt; * superficial thread-local caching (1 affected file)\n&gt; &gt;&gt; **  the ServerC=\r\nache lookups are done in many code locations, so it\n&gt; &gt;&gt; seems each thread =\r\nprocessing a uri might repeatedly do the same\nlookup\n&gt; &gt;&gt; and get stuck wai=\r\nting\n&gt; &gt;&gt; **  a ThreadLocal cache of one key-value pair could be checked be=\r\nfore\n&gt; &gt;&gt; the Maps in ServerCache.getServerFor(String) before synchronizing=\r\n on\n&gt; &gt;&gt; this instance of ServerCache.\n&gt; &gt;&gt; **  this must not interfere wit=\r\nh soft reference tracking, of course\n&gt; &gt;&gt;\n&gt; &gt;&gt; * deep un-knotting by lock-s=\r\nplitting and enabling more concurrency in\n&gt; &gt;&gt; ServerCache, CachedBdbMap, a=\r\nnd BDB.\n&gt; &gt;&gt; ** drop synchronization of ServerCache.getServerFor(String)\n&gt; =\r\n&gt;&gt; ** drop synchronization of  CachedBdbMap.get(Object)\n&gt; &gt;&gt; ** use Concurr=\r\nentHashMap for CachedBdbMap.memMap\n&gt; &gt;&gt; ** drop synchronization of CachedBd=\r\nbMap.put(K,V) and expose\n&gt; &gt;&gt; putIfAbsent(K,V) if needed.\n&gt; &gt;&gt; *** ServerCa=\r\nche.createServerFor(String) loses synchronization when\n&gt; &gt;&gt; ServerCache.get=\r\nServerFor(String) drops it.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; My particular crawl job exercise=\r\ns the ServerCache more than most\njobs,\n&gt; &gt;&gt; but it is analogous to having a=\r\n very wide, breadth-first crawl. \n&gt; &gt;&gt; Characteristics of this performance =\r\ncase are shared by all jobs that\n&gt; &gt;&gt; crawl many thousands of hosts.\n&gt; &gt;&gt;\n&gt;=\r\n &gt;&gt; Since full GC was occurring about every 10 minutes, the lock\n&gt; &gt;&gt; conte=\r\nntion was not due to full GC frequency.  A heap histogram showed\n&gt; &gt;&gt; about=\r\n 3700 CrawlServer instances at the end of the run.\n&gt; &gt;&gt;\n&gt; &gt;&gt; If this un-kno=\r\ntting can work, there should be substantially better\n&gt; &gt;&gt; disk and network =\r\nutilization. \n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; Paul\n&gt; &gt;&gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; ------------=\r\n------------------------\n&gt; &gt; \n&gt; &gt; Yahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}