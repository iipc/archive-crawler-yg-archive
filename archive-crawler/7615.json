{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":509984195,"authorName":"thomas.zeithaml","from":"&quot;thomas.zeithaml&quot; &lt;thomas.zeithaml@...&gt;","profile":"thomas.zeithaml","replyTo":"LIST","senderId":"ZM9IRpDdRlV4cu7t8mDyJRN3LZqryi5ERJjyY08NTSxK3Lnq7bPlsLol9Plrlolw8kGxZOiEYMgFmQ67sH_XR6QUu5tkMGY8VXuof0BI4pGGAds","spamInfo":{"isSpam":false,"reason":"6"},"subject":"High Performance Crawl for a single site","postDate":"1328878485","msgId":7615,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpoMzQybCsyc3BzQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":7616,"prevInTime":7614,"nextInTime":7616,"topicId":7615,"numMessagesInTopic":2,"msgSnippet":"Hi All, i plan to crawl my own site with a high performance job. But i only have 1 Queue Load 1 active of 25 threads; 1 congestion ratio; 6283 deepest queue;","rawEmail":"Return-Path: &lt;thomas.zeithaml@...&gt;\r\nX-Sender: thomas.zeithaml@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 35732 invoked from network); 10 Feb 2012 12:54:48 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m13.grp.sp2.yahoo.com with QMQP; 10 Feb 2012 12:54:48 -0000\r\nX-Received: from unknown (HELO ng20-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.252)\n  by mta1.grp.sp2.yahoo.com with SMTP; 10 Feb 2012 12:54:47 -0000\r\nX-Received: from [98.137.0.87] by ng20.bullet.mail.gq1.yahoo.com with NNFMP; 10 Feb 2012 12:54:46 -0000\r\nX-Received: from [69.147.65.150] by tg7.bullet.mail.gq1.yahoo.com with NNFMP; 10 Feb 2012 12:54:46 -0000\r\nX-Received: from [98.137.34.35] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 10 Feb 2012 12:54:46 -0000\r\nDate: Fri, 10 Feb 2012 12:54:45 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jh342l+2sps@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;thomas.zeithaml&quot; &lt;thomas.zeithaml@...&gt;\r\nSubject: High Performance Crawl for a single site\r\nX-Yahoo-Group-Post: member; u=509984195; y=sXwx4nNjgfLYs661sVLeScKJHxbuHCWDSJyP_qrWU2lqM5zWOFCauW-_\r\nX-Yahoo-Profile: thomas.zeithaml\r\n\r\nHi All,\ni plan to crawl my own site with a high performance job.\n\nBut i onl=\r\ny have 1 Queue\n\nLoad\n   1 active of 25 threads; 1 congestion ratio; 6283 de=\r\nepest queue; 6283 average depth \n\nMy Config are: \n\nI enable the queueAssign=\r\nmentPolicy in candidateScoper\n\n-------------- Code\n\n    &lt;!-- CANDIDATE CHAI=\r\nN --&gt;\n    &lt;!-- first, processors are declared as top-level named beans --&gt;\n=\r\n    &lt;bean id=3D&quot;candidateScoper&quot; class=3D&quot;org.archive.crawler.prefetch.Cand=\r\nidateScoper&quot;&gt;\n    &lt;/bean&gt;\n    &lt;bean id=3D&quot;preparer&quot; class=3D&quot;org.archive.cr=\r\nawler.prefetch.FrontierPreparer&quot;&gt;\n        &lt;!-- &lt;property name=3D&quot;preference=\r\nDepthHops&quot; value=3D&quot;-1&quot; /&gt; --&gt;\n        &lt;!-- &lt;property name=3D&quot;preferenceEmb=\r\nedHops&quot; value=3D&quot;1&quot; /&gt; --&gt;\n        &lt;property name=3D&quot;canonicalizationPolicy=\r\n&quot;&gt;\n            &lt;ref bean=3D&quot;canonicalizationPolicy&quot;/&gt;\n        &lt;/property&gt;\n =\r\n       &lt;property name=3D&quot;queueAssignmentPolicy&quot;&gt;\n            &lt;ref bean=3D&quot;q=\r\nueueAssignmentPolicy&quot; /&gt;\n        &lt;/property&gt;\n\n-------------- Code\n\nand have=\r\n assigned more parallelQueues\n\n-------------- Code\n    &lt;bean id=3D&quot;queueAss=\r\nignmentPolicy&quot;     class=3D&quot;org.archive.crawler.frontier.SurtAuthorityQueue=\r\nAssignmentPolicy&quot;&gt;\n        &lt;property name=3D&quot;forceQueueAssignment&quot; value=3D=\r\n&quot;&quot;/&gt;\n        &lt;property name=3D&quot;deferToPrevious&quot; value=3D&quot;false&quot;/&gt;\n        &lt;=\r\nproperty name=3D&quot;parallelQueues&quot; value=3D&quot;25&quot;/&gt;\n    &lt;/bean&gt;\n-------------- =\r\nCode\n\nand\n\n-------------- Code\n    &lt;bean id=3D&quot;disposition&quot; class=3D&quot;org.ar=\r\nchive.crawler.postprocessor.DispositionProcessor&quot;&gt;\n        &lt;property name=\r\n=3D&quot;delayFactor&quot; value=3D&quot;0.0&quot; /&gt;\n        &lt;property name=3D&quot;minDelayMs&quot; val=\r\nue=3D&quot;0&quot; /&gt;\n        &lt;property name=3D&quot;respectCrawlDelayUpToSeconds&quot; value=\r\n=3D&quot;300&quot; /&gt;\n        &lt;property name=3D&quot;maxDelayMs&quot; value=3D&quot;0&quot; /&gt;\n        &lt;p=\r\nroperty name=3D&quot;maxPerHostBandwidthUsageKbSec&quot; value=3D&quot;0&quot; /&gt;\n    &lt;/bean&gt;\n-=\r\n------------- Code\n\nwhat i&#39;m doing wrong to get more Queues for one Domain =\r\n?\nIs there maybe a tutorial for crawling only one website ?\n\nbest regards\nT=\r\nom\n\n\n"}}