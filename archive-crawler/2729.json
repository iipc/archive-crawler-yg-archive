{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (archive.org)","from":"&quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"JGZ8FAfcFnIbM3gxwuWz7EZSZ4ClIXAvuD49v9boK7_Rt1_hde9ICQQVkcR9HZ_R6x8bGA_UPzkg2Df0sjCdcPiLmFeaWqRx7EoN62pLpIC0R8Q4X-Cc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] TooManyHopsDecideRule and crawl speed","postDate":"1141237951","msgId":2729,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0MDVFOEJGLjEwMTA3MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGR1NGtkZCtvYnBuQGVHcm91cHMuY29tPg==","referencesHeader":"PGR1NGtkZCtvYnBuQGVHcm91cHMuY29tPg=="},"prevInTopic":2725,"nextInTopic":2730,"prevInTime":2728,"nextInTime":2730,"topicId":2725,"numMessagesInTopic":3,"msgSnippet":"... I m surprised it s crawling anything of interest at all: 2 hops deep is not very much, and even a broad crawl started from a massive directory page that","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 65633 invoked from network); 1 Mar 2006 18:32:40 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m32.grp.scd.yahoo.com with QMQP; 1 Mar 2006 18:32:40 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta1.grp.scd.yahoo.com with SMTP; 1 Mar 2006 18:32:40 -0000\r\nReceived: (qmail 12399 invoked by uid 100); 1 Mar 2006 18:25:12 -0000\r\nReceived: from adsl-71-130-102-77.dsl.pltn13.pacbell.net (HELO ?192.168.1.10?) (gojomo@...@71.130.102.77)\n  by mail-dev.archive.org with SMTP; 1 Mar 2006 18:25:12 -0000\r\nMessage-ID: &lt;4405E8BF.1010704@...&gt;\r\nDate: Wed, 01 Mar 2006 10:32:31 -0800\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc4 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;du4kdd+obpn@...&gt;\r\nIn-Reply-To: &lt;du4kdd+obpn@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-88.4 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 2:12:4:0\r\nFrom: &quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] TooManyHopsDecideRule and crawl speed\r\nX-Yahoo-Group-Post: member; u=137285340; y=IWLl5u7X41jlIJ9tBOfq397I7LJ2kfifZPgiZOZSil7B\r\nX-Yahoo-Profile: gojomo\r\n\r\nAdam Fisk wrote:\n&gt; I&#39;ve set max-hops to 2 in my TooManyHopsDecideRule in an effort to \n&gt; start from small sample crawls and build up from there, in large part \n&gt; based on research saying that 80%-90% of visits to web sites occur at \n&gt; hops &lt; 5.  \n&gt; \n&gt; After running the crawl for about 1 day, though, I&#39;m surprised to find \n&gt; that the speed of the crawld does not seem to have increased as much \n&gt; as I would have expected.  I&#39;m not downloading nearly the volume of \n&gt; data into my ARC files as I was with max-hops=20.  However, the \n&gt; crawler will spend a great deal of time idling, not downloading any \n&gt; pages.  It will often go a minute or more without downloading \n&gt; anything.\n\nI&#39;m surprised it&#39;s crawling anything of interest at all: 2 hops deep \nis not very much, and even a broad crawl started from a massive \ndirectory page that only goes 2 hops deep would likely finish all \nresponsive sites within a few hours.\n\n&#39;Responsive&#39; being the key word, however. For servers that don&#39;t \nrespond, the default number of retries and delay between retries are \nboth quite large, on the theory that there are many transient (few \nminutes to half-day or more) outages that the crawler should patiently \nwait to resolve themselves.\n\nI suspect that&#39;s what you&#39;re seeing: all the responsive sites have \nbeen finished, the crawler is just returning to a small number of \nunresponsive sites with a ~15 minute pause between retries to the sites.\n\n(I think 9 times out of 10 that someone has been surprised at how slow \nthe crawler is going, it&#39;s been because given the prevailing rules for \nwhat it may crawl -- politeness pauses, retry pauses, no simultaneous \nhits against the same site -- there&#39;s a dearth of eligible material to \ncrawl.)\n\nThe Frontier report should make it clear what queues are in what \nstate. If they&#39;re all in &#39;snoozed&#39; state with long times-til-wake, \nthen every queue with items remaining to try has been trying \nunresponsive servers.\n\n&gt; It appears to me it must still be evaluating the same number of \n&gt; overall URIs as it was before, but just rejecting the vast majority of \n&gt; them.  Is this accurate?  I guess I expected it would avoid evaluating \n&gt; the same number of URIs because it could reject so many right away \n&gt; that are only reachable beyond a certain number of hops.  Given that \n&gt; decide rules only come into play after the crawler has reached a given \n&gt; URI, though, I suspect this is not the case.\n&gt; \n&gt; Any way I might be able to tweak things so setting the hops that low \n&gt; makes the crawler not even consider a huge set of URIs?\n&gt; \n&gt; Sorry if I&#39;m missing something obvious here.\n\nI can&#39;t quite follow what you&#39;re asking. If your rules are set to \nreject everything after 2 hops, then on a page that&#39;s exactly two hops \nfrom the start, every discovered outlink on that page will be \nconsidered, but then rejected for being too many hops (3) away. As a \nresult, no outlink that&#39;s 4 hops away will ever even be \ndiscovered/considered.\n\nHope this helps,\n\n- Gordon @ IA\n\n"}}