{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"P40wFtLn3yfzOYf2mi990z6c-vqFp7zwJfcYeu9vOORcqq5Q5zXfCI-GU4Vvg1FKPRzh_APFCWJyMQpr7Dmx4BdjgLYwXW4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] robots.txt problem","postDate":"1324592683","msgId":7478,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFRjNBRTJCLjUwMTA5MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRFRjM4RDk5LjEwODA0MDZAY3MuY211LmVkdT4=","referencesHeader":"PGpjNThodCt1Z3VwQGVHcm91cHMuY29tPiA8NEVFODA5RDcuNzAxMDEwMEBhcmNoaXZlLm9yZz4gPDRFRjM4RDk5LjEwODA0MDZAY3MuY211LmVkdT4="},"prevInTopic":7477,"nextInTopic":7479,"prevInTime":7477,"nextInTime":7479,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"Robots-enforcement is completely separate from scoping/DecideRules, so changes to SurtPrefixedDecideRule couldn t be responsible for robots issues. (Though, if","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 26636 invoked from network); 22 Dec 2011 22:24:45 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m10.grp.sp2.yahoo.com with QMQP; 22 Dec 2011 22:24:45 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta6.grp.sp2.yahoo.com with SMTP; 22 Dec 2011 22:24:45 -0000\r\nX-Received: (qmail 69748 invoked by uid 0); 22 Dec 2011 22:24:44 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay00.pair.com with SMTP; 22 Dec 2011 22:24:44 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4EF3AE2B.5010907@...&gt;\r\nDate: Thu, 22 Dec 2011 14:24:43 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: David Pane &lt;dpane@...&gt;\r\nReferences: &lt;jc58ht+ugup@...&gt; &lt;4EE809D7.7010100@...&gt; &lt;4EF38D99.1080406@...&gt;\r\nIn-Reply-To: &lt;4EF38D99.1080406@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] robots.txt problem\r\nX-Yahoo-Group-Post: member; u=137285340; y=jxKEm7EA10aJtA5FkCR1jZYhgc05Mjqto9WvX0_iAxH5\r\nX-Yahoo-Profile: gojomo\r\n\r\nRobots-enforcement is completely separate from scoping/DecideRules, so \nchanges to SurtPrefixedDecideRule couldn&#39;t be responsible for robots \nissues.\n\n(Though, if your changes are a significant new capability, such as \ntriggering DNS lookups or even consulting the existing cached CrawlHost \nlookups during scoping, you might want to package them as another rule \nthat could be applied in serial, rather than an extension of \nSurtPrefixedDecideRule.)\n\n From the description of the URLs and the site&#39;s robots.txt, it seems \nthey should be prohibited.\n\nHeritrix robots.txt-parsing does not yet support the Google-originated \nconvention of internal &#39;*&#39; wildcards in declared paths, but we tolerate \n&#39;*&#39; at the end (as being equivalent to not being there at all). And, we \nshould simply interpret internal &#39;*&#39; literally (and continue respecting \nall other rules, such as the ones that are relevant here). So this is \nprobably not a factor, even though some internal wildcards are used in \nthis site&#39;s robots.txt.\n\nCan you confirm from your own crawl.log that you visited the relevant \nrobots.txt, received it (as a 200 of appropriate size), and then fetched \nthe other URIs that should have been precluded?\n\nAre there any other changes to your code or configuration related to \nrobots or the PreconditionEnforcer processor?\n\nCan you reproduce the same behavior when trying to crawl a single one of \nthe problem URIs, such as for example....\n\nhttp:/tmbw.net/wiki/index.php?title=1996&action=edit\n\n...with either the default configuration or your usual configuration \nwith just this one seed?\n\nFYI, whether a mid-crawl change has taken effect the way you&#39;d like \ndepends on how you added it. The rules that take their starting \nconfiguration from lists of URIs/patterns (in either CXML or from source \nfiles on disk) are *not* constantly monitoring those files for \nchanges... so simply editing those files on disk *won&#39;t* take immediate \neffect.\n\nHowever, if you drop a &quot;updates.seeds&quot; file into the &#39;action&#39; directory, \nseeds and directive lines (starting with &#39;+&#39; or &#39;-&#39;) do get announced to \nthe crawl in a way that (in the usual configuration) will be noticed by \naccept/reject SurtPrefixedDecideRules, in which case they do live-update \ntheir in-memory rule-in/rule-out lists.\n\n- Gordon\n\nOn 12/22/11 12:05 PM, David Pane wrote:\n&gt; I am running a crawl using heritrix.3.1.1 from the git repository with\n&gt; some minor changes that I do not believe is the cause of this problem.\n&gt; (I made changes in SurtPrefixedDecideRule.java to accommodate the\n&gt; checking of ip addresses as well as surts in my black list surt file. )\n&gt;\n&gt; I had a webmaster complain that the crawler was not obeying his\n&gt; robots.txt file. He reported that the crawler was requesting pages that\n&gt; should be dissallowed by directives for &quot;*&quot; user agent.\n&gt;\n&gt; Here are some sample pages that it is requesting:\n&gt; /wiki/index.php?title=They_Might_Be_Giants&action=edit\n&gt; Http Code: 200 Date: Dec 22 13:04:15 Http Version: HTTP/1.0 Size in\n&gt; Bytes: 54449\n&gt; Referer: http://tmbw.net/wiki/They_Might_Be_Giants\n&gt;\n&gt; /wiki/index.php?title=Category:Sung_By_John_Flansburgh&action=edit\n&gt; Http Code: 200 Date: Dec 22 13:04:54 Http Version: HTTP/1.0 Size in\n&gt; Bytes: 21164\n&gt; Referer: http://tmbw.net/wiki/Category:Sung_By_John_Flansburgh\n&gt;\n&gt; /wiki/index.php?title=Help_talk:Contents&action=edit\n&gt; Http Code: 200 Date: Dec 22 13:05:23 Http Version: HTTP/1.0 Size in\n&gt; Bytes: 22155\n&gt; Referer: http://tmbw.net/wiki/Help_talk:Contents\n&gt;\n&gt; /wiki/index.php?title=1996&action=edit\n&gt; Http Code: 200 Date: Dec 22 13:05:48 Http Version: HTTP/1.0 Size in\n&gt; Bytes: 20813\n&gt; Referer: http://tmbw.net/wiki/1996\n&gt;\n&gt; /wiki/index.php?title=Here_Comes_Science/Charts&action=render&chartOnly=Billboard%20Kids%20Albums\n&gt;\n&gt; Http Code: 200 Date: Dec 22 13:06:18 Http Version: HTTP/1.0 Size in\n&gt; Bytes: 1069\n&gt; Referer: http://tmbw.net/wiki/Here_Comes_Science\n&gt;\n&gt; /wiki/index.php?title=Dial-A-Song&action=edit\n&gt; Http Code: 200 Date: Dec 22 13:06:46 Http Version: HTTP/1.0 Size in\n&gt; Bytes: 29906\n&gt; Referer: http://tmbw.net/wiki/Dial-A-Song\n&gt;\n&gt;\n&gt; His robots.txt file is here: http://tmbw.net/robots.txt\n&gt;\n&gt; I added his domain to my surt black list file while the crawler is\n&gt; running. I am assuming that this update will be applied. Please let me\n&gt; know if I am correct in this assumption.\n&gt;\n&gt; Thanks,\n&gt;\n&gt; David\n\n"}}