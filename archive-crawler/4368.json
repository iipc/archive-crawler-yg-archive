{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":91078969,"authorName":"Jigar Patel","from":"Jigar Patel &lt;jigar_bca@...&gt;","profile":"jigar_bca","replyTo":"LIST","senderId":"5_1EF3vTsbEcyMAfXiD7a3e5-IRh5Jr_OklQffc3HVHrsX9fX6V84tcDDHrAtlj2jZdMe0venWDJnO1R1-86PugMX2tZkTvm","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling","postDate":"1182922017","msgId":4368,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDkyOTI4NS41MDQ4Ny5xbUB3ZWI1MDMwNi5tYWlsLnJlMi55YWhvby5jb20+","inReplyToHeader":"PDQ2ODE2QzU4LjYwMjAwMDBAYXJjaGl2ZS5vcmc+"},"prevInTopic":4364,"nextInTopic":4373,"prevInTime":4367,"nextInTime":4369,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"Thanks a lot Gordon, You solved my problem. One more thing I want to know that, Can I use same settings for two different independent machines for distributed","rawEmail":"Return-Path: &lt;jigar_bca@...&gt;\r\nX-Sender: jigar_bca@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 78860 invoked from network); 27 Jun 2007 05:27:15 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m50.grp.scd.yahoo.com with QMQP; 27 Jun 2007 05:27:15 -0000\r\nReceived: from unknown (HELO web50306.mail.re2.yahoo.com) (206.190.38.60)\n  by mta10.grp.scd.yahoo.com with SMTP; 27 Jun 2007 05:27:15 -0000\r\nReceived: (qmail 51219 invoked by uid 60001); 27 Jun 2007 05:26:57 -0000\r\nX-YMail-OSG: Zy81sSwVM1kTBBAtHATzLQxgRrG5Nfuqmdo4aQXl\r\nReceived: from [203.199.114.33] by web50306.mail.re2.yahoo.com via HTTP; Tue, 26 Jun 2007 22:26:57 PDT\r\nDate: Tue, 26 Jun 2007 22:26:57 -0700 (PDT)\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;46816C58.6020000@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-1502971677-1182922017=:50487&quot;\r\nContent-Transfer-Encoding: 8bit\r\nMessage-ID: &lt;929285.50487.qm@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Jigar Patel &lt;jigar_bca@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=91078969; y=HbRvhC-3LlbG29Ts26gYYP_eMiF6DBZp4SIZdp2xui4jUbJi\r\nX-Yahoo-Profile: jigar_bca\r\n\r\n\r\n--0-1502971677-1182922017=:50487\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nThanks a lot Gordon,\n   \n  You solved my problem.\n   \n  One more thing I want to know that, \n  Can I use same settings for two different independent machines for distributed crawling ?\n   \n  Please tell me how does it work and coordinate with different machine ?\n   \n  Thanks\n   \n  Jigar\n\nGordon Mohr &lt;gojomo@...&gt; wrote:\n          Jigar Patel wrote:\n&gt; Presently I am running two heritrix instances on the same machine on \n&gt; different port...\n\nIn general, you only want to use distributed crawling, with URIs \npartitioned across separate cooperating crawlers, to spread a crawl over \nmultiple independent machines. If using a single machine, a single \ncrawler instance will be more efficient.\n\n&gt; I am using decidingScope and inside it I apply SurtPrefixRule\n&gt; I added HashCrawlMapper at two places as you suggested\n&gt; I made same configuration setting and seed file at each place.\n&gt; \n&gt; But as I run my job it gives me following error in seed file and \n&gt; nothing was crawled.\n&gt; \n&gt; Heritrix(-5002)-Blocked by custom prefetch processor \n&gt; \n&gt; Please let me know why I am getting such error...\n&gt; \n&gt; Is anything missing ?\n\nThis is the expected crawl.log result for URIs that are considered by a \ncrawler, but mapped to be handled by one of the others in the group of \ncrawlers. With a proper configuration, some but not all lines in your \ncrawl.log will have this code.\n\nFor example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and \nthe other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; of &#39;2&#39;.\n\nEvery URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, but \nwas fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the \ncrawl.log as &#39;blocked by custom processor&#39;. It is then up to the \noperator if they want to cross-feed those URIs to the &#39;1&#39; crawler.\n\n- Gordon @ IA\n\n&gt; Regards,\n&gt; \n&gt; Jigar Patel\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; \n&gt; wrote:\n&gt;&gt; nt_bdr wrote:\n&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we \n&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600 \n&gt;&gt; million URLs visited), and know of work elsewhere over up to 8 \n&gt; machines \n&gt;&gt; (&gt;1 billion URLs fetched).\n&gt;&gt;\n&gt;&gt; For background see some previous threads including:\n&gt;&gt;\n&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2909\n&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/3060\n&gt;&gt;\n&gt;&gt; Roughly how we do it:\n&gt;&gt;\n&gt;&gt; - Use BloomFilterUriUniqFilter with its defaults -- which devotes \n&gt;&gt; about 500MB to this structure and keeps the false-positive \n&gt; (mistakenly \n&gt;&gt; believed to have been previously-scheduled) rate under 1-in-4-\n&gt; million up \n&gt;&gt; through 125 million URIs discovered.\n&gt;&gt;\n&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+ \n&gt; heap\n&gt;&gt; - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in \n&gt; queues \n&gt;&gt; named by the reversed-host (com,example,) rather than usual host \n&gt;&gt; (example.com)\n&gt;&gt;\n&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor \n&gt; chain:\n&gt;&gt; * Once, immediately before the PreconditionEnforcer. This one \n&gt; has \n&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any \n&gt; scheduled \n&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)\n&gt;&gt; * Again, immediately before the FrontierScheduler. This one has \n&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any \n&gt; discovered \n&gt;&gt; outlinks before they are scheduled.)\n&gt;&gt;\n&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a \n&gt; number 0 \n&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and \n&gt; all \n&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers, \n&gt; n).\n&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the \n&gt; SURT \n&gt;&gt; authority part, because of the above choice -- and decides if a URI \n&gt; is \n&gt;&gt; handled by the current crawler or one of its siblings. If mapped to \n&gt; a \n&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally. \n&gt;&gt; Depending on the character of your crawl, you may want to feed \n&gt; these \n&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore \n&gt; them.\n&gt;&gt; The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key \n&gt; before \n&gt;&gt; mapping -- used to ensure that all subdomains of example.com are \n&gt; treated \n&gt;&gt; the same as example.com for mapping purposes. The first match of \n&gt; this \n&gt;&gt; pattern, if present, is what is used for mapping purposes. A small \n&gt;&gt; example would be:\n&gt;&gt;\n&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n&gt;&gt;\n&gt;&gt; For 3-letter domains (com, org, net), this uses everything \n&gt; through \n&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it \n&gt; uses \n&gt;&gt; the first 9 characters. You could imagine more complicated patterns \n&gt; that \n&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs, \n&gt; like \n&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level \n&gt;&gt; domains.)\n&gt;&gt;\n&gt;&gt; - All crawlers are launched with the same configuration, \n&gt; including \n&gt;&gt; the same seeds, but otherwise do not (themselves) communicate. \n&gt; Seeds \n&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early \n&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed \n&gt; are \n&gt;&gt; done so by an external process/scripts.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n\n         \n\n \n---------------------------------\nFood fight? Enjoy some healthy debate\nin the Yahoo! Answers Food & Drink Q&A.\r\n--0-1502971677-1182922017=:50487\r\nContent-Type: text/html; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\n&lt;div&gt;Thanks a lot Gordon,&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;You solved my problem.&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;One more thing I want to know that, &lt;/div&gt;  &lt;div&gt;Can I use same settings for two different independent machines for distributed crawling ?&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;Please tell me how does it work and coordinate with different machine ?&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;Thanks&lt;/div&gt;  &lt;div&gt;&nbsp;&lt;/div&gt;  &lt;div&gt;Jigar&lt;BR&gt;&lt;BR&gt;&lt;B&gt;&lt;I&gt;Gordon Mohr &lt;gojomo@...&gt;&lt;/I&gt;&lt;/B&gt; wrote:&lt;/div&gt;  &lt;BLOCKQUOTE class=replbq style=&quot;PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #1010ff 2px solid&quot;&gt;&lt;!-- Network content --&gt;  &lt;DIV id=ygrp-text&gt;  &lt;div&gt;Jigar Patel wrote:&lt;BR&gt;&gt; Presently I am running two\n heritrix instances on the same machine on &lt;BR&gt;&gt; different port...&lt;BR&gt;&lt;BR&gt;In general, you only want to use distributed crawling, with URIs &lt;BR&gt;partitioned across separate cooperating crawlers, to spread a crawl over &lt;BR&gt;multiple independent machines. If using a single machine, a single &lt;BR&gt;crawler instance will be more efficient.&lt;BR&gt;&lt;BR&gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule&lt;BR&gt;&gt; I added HashCrawlMapper at two places as you suggested&lt;BR&gt;&gt; I made same configuration setting and seed file at each place.&lt;BR&gt;&gt; &lt;BR&gt;&gt; But as I run my job it gives me following error in seed file and &lt;BR&gt;&gt; nothing was crawled.&lt;BR&gt;&gt; &lt;BR&gt;&gt; Heritrix(-5002)&lt;WBR&gt;-Blocked by custom prefetch processor &lt;BR&gt;&gt; &lt;BR&gt;&gt; Please let me know why I am getting such error...&lt;BR&gt;&gt; &lt;BR&gt;&gt; Is anything missing ?&lt;BR&gt;&lt;BR&gt;This is the expected crawl.log result for URIs that are considered by a &lt;BR&gt;crawler, but mapped to be handled by one of the others in the group\n of &lt;BR&gt;crawlers. With a proper configuration, some but not all lines in your &lt;BR&gt;crawl.log will have this code.&lt;BR&gt;&lt;BR&gt;For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and &lt;BR&gt;the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; of &#39;2&#39;.&lt;BR&gt;&lt;BR&gt;Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, but &lt;BR&gt;was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the &lt;BR&gt;crawl.log as &#39;blocked by custom processor&#39;. It is then up to the &lt;BR&gt;operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.&lt;BR&gt;&lt;BR&gt;- Gordon @ IA&lt;BR&gt;&lt;BR&gt;&gt; Regards,&lt;BR&gt;&gt; &lt;BR&gt;&gt; Jigar Patel&lt;BR&gt;&gt; &lt;BR&gt;&gt; --- In &lt;A href=&quot;mailto:archive-crawler%40yahoogroups.com&quot;&gt;archive-crawler@&lt;WBR&gt;yahoogroups.&lt;WBR&gt;com&lt;/A&gt;, Gordon Mohr &lt;gojomo@...&gt; &lt;BR&gt;&gt; wrote:&lt;BR&gt;&gt;&gt; nt_bdr wrote:&lt;BR&gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?&lt;BR&gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we\n &lt;BR&gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600 &lt;BR&gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8 &lt;BR&gt;&gt; machines &lt;BR&gt;&gt;&gt; (&gt;1 billion URLs fetched).&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; For background see some previous threads including:&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; &lt;A href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&quot;&gt;http://tech.&lt;WBR&gt;groups.yahoo.&lt;WBR&gt;com/group/&lt;WBR&gt;archive-crawler/&lt;WBR&gt;message/2909&lt;/A&gt;&lt;BR&gt;&gt;&gt; &lt;A href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&quot;&gt;http://tech.&lt;WBR&gt;groups.yahoo.&lt;WBR&gt;com/group/&lt;WBR&gt;archive-crawler/&lt;WBR&gt;message/3060&lt;/A&gt;&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; Roughly how we do it:&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; - Use BloomFilterUriUniqF&lt;WBR&gt;ilter with its defaults -- which devotes &lt;BR&gt;&gt;&gt; about 500MB to this structure and keeps the false-positive &lt;BR&gt;&gt; (mistakenly &lt;BR&gt;&gt;&gt; believed to have been previously-schedule&lt;WBR&gt;d) rate under 1-in-4-&lt;BR&gt;&gt; million\n up &lt;BR&gt;&gt;&gt; through 125 million URIs discovered.&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+ &lt;BR&gt;&gt; heap&lt;BR&gt;&gt;&gt; - Use SurtAuthorityAssign&lt;WBR&gt;mentPolicy, so URIs are grouped in &lt;BR&gt;&gt; queues &lt;BR&gt;&gt;&gt; named by the reversed-host (com,example,&lt;WBR&gt;) rather than usual host &lt;BR&gt;&gt;&gt; (example.com)&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor &lt;BR&gt;&gt; chain:&lt;BR&gt;&gt;&gt; * Once, immediately before the PreconditionEnforce&lt;WBR&gt;r. This one &lt;BR&gt;&gt; has &lt;BR&gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any &lt;BR&gt;&gt; scheduled &lt;BR&gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)&lt;BR&gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has &lt;BR&gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any &lt;BR&gt;&gt; discovered &lt;BR&gt;&gt;&gt; outlinks before they are scheduled.)&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; Both\n HashCrawlMappers should have the same &#39;local-name&#39; (a &lt;BR&gt;&gt; number 0 &lt;BR&gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and &lt;BR&gt;&gt; all &lt;BR&gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers, &lt;BR&gt;&gt; n).&lt;BR&gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the &lt;BR&gt;&gt; SURT &lt;BR&gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI &lt;BR&gt;&gt; is &lt;BR&gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to &lt;BR&gt;&gt; a &lt;BR&gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally. &lt;BR&gt;&gt;&gt; Depending on the character of your crawl, you may want to feed &lt;BR&gt;&gt; these &lt;BR&gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore &lt;BR&gt;&gt; them.&lt;BR&gt;&gt;&gt; The &#39;reduce-prefix-&lt;WBR&gt;pattern&#39; may be used to trim the queue key &lt;BR&gt;&gt; before &lt;BR&gt;&gt;&gt; mapping -- used to ensure that all subdomains of example.com are &lt;BR&gt;&gt; treated\n &lt;BR&gt;&gt;&gt; the same as example.com for mapping purposes. The first match of &lt;BR&gt;&gt; this &lt;BR&gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small &lt;BR&gt;&gt;&gt; example would be:&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*&lt;WBR&gt;)|[&#92;w,]{9}&lt;WBR&gt;)&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything &lt;BR&gt;&gt; through &lt;BR&gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it &lt;BR&gt;&gt; uses &lt;BR&gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns &lt;BR&gt;&gt; that &lt;BR&gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs, &lt;BR&gt;&gt; like &lt;BR&gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level &lt;BR&gt;&gt;&gt; domains.)&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; - All crawlers are launched with the same configuration, &lt;BR&gt;&gt; including &lt;BR&gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate. &lt;BR&gt;&gt; Seeds &lt;BR&gt;&gt;&gt; that don&#39;t belong on any one crawler\n are dropped out by the early &lt;BR&gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed &lt;BR&gt;&gt; are &lt;BR&gt;&gt;&gt; done so by an external process/scripts.&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt;&gt; - Gordon @ IA&lt;BR&gt;&gt;&gt;&lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; Yahoo! Groups Links&lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&gt; &lt;BR&gt;&lt;BR&gt;&lt;/div&gt;&lt;/DIV&gt;&lt;!--End group email --&gt;&lt;/BLOCKQUOTE&gt;&lt;BR&gt;&lt;p&gt;&#32;\n\n&lt;hr size=1&gt;&lt;a href=&quot;http://answers.yahoo.com/dir/index;_ylc=X3oDMTFvbGNhMGE3BF9TAzM5NjU0NTEwOARfcwMzOTY1NDUxMDMEc2VjA21haWxfdGFnbGluZQRzbGsDbWFpbF90YWcx?link=ask&sid=396545367&quot;&gt;Food fight?&lt;/a&gt; Enjoy some healthy debate&lt;br&gt;in the &lt;a href=&quot;http://answers.yahoo.com/dir/index;_ylc=X3oDMTFvbGNhMGE3BF9TAzM5NjU0NTEwOARfcwMzOTY1NDUxMDMEc2VjA21haWxfdGFnbGluZQRzbGsDbWFpbF90YWcx?link=ask&sid=396545367&quot;&gt;Yahoo! Answers Food & Drink Q&A.&lt;/a&gt;\r\n--0-1502971677-1182922017=:50487--\r\n\n"}}