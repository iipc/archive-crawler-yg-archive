{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":278782393,"authorName":"Paul Jack","from":"Paul Jack &lt;pjack@...&gt;","profile":"poetbeware","replyTo":"LIST","senderId":"07QwvrFIxSalLbrWN9vVXBIzCxUaKTaWUyK-ybqbvE0Ofcx0Qj-jiLM0g94sM3TixJM7qydGfEWjPn7Z9XuiCE_YLOU","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] increase crawling problem","postDate":"1177089607","msgId":4148,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDk5ODY3Q0FGLUY4N0MtNEI1Qi04NjE3LTQ4REM3MDUyNEY4M0BhcmNoaXZlLm9yZz4=","inReplyToHeader":"PGYwNGppZStjNnVzQGVHcm91cHMuY29tPg==","referencesHeader":"PGYwNGppZStjNnVzQGVHcm91cHMuY29tPg=="},"prevInTopic":4135,"nextInTopic":0,"prevInTime":4147,"nextInTime":4149,"topicId":4135,"numMessagesInTopic":2,"msgSnippet":"We ve found that performing one-off collections makes it easier to create search indexes and perform other analysis of the collected sites. It can be easier to","rawEmail":"Return-Path: &lt;pjack@...&gt;\r\nX-Sender: pjack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 12614 invoked from network); 20 Apr 2007 17:20:39 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m48.grp.scd.yahoo.com with QMQP; 20 Apr 2007 17:20:39 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta7.grp.scd.yahoo.com with SMTP; 20 Apr 2007 17:20:39 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id EB124141A56FB\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 20 Apr 2007 10:20:08 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 16525-01-86 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tFri, 20 Apr 2007 10:20:08 -0700 (PDT)\r\nReceived: from [192.168.1.22] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id A918614182DA7\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 20 Apr 2007 10:20:08 -0700 (PDT)\r\nMime-Version: 1.0 (Apple Message framework v752.3)\r\nIn-Reply-To: &lt;f04jie+c6us@...&gt;\r\nReferences: &lt;f04jie+c6us@...&gt;\r\nContent-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed\r\nMessage-Id: &lt;99867CAF-F87C-4B5B-8617-48DC70524F83@...&gt;\r\nContent-Transfer-Encoding: 7bit\r\nDate: Fri, 20 Apr 2007 10:20:07 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nX-Mailer: Apple Mail (2.752.3)\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Paul Jack &lt;pjack@...&gt;\r\nSubject: Re: [archive-crawler] increase crawling problem\r\nX-Yahoo-Group-Post: member; u=278782393; y=WQklKTZMrWxzuqcUgcxQ6Hv9xyRzq6xNEjd4dVwuwdzESgCNBQ\r\nX-Yahoo-Profile: poetbeware\r\n\r\nWe&#39;ve found that performing one-off collections makes it easier to  \ncreate search indexes and perform other analysis of the collected  \nsites. It can be easier to set up a pipeline where you have some  \nmachines dedicated to crawling, and once a crawl finishes, the ARCS  \nare moved off to machines that are dedicated to indexing. Updating a  \nsearch index as ARCs are found can be done, but it adds load to your  \ncrawling machines and makes it more complex to troubleshoot when  \nsomething goes wrong.\n\nHope that helps,\n\n-Paul\n\n\nOn Apr 18, 2007, at 1:06 AM, vretr wrote:\n&gt; hi,all.\n&gt; If we collect the webpages of the website by gradual increase, what\n&gt; are the differences between this method and one-off collection of\n&gt; webpages? who can give me some detailed explanations about crawling\n&gt; the webpages of the website by gradual increase?\n\n"}}