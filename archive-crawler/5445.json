{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"j94oWR-nodgjNpd6LrvDP10lHH41Qr7C3bIghkaBsq1EqViWougKEkA97ZrmRFKFlDnYD0KbsKzyYXmKJDkK3B0wcMsu-iQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Crawling Stopped after the Seeds completed","postDate":"1220496993","msgId":5445,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4QkY0RTYxLjYwNjA4MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGc5aGE1ZStkbWNuQGVHcm91cHMuY29tPg==","referencesHeader":"PGc5aGE1ZStkbWNuQGVHcm91cHMuY29tPg=="},"prevInTopic":5438,"nextInTopic":5447,"prevInTime":5444,"nextInTime":5446,"topicId":5438,"numMessagesInTopic":3,"msgSnippet":"It s not an issue with your scoping/prefixes. I tried crawling , and the result line in the crawl.log was: ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 72889 invoked from network); 4 Sep 2008 02:56:31 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m43.grp.scd.yahoo.com with QMQP; 4 Sep 2008 02:56:31 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta16.grp.scd.yahoo.com with SMTP; 4 Sep 2008 02:56:31 -0000\r\nX-Received: (qmail 44327 invoked from network); 4 Sep 2008 02:56:27 -0000\r\nX-Received: from unknown (HELO ?10.0.10.7?) (unknown)\n  by unknown with SMTP; 4 Sep 2008 02:56:27 -0000\r\nX-pair-Authenticated: 70.137.149.117\r\nMessage-ID: &lt;48BF4E61.6060804@...&gt;\r\nDate: Wed, 03 Sep 2008 19:56:33 -0700\r\nUser-Agent: Thunderbird 2.0.0.16 (Windows/20080708)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g9ha5e+dmcn@...&gt;\r\nIn-Reply-To: &lt;g9ha5e+dmcn@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Crawling Stopped after the Seeds completed\r\nX-Yahoo-Group-Post: member; u=137285340; y=2TM88-Kfpb7rwE25fLFnYfF-NuG01xg8r2QELniwW9OF\r\nX-Yahoo-Profile: gojomo\r\n\r\nIt&#39;s not an issue with your scoping/prefixes.\n\nI tried crawling &lt;http://www.genealogy.ams.org/id.php?id=123&gt;, and the \nresult line in the crawl.log was:\n\n2008-09-04T00:16:11.218Z   400        312 \nhttp://www.genealogy.ams.org/id.php?id=123 - - text/html #004 \n20080904001610953+265 sha1:UNXDSRUEFN4HMW5UFGQJQMM6VTBUYLY7 - 3t\n\nNote the &#39;400&#39; status code: that&#39;s an HTTP failure code. The same code \nwas shown for the fetch of /robots.txt, and examining the ARC file \nshowed an error from the server which included the message:\n\n&quot;&lt;h1&gt;Bad Request&lt;/h1&gt;\n&lt;p&gt;Your browser sent a request that this server could not understand.&quot;\n\nThat indicates the server is for some reason rejecting the Heritrix \nrequest. However, the same URL can easily be visited with a browser, and \neven setting the browser User-Agent to the same as the crawler doesn&#39;t \ncause the same &#39;400&#39; error -- so they&#39;re not specifically blocking the \ncrawler.\n\nVia trial and error, I discovered this site requires an HTTP &#39;Accept&#39; \nheader on the request. The &#39;Accept&#39; header isn&#39;t required by HTTP \nspecifications, and when it&#39;s absent a site is supposed to assume any \nreturn content-type is OK. See...\n\n   http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.1\n\nWe leave it off crawler requests because we want to make the smallest, \nmost simple requests necessary.\n\nHowever, you can add an &#39;Accept&#39; header (or other headers) via the \ncrawler configuration. Use the &#39;accept-headers&#39; setting under the \nFetchHTTP processor -- you can specify a list of full headers to add to \nrequests (not just &#39;accept&#39; headers) here. (In Heritrix 2.0, go to the \n&#39;details&#39; for that setting to add to the list.)\n\nI added the header...\n\n&quot;Accept: */*&quot;\n\n...and then had no problems crawling the URL you gave. (And, because it \ncould then discover the included links, it continued to crawl other \npages on the same site.)\n\nIf in fact there are other sites that throw an error on requests without \nand &#39;Accept&#39; header, we&#39;ll change the default in Heritrix to include \nthis header. I&#39;ve made this issue to track the matter:\n\n   http://webteam.archive.org/jira/browse/HER-1547\n\n- Gordon @ IA\n\n\nliangjie.hong wrote:\n&gt; Hi:\n&gt; \n&gt; I downloaded and installed Heritrix 2 on my machine. I followed the \n&gt; guide for Version 2 and only changed contact url and email.\n&gt; \n&gt; My goal is to crawl urls like:http://www.genealogy.ams.org/id.php?id=XXX\n&gt; \n&gt; So I setup the seed url as:http://www.genealogy.ams.org/id.php?id=123. \n&gt; However, no matter I setup the prefix associates to \n&gt; &quot;http://(org,ams,genealogy,www,)/&quot; or &quot;http://\n&gt; (org,ams,genealogy,www,)/id.php&quot; or I don&#39;t have the prefix, the \n&gt; program only crawl ONE file, which is the seed url and then stops.\n&gt; \n&gt; \n&gt; Shall I change what configuration? I am a newbie to Heritrix.\n&gt; \n&gt; Thank you very much.\n\n"}}