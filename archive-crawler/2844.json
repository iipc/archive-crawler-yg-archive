{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"zLFgBbRoZgO1JOiMxSc_Ie_6cddmigDn9dDpNN8cziLUwJORHKpRaqFlv3-gIJpsa46bwBJUVAxXrY37psPCIrD6z8DlwdY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Crawling whole czech domain","postDate":"1147204184","msgId":2844,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0NjBGMjU4LjgwNzA4QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGUzcTRkdStxY2c1QGVHcm91cHMuY29tPg==","referencesHeader":"PGUzcTRkdStxY2c1QGVHcm91cHMuY29tPg=="},"prevInTopic":2842,"nextInTopic":0,"prevInTime":2843,"nextInTime":2845,"topicId":2837,"numMessagesInTopic":5,"msgSnippet":"These java.lang.OutOfMemoryError: unable to create new native thread errors are exactly the sort of OOMEs I mentioned that are *not* due to a shortage of","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 25879 invoked from network); 9 May 2006 19:50:45 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m28.grp.scd.yahoo.com with QMQP; 9 May 2006 19:50:45 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta1.grp.scd.yahoo.com with SMTP; 9 May 2006 19:50:45 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id ADE16140FF92A\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue,  9 May 2006 12:49:40 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 29629-04-13 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 9 May 2006 12:49:37 -0700 (PDT)\r\nReceived: from [192.168.1.9] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id B7F53140FC165\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue,  9 May 2006 12:49:37 -0700 (PDT)\r\nMessage-ID: &lt;4460F258.80708@...&gt;\r\nDate: Tue, 09 May 2006 12:49:44 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e3q4du+qcg5@...&gt;\r\nIn-Reply-To: &lt;e3q4du+qcg5@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Crawling whole czech domain\r\nX-Yahoo-Group-Post: member; u=137285340; y=jDwTAqhdCtR4Mtm1ARRTNQ7K-dD17d0N-n_MW-TcVD0O\r\nX-Yahoo-Profile: gojomo\r\n\r\nThese &quot;java.lang.OutOfMemoryError: unable to create new native thread&quot; \nerrors are exactly the sort of OOMEs I mentioned that are *not* due to a \nshortage of Java heap (java object) space.\n\nIn fact, increasing Java heap space can make these sorts of errors more \ncommon -- because memory allocated to the heap cannot be used for native \nthread overhead. A 32bit JVM on a 32bit OS will often only have 2GB of \nmemory total, so assigning 1800M to the Java heap only leaves 200M for \nthe JVM&#39;s native structures and native threads. Plus if you&#39;re still \nusing a &#39;-Xss&#39; of 512K, every 2 threads will use up at least 1MB of that \nspace.\n\nSo you should be trying smaller heaps (and no &#39;-Xss&#39; setting unless \nyou&#39;re sure you need it) rather than larger heaps to resolve this kind \nof OOME.\n\nThat seems to be the main problem.\n\nThe NullPointerException in ARCWriterProcessor may not be related. It&#39;s \nquite odd because it only looks like an NPE could occur at that line if \nthe Processor was never initialized properly... so I would not expect to \nsee that NPE as the first indication of a problem. Was there any \nprogress in the crawl before these errors?\n\n- Gordon @ IA\n\ngoblin_cz wrote:\n&gt; Hi,\n&gt; thanks a lot for your advices. Today I have started heritrix 1.8.0.\n&gt; With this options JAVA_OPTS= -Xmx1800m on Java(TM) 2 Runtime\n&gt; Environment, Standard Edition (build 1.5.0_06-b05). After configure\n&gt; job according to your recommendations. I try DomainSensitiveFrontier\n&gt; to set the number of documents to download (counter-per-domain instead\n&gt; of QuotaEnforcer). The seeds list have got 220 000 seeds. But after\n&gt; preparing phase I have got recieveds some alerts:\n&gt; \n&gt; java.lang.NullPointerException\n&gt;         at\n&gt; org.archive.crawler.writer.ARCWriterProcessor.write(ARCWriterProcessor.java:457)\n&gt;         at\n&gt; org.archive.crawler.writer.ARCWriterProcessor.writeDns(ARCWriterProcessor.java:444)\n&gt;         at\n&gt; org.archive.crawler.writer.ARCWriterProcessor.innerProcess(ARCWriterProcessor.java:392)\n&gt;         at\n&gt; org.archive.crawler.framework.Processor.process(Processor.java:103)\n&gt;         at\n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:306)\n&gt;         at org.archive.crawler.framework.ToeThread.run(ToeThread.java:153)\n&gt; 05/09/2006 14:42:32 +0000 SEVERE\n&gt; org.archive.crawler.framework.ToeThread recoverableProblem Problem\n&gt; java.lang.NullPointerException occured when trying to pro\n&gt; cess &#39;dns:1-1.cz&#39; at step ABOUT_TO_BEGIN_PROCESSOR in Archiver\n&gt; \n&gt; /// 15x (on different host of course)\n&gt; \n&gt; 05/09/2006 14:42:37 +0000 SEVERE\n&gt; org.archive.crawler.framework.ToeThread run Fatal exception in\n&gt; ToeThread #14: dns:1-000-000.cz\n&gt; java.lang.NullPointerException\n&gt;         at\n&gt; org.archive.crawler.framework.CrawlController.freeReserveMemory(CrawlController.java:1788)\n&gt;         at\n&gt; org.archive.crawler.framework.ToeThread.seriousError(ToeThread.java:225)\n&gt;         at\n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:329)\n&gt;         at org.archive.crawler.framework.ToeThread.run(ToeThread.java:153)\n&gt; \n&gt; and finaly..\n&gt; \n&gt; 14:46:20.991 WARN!!\n&gt; java.lang.OutOfMemoryError: unable to create new native thread\n&gt;         at java.lang.Thread.start0(Native Method)\n&gt;         at java.lang.Thread.start(Thread.java:574)\n&gt;         at\n&gt; org.mortbay.util.ThreadPool$PoolThread.enterPool(ThreadPool.java:423)\n&gt;         at org.mortbay.util.Pool.newPondLife(Pool.java:343)\n&gt;         at org.mortbay.util.Pool.get(Pool.java:279)\n&gt;         at org.mortbay.util.ThreadPool.run(ThreadPool.java:345)\n&gt;         at\n&gt; org.mortbay.util.ThreadedServer$Acceptor.run(ThreadedServer.java:559)\n&gt; \n&gt; 14:46:20.993 WARN!! Stopping Acceptor\n&gt; ServerSocket[addr=0.0.0.0/0.0.0.0,port=0,localport=7090]\n&gt; \n&gt; Can anyone help me with this?\n&gt; Thanks.\n&gt; AB\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt; Adam Brokes wrote:\n&gt;&gt;&gt; Hallo,\n&gt;&gt;&gt; I know the OutOfMemory problem was there mentioned many times, but\n&gt; i could not find any topic that can solve my problem.\n&gt;&gt;&gt; I am working on crawling whole czech domain. I have got list of\n&gt; 220 000 seeds (2.level domains) and I do it by SurtPrefixScope.\n&gt;&gt;&gt; Machine specs:\n&gt;&gt;&gt; 900 MHz PIII\n&gt;&gt;&gt; 4GB RAM\n&gt;&gt;&gt; 4GB swap\n&gt;&gt;&gt; Debian Linux\n&gt;&gt;&gt; 32b JVM\n&gt;&gt;&gt; I started java with JAVA_OPTS=&quot; -Xms1500m -Xmx1500m -Xss512k&quot;.\n&gt;&gt;&gt; But after few hours i have got alerts with OOM exceptions and\n&gt; crawl stoped. I set 5 toethreads and I hope the crawl will go on\n&gt; without errors.\n&gt;&gt;&gt; If it will be necessary I will copy my order file.\n&gt;&gt;&gt; So question is: what is the most important settings in so big crawl?\n&gt;&gt;&gt; Thanks a lot. Adam B.\n&gt;&gt; Some suggestions regarding the OOME:\n&gt;&gt;\n&gt;&gt;   - If you are using the ExtractorSWF, use the latest release (1.8.0, \n&gt;&gt; officially out just today) or disable the extractor. The 3rd-party \n&gt;&gt; library used can inflate small bitmaps (&lt;100K) in the source SWF to \n&gt;&gt; 100MB or more of heap space momentarily, which has been implicated in \n&gt;&gt; some OOMEs we&#39;ve seen. (By the time the OOME is caught and the heap \n&gt;&gt; examined, the temporary giant inflated maps are gone, which made this \n&gt;&gt; pretty hard to track down.)\n&gt;&gt;\n&gt;&gt;   - See this blog entry at Sleepycat:\n&gt;&gt;     http://blog.sleepycat.com/2006/04/adler32-vs-gc.html\n&gt;&gt;     In Heritrix 1.8, our launcher script sets this workaround flag \n&gt;&gt; always just to be safe. According to Sun, the underlying bug has been \n&gt;&gt; fixed in Java 1.4.2_11 and the latest 1.6 beta (&quot;mustang&quot;) releases... \n&gt;&gt; but not yet any 1.5.0 release. Unfortunately, I believe there is a risk \n&gt;&gt; that our heavy use of Java&#39;s gzip/deflate functionality would be\n&gt; another \n&gt;&gt; way to trigger this JVM bug, and there is no easy workaround. So if \n&gt;&gt; mysterious OOMEs continue and you are using a 1.5.0 release, consider \n&gt;&gt; rolling back to 1.4.2_11. (We use 1.5.0_06, though.)\n&gt;&gt;\n&gt;&gt;   - There are several kinds of OOMEs -- some have to do with exhaustion \n&gt;&gt; of native resources (sometimes not even memory) other than the Java \n&gt;&gt; heap. Always make note of the exact message. (Though Java 1.6 beta is \n&gt;&gt; not yet officially supported for Heritrix, if you have a reliably \n&gt;&gt; reproduceable OOME its improved OOME stacks and \n&gt;&gt; -XX:+HeapDumpOnOutOfMemoryError are valuable for a Java head to get to \n&gt;&gt; the bottom of the problem).\n&gt;&gt;\n&gt;&gt; - Is there a reason you&#39;ve set a non-default -Xss stack size? (We&#39;ve \n&gt;&gt; only seen StackOverflowErrors very rarely, on pathologically odd \n&gt;&gt; content, and there&#39;s handling code around the places where they might \n&gt;&gt; occur that should recover cleanly.) A large number here increases the \n&gt;&gt; overhead per thread, and this comes out of non-heap memory.. so can \n&gt;&gt; sometimes lead to non-heap OOMEs.\n&gt;&gt;\n&gt;&gt; Other comments:\n&gt;&gt;\n&gt;&gt; - That should be enough memory to run 200 threads or more, though that \n&gt;&gt; might be an excessive number for that processor. (The point of \n&gt;&gt; diminishing or negative returns to a larger number of threads might \n&gt;&gt; begin much lower than 200 on your system: it&#39;s still just trial and \n&gt;&gt; error to discover the best tally for good throughput.)\n&gt;&gt;\n&gt;&gt; - If you are certain you will end your crawl at a specific size, you \n&gt;&gt; might prefer to use a Bloom-filter based &#39;already-seen&#39; structure \n&gt;&gt; (UriUniqFilter) rather than the default BDB approach. See this prior \n&gt;&gt; message for details:\n&gt;&gt;    http://groups.yahoo.com/group/archive-crawler/message/2450\n&gt;&gt; (If you do, you should also decrease the BDB cache percentage, which is \n&gt;&gt; by default 60% of heap, to 30% or lower because it&#39;s no longer doing\n&gt; one \n&gt;&gt; of its biggest jobs and the Bloom filter needs the space instead.)\n&gt;&gt;\n&gt;&gt; Hope this helps,\n&gt;&gt;\n&gt;&gt; - Gordon\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n\n\n"}}