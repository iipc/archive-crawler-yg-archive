{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","replyTo":"LIST","senderId":"iToARIsyjvXlzAg3QAPkuzjbK2LXqnvk8PcX9ex_H93RIns7J-BUvx_bFq-dDxBF93Cs3C_7JBohwAwR5-vu96x_e3AdPca2","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix - use...","postDate":"1085700935","msgId":465,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwQjY3QjQ3LjUwNzAyMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDAwMDAwMWM0NDQ0MiQ3M2RlMzgxMCQwMzAxYThjMEBNZXNhLmNvbT4=","referencesHeader":"PDAwMDAwMWM0NDQ0MiQ3M2RlMzgxMCQwMzAxYThjMEBNZXNhLmNvbT4="},"prevInTopic":464,"nextInTopic":466,"prevInTime":464,"nextInTime":466,"topicId":455,"numMessagesInTopic":37,"msgSnippet":"... Use heritrix to crawl the sites to get the raw pages.  The raw pages are saved to files known as ARC files. One file has hundreds of pages.  The cutoff","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 89237 invoked from network); 27 May 2004 23:42:04 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m22.grp.scd.yahoo.com with QMQP; 27 May 2004 23:42:04 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 27 May 2004 23:42:04 -0000\r\nReceived: (qmail 26654 invoked by uid 100); 27 May 2004 23:35:00 -0000\r\nReceived: from b116-dyn-36.archive.org (HELO archive.org) (stack@...@209.237.240.36)\n  by mail-dev.archive.org with SMTP; 27 May 2004 23:35:00 -0000\r\nMessage-ID: &lt;40B67B47.5070206@...&gt;\r\nDate: Thu, 27 May 2004 16:35:35 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.6) Gecko/20040526 Debian/1.6-6\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;000001c44442$73de3810$0301a8c0@...&gt;\r\nIn-Reply-To: &lt;000001c44442$73de3810$0301a8c0@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.4 required=6.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix - use...\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\nbruce wrote:\n\n&gt;hi...\n&gt;\n&gt;we&#39;re looking at heritrix and it&#39;s possible uses as a crawling/spidering\n&gt;application. we&#39;re considering an application that would have to parse\n&gt;various university/college sites for registrar information.\n&gt;\n&gt;as such, we basically would know prior to searching what the various\n&gt;tags/text for a given element would be that we want to extract the\n&gt;information for. we&#39;re curious as to whether heritrix might be able to be\n&gt;easily modified for this kind of purpose. we haven&#39;t yet had an opportunity\n&gt;to get into the docs/code to really look at what it does.\n&gt;  \n&gt;\nUse heritrix to crawl the sites to get the raw pages.  The raw pages are \nsaved to files known as ARC files. One file has hundreds of pages.  The \ncutoff point is hit when the file is (configurable) 100M in size.  These \nARC files would then serve as input to a parser of your writing that \nwould extract the pages/info you&#39;re trying to harvest.  To parse the ARC \nfiles, you need a reader.  There are a few options outlined here: \nhttp://crawler.archive.org/articles/developer_manual.html#arcreader.\n\nAsk more questions.\n\nYours,\nSt.Ack\n\n\n&gt;to be frank, we&#39;re looking for a solution that would allow us to not have to\n&gt;write unique perl scripts/parsers for each site!\n&gt;\n&gt;if there is someone who&#39;s familiar with this app that we could talk to, we\n&gt;would greatly appreciate a short conversation to get a better understanding\n&gt;of what the app does, and how it works.\n&gt;\n&gt;regards,\n&gt;\n&gt;bruce douglas\n&gt;bedouglas@...\n&gt;(925) 866-2790\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;\n&gt;  \n&gt;\n\n\n"}}