{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"david_pane1","from":"&quot;david_pane1&quot; &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"XsfQJ5Wh4ZLWTnFEbue--m-_EX1xqD6eiUUn4ceTfFD8cE7ob4TpvVQ1P0XsjYWLeFwwl6PaWKcCm7D0IgXL9F_rGZNljOU","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: questions before we restart the crawl","postDate":"1327954577","msgId":7588,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpnNnRxaCt0Z2tsQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRGMjZGNzFGLjgwM0BnbWFpbC5jb20+"},"prevInTopic":7587,"nextInTopic":7589,"prevInTime":7587,"nextInTime":7589,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"Kenji, Can I just change this value to something greater than 6 and the change will take effect or do I need to stop the crawler? --David","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 62090 invoked from network); 30 Jan 2012 20:16:19 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m5.grp.sp2.yahoo.com with QMQP; 30 Jan 2012 20:16:19 -0000\r\nX-Received: from unknown (HELO ng7-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.48)\n  by mta5.grp.sp2.yahoo.com with SMTP; 30 Jan 2012 20:16:19 -0000\r\nX-Received: from [98.139.164.124] by ng7.bullet.mail.bf1.yahoo.com with NNFMP; 30 Jan 2012 20:16:18 -0000\r\nX-Received: from [69.147.65.147] by tg5.bullet.mail.bf1.yahoo.com with NNFMP; 30 Jan 2012 20:16:18 -0000\r\nX-Received: from [98.137.34.36] by t10.bullet.mail.sp1.yahoo.com with NNFMP; 30 Jan 2012 20:16:18 -0000\r\nDate: Mon, 30 Jan 2012 20:16:17 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jg6tqh+tgkl@...&gt;\r\nIn-Reply-To: &lt;4F26F71F.803@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;david_pane1&quot; &lt;dpane@...&gt;\r\nSubject: Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=500983475; y=_T0HqqZ-WZLUl8FjHo63Hux_h1CgHavEshvkIATy5TT23lWDo0qxdw\r\nX-Yahoo-Profile: david_pane1\r\n\r\nKenji,\n\nCan I just change this value to something greater than 6 and the ch=\r\nange will take effect or do I need to stop the crawler?\n\n--David\n\n--- In ar=\r\nchive-crawler@yahoogroups.com, Kenji Nagahashi &lt;knagahashi@...&gt; wrote:\n&gt;\n&gt; =\r\nDavid,\n&gt; \n&gt; Ah, then you must have poolMaxActive=3D1. Try increasing that n=\r\number to \n&gt; allow Heritirx to write into multiple warc files in parallel. I=\r\nt should \n&gt; improve your throughput even if you&#39;re writing into single disk=\r\n. I&#39;m \n&gt; using 6. Only downside of poolMaxActive&gt;1 is crawled resources are=\r\n \n&gt; scattered across multiple WARC files.\n&gt; \n&gt; --Kenji\n&gt; \n&gt; (1/30/12 7:27 A=\r\nM), david_pane1 wrote:\n&gt; &gt; Kenji,\n&gt; &gt;\n&gt; &gt; I didn&#39;t change any of the values=\r\n in WARCWriterProcessor. It is the default.\n&gt; &gt;\n&gt; &gt; &lt;bean id=3D&quot;warcWriter&quot;=\r\n\n&gt; &gt; class=3D&quot;org.archive.modules.writer.WARCWriterProcessor&quot;&gt;\n&gt; &gt; &lt;!-- &lt;pr=\r\noperty name=3D&quot;compress&quot; value=3D&quot;true&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;property name=3D&quot;p=\r\nrefix&quot; value=3D&quot;IAH&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;property name=3D&quot;suffix&quot; value=3D&quot;${H=\r\nOSTNAME}&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;property name=3D&quot;maxFileSizeBytes&quot; value=3D&quot;1000=\r\n000000&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;property name=3D&quot;poolMaxActive&quot; value=3D&quot;1&quot; /&gt; --&gt;=\r\n\n&gt; &gt; &lt;!-- &lt;property name=3D&quot;MaxWaitForIdleMs&quot; value=3D&quot;500&quot; /&gt; --&gt;\n&gt; &gt; &lt;!--=\r\n &lt;property name=3D&quot;skipIdenticalDigests&quot; value=3D&quot;false&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;p=\r\nroperty name=3D&quot;maxTotalBytesToWrite&quot; value=3D&quot;0&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;property=\r\n name=3D&quot;directory&quot; value=3D&quot;${launchId}&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;property name=3D=\r\n&quot;storePaths&quot;&gt;\n&gt; &gt; &lt;list&gt;\n&gt; &gt; &lt;value&gt;warcs&lt;/value&gt;\n&gt; &gt; &lt;/list&gt;\n&gt; &gt; &lt;/propert=\r\ny&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;property name=3D&quot;writeRequests&quot; value=3D&quot;true&quot; /&gt; --&gt;\n&gt; &gt; =\r\n&lt;!-- &lt;property name=3D&quot;writeMetadata&quot; value=3D&quot;true&quot; /&gt; --&gt;\n&gt; &gt; &lt;!-- &lt;prope=\r\nrty name=3D&quot;writeRevisitForIdenticalDigests&quot; value=3D&quot;true&quot; /&gt; --&gt;\n&gt; &gt; &lt;!--=\r\n &lt;property name=3D&quot;writeRevisitForNotModified&quot; value=3D&quot;true&quot; /&gt; --&gt;\n&gt; &gt; &lt;/=\r\nbean&gt;\n&gt; &gt;\n&gt; &gt; --David\n&gt; &gt;\n&gt; &gt; --- In archive-crawler@yahoogroups.com\n&gt; &gt; &lt;m=\r\nailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt; &gt; &lt;knagahashi@&gt;=\r\n wrote:\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; David,\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; It is fairly common H3 goes 2x~3=\r\nx faster at the beginning, where URI/s\n&gt; &gt;  &gt; figure includes lots of DNS q=\r\nueries, many queues are ready, state\n&gt; &gt;  &gt; database is smaller, etc. I oft=\r\nen see my crawlers running at &gt;200URI/s,\n&gt; &gt;  &gt; too. (Also URI/s shown on t=\r\nhe web UI is not really reliable.)\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; It is difficult to tell ho=\r\nw many queues are enough to keep all threads\n&gt; &gt;  &gt; busy, as there are many=\r\n factors affecting crawl speed. Assuming 0.5 sec\n&gt; &gt;  &gt; processing time per=\r\n URI and 100% concurrency, 1200 thread could do 2400\n&gt; &gt;  &gt; URI/s. On the o=\r\nther hand, assuming constant crawl delay of 3sec, 4000\n&gt; &gt;  &gt; active queues=\r\n can only emit 1333 URI/s. In this case, active queue size\n&gt; &gt;  &gt; becomes t=\r\nhe limiting factor. If processing time per URI becomes 1 sec,\n&gt; &gt;  &gt; then p=\r\nrocessing time becomes a bottleneck. Some queues get snoozed much\n&gt; &gt;  &gt; lo=\r\nnger than 3 sec and it makes URI emit rate lower. I tried drawing a\n&gt; &gt;  &gt; =\r\nactive-queue to crawl-speed graph, but what I can say is that there is a\n&gt; =\r\n&gt;  &gt; strong correlation between them (roughly, 5000 queues -&gt; 40URI/s, 8000=\r\n\n&gt; &gt;  &gt; queue -&gt; 70URI/s ; variance is really big).\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Looking a=\r\nt your thread and frontier report, I noticed:\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; - 994 queues ar=\r\ne &quot;ready&quot;\n&gt; &gt;  &gt; - 911 threads are in warcWriter\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; that is, you=\r\n have enough active queues to keep threads busy, but threads\n&gt; &gt;  &gt; are spe=\r\nnding too much time on writing WARCs to process URIs on time.\n&gt; &gt;  &gt; WarcWr=\r\niter is the bottleneck in this case, probably because of small\n&gt; &gt;  &gt; numbe=\r\nr of concurrent writers? What is your warcWriter.poolMaxActive?\n&gt; &gt;  &gt;\n&gt; &gt; =\r\n &gt; --Kenji\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; (1/27/12 7:32 AM), david_pane1 wrote:\n&gt; &gt;  &gt; &gt;\n&gt; &gt;=\r\n  &gt; &gt;\n&gt; &gt;  &gt; &gt; Kenji,\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; You understood correctly 25M pages/=\r\nday for 5 machines. Right now, with\n&gt; &gt;  &gt; &gt; three instances running, we ar=\r\ne seeing around 17M pages per day. This\n&gt; &gt;  &gt; &gt; makes me think that we may=\r\n be saturating our network throughput.\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; But, we have seen,=\r\n at the beginning of the crawl, 250-300 URIs/second\n&gt; &gt;  &gt; &gt; (an average of=\r\n 52M pages/day over the first 3 days of the crawl.\n&gt; &gt; Once we\n&gt; &gt;  &gt; &gt; get=\r\n past the first few days, the crawl slows. During mid crawl, we\n&gt; &gt; found\n&gt;=\r\n &gt;  &gt; &gt; that stopping the crawl and restarting it improves the throughput b=\r\nut\n&gt; &gt;  &gt; &gt; never back to 50M pages/day.\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; How many queues =\r\ndo you need active per thread? Wouldn&#39;t Heritrix\n&gt; &gt;  &gt; &gt; activate more que=\r\nues if there are enough threads to handle them? We\n&gt; &gt;  &gt; &gt; certainly have =\r\na lot of queues.\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; As an example, on one instance we are se=\r\neing a rate of 93.25 URIs/sec\n&gt; &gt;  &gt; &gt; average.\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; Load:\n&gt; &gt;=\r\n  &gt; &gt; 1176 active of 1176 threads; 3,456.5 congestion ration 13688365 deepe=\r\nst\n&gt; &gt;  &gt; &gt; queue; 62 average depth\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; Threads:\n&gt; &gt;  &gt; &gt; 117=\r\n6 threads: 1176 ABOUT_TO_BEGIN_PROCESSOR; 911 warcWriter, 262\n&gt; &gt;  &gt; &gt; fetc=\r\nhHttp, 2 candidates, 1 extractorHtml\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; Frontier:\n&gt; &gt;  &gt; &gt;\n&gt;=\r\n &gt;  &gt; &gt; RUN 12955728 URI queues: 4302 active (1200 in-process; 994 ready; 2=\r\n108\n&gt; &gt;  &gt; &gt; snoozed); 11429799 inactive; 0 ineligible; 0 retired; 1521627\n=\r\n&gt; &gt; exhausted.\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; --David\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; --- In archive-c=\r\nrawler@yahoogroups.com\n&gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt; &gt;  =\r\n&gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt; &gt;  &gt; &gt; &lt;k=\r\nnagahashi@&gt; wrote:\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; David,\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; I und=\r\nerstood &quot;25M + page/day&quot; was total for 5 machines, as you wrote\n&gt; &gt;  &gt; &gt; &gt; =\r\n&quot;(these numbers are totals of all 5 instances combined)&quot;. Did you\n&gt; &gt; mean\n=\r\n&gt; &gt;  &gt; &gt; &gt; 25M+ page/day/instance? If so, my &quot;100 threads&quot; comment is point=\r\nless.\n&gt; &gt;  &gt; &gt; &gt; Please disregard it.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; I&#39;m running bro=\r\nad crawl that captures everything linked: images,\n&gt; &gt; script,\n&gt; &gt;  &gt; &gt; &gt; CS=\r\nS, PDF, Excel, ... even mpeg4 videos. Our Heritrix 3 runs on 8GB\n&gt; &gt;  &gt; &gt; &gt;=\r\n memory + 4 core virtual machine (KVM), with 100 threads. it goes\n&gt; &gt;  &gt; &gt; =\r\n&gt; ~60URI/s on average (per instance). Probably we could go as high\n&gt; &gt; as 1=\r\n50\n&gt; &gt;  &gt; &gt; &gt; threads to get higher crawl speed, but it comes with higher r=\r\nisk of\n&gt; &gt;  &gt; &gt; &gt; dying of OutOfMemoryError, empirically. Crawl speed is al=\r\nso\n&gt; &gt; limited by\n&gt; &gt;  &gt; &gt; &gt; lower disk I/O performance of VMs. 100 seems t=\r\no be a good number\n&gt; &gt; for us.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; Yes, increasing thread=\r\ns brings significant increase in crawl\n&gt; &gt; speed, to\n&gt; &gt;  &gt; &gt; &gt; certain ext=\r\nent. If you don&#39;t have enough &quot;active queues,&quot; threads are\n&gt; &gt;  &gt; &gt; &gt; just =\r\nwasted. There are other bottleneck, too, and it can change over\n&gt; &gt;  &gt; &gt; ti=\r\nme.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; At least we&#39;re getting sustained 60URI/s level of=\r\n speed with 100\n&gt; &gt;  &gt; &gt; &gt; threads. With 1,200 threads and enough active qu=\r\neues, you should be\n&gt; &gt;  &gt; &gt; &gt; getting crawl speed much much higher than th=\r\nat (I&#39;ve never been\n&gt; &gt; able to\n&gt; &gt;  &gt; &gt; &gt; run my crawler with 1200 threads=\r\n, though!)\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; --Kenji\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; (1/26/12 10:=\r\n31 AM), david_pane1 wrote:\n&gt; &gt;  &gt; &gt; &gt; &gt; Kenji,\n&gt; &gt;  &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; Ar=\r\ne you saying that you can get 25M pages per day on 100\n&gt; &gt; threads and 1\n&gt; =\r\n&gt;  &gt; &gt; &gt; &gt; instance or 25M URIs/day? Do you capture all images, pdfs, and\n&gt;=\r\n &gt;  &gt; &gt; &gt; &gt; supporting page documents or are you just capturing html pages?=\r\n\n&gt; &gt;  &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; My test crawls before running our large crawl sh=\r\nowed significant\n&gt; &gt;  &gt; &gt; &gt; &gt; increase in the number of pages captured when=\r\n we increased the\n&gt; &gt;  &gt; &gt; number of\n&gt; &gt;  &gt; &gt; &gt; &gt; threads.\n&gt; &gt;  &gt; &gt; &gt; &gt;\n&gt; &gt;=\r\n  &gt; &gt; &gt; &gt; --David\n&gt; &gt;  &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; --- In archive-crawler@yahoogro=\r\nups.com\n&gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt; &gt;  &gt; &gt; &lt;mailto:arc=\r\nhive-crawler%40yahoogroups.com&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &lt;mailto:archive-crawler%40yaho=\r\nogroups.com&gt;, Kenji Nagahashi\n&gt; &gt;  &gt; &gt; &gt; &gt; &lt;knagahashi@&gt; wrote:\n&gt; &gt;  &gt; &gt; &gt; =\r\n&gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; Hi,\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; May be a bit off-topic=\r\n, but 25M/day with 5 machine is average\n&gt; &gt;  &gt; &gt; 58/s per\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; ma=\r\nchine. Since I know Heritrix-3 can crawl at this speed with\n&gt; &gt;  &gt; &gt; just 1=\r\n00\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; ToeThreads, I wonder if most of your 1200 ToeThreads are =\r\nidle.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; While why you don&#39;t get much higher spe=\r\ned with 1200 threads\n&gt; &gt; is a big\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; question, it may make sens=\r\ne to cut down the number of\n&gt; &gt; ToeThreads if\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; you&#39;re okay wi=\r\nth current crawl speed. Less threads will make\n&gt; &gt; H3 less\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; s=\r\nusceptible to memory problems... Just a thought.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; =\r\n&gt; &gt; --Kenji\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; (1/20/12 9:54 PM), David Pane wro=\r\nte:\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; Gordon,\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; Thank you fo=\r\nr your response. And I am sorry for the\n&gt; &gt;  &gt; &gt; overwhelming amount\n&gt; &gt;  &gt;=\r\n &gt; &gt; &gt; &gt; &gt; of information...I think I am a little overwhelmed.... and\n&gt; &gt;  =\r\n&gt; &gt; feeling the\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; pressure.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; =\r\n&gt; 1) Our Bloom filter configuration:\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;bea=\r\nn id=3D&quot;uriUniqFilter&quot;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; class=3D&quot;org.archive.crawler.util.B=\r\nloomUriUniqFilter&quot;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;property name=3D&quot;bloomFilter&quot;&gt;\n&gt; &gt;  &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &lt;bean class=3D&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &lt;constructor-arg value=3D&quot;400000000&quot;/&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;constructor-ar=\r\ng value=3D&quot;30&quot;/&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;/property&gt;\n&gt; &gt; =\r\n &gt; &gt; &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; 2) We are writing th=\r\ne crawl data to a NAS configured with\n&gt; &gt; RAID 6.\n&gt; &gt;  &gt; &gt; &gt; &gt; We did\n&gt; &gt;  =\r\n&gt; &gt; &gt; &gt; &gt; &gt; see some problems with disk errors on the NAS earlier in\n&gt; &gt; th=\r\ne crawl\n&gt; &gt;  &gt; &gt; &gt; &gt; (late\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; Dec ). I recently found this ou=\r\nt. We were/are running in a\n&gt; &gt; degraded\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; raid state - a fe=\r\nw of the disks have been replaced and the\n&gt; &gt; RAID is\n&gt; &gt;  &gt; &gt; &gt; &gt; being\n&gt; =\r\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; rebuilt. We didn&#39;t see any block device errors in the logs o=\r\nn\n&gt; &gt;  &gt; &gt; the NAS\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; so the write failures we saw are probab=\r\nly not related to the\n&gt; &gt;  &gt; &gt; &gt; &gt; rebuild. We\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; did see som=\r\ne network hiccups (no outright failures) in the\n&gt; &gt; logs.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; =\r\nSo, this may be the culprit for some of the\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt;=\r\n &gt; 3) Yes, we have been cross-feeding URIs.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt;=\r\n &gt; --David\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; On 1/21/12 12:22 AM, Gordon Mo=\r\nhr wrote:\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; You&#39;ve provided an overwhelming amount of info=\r\nrmation and we\n&gt; &gt;  &gt; &gt; may be\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; dealing with multiple iss=\r\nues, some of which have roots\n&gt; &gt; going back\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; earlier tha=\r\nn the diagnostic data we now have available.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; A few key points of emphasis:\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n - we&#39;ve not run crawls with 1200 threads before, or on\n&gt; &gt; hardware\n&gt; &gt;  &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt; similar to yours, so our experience is only vaguely\n&gt; &gt; sugges=\r\ntive\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; - it&#39;s not the lower thread coun=\r\nts that are the real\n&gt; &gt; source of\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; concern; you can even=\r\n adjust the number of threads mid-crawl.\n&gt; &gt;  &gt; &gt; It&#39;s\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; t=\r\nhat the error that killed the threads almost certainly left\n&gt; &gt;  &gt; &gt; a queu=\r\ne\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; in a &#39;phantom&#39; state where no progress would be made\n&gt;=\r\n &gt; crawling its\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; URIs, each time it happened, on each res=\r\nume leading to the\n&gt; &gt;  &gt; &gt; &gt; &gt; current state.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; - without having understood and fixed whatever software\n&gt; &gt; or sy=\r\nstem\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; problems caused the earliest/most-foundational erro=\r\nrs in your\n&gt; &gt;  &gt; &gt; crawl,\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; it&#39;s impossible to say how li=\r\nkely they are to recur.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; With that in =\r\nmind, I&#39;ll try to provide quick answers to your\n&gt; &gt;  &gt; &gt; other\n&gt; &gt;  &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; questions...\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; On 1/20/12 4:20 P=\r\nM, David Pane wrote:\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; We have collec=\r\nted about 550 million pages along with the\n&gt; &gt;  &gt; &gt; images and\n&gt; &gt;  &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt;&gt; supporting documents on our 5 instance crawl that was\n&gt; &gt; started\n&gt;=\r\n &gt;  &gt; &gt; &gt; &gt; Dec. 23rd.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; Although we are please with the =\r\namount of data we\n&gt; &gt; captured to\n&gt; &gt;  &gt; &gt; &gt; &gt; date, we\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;=\r\n are very concerned about the state of the Heritrix\n&gt; &gt; instances. If\n&gt; &gt;  =\r\n&gt; &gt; &gt; &gt; fact,\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; we aren&#39;t very confident that the instanc=\r\nes will last\n&gt; &gt; until the\n&gt; &gt;  &gt; &gt; &gt; &gt; end of\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; February=\r\n. We are now running on a total of over 500 less\n&gt; &gt; threads\n&gt; &gt;  &gt; &gt; &gt; &gt; t=\r\nhan\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; the configured 1200 threads/instance.\n&gt; &gt;  &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 0 - not running right now.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 1=\r\n - running on 1198 ( 2 less)\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 2 - running on 931 (269 le=\r\nss)\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 3 - running on 987 (213 less)\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 4=\r\n - running on 1170 (30 less)\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; Since =\r\nwe are seriously considering throwing away this past\n&gt; &gt;  &gt; &gt; &gt; &gt; month&#39;s w=\r\nork\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; and starting over, we would like to pick your brain=\r\n on some\n&gt; &gt;  &gt; &gt; &gt; &gt; strategies\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; that will help us avoi=\r\nd getting into this situation again.\n&gt; &gt;  &gt; &gt; We were\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; h=\r\noping to be done crawling by the end of February so this\n&gt; &gt;  &gt; &gt; &gt; &gt; resta=\r\nrt will\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; put us behind schedule.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;=\r\n  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 1) Can we continue from here but with &quot;clean&quot; Heritrix\n&gt; &gt;=\r\n  &gt; &gt; instances?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; Is there a way tha=\r\nt we can continue from the this point\n&gt; &gt;  &gt; &gt; forward, but\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt;&gt; start with Heritrix instances that will not be corrupt due\n&gt; &gt;  &gt; &gt; to=\r\n sever\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; error? (e.g. using the\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  =\r\n&gt; &gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &gt; &lt;h=\r\nttps://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt; &gt;  &gt; &gt; &lt;=\r\nhttps://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &gt; &lt;https=\r\n://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt;\n=\r\n&gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &gt; &lt;=\r\nhttps://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt; &gt;  &gt; &gt; =\r\n&lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &gt; &lt;http=\r\ns://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; =\r\n&gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Rec=\r\novery\n&gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery=\r\n&gt;\n&gt; &gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recover=\r\ny\n&gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;\n&gt;=\r\n &gt;  &gt; &gt; &gt; &gt;\n&gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Re=\r\ncovery\n&gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recover=\r\ny&gt;\n&gt; &gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recove=\r\nry\n&gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;&gt;=\r\n&gt; ) If\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; so, would you recommend doing this? You mentione=\r\nd that this\n&gt; &gt;  &gt; &gt; could be\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; time consuming. Each of o=\r\nur instances has downloaded\n&gt; &gt; around 170M\n&gt; &gt;  &gt; &gt; &gt; &gt; URIs,\n&gt; &gt;  &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt;&gt; they have over 700M queued URIs, what is your time\n&gt; &gt; estimate for=\r\n\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; something this large?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt;&gt; We are willing to sacrifice a few days to get our crawler to\n&gt; &gt;  &gt;=\r\n &gt; a clean\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; state again so we can crawl for another 30 d=\r\nays at the\n&gt; &gt; pace we\n&gt; &gt;  &gt; &gt; &gt; &gt; have been\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; crawling.=\r\n\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; You can do a big &#39;frontier-recover&#39; =\r\nlog replay to avoid\n&gt; &gt;  &gt; &gt; &gt; &gt; recrawling the\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; same URI=\r\ns, and approximate the earlier queue state.\n&gt; &gt;  &gt; &gt; Splitting/filters\n&gt; &gt; =\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; the logs manually beforehand as alluded to in the wiki page\n=\r\n&gt; &gt;  &gt; &gt; can speed\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; this process somewhat... but given th=\r\ne size of all your\n&gt; &gt;  &gt; &gt; log-segments\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; that log groomi=\r\nng beforehand is itself likely to be a lengthy\n&gt; &gt;  &gt; &gt; &gt; &gt; process.\n&gt; &gt;  &gt;=\r\n &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; I don&#39;t think we&#39;ve ever done it with logs =\r\nof 170M\n&gt; &gt; crawled / 870M\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; discovered before, nor on any=\r\n hardware comparable to yours.\n&gt; &gt;  &gt; &gt; So it&#39;s\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; impossib=\r\nle to project its duration in your environment. It&#39;s\n&gt; &gt;  &gt; &gt; &gt; &gt; taken 2-3=\r\n\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; days for us on smaller crawls, slower hardware.\n&gt; &gt;  &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; An added complication is that this older fro=\r\nntier-recover-log\n&gt; &gt;  &gt; &gt; replay\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; technique happens in i=\r\nts own thread separate from the\n&gt; &gt;  &gt; &gt; checkpointing\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; p=\r\nrocess, so it is not, itself, accurately checkpointed\n&gt; &gt; during the\n&gt; &gt;  &gt;=\r\n &gt; &gt; &gt; long\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; reload process.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; At nearly 1B discovered URIs per node, even if you are\n&gt; &gt; using =\r\nthe\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; alternate BloomUriUniqFilter, if you are using it at=\r\n its\n&gt; &gt;  &gt; &gt; default size\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; (~500MB) it will now be heavi=\r\nly saturated and thus\n&gt; &gt; returning many\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; false-positives=\r\n causing truly unique URIs to be rejected as\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; duplicates.=\r\n (If you&#39;re using a significantly larger filter,\n&gt; &gt;  &gt; &gt; you may\n&gt; &gt;  &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt; not yet be at a high false-positive rate: you&#39;d have to do\n&gt; &gt;  &gt;=\r\n &gt; the bloom\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; filter math. If you&#39;re still using BdbUriUn=\r\niqFilter, you&#39;re\n&gt; &gt;  &gt; &gt; way way\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; past the point where i=\r\nts disk seeks have usually made it too\n&gt; &gt;  &gt; &gt; slow for\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n our purposes.)\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 2) What can be done =\r\nto avoid corrupting the Heritrix\n&gt; &gt; instances?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt;&gt; - What kind of strategies might we take to keep the\n&gt; &gt; crawl =\r\nerror\n&gt; &gt;  &gt; &gt; &gt; &gt; free?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Do you t=\r\nhink the SEVER errors that we have seen are\n&gt; &gt;  &gt; &gt; &gt; &gt; deterministic or\n&gt;=\r\n &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; random (e.g., triggered by occasional flaky network\n&gt; &gt; =\r\nconditions,\n&gt; &gt;  &gt; &gt; &gt; &gt; disks,\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; race conditions, or wha=\r\ntever)?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Hard to say. The main thing I=\r\n could suggest is watch very\n&gt; &gt;  &gt; &gt; closely and\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; when a=\r\n SEVERE error occurs, prioritize diagnosing and\n&gt; &gt;  &gt; &gt; resolving the\n&gt; &gt; =\r\n &gt; &gt; &gt; &gt; &gt; &gt; &gt; cause while the info is fresh.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt;&gt; - Do you believe that we can reliably backup to the previous\n&gt; &gt; =\r\n &gt; &gt; &gt; &gt; checkpoint\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; if we watch the logs and stop as so=\r\non as we see the\n&gt; &gt; first SEVER\n&gt; &gt;  &gt; &gt; &gt; &gt; error?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; If=\r\n we do this, do you speculate that the same SEVER will\n&gt; &gt; occur\n&gt; &gt;  &gt; &gt; &gt;=\r\n &gt; again?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Resuming from the latest ch=\r\neckpoint before an error\n&gt; &gt; believed to\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; corrupt the on-=\r\ndisk state will be the best strategy.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;=\r\n If we never figure out the real cause, but run the same\n&gt; &gt;  &gt; &gt; software =\r\non\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; the same machine, yes, I expect the same problem will=\r\n recur!\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Is there any reason why a =\r\nHeritrix instance that is\n&gt; &gt; run while\n&gt; &gt;  &gt; &gt; &gt; &gt; binded\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; =\r\n&gt; &gt;&gt; to one ip address can&#39;t be resumed binded to a different ip\n&gt; &gt;  &gt; &gt; a=\r\nddress?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Only the web UI to my knowled=\r\nge binds to a chosen address,\n&gt; &gt;  &gt; &gt; and it is\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; common =\r\nto have it bind to all. I don&#39;t expect the outbound\n&gt; &gt;  &gt; &gt; requests\n&gt; &gt;  =\r\n&gt; &gt; &gt; &gt; &gt; &gt; &gt; would be hurt by a machine changing its IP address while the\n=\r\n&gt; &gt;  &gt; &gt; &gt; &gt; crawl was\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; running, but I would run a test t=\r\no be sure if that was an\n&gt; &gt;  &gt; &gt; important,\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; expected tr=\r\nansition.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 3) Should we configure the=\r\n crawler with more instances and\n&gt; &gt;  &gt; &gt; switch\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; betwee=\r\nn them?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; We have seen that we can ru=\r\nn a single instance to 100M\n&gt; &gt; pages +\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; supporting imag=\r\nes and documents. Perhaps this means that\n&gt; &gt; we need\n&gt; &gt;  &gt; &gt; &gt; &gt; 10 or\n&gt; =\r\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; more instances instead of 5. That raises the possibility =\r\nof\n&gt; &gt;  &gt; &gt; &gt; &gt; running 2\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; instances per machine. If we =\r\ncould run 2, or even 4,\n&gt; &gt;  &gt; &gt; instances on a\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; single =\r\nmachine, they would each run half as long.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; I don&#39;t think the problems as reported are specifically due\n&gt; &gt;  &gt; &gt; =\r\nto one\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; node&#39;s progress growing beyond a certain size, bu=\r\nt it might\n&gt; &gt;  &gt; &gt; be the\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; case that giant instances are=\r\n more likely to suffer from, and\n&gt; &gt;  &gt; &gt; harder\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; to reco=\r\nver from, single glitches (eg a single disk\n&gt; &gt; error). On the\n&gt; &gt;  &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt; other hand, many instances introduce more redundant overhead\n&gt; &gt;  &gt; =\r\n&gt; costs\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; (certain data structures, cross-feeding URIs if =\r\nyou&#39;re doing\n&gt; &gt;  &gt; &gt; &gt; &gt; that, etc.).\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt;&gt; - Can you suggest a way to start/stop instances from a\n&gt; &gt; script so\n&gt; &gt;=\r\n  &gt; &gt; &gt; &gt; we can\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; change between instances automatically=\r\n?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Not a mode I&#39;ve thought much about.=\r\n\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Have you seen frequent starting /=\r\n stopping of instances\n&gt; &gt;  &gt; &gt; introduce\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; instability?\n=\r\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; No... but it might make you notice la=\r\ntent issues sooner.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 4) Crawl slows b=\r\nut restarting seems to improve the speed\n&gt; &gt; again.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;=\r\n  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; We noticed that the all of our instances would initially\n&gt;=\r\n &gt; run at\n&gt; &gt;  &gt; &gt; &gt; &gt; a fast\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; pace. We would collect an=\r\n average of 25M + pages/day for 2-3\n&gt; &gt;  &gt; &gt; &gt; &gt; days and\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt;&gt; then the crawl would slow down to 10M pages/day over the\n&gt; &gt; next\n&gt; &gt;  &gt;=\r\n &gt; &gt; &gt; few days.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; (these numbers are totals of all 5 ins=\r\ntances combined).\n&gt; &gt; When we\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; restarted the instances, =\r\nthe average pages would improve\n&gt; &gt; back to\n&gt; &gt;  &gt; &gt; &gt; &gt; 25M +\n&gt; &gt;  &gt; &gt; &gt; &gt;=\r\n &gt; &gt; &gt;&gt; pages/day. The total crawled numbers (TiB) also\n&gt; &gt; reflected the\n&gt;=\r\n &gt;  &gt; &gt; &gt; &gt; slow down.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Is this so=\r\nmething that others have experienced as well?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt; I don&#39;t recall hearing other reports of speed boosts after\n&gt; &gt;  &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt; checkpoint-resumes but others may have more experience.\n&gt; &gt;  &gt; =\r\n&gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 5) We are capturing tweets from twitter, ha=\r\nrvesting the\n&gt; &gt; urls and\n&gt; &gt;  &gt; &gt; &gt; &gt; want to\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; crawl th=\r\nose urls within 1 day of receiving the tweet.\n&gt; &gt; Can you\n&gt; &gt;  &gt; &gt; &gt; &gt; reco=\r\nmmend\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; a strategy for doing this with the 5 instances we=\r\n are\n&gt; &gt; running?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Do we need to r=\r\nun a separate crawler dedicated to\n&gt; &gt; this? If so,\n&gt; &gt;  &gt; &gt; &gt; &gt; can you\n&gt; =\r\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; suggest a way to crawl out from the tweeted urls but\n&gt; &gt; =\r\nwhen we get\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; additional urls from the tweets, quickly ch=\r\nange focus to\n&gt; &gt;  &gt; &gt; these urls\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; instead of the ones b=\r\nranching out. When adding urls as\n&gt; &gt; seeds,\n&gt; &gt;  &gt; &gt; &gt; &gt; can you\n&gt; &gt;  &gt; &gt; =\r\n&gt; &gt; &gt; &gt; &gt;&gt; set a high priority to crawl those before the discovered\n&gt; &gt; url=\r\ns?\n&gt; &gt;  &gt; &gt; &gt; &gt; Do you\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; recommend maybe setting up a spe=\r\ncific crawl for these\n&gt; &gt; urls and\n&gt; &gt;  &gt; &gt; &gt; &gt; then only\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt;&gt; crawl a few hopes from the seeds - injecting the urls\n&gt; &gt; from the\n&gt; &gt;  =\r\n&gt; &gt; &gt; &gt; tweets as\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; seeds?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; Dedicating a special script or crawler to URIs that come from\n&gt; &gt;  =\r\n&gt; &gt; such a\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; constrained source (Twitter feeds), or that n=\r\need to be\n&gt; &gt;  &gt; &gt; crawled in a\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; special timeframe, or ac=\r\ncording to other special limits\n&gt; &gt;  &gt; &gt; (fewer hops),\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; c=\r\nould make sense.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; It would take some c=\r\nustomization of the queueing-policy or\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; &#39;precedence&#39; feat=\r\nures of Heritrix to allow URIs added\n&gt; &gt;  &gt; &gt; mid-crawl to be\n&gt; &gt;  &gt; &gt; &gt; &gt; =\r\n&gt; &gt; &gt; prioritized above those already discovered and queued.\n&gt; &gt; The most\n&gt;=\r\n &gt;  &gt; &gt; &gt; &gt; simple\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; possible customization might be a Uri=\r\nPrecedencePolicy that\n&gt; &gt;  &gt; &gt; takes all\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; zero-hop URIs (=\r\nwhich all seeds and most direct-fed URIs would\n&gt; &gt;  &gt; &gt; be) and\n&gt; &gt;  &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt; gives them a higher precedence (lower precedence number)\n&gt; &gt; than a=\r\nll\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; other URIs.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 6)=\r\n I think the answer is no for this question, but I\n&gt; &gt; will ask it\n&gt; &gt;  &gt; &gt;=\r\n &gt; &gt; anyway.\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; If you have a Heritrix instance that is co=\r\nnfigured for 1200\n&gt; &gt;  &gt; &gt; &gt; &gt; threads on\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; one machine, =\r\ncan you recover from a checkpoint from that\n&gt; &gt;  &gt; &gt; 1200 thread\n&gt; &gt;  &gt; &gt; &gt;=\r\n &gt; &gt; &gt; &gt;&gt; configuration on a different machine with an Heritrix\n&gt; &gt; instanc=\r\ne\n&gt; &gt;  &gt; &gt; &gt; &gt; that is\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; configured for less threads (e.g=\r\n. the default 25 threads)?\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Yes - ther=\r\ne&#39;s no need to keep the thread count the same\n&gt; &gt; after a\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; =\r\n&gt; resume. None of the checkpoint structures (or usual disk\n&gt; &gt;  &gt; &gt; structu=\r\nres)\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; are based on the number of worker threads\n&gt; &gt; (&#39;Toe=\r\nThreads&#39;)... as\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; mentioned above you can even vary the nu=\r\nmber of threads in a\n&gt; &gt;  &gt; &gt; running\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; crawl.\n&gt; &gt;  &gt; &gt; &gt; =\r\n&gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; - Gordon\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;=\r\n  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt;\n=\r\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}