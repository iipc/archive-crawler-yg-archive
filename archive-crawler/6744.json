{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":465980601,"authorName":"Zach Bailey","from":"Zach Bailey &lt;zach.bailey@...&gt;","replyTo":"LIST","senderId":"4ipg1HYWZ-AonLWQ9NlWtJIAz2JvpaSWN3pFqw2SvkkiNAMHKDM-iypHq090HuOGHn_q4ruG1GntNZolxc-sDm1Bw4FugH1p91s94CI","spamInfo":{"isSpam":false,"reason":"2"},"subject":"WriterPool Members getting interrupted at conclusion of crawl?","postDate":"1285257496","msgId":6744,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOUI3OTE4LjQwMzAzMDFAZGF0YWNsaXAuY29tPg=="},"prevInTopic":0,"nextInTopic":6745,"prevInTime":6743,"nextInTime":6745,"topicId":6744,"numMessagesInTopic":2,"msgSnippet":"I wanted to run this by you guys and get your thoughts - I am running into a problem with the HDFS writer I m using to write crawl results from heritrix 3. If","rawEmail":"Return-Path: &lt;zach.bailey@...&gt;\r\nX-Sender: zach.bailey@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 98627 invoked from network); 23 Sep 2010 15:58:20 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m1.grp.sp2.yahoo.com with QMQP; 23 Sep 2010 15:58:20 -0000\r\nX-Received: from unknown (HELO mail-gx0-f170.google.com) (209.85.161.170)\n  by mta2.grp.sp2.yahoo.com with SMTP; 23 Sep 2010 15:58:20 -0000\r\nX-Received: by gxk25 with SMTP id 25so753524gxk.29\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Sep 2010 08:58:20 -0700 (PDT)\r\nX-Received: by 10.150.182.6 with SMTP id e6mr3111966ybf.157.1285257500011;\n        Thu, 23 Sep 2010 08:58:20 -0700 (PDT)\r\nReturn-Path: &lt;zach.bailey@...&gt;\r\nX-Received: from znbailey-2.local ([173.160.76.185])\n        by mx.google.com with ESMTPS id q21sm1120234ybk.11.2010.09.23.08.58.18\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Thu, 23 Sep 2010 08:58:19 -0700 (PDT)\r\nMessage-ID: &lt;4C9B7918.4030301@...&gt;\r\nDate: Thu, 23 Sep 2010 11:58:16 -0400\r\nUser-Agent: Postbox 2.0.0b4 (Macintosh/20100915)\r\nMIME-Version: 1.0\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------000909000700090305050007&quot;\r\nX-eGroups-Msg-Info: 2:2:2:0:2\r\nFrom: Zach Bailey &lt;zach.bailey@...&gt;\r\nSubject: WriterPool Members getting interrupted at conclusion of crawl?\r\nX-Yahoo-Group-Post: member; u=465980601\r\n\r\n\r\n--------------000909000700090305050007\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nI wanted to run this by you guys and get your thoughts - I am running \ninto a problem with the HDFS writer I&#39;m using to write crawl results \nfrom heritrix 3. If you&#39;re curious you can find the code here: \nhttp://github.com/openplaces/heritrix-hdfs-writer\n\nThe problem I&#39;m seeing is this - at the end of the crawl, it appears the \njob-related threads are being interrupted before they&#39;re completely \nfinished processing (though I have not verified this by tracing through \nthe code). Here is a warning I see pop up in the logs at the end of the \ncrawl:\n\n2010-09-23 15:05:13.125 INFO thread-18 \norg.archive.crawler.framework.CrawlJob.onApplicationEvent() STOPPING\n2010-09-23 15:05:13.126 INFO thread-18 \norg.archive.crawler.framework.CrawlJob.onApplicationEvent() FINISHED\n10/09/23 15:05:13 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor \nexception  for block \nblk_5887928276646679438_1127java.io.InterruptedIOException: Interruped \nwhile waiting for IO on channel \njava.nio.channels.SocketChannel[connected local=/127.0.0.1:57686 \nremote=/127.0.0.1:50010]. 46903 millis timeout left.\n     at \norg.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:349)\n     at \norg.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n     at \norg.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)\n     at \norg.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\n     at java.io.DataInputStream.readFully(DataInputStream.java:178)\n     at java.io.DataInputStream.readLong(DataInputStream.java:399)\n     at \norg.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2397)\n\nThis is a problem because it causes an abnormal termination of \ncommunication between the DFSClient and the HDFS server, which results \nin the crawl file not being closed correctly. Examining the state of the \nHDFS file after this happens we see it has a file size of 0:\n\nzach@znbailey-2:bin$ ./hadoop fs -ls /crawl\nFound 1 items\n-rw-r--r--   3 zach supergroup          0 2010-09-23 10:55 \n/crawl/CrawlData-20100923145529-00000\n\nand running an HDFS &quot;fsck&quot; shows us that HDFS thinks the file is still \n&quot;open&quot;:\n\nzach@znbailey-2:bin$ ./hadoop fsck /\nStatus: HEALTHY\n  Total size:    0 B\n  Total dirs:    3\n  Total files:    0 *(Files currently being written: 1)*\n  Total blocks (validated):    0 *(Total open file blocks (not \nvalidated): 1)*\n  Minimally replicated blocks:    0\n  Over-replicated blocks:    0\n  Under-replicated blocks:    0\n  Mis-replicated blocks:        0\n  Default replication factor:    1\n  Average block replication:    0.0\n  Corrupt blocks:        0\n  Missing replicas:        0\n  Number of data-nodes:        1\n  Number of racks:        1\n\nAs I said I have not verified this behavior 100% and I&#39;m only going off \nthe evidence in the stack traces, so I&#39;m not sure if my hypothesis about \nheritrix interrupting these threads is right or not. So, I was hoping \nyou could shed some light on that.\n\nIf it is indeed the case that this is how heritrix is behaving, then do \nI need to see about modifying the HDFS writer code to handle these \nthread interrupts better?\n\nThanks,\n-Zach\n\r\n--------------000909000700090305050007\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n  &lt;head&gt;\n\n    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=ISO-8859-1&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#ffffff&quot; text=&quot;#000000&quot;&gt;\n    I wanted to run this by you guys and get your thoughts - I am\n    running into a problem with the HDFS writer I&#39;m using to write crawl\n    results from heritrix 3. If you&#39;re curious you can find the code\n    here: &lt;a class=&quot;moz-txt-link-freetext&quot;\n      href=&quot;http://github.com/openplaces/heritrix-hdfs-writer&quot;&gt;http://github.com/openplaces/heritrix-hdfs-writer&lt;/a&gt;&lt;br&gt;\n    &lt;br&gt;\n    The problem I&#39;m seeing is this - at the end of the crawl, it appears\n    the job-related threads are being interrupted before they&#39;re\n    completely finished processing (though I have not verified this by\n    tracing through the code). Here is a warning I see pop up in the\n    logs at the end of the crawl:&lt;br&gt;\n    &lt;br&gt;\n    &lt;tt&gt;2010-09-23 15:05:13.125 INFO thread-18\n      org.archive.crawler.framework.CrawlJob.onApplicationEvent()\n      STOPPING&lt;br&gt;\n      2010-09-23 15:05:13.126 INFO thread-18\n      org.archive.crawler.framework.CrawlJob.onApplicationEvent()\n      FINISHED&lt;br&gt;\n      10/09/23 15:05:13 WARN hdfs.DFSClient: DFSOutputStream\n      ResponseProcessor exception&nbsp; for block\n      blk_5887928276646679438_1127java.io.InterruptedIOException:\n      Interruped while waiting for IO on channel\n      java.nio.channels.SocketChannel[connected local=/127.0.0.1:57686\n      remote=/127.0.0.1:50010]. 46903 millis timeout left.&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; at\norg.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:349)&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; at\norg.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; at\n      org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; at\n      org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:178)&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; at java.io.DataInputStream.readLong(DataInputStream.java:399)&lt;br&gt;\n      &nbsp;&nbsp;&nbsp; at\norg.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2397)&lt;/tt&gt;&lt;br&gt;\n    &lt;br&gt;\n    This is a problem because it causes an abnormal termination of\n    communication between the DFSClient and the HDFS server, which\n    results in the crawl file not being closed correctly. Examining the\n    state of the HDFS file after this happens we see it has a file size\n    of 0:&lt;br&gt;\n    &lt;br&gt;\n    &lt;tt&gt;zach@znbailey-2:bin$ ./hadoop fs -ls /crawl&lt;br&gt;\n      Found 1 items&lt;br&gt;\n      -rw-r--r--&nbsp;&nbsp; 3 zach supergroup&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 2010-09-23 10:55\n      /crawl/CrawlData-20100923&lt;span\n        __postbox-detected-content=&quot;__postbox-detected-phone&quot;\n        class=&quot;__postbox-detected-content __postbox-detected-phone&quot;\n        style=&quot;display: inline; font-size: inherit; padding: 0pt;&quot;&gt;145529-0000&lt;/span&gt;0&lt;br&gt;\n    &lt;/tt&gt;&lt;br&gt;\n    and running an HDFS &quot;fsck&quot; shows us that HDFS thinks the file is\n    still &quot;open&quot;:&lt;br&gt;\n    &lt;br&gt;\n    &lt;tt&gt;zach@znbailey-2:bin$ ./hadoop fsck /&lt;br&gt;\n      Status: HEALTHY&lt;br&gt;\n      &nbsp;Total size:&nbsp;&nbsp;&nbsp; 0 B&lt;br&gt;\n      &nbsp;Total dirs:&nbsp;&nbsp;&nbsp; 3&lt;br&gt;\n      &nbsp;Total files:&nbsp;&nbsp;&nbsp; 0 &lt;b&gt;(Files currently being written: 1)&lt;/b&gt;&lt;br&gt;\n      &nbsp;Total blocks (validated):&nbsp;&nbsp;&nbsp; 0 &lt;b&gt;(Total open file blocks (not\n        validated): 1)&lt;/b&gt;&lt;br&gt;\n      &nbsp;Minimally replicated blocks:&nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n      &nbsp;Over-replicated blocks:&nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n      &nbsp;Under-replicated blocks:&nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n      &nbsp;Mis-replicated blocks:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n      &nbsp;Default replication factor:&nbsp;&nbsp;&nbsp; 1&lt;br&gt;\n      &nbsp;Average block replication:&nbsp;&nbsp;&nbsp; 0.0&lt;br&gt;\n      &nbsp;Corrupt blocks:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n      &nbsp;Missing replicas:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 0&lt;br&gt;\n      &nbsp;Number of data-nodes:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 1&lt;br&gt;\n      &nbsp;Number of racks:&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 1&lt;/tt&gt;&lt;br&gt;\n    &lt;br&gt;\n    As I said I have not verified this behavior 100% and I&#39;m only going\n    off the evidence in the stack traces, so I&#39;m not sure if my\n    hypothesis about heritrix interrupting these threads is right or\n    not. So, I was hoping you could shed some light on that.&lt;br&gt;\n    &lt;br&gt;\n    If it is indeed the case that this is how heritrix is behaving, then\n    do I need to see about modifying the HDFS writer code to handle\n    these thread interrupts better?&lt;br&gt;\n    &lt;br&gt;\n    Thanks,&lt;br&gt;\n    -Zach\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------000909000700090305050007--\r\n\n"}}