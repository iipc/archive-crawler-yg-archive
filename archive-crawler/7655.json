{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":385173050,"authorName":"Coram, Roger","from":"&quot;Coram, Roger&quot; &lt;Roger.Coram@...&gt;","replyTo":"LIST","senderId":"2DWaE8Zzk8XDN_1oNVR315KoMVT6DcRLXwejE9Bo1nDu1PwJ_3JRp-rMe7s4DNe8zMciPg78Y1AFLbPnMM1gFAo-TViF1a62iw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [archive-crawler] Slow (?) loading millions of seeds","postDate":"1334656861","msgId":7655,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MEVDNTIyQTVAdzJrMy1ic3BleDE+","inReplyToHeader":"PDRGODc0QTZDLjMwNjAxMDNAYXJjaGl2ZS5vcmc+","referencesHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MEVBNDVCQ0VAdzJrMy1ic3BleDE+IDw3RDMxNjA3RS0yNkQ4LTRGRUEtQTE0Ni05MUQ0NjBBREREOEFAYXJjaGl2ZS5vcmc+IDxDNjMxNzQ2QS1BQkJDLTQ1N0UtOENDRC1ERTUxNDAyRjg4NDZAYXJjaGl2ZS5vcmc+IDw3NEM5N0U3REY1QTc3ODREOTk3MjE3RkY3NUQxMjE2NjBFQTQ1RDQ4QHcyazMtYnNwZXgxPiA8NEY4NzRBNkMuMzA2MDEwM0BhcmNoaXZlLm9yZz4="},"prevInTopic":7652,"nextInTopic":7698,"prevInTime":7654,"nextInTime":7656,"topicId":7645,"numMessagesInTopic":9,"msgSnippet":"Thanks, Gordon. We ve amended the scoping rules and taken to feeding the seeds in batches which has allowed things to get underway. Roger ... From: Gordon Mohr","rawEmail":"Return-Path: &lt;Roger.Coram@...&gt;\r\nX-Sender: Roger.Coram@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 59278 invoked from network); 17 Apr 2012 10:01:04 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m8.grp.sp2.yahoo.com with QMQP; 17 Apr 2012 10:01:04 -0000\r\nX-Received: from unknown (HELO w2k3-bspex1.bl.uk) (194.66.236.30)\n  by mta5.grp.sp2.yahoo.com with SMTP; 17 Apr 2012 10:01:04 -0000\r\nX-MimeOLE: Produced By Microsoft Exchange V6.5\r\nContent-class: urn:content-classes:message\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nDate: Tue, 17 Apr 2012 11:01:01 +0100\r\nMessage-ID: &lt;74C97E7DF5A7784D997217FF75D121660EC522A5@w2k3-bspex1&gt;\r\nIn-Reply-To: &lt;4F874A6C.3060103@...&gt;\r\nX-MS-Has-Attach: \r\nX-MS-TNEF-Correlator: \r\nThread-Topic: [archive-crawler] Slow (?) loading millions of seeds\r\nThread-Index: Ac0Y9BS93/ybj1dTRIe0Lsge06qkeADjIdAg\r\nReferences: &lt;74C97E7DF5A7784D997217FF75D121660EA45BCE@w2k3-bspex1&gt; &lt;7D31607E-26D8-4FEA-A146-91D460ADDD8A@...&gt; &lt;C631746A-ABBC-457E-8CCD-DE51402F8846@...&gt; &lt;74C97E7DF5A7784D997217FF75D121660EA45D48@w2k3-bspex1&gt; &lt;4F874A6C.3060103@...&gt;\r\nTo: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;,\n\t&lt;archive-crawler@yahoogroups.com&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Coram, Roger&quot; &lt;Roger.Coram@...&gt;\r\nSubject: RE: [archive-crawler] Slow (?) loading millions of seeds\r\nX-Yahoo-Group-Post: member; u=385173050\r\n\r\nThanks, Gordon.\n\nWe&#39;ve amended the scoping rules and taken to feeding the s=\r\needs in\nbatches which has allowed things to get underway.\n\nRoger\n\n-----Orig=\r\ninal Message-----\nFrom: Gordon Mohr [mailto:gojomo@...] \nSent: 12 A=\r\npril 2012 22:35\nTo: archive-crawler@yahoogroups.com\nCc: Coram, Roger\nSubjec=\r\nt: Re: [archive-crawler] Slow (?) loading millions of seeds\n\nWe&#39;d need to k=\r\nnow more about your crawl design and goals, and maybe\ncollect some info abo=\r\nut the process during the &#39;slow&#39; period, to\nunderstand the bottleneck you&#39;r=\r\ne hitting.\n\nStill, some other ideas not yet mentioned:\n\nWith millions of se=\r\neds, you&#39;ll *not* want to use any of the scoping\nrules that auto-update bas=\r\ned on each seed -- like SurtPrefixedDecideRule\nwith seedsAsSurtPrefixes=3D=\r\n=3Dtrue.\n\n(If you do, a giant in-memory sorted map of acceptable prefixes b=\r\nased on\nyour seeds is being built... perhaps hundreds of MB of heap for you=\r\nr\nseed list.)\n\nFor a &#39;slash&#39; (aka root) page crawl, you probably want to st=\r\nart broad\n(with an accept-all rule) then have the MaxHops rule at &#39;0&#39; -- al=\r\nl\ndiscovered navigational outlinks (hop-type &#39;L&#39;) will be considered too\nfa=\r\nr, while inline outlinks may still be retrieved based on whether you\ntune t=\r\nhe TransclusionDecideRule.\n\nAs just a slash-page crawl, you might not need =\r\nto split it to multiple\nmachines just yet unless you want to increase the p=\r\narallelism (compress\nthe time-skew) beyond what the threads of one machine =\r\ncan do.\n\nYou might want to feed in batches via the action directory, for ad=\r\nded\nunderstanding of what&#39;s been done, or dynamic adds after initial launch=\r\n.\n\nBut on thing to watch out with that technique is that it is totally\nasyn=\r\nchronous/oblivious to checkpointing, so you&#39;d want to make sure any\nparticu=\r\nlar file you&#39;d dropped there was completed (renamed to &#39;done&#39; \nsubdirectory=\r\n) before creating a checkpoint.\n\nOften with large seed lists, large numbers=\r\n of the seeds are\nobsolete/unresponsive hosts, which can also lead to large=\r\n lulls in\ncrawl-rate. (They&#39;ll appear as queued URIs reasonably fast, but a=\r\nll of\nyour worker threads might be in some sort of slow-connection-timeout\n=\r\n(20-seconds-plus) with unresponsive servers at once.) This can be\nespeciall=\r\ny bad when the seed list is sorted and thousands of\nequivalently-unresponsi=\r\nve hosts are listed alongside each other. When\nyou see or expect this probl=\r\nem, some ways to minimize it are:\n\n* randomize the seed order\n* decrease co=\r\nnfigurable connection timeouts (fewer thread-seconds lost\nper failure) * de=\r\ncrease the retry counts (fewer repeated delays) *\nincrease the slow-retry i=\r\nnterval (essentially so more other URIs can be\ntried in the time between tr=\r\nies of probably-hopeless URIs) * be a bit\nmore aggressive on thread count, =\r\nsince many will be doing a slow (but\nnot CPU/RAM-intensive) timeout for mor=\r\ne of the crawl\n\n- Gordon\n\nOn 4/12/12 5:18 PM, Coram, Roger wrote:\n&gt;\n&gt;\n&gt; Hi =\r\nKris,\n&gt;\n&gt; Many thanks for the advice. We&#39;ll amend our configuration and see=\r\n what\n\n&gt; happens.\n&gt;\n&gt; Thanks,\n&gt;\n&gt; Roger\n&gt;\n&gt; *From:*archive-crawler@yahoogro=\r\nups.com\n&gt; [mailto:archive-crawler@yahoogroups.com] *On Behalf Of *Kris Carp=\r\nenter\n\n&gt; Negulescu\n&gt; *Sent:* 11 April 2012 23:22\n&gt; *To:* archive-crawler@ya=\r\nhoogroups.com\n&gt; *Cc:* Kris Carpenter Negulescu\n&gt; *Subject:* Re: [archive-cr=\r\nawler] Slow (?) loading millions of seeds\n&gt;\n&gt; You might also want to look a=\r\nt leveraging the Action Directory and \n&gt; splitting the seeds into multiple =\r\nfiles that get loaded incrementally \n&gt; vs as a single monolithic file....\n&gt;=\r\n\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Cra\n&gt;=\r\n wl+Operators#H3DevNotesforCrawlOperators-MillionsofseedsOK\n&gt; &lt;https://weba=\r\nrchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Cr\n&gt; awl+Opera tors#=\r\nH3DevNotesforCrawlOperators-MillionsofseedsOK&gt;\n&gt;\n&gt;\n&gt;       ActionDirectory =\r\nfor post-launch URI-loading\n&gt;\n&gt; A bean class ActionDirectory, if present in=\r\n a crawl configuration (and\n\n&gt; it is recommended to become part of all stan=\r\ndard configurations), \n&gt; watches a configured &#39;action&#39; directory for any fi=\r\nles which appear \n&gt; (rechecking a configurable interval, default 30 seconds=\r\n). For each \n&gt; file, an action is taken in accordance with the file&#39;s suffi=\r\nx, then \n&gt; the file is moved to a &#39;done&#39; directory.\n&gt;\n&gt; A file ending &#39;.see=\r\nds&#39; will trigger the addition of more seeds. A file\n\n&gt; ending &#39;.recover&#39; wi=\r\nll be treated as a traditional recovery log -- \n&gt; with all &#39;Fs&#39; lines consi=\r\ndered included (to suppress recrawling) then \n&gt; all &#39;F+&#39; lines rescheduled.=\r\n A file ending &#39;.include&#39;, &#39;.schedule&#39;, or \n&gt; &#39;.force&#39; respectively will be=\r\n treated as if a recovery-log format \n&gt; (with 3-character prefix tag per li=\r\nne), but all URIs listed \n&gt; (regardless of\n&gt; prefix-tag) will be considered=\r\n-included, scheduled, or force-scheduled\n\n&gt; respectively.\n&gt;\n&gt; Any of these =\r\nfiles may be gzip-compressed (with a &#39;.gz&#39; extension), \n&gt; and those in reco=\r\nvery-log-format may have a &#39;.s.&#39; inserted prior to \n&gt; the functional suffix=\r\n (eg &#39;frontier.s.recover.gz&#39;) to indicate that \n&gt; prior to other steps, sco=\r\nping should be attempted against the included\nURIs.\n&gt;\n&gt; Drop ping the prope=\r\nr files (possibly filtered) into this directory \n&gt; will likely be the recom=\r\nmended way to recover prior crawl frontier \n&gt; state, or perform other bulk =\r\nadds to a running crawler.\n&gt;\n&gt; On Apr 11, 2012, at 3:15 PM, Kris Carpenter =\r\nNegulescu wrote:\n&gt;\n&gt;\n&gt;\n&gt; For very large crawls you will want to consider us=\r\ning the \n&gt; HashCrawlMapper to spread the crawl over multiple crawl instance=\r\ns.\n&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Multiple+Machine+C=\r\nra\n&gt; wling\n&gt;\n&gt; For the volume of seeds you describe, you will want to distr=\r\nibute \n&gt; across\n&gt; 10 or more crawl instances.\n&gt;\n&gt; Let us know if you need m=\r\nore detail/help in getting started.\n&gt;\n&gt; Kris\n&gt;\n&gt; Kris Carpenter Negulescu\n&gt;=\r\n Director, Web Group\n&gt; Internet Archive\n&gt; kcarpenter@... &lt;mailto:kc=\r\narpenter@...&gt;\n&gt; skypeid: kris.carpenter\n&gt;\n&gt; On Apr 11, 2012, at 12:=\r\n55 AM, Coram, Roger wrote:\n&gt;\n&gt;\n&gt;\n&gt; We&#39;re trying to begin a slash-page crawl=\r\n of 10,000,000+ seeds using \n&gt; Heritrix 3.0.0. While the initial few millio=\r\nn load relatively quickly \n&gt; after a few hours the rate slows to a (pardon =\r\nthe pun) crawl. The time\n\n&gt; take thus far is now measurable in days.\n&gt;\n&gt; We=\r\n&#39;ve tried doubling the memory allocated to the JVM, switching to the\n\n&gt; lat=\r\ne st version of Java and tried launching a crawl with Heritrix \n&gt; 3.1.0 for=\r\n comparison but the results don&#39;t seem noticeably improved. \n&gt; Is this to b=\r\ne expected? How long should 10 or even 20 million seeds \n&gt; optimally take t=\r\no read? Is there anything obvious we can do to improve\nthings?\n&gt;\n&gt; Many tha=\r\nnks,\n&gt;\n&gt; Roger\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}