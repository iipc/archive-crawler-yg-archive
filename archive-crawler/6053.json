{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":415459286,"authorName":"shichuanwuhan@yahoo.cn","from":"&quot;shichuanwuhan@...&quot; &lt;shichuanwuhan@...&gt;","profile":"shichuanwuhan@yahoo.cn","replyTo":"LIST","senderId":"8TShUpmT9AosRkBjo-mJ-W71gD6cH8njHIo62YWDgmjdouDe3xzZx-ibTo8pjo-iwc0vw3MIpOa0Qii1NhFCeS1Q_ACVvMKoIx6b0uV80hYkuJixup3dgU4","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Basic question: How to limit the crawling scope within a host?","postDate":"1254017067","msgId":6053,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg5bWg3YitzZWlsQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRBQkU0Nzg4LjMwNDAzMDRAYXJjaGl2ZS5vcmc+"},"prevInTopic":6052,"nextInTopic":6055,"prevInTime":6052,"nextInTime":6054,"topicId":6051,"numMessagesInTopic":6,"msgSnippet":"Dear Gordon, Thanks a lot. Your answer gives me a deeper understanding of Heritrix. But I still can t configure it correctly following your instruction so I","rawEmail":"Return-Path: &lt;shichuanwuhan@...&gt;\r\nX-Sender: shichuanwuhan@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 60698 invoked from network); 27 Sep 2009 02:04:56 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m3.grp.re1.yahoo.com with QMQP; 27 Sep 2009 02:04:56 -0000\r\nX-Received: from unknown (HELO n45b.bullet.mail.sp1.yahoo.com) (66.163.168.159)\n  by mta1.grp.re1.yahoo.com with SMTP; 27 Sep 2009 02:04:56 -0000\r\nX-Received: from [69.147.65.174] by n45.bullet.mail.sp1.yahoo.com with NNFMP; 27 Sep 2009 02:04:29 -0000\r\nX-Received: from [98.137.34.34] by t12.bullet.mail.sp1.yahoo.com with NNFMP; 27 Sep 2009 02:04:28 -0000\r\nDate: Sun, 27 Sep 2009 02:04:27 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h9mh7b+seil@...&gt;\r\nIn-Reply-To: &lt;4ABE4788.3040304@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;shichuanwuhan@...&quot; &lt;shichuanwuhan@...&gt;\r\nSubject: Re: Basic question: How to limit the crawling scope within a host?\r\nX-Yahoo-Group-Post: member; u=415459286; y=DnJWxT4p1GjobBqsm-mNUAMRC45pr0XZ27swd_E5C5Fadkyyl8AcLE69FGe4su0NLw\r\nX-Yahoo-Profile: shichuanwuhan@...\r\n\r\nDear Gordon,\n\n    Thanks a lot. Your answer gives me a deeper understanding=\r\n of Heritrix. \n      \n    But I still can&#39;t configure it correctly followin=\r\ng your instruction so I turn to you for help. Sorry about that :)\n\n    1.Is=\r\n it optional to use &#39;surts-source-file&#39;? I add the URL:&#39;+http://people.cs.c=\r\nmu.edu/faculty/index.html&#39; in the &#39;seeds&#39; table. Is it enough?\n\n    2. When=\r\n I want to use &#39;surts-source-file&#39;, I don&#39;t know where to put it on my disk=\r\n. I am using Windows XP. I try to put it in the same directory with &#39;seeds.=\r\ntxt&#39;, but it doesn&#39;t work.\n\n    3.Would you please give me an example of wr=\r\niting a correct SURT to\ncrawl all html files under &#39;http://people.cs.cmu.ed=\r\nu/faculty/&#39;?\n\nHere is my crawl order:\n\n17 Admin 20090927015119 settings log=\r\ns checkpoints state scratch 0 0 0 100 4096 65536 0 true seeds.txt true ACCE=\r\nPT true result.txt false true Mozilla/5.0 (compatible; heritrix/@1.14.3@ +h=\r\nttp://192.168.0.1) test@... ignore false 5.0 30000 3000 300 30 900 1 0=\r\n 0 org.archive.crawler.frontier.HostnameQueueAssignmentPolicy false false f=\r\nalse true true 3000 100 -1 org.archive.crawler.frontier.UnitCostAssignmentP=\r\nolicy 300000 50 org.archive.crawler.util.BdbUriUniqFilter false true false =\r\nfalse false true 21600 86400 false true false true sha1 true 1200 20000 0 0=\r\n false true open ISO-8859-1 true sha1 true true true true false true true t=\r\nrue true false true true true true true index.html %2E . true mirror 1023 2=\r\n55 false true LONG true true false true -1 true true false true true\n\nThank=\r\n you in advance.\n\nChuan\n\n\n--- In archive-crawler@yahoogroups.com, Gordon Mo=\r\nhr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; shichuanwuhan@... wrote:\n&gt; &gt; Hi all,\n&gt; &gt; \n&gt; &gt; I =\r\nam using version 1.14.3. My final goal is to get all the URLs of professors=\r\n&#39; mainpages on one host i.e www.cs.cmu.edu. So firstly, I plan to fetch all=\r\n pages that are within the host. However, I fail to do that.\n&gt; &gt; \n&gt; &gt; I tri=\r\ned both the traditional &#39;hostscope&#39; and recommended &#39;decidingscope&#39; but I s=\r\ntill cannot achieve my goal. As English is not my native language, maybe I =\r\nmisunderstand something in &#39;user manual&#39;. Would someone kindly answer sever=\r\nal questions?\n&gt; \n&gt; The included &#39;deciding-default&#39; profile should work for =\r\nthe purpose of \n&gt; getting all pages on a single web host.\n&gt; \n&gt; &gt; 1.My seed =\r\nis simply: &#39;http://www.cs.cmu.edu/&#39;. Is it right?\n&gt; \n&gt; Supplying that as a =\r\nseed, with the deciding-default settings, should \n&gt; cause the crawler to:\n&gt;=\r\n \n&gt; (1) start by visiting &quot;http://www.cs.cmu.edu/&quot;, examining the outlinks =\r\n\n&gt; of that page for &quot;in-scope&quot; URIs\n&gt; \n&gt; (2) evaluate any URIs that begin &quot;=\r\nhttp://www.cs.cmu.edu/&quot; as being \n&gt; &quot;in-scope&quot; (along with some other rules=\r\n), and thus eligible for \n&gt; recursive fetching\n&gt; \n&gt; The default configurati=\r\non, with only the single seed, will only wander \n&gt; off &quot;www.cs.cmu.edu&quot; to =\r\nfetch URIs that appears necessary to render \n&gt; another page (like inline re=\r\nferences to scripts, images, frames, etc., \n&gt; or URLs found in Javascript t=\r\nhat may auto-load).\n&gt; \n&gt; I see that the &#39;faculty&#39; link from &#39;www.cs.cmu.edu=\r\n&#39; goes to another \n&gt; host, &#39;people.cs.cmu.edu. Your crawl will not in gener=\r\nal visit that \n&gt; other host without additional scope customization to say t=\r\nhose URLs are \n&gt; of interest. Also, it appears that faculty web pages are o=\r\nn a variety of \n&gt; hosts (including &#39;www-2&#39; and other departmental servers).=\r\n\n&gt; \n&gt; &gt; 2.Is it possible that I simply use &#39;hostscope&#39; to achieve my goal? =\r\n\n&gt; \n&gt; It might be possible, but it is not recommended -- HostScope is \n&gt; de=\r\nprecated, less efficient and flexible than the DecidingScope + \n&gt; SurtPrefi=\r\nxedDecideRule mechanism.\n&gt; \n&gt; &gt; 3.If not, how to configure &#39;decidingscope&#39;?=\r\n\n&gt; \n&gt; You probably want to tell your crawl that begins at www.cs.cmu.edu th=\r\nat \n&gt; it may accept URIs on other hosts, like &#39;people.cs.cmu.edu&#39; and other=\r\ns, \n&gt; as &#39;in-scope&#39;. This involves giving the SurtPrefixedDecideRule more \n=\r\n&gt; acceptable &#39;SURT&#39; (URI-like) prefixes.\n&gt; \n&gt; This can be done either by sp=\r\necifying a file with such prefixes as the \n&gt; SurtPrefixedDecideRule&#39;s &#39;surt=\r\n-source-file&#39;, or by adding lines to your \n&gt; seeds list that begin &#39;+&#39;.\n&gt; \n=\r\n&gt; You can read more about SURTs as a means of scoping at:\n&gt; \n&gt; http://crawl=\r\ner.archive.org/articles/user_manual/config.html#surtprefixscope\n&gt; \n&gt; Anothe=\r\nr strategy might be to start your crawling at the faculty directory:\n&gt; \n&gt; h=\r\nttp://people.cs.cmu.edu/faculty/index.html\n&gt; \n&gt; Supplying that URI as a see=\r\nd will cause the &quot;implied scope&quot; to be all \n&gt; URLs beginning &quot;http://people=\r\n.cs.cmu.edu/faculty/&quot; -- which should get \n&gt; all the other pages of the dir=\r\nectory, as well. *If* you are confident \n&gt; all homepages always contain the=\r\n &#39;~&#39; character, you could also add a new \n&gt; rule to the list of rules, such=\r\n as a MatchesRegExpDecideRule, that \n&gt; always ACCEPTs any URI with a &#39;~&#39; ch=\r\naracter. That would get the \n&gt; directory, and all the linked pages with &#39;~&#39;=\r\n anywhere in their URI (and \n&gt; quite probably other pages, at other hosts a=\r\nnd universities, when their \n&gt; URIs with &#39;~&#39; are discovered).\n&gt; \n&gt; Hope thi=\r\ns helps,\n&gt; \n&gt; - Gordon @ IA\n&gt;\n\n\n\n"}}