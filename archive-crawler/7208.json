{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":289645082,"authorName":"helloitsmaxine","from":"&quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;","profile":"helloitsmaxine","replyTo":"LIST","senderId":"0X3zcTdFoX3dvhIT345u98-vWeuoBJZS7gt3G4g280gus3sADemHRjDI6HxzBxOJELelO6A1TCQ-knYgpTmPmcmhSD6J7lMukuYFrnc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: settings to maximize number of unique domains crawled?","postDate":"1310602023","msgId":7208,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGl2bGJ2OCthdDBvQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRFMUUzMDFBLjkwMzA3MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":7207,"nextInTopic":7209,"prevInTime":7207,"nextInTime":7209,"topicId":7206,"numMessagesInTopic":5,"msgSnippet":"Hi Gordon, Thanks for your advice. I ve changed my scope now to Deciding with the AcceptRule on. As for the first two suggestions, they make sense but I can t","rawEmail":"Return-Path: &lt;itsmaxine@...&gt;\r\nX-Sender: itsmaxine@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 11875 invoked from network); 14 Jul 2011 00:07:04 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m3.grp.sp2.yahoo.com with QMQP; 14 Jul 2011 00:07:04 -0000\r\nX-Received: from unknown (HELO n37b.bullet.mail.sp1.yahoo.com) (66.163.168.151)\n  by mta1.grp.sp2.yahoo.com with SMTP; 14 Jul 2011 00:07:04 -0000\r\nX-Received: from [69.147.65.148] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 14 Jul 2011 00:07:04 -0000\r\nX-Received: from [98.137.34.36] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 14 Jul 2011 00:07:04 -0000\r\nDate: Thu, 14 Jul 2011 00:07:03 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;ivlbv8+at0o@...&gt;\r\nIn-Reply-To: &lt;4E1E301A.9030707@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;\r\nSubject: Re: settings to maximize number of unique domains crawled?\r\nX-Yahoo-Group-Post: member; u=289645082; y=AU2P1l5tTSXYkMvK7bSHTaXWucOInFGu8UchCmmxRlkb2FTsR-se5kOedhwKuH0cHsQLJwmn3HcOdDA\r\nX-Yahoo-Profile: helloitsmaxine\r\n\r\nHi Gordon,\n\nThanks for your advice.\n\nI&#39;ve changed my scope now to Deciding =\r\nwith the AcceptRule on. As for the first two suggestions, they make sense b=\r\nut I can&#39;t seem to figure out how to implement those configurations through=\r\n the web UI. Searching about the CostAssignmentPolicy has turned up somethi=\r\nng to do with a WorkQueueFrontier, which doesn&#39;t seem to come as an option?=\r\n Am I just looking in the wrong place or are there certain features that ne=\r\ned some additional work to set up?\n\n--- In archive-crawler@yahoogroups.com,=\r\n Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; The best adjustment you could make to =\r\nencourage wandering to a wide \n&gt; number of different hosts is to:\n&gt; \n&gt; (1) =\r\nmake sure the &#39;budgeting&#39; for allocating frontier effort among \n&gt; queues is=\r\n in effect, by having a non-zero CostAssignmentPolicy. \n&gt; (UnitCostAssignme=\r\nntPolicy, treating each URI as &#39;1&#39;, is fine.)\n&gt; \n&gt; (2) then, make the &#39;bala=\r\nnce-replenish-amount&#39; very small. Each queue \n&gt; gets devoted frontier atten=\r\ntion until this amount is &#39;spent&#39;, then that \n&gt; queue goes to the back of a=\r\nll other queues. (Usually, we want to \n&gt; intensely crawl a site for a while=\r\n, ideally even finishing small sites, \n&gt; so that all the archived captures =\r\nare close together in time... thus the \n&gt; 3000 default value here. But it c=\r\nould be 50, or 10, or even 1. Note that \n&gt; at &#39;1&#39; behavior between queues i=\r\ns completely round-robin, which spread \n&gt; attention across the widest numbe=\r\nr of sites but also generally keeps \n&gt; memory caches from getting any benef=\r\nit from lots of crawling of the same \n&gt; site in rapid succession.)\n&gt; \n&gt; Lot=\r\ns of other refinements are possible, with advanced configuration \n&gt; tweaks =\r\n(including some, like custom &#39;queue precedence policies&#39;, that \n&gt; theoretic=\r\nally should work but are so seldom used they&#39;re not \n&gt; battle-tested). But =\r\njust that change biases things a lot more towards \n&gt; host diversity early, =\r\nrather than eventually.\n&gt; \n&gt; Separately, the &#39;BroadScope&#39; and other ____Sco=\r\npe classes (other than \n&gt; DecidingScope) are discouraged; if you make a Dec=\r\nidingScope which starts \n&gt; with an AcceptDecideRule, then focus the other r=\r\nules on knocking out \n&gt; things you don&#39;t want, you&#39;ll have a more modern (a=\r\nnd H3-ready) \n&gt; broadly-scoped crawl.\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; On 7/13/11 3:=\r\n03 PM, helloitsmaxine wrote:\n&gt; &gt; I&#39;m doing a crawl and interested in maximu=\r\nm the number of unique\n&gt; &gt; domains crawled, ie. I&#39;d rather crawl one page f=\r\nrom each of 5 unique\n&gt; &gt; domains than have 10 pages from one domain. Right =\r\nnow it&#39;s weird\n&gt; &gt; because I have pretty standard/default settings, ie. Bro=\r\nadScope and\n&gt; &gt; high max hops and such, but out of ~50gb I&#39;ve crawled, less=\r\n than 1000\n&gt; &gt; unique domains have been produced. I&#39;m counting by counting =\r\nthe\n&gt; &gt; number of folders in the mirror folder (each of which seems to\n&gt; &gt; =\r\nrepresent the content from one unique domain), as I&#39;m using the\n&gt; &gt; MirrorW=\r\nriter.\n&gt; &gt;\n&gt; &gt; My question is if anyone knows what the problem could be and=\r\n how to\n&gt; &gt; fix it? I was wondering if there were possible a maximum bytes =\r\nper\n&gt; &gt; domain limit to set (I know there&#39;s an overall max bytes but that\n&gt;=\r\n &gt; wouldn&#39;t seem to help) or some other settings that could help me get\n&gt; &gt;=\r\n more domains per amount of space crawled?\n&gt; &gt;\n&gt; &gt; Thanks for any help!\n&gt; &gt;=\r\n\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ------------------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Lin=\r\nks\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}