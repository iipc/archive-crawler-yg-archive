{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"_CHU8GpWDkJK3Q2td7wnqrwLeQ6QHMsmAe2EaE9disxjgkW9dGAULP4toinjWsk0nxRh3mb3vOwT3biwFhuQchv-NrtiY_Q","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Meaning of &quot;Job&quot; and params tweaking","postDate":"1380664718","msgId":8353,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUyNEI0NThFLjIwMEBhcmNoaXZlLm9yZz4=","inReplyToHeader":"PGwyYjlhMysxNDk5amxhQFlhaG9vR3JvdXBzLmNvbT4=","referencesHeader":"PGwyYjlhMysxNDk5amxhQFlhaG9vR3JvdXBzLmNvbT4="},"prevInTopic":8352,"nextInTopic":8354,"prevInTime":8352,"nextInTime":8354,"topicId":8352,"numMessagesInTopic":3,"msgSnippet":"... A Heritrix job is a grouping of configuration for a particular purpose. When you launch it, you start a crawl with those settings. Only some very small","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 66678 invoked by uid 102); 1 Oct 2013 21:58:43 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m10.grp.bf1.yahoo.com with SMTP; 1 Oct 2013 21:58:43 -0000\r\nX-Received: (qmail 30387 invoked from network); 1 Oct 2013 21:58:43 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 1 Oct 2013 21:58:43 -0000\r\nX-Received: (qmail 45750 invoked by uid 0); 1 Oct 2013 21:58:42 -0000\r\nX-Received: from 174.240.14.132 (HELO silverbook.local) (174.240.14.132)\n  by relay01.pair.com with SMTP; 1 Oct 2013 21:58:42 -0000\r\nX-pair-Authenticated: 174.240.14.132\r\nMessage-ID: &lt;524B458E.200@...&gt;\r\nDate: Tue, 01 Oct 2013 14:58:38 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:17.0) Gecko/20130801 Thunderbird/17.0.8\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;l2b9a3+1499jla@...&gt;\r\nIn-Reply-To: &lt;l2b9a3+1499jla@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Meaning of &quot;Job&quot; and params tweaking\r\nX-Yahoo-Group-Post: member; u=137285340; y=kEbw4Br4IhCQrzQddGi7fr6CpGrOYhFSz88tOFydEnaS\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 9/30/13 12:30 AM, james@... wrote:\n&gt; Hello everyone\n&gt;\n&gt;\n&gt; I just set up heritrix in eclipse and wrote a sample Processor,\n&gt; configured a job that uses this processor and started a crawling job.\n&gt; Everything works ok so far.\n&gt;\n&gt; I&#39;m a still not sure about the meaning of a &quot;job&quot;. I setup a test crawl\n&gt; job for 100 URLs and let it run over the weekend. On monday morning, the\n&gt; job still was running. It still tried to crawl some pages that probably\n&gt; had some errors while fetching.\n\nA Heritrix &#39;job&#39; is a grouping of configuration for a particular \npurpose. When you &#39;launch&#39; it, you start a crawl with those settings.\n\nOnly some very small and simple jobs, of very narrowly-defined scopes \n(URIs/domains of interest), are likely to run to definitive completion \nin a small amount of time.\n\n&gt; So if a job with 100 URLs won&#39;t finish after two days, a job with\n&gt; ~100&#39;000 will definitively will run for weeks. I&#39;m not quite sure how to\n&gt; understand a job. Is it something that is long running and new URLs etc.\n&gt; should be added to a running job? How do I know when it&#39;s a good time to\n&gt; terminate a job, e.g. so that only previously failed URLs will be\n&gt; enqueued again?\n\nIt depends on your goals and judgement. There are infinite paths on the \nweb - crawl traps - that can only be heuristically and imperfectly \nassessed. There are sites that aren&#39;t responding.... but might come back \ntomorrow or next week - so it&#39;s a judgement call how many retries to \nallow the crawler.\n\nHistorically we thought of jobs as something that would run to either \ncompletion (emptying of all known queues, via either URI success or \nfailure-after-configured-retries), or to a time limit, or to a point \nwhere the crawl operator looked at the current queues/activity/errors \nand decided, &quot;enough for this one&quot;. (Usually, the pattern of \nsuccess/failure shown in the crawl.log, and names/sizes of remaining \nnon-empty queues, are the major factors in such a decision.)\n\nA later launch could either start fresh from the same configuration and \nseeds, as if the prior crawl hadn&#39;t happened, or with some extra effort \nimport some state from the prior launch to help avoid collecting or \nstoring duplicate content.\n\nThe faster checkpointing in Heritrix post-3.0, and some other features, \ncreate the possibility for running an &quot;eternal&quot; job. Sometimes you might \npause it, checkpoint it, turn off its hardware or move it to entirely \nnew systems... but then it could resume from where it left off, and \nperhaps then be prodded into revisiting URIs occording to some desired \nschedule. I don&#39;t know of a group doing this quite yet, but it remains a \npossible idealized mode of operation.\n\n&gt; Which params would I have to tweak to achieve what we are planning to do?\n&gt; Our use case looks as follows:\n&gt;\n&gt;   * Fetch the content of ~100&#39;000 URLs (seeds) once a month\n&gt;   * Post-process and analyze the content of those URLs and write an\n&gt;     index in solr\n\nDo you want to fetch only those exact 100,000 URIs, or a much larger set \nof URIs discoverable from those seed points? That question will \ndetermine your &#39;scope&#39;. (You could get just those URIs. Or those URIs \nplus &#39;inline&#39; embeds, like images, CSS, and JS. Or those URIs and N \nextra link-hops away (no matter what domains). Or those URIs and other \nURIs that seem to share the same URI-domain/path-prefix as the seeds. Etc.)\n\nWatching what your initial setup achieves after a few days or weeks may \ngive you ides for how the various politeness, retry, and scope-filtering \nsettings should be adjusted to get more of what you want, less of what \nyou don&#39;t, according to your own preferences for \nerring-towards-inclusion or erring-towards-exclusion. Your target sites \nmight be very different in size, responsiveness, crawl-friendliness, etc \nthan other projects&#39; sites... so there&#39;s no one-size-fits-all approach.\n\nPost-processing/indexing of the content is a matter for other tools and \nprocesses... you might be able to feed partial results (while the crawl \nis still in progress) to your indexing, or you might need a &quot;complete&quot; \ndataset.\n\n- Gordon\n\n"}}