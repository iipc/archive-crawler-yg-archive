{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":363624086,"authorName":"Ko, Lauren","from":"&quot;Ko, Lauren&quot; &lt;lauren.ko@...&gt;","profile":"laurendko","replyTo":"LIST","senderId":"4a7PUGR_kecYWwnoNGuwqJUyP3tJ9vJO1QZC2q2228xmu5Of9z1Bs_RTC_Ze_N5aIIMQ7sLdCzdskVVW5_5lWsDx1csZCwk","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [archive-crawler] Re: Problem with robots.txt IGNORE policy","postDate":"1324569962","msgId":7472,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDc5ODk2RDkxQTNFQjUzNEM5N0RDODhFOEU2OTQ3RDNGNDJGMkVEQEdBQk1CeDA0LmFkLnVudC5lZHU+","inReplyToHeader":"PGpjdjNxMSs4OThmQGVHcm91cHMuY29tPg==","referencesHeader":"PDRDNjQ0NUQ5LjYwNzAxQGFyY2hpdmUub3JnPiw8amN2M3ExKzg5OGZAZUdyb3Vwcy5jb20+"},"prevInTopic":7471,"nextInTopic":7475,"prevInTime":7471,"nextInTime":7473,"topicId":6671,"numMessagesInTopic":9,"msgSnippet":"For Heritrix 3.1: Put this in SETTINGS OVERLAY SHEETS section of crawler-beans.cxml: ","rawEmail":"Return-Path: &lt;Lauren.Ko@...&gt;\r\nX-Sender: Lauren.Ko@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 52932 invoked from network); 22 Dec 2011 16:06:04 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m6.grp.sp2.yahoo.com with QMQP; 22 Dec 2011 16:06:04 -0000\r\nX-Received: from unknown (HELO mailhost.unt.edu) (129.120.188.67)\n  by mta4.grp.sp2.yahoo.com with SMTP; 22 Dec 2011 16:06:04 -0000\r\nX-Received-SPF: None (mailhost.unt.edu: no sender authenticity\n  information available from domain of Lauren.Ko@...)\n  identity=pra; client-ip=129.120.209.40;\n  receiver=mailhost.unt.edu; envelope-from=&quot;Lauren.Ko@...&quot;;\n  x-sender=&quot;Lauren.Ko@...&quot;; x-conformance=sidf_compatible\r\nX-Received-SPF: None (mailhost.unt.edu: no sender authenticity\n  information available from domain of Lauren.Ko@...)\n  identity=mailfrom; client-ip=129.120.209.40;\n  receiver=mailhost.unt.edu; envelope-from=&quot;Lauren.Ko@...&quot;;\n  x-sender=&quot;Lauren.Ko@...&quot;; x-conformance=sidf_compatible\r\nX-Received-SPF: None (mailhost.unt.edu: no sender authenticity\n  information available from domain of\n  postmaster@...) identity=helo;\n  client-ip=129.120.209.40; receiver=mailhost.unt.edu;\n  envelope-from=&quot;Lauren.Ko@...&quot;;\n  x-sender=&quot;postmaster@...&quot;;\n  x-conformance=sidf_compatible\r\nX-SBRS: 2.9\r\nX-Policy: INTERNAL_RELAY-$RELAY\r\nX-ExtLoopCount1: 1\r\nX-IronPort-AV: E=Sophos;i=&quot;4.71,392,1320645600&quot;; \n   d=&quot;scan&#39;208&quot;;a=&quot;344768701&quot;\r\nX-Received: from gabvmhub02.ad.unt.edu ([129.120.209.40])\n  by mailhost.unt.edu with ESMTP/TLS/AES128-SHA; 22 Dec 2011 10:06:02 -0600\r\nX-Received: from GABMBX04.ad.unt.edu ([169.254.4.249]) by GABvmHUB02.ad.unt.edu\n ([129.120.209.40]) with mapi id 14.01.0339.001; Thu, 22 Dec 2011 10:06:02\n -0600\r\nTo: &quot;archive-crawler@yahoogroups.com&quot; &lt;archive-crawler@yahoogroups.com&gt;\r\nThread-Topic: [archive-crawler] Re: Problem with robots.txt IGNORE policy\r\nThread-Index: AQHMwJvDqCgB8KlJOE6kggK4QOl9GJXoBNhM\r\nDate: Thu, 22 Dec 2011 16:06:02 +0000\r\nMessage-ID: &lt;79896D91A3EB534C97DC88E8E6947D3F42F2ED@...&gt;\r\nReferences: &lt;4C6445D9.60701@...&gt;,&lt;jcv3q1+898f@...&gt;\r\nIn-Reply-To: &lt;jcv3q1+898f@...&gt;\r\nAccept-Language: en-US\r\nContent-Language: en-US\r\nX-MS-Has-Attach:\r\nX-MS-TNEF-Correlator:\r\nContent-Type: text/plain; charset=&quot;Windows-1252&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nMIME-Version: 1.0\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Ko, Lauren&quot; &lt;lauren.ko@...&gt;\r\nSubject: RE: [archive-crawler] Re: Problem with robots.txt IGNORE policy\r\nX-Yahoo-Group-Post: member; u=363624086; y=9vvHPCInoN35ouy9OUCZv45Em_RKF-4AOV7BLc53j8-foZMx\r\nX-Yahoo-Profile: laurendko\r\n\r\nFor Heritrix 3.1:\nPut this in SETTINGS OVERLAY SHEETS section of crawler-be=\r\nans.cxml:\n\n&lt;!-- ignoreRobots: any URI to which this sheet&#39;s settings are ap=\r\nplied\n     will ignore robots.txt rules. For use when we are obeying robots=\r\n.txt\n     but perhaps have permission to ignore for a domain. --&gt;\n&lt;bean id=\r\n=3D&#39;ignoreRobots&#39; class=3D&#39;org.archive.spring.Sheet&#39;&gt;\n &lt;property name=3D&#39;ma=\r\np&#39;&gt;\n  &lt;map&gt;\n   &lt;entry key=3D&#39;metadata.robotsPolicyName&#39; value=3D&#39;ignore&#39;/&gt;\n=\r\n  &lt;/map&gt;\n &lt;/property&gt;\n&lt;/bean&gt;\n\nPut the following (replace surtPrefixes valu=\r\nes as needed) in SETTINGS OVERLAY SHEET-ASSOCIATION section of crawler-bean=\r\ns.cxml:\n\n&lt;bean class=3D&#39;org.archive.crawler.spring.SurtPrefixesSheetAssocia=\r\ntion&#39;&gt;\n &lt;property name=3D&#39;surtPrefixes&#39;&gt;\n  &lt;list&gt;\n   &lt;value&gt;http://(com,knt=\r\nu,&lt;/value&gt;\n  &lt;/list&gt;\n &lt;/property&gt;\n &lt;property name=3D&#39;targetSheetNames&#39;&gt;\n  &lt;=\r\nlist&gt;\n   &lt;value&gt;ignoreRobots&lt;/value&gt;\n  &lt;/list&gt;\n &lt;/property&gt;\n&lt;/bean&gt;\n\n\nLaure=\r\nn Ko\nWeb Archiving Programmer\nUNT Libraries\n\n______________________________=\r\n__________\nFrom: archive-crawler@yahoogroups.com [archive-crawler@yahoogrou=\r\nps.com] on behalf of Mahmoud A. Mubarak [mahmoud.mubarak@...]\nSent:=\r\n Thursday, December 22, 2011 5:20 AM\nTo: archive-crawler@yahoogroups.com\nSu=\r\nbject: [archive-crawler] Re: Problem with robots.txt IGNORE policy\n\n--- In =\r\narchive-crawler@yahoogroups.com&lt;mailto:archive-crawler%40yahoogroups.com&gt;, =\r\nGordon Mohr &lt;gojomo@...&gt; wrote:\n\n&gt; An option for simulating what you want w=\r\nithout changing the current\n&gt; IGNORE or adding a new policy would be to run=\r\n your crawl with a CLASSIC\n&gt; robots-respecting policy, but set the &#39;calcula=\r\nteRobotsOnly&#39; flag on\n&gt; PreconditionEnforcer. Rather than canceling the fet=\r\nching of URIs that\n&gt; are robots-precluded, this setting merely marks them u=\r\np with an annotation.\n&gt;\n&gt; - Gordon @ IA\n&gt;\n\nI have set the &#39;calculateRobotsO=\r\nnly&#39; flag on PreconditionEnforcer and It worked. But, how can I ignore robo=\r\nts.txt for one or more seeds, not all of them?\n\nThanks in advance.\n\nMahmoud=\r\n A. Mubarak\nBibliotheca Alexandrina\n\n\n\n\n"}}