{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"_N5dEwq8TI1wb4UphdEXZ3RtevljRNJvm6fykRUxFIyeT87nOJqB5qxchAetfvDY3MK61_FP2Y0-EGEANt2cj68eiscv0BQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] using regular expressions to filter fetched content","postDate":"1231192351","msgId":5627,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5NjI4MTFGLjMwMjAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGdqcmg0MCtvcDBiQGVHcm91cHMuY29tPg==","referencesHeader":"PGdqcmg0MCtvcDBiQGVHcm91cHMuY29tPg=="},"prevInTopic":5626,"nextInTopic":5649,"prevInTime":5626,"nextInTime":5628,"topicId":5626,"numMessagesInTopic":6,"msgSnippet":"I recommend using 1.14.x unless you specifically need 2.0 features. The documentation (both in the official user manual and various notes/threads) is better;","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 73734 invoked from network); 5 Jan 2009 21:52:31 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m51.grp.scd.yahoo.com with QMQP; 5 Jan 2009 21:52:31 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta17.grp.scd.yahoo.com with SMTP; 5 Jan 2009 21:52:31 -0000\r\nX-Received: (qmail 93911 invoked from network); 5 Jan 2009 21:52:25 -0000\r\nX-Received: from 67.170.220.186 (HELO ?192.168.1.10?) (67.170.220.186)\n  by relay00.pair.com with SMTP; 5 Jan 2009 21:52:25 -0000\r\nX-pair-Authenticated: 67.170.220.186\r\nMessage-ID: &lt;4962811F.3020303@...&gt;\r\nDate: Mon, 05 Jan 2009 13:52:31 -0800\r\nUser-Agent: Thunderbird 2.0.0.19 (Windows/20081209)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;gjrh40+op0b@...&gt;\r\nIn-Reply-To: &lt;gjrh40+op0b@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] using regular expressions to filter fetched\n content\r\nX-Yahoo-Group-Post: member; u=137285340; y=LWlkQt_W_EVpeI8L-Z18tf4g8eiZIRrOzSsU_Dw6hP6d\r\nX-Yahoo-Profile: gojomo\r\n\r\nI recommend using 1.14.x unless you specifically need 2.0 features. The \ndocumentation (both in the official user manual and various \nnotes/threads) is better; there are still areas of 2.0.x with less \ntesting and awkward usability compared to 1.x; and changes planned in \n2.2 mean migration from 1.14 will be just as easy (if not easier via a \nplanned configuration-conversion tool) than 2.0-&gt;2.2.\n\nStill, the techniques described in Tom Emerson&#39;s writeup should work in \n2.0; only the manner of setting it up is a slightly different.\n\nTo prevent certain material from being saved, install rules on the \nProcessor that does the saving (usually ArcWriterProcessor). If those \nrules &#39;REJECT&#39; the URI, that Processor will be skipped.\n\nSo there are at least 3 different places where you can put rules to help \nachieve your desired aim:\n\n(1) In the scope, to prevent a URI from being fetched at all. Of course, \nthese rules can only consider the URI&#39;s string form and discovery \ncontext, because no fetching has yet occurred.\n\n(2) In the &#39;mid-fetch&#39; rules of FetchHTTP, to abort a fetch after the \nheaders are available, but before the full (potentially large) content \nis fetched.\n\n(3) In the processor rules of the writing Processor (such as \nArcWriterProcessor), to skip writing of certain content. (These rules \ncould even potentially take into account fetched content bodies.)\n\nHope this helps,\n\n- Gordon @ IA\n\npeterlikarish wrote:\n&gt; Hello all,\n&gt; \n&gt; I hope I am not missing something obvious. I am using Heritrix 2.0.2\n&gt; and have successfully installed/run simple crawls. Now, I only want to\n&gt; save particular types of content to save space/bandwidth. For\n&gt; instance, I don&#39;t need any media content. I&#39;ve seen the Tom Emerson&#39;s\n&gt; blog on the topic\n&gt; (http://www.dreamersrealm.net/~tree/blog/?s=text%2Fhtml&submit=GO) as\n&gt; well as the FAQ entry but these seem tailored for the 1.* versions of\n&gt; heritrix. I can switch back and follow the directions but wanted to\n&gt; give 2.0 a shot. I am guessing that I need to institute a rule under\n&gt; the Scope-&gt;Rules-&gt;list of decide rules.\n&gt; \n&gt; The logical one to pick would be the &quot;MatchesRegExpDecideRule&quot;. I then\n&gt; place the regex filtering the unwanted extensions, move the rule up in\n&gt; the list and set the decision to &quot;reject&quot;. I noted in Mr. Emerson&#39;s\n&gt; blog that he also filtered the saved content. Is there a comparable\n&gt; way to do this in Heritrix 2? Is this the intended mechanism by which\n&gt; I should be filtering content? Thank you for the time and effort both\n&gt; on the crawler and reading my question,\n&gt; \n&gt; Peter\n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}