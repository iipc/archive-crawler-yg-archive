{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"HxrWt3CYd1AgH_6N9gyow5uEUmtRsm4ntiABLL3Kbze6VTV-75GF5oB6paropGnGU8cfiKwad_1vZIeHvXFrOWDw7XBShHI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"OOME Re: [archive-crawler] Re: Distributed Crawling","postDate":"1176241767","msgId":4078,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2MUMwNjY3LjcwNjAxQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGV2Z2czNSttOGFwQGVHcm91cHMuY29tPg==","referencesHeader":"PGV2Z2czNSttOGFwQGVHcm91cHMuY29tPg=="},"prevInTopic":4076,"nextInTopic":4080,"prevInTime":4077,"nextInTime":4079,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"... This is likely a contributor to the problem. The hold-queues setting originally made a giant difference: for queues that went inactive , only their name","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 21149 invoked from network); 10 Apr 2007 21:48:12 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m48.grp.scd.yahoo.com with QMQP; 10 Apr 2007 21:48:12 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta7.grp.scd.yahoo.com with SMTP; 10 Apr 2007 21:48:12 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id D7436141C6EE9\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 10 Apr 2007 14:46:59 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 24094-07-68 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 10 Apr 2007 14:46:57 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 5752D141C6EE0\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 10 Apr 2007 14:46:57 -0700 (PDT)\r\nMessage-ID: &lt;461C0667.70601@...&gt;\r\nDate: Tue, 10 Apr 2007 14:49:27 -0700\r\nUser-Agent: Thunderbird 1.5.0.10 (X11/20070306)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;evgg35+m8ap@...&gt;\r\nIn-Reply-To: &lt;evgg35+m8ap@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: OOME Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=137285340; y=wCPzR1UfWj1vt4te0wBU7yovtJu8SERwNRXlxEUXTlgY\r\nX-Yahoo-Profile: gojomo\r\n\r\njoehung302 wrote:\n&gt; so far there I&#39;ve got the first proof crawling (single instance but \n&gt; HashMap&#39;ed) going for 4 days with 12M docs downloaded. I just got the \n&gt; first OOME error. I wonder if it&#39;s because that I turned the option\n&gt; \n&gt;  &lt;boolean name=&quot;hold-queues&quot;&gt;false&lt;/boolean&gt;\n&gt; \n&gt; to false. We were using &quot;true&quot; for the last crawl but we thought it \n&gt; might be a better practice to rotate the queues more frequently (to \n&gt; prevent the busy crawling on certain sites).\n\nThis is likely a contributor to the problem.\n\nThe &#39;hold-queues&#39; setting originally made a giant difference: for queues \nthat went &#39;inactive&#39;, only their name (queue key) was held in memory, \nwhile all &#39;ready&#39; queue objects (significantly larger than just the key) \nwere held in memory. (In both cases, the actual URI contents of the \nqueues are on disk until needed.)\n\nNow, both the queue of &#39;ready&#39; queues and the queue of &#39;inactive&#39; queues \nare handled the same way, with only the queue name definitely in memory \nuntil the queue is needed.\n\nHowever, there will still be significant indirect effects. With \n&#39;hold-queues&#39; as false, all queues are created in the &#39;ready&#39; state. \nEssentially, *all* queues are round-robined for providing a URI to \ncrawl. (As soon as one URI finishes, then the queue politeness wait, the \nqueue goes to the back of the &#39;ready&#39; rotation.)\n\nThere will be at least two large effects of this in a broad crawl:\n\n(1) little chance of keeping important in-memory object caches &#39;warm&#39;: \nsome queue/host/server objects are in soft-reference caches because \nwhile they go unused (during politeness delay and waiting in the &#39;ready&#39; \nqueue) for a while they&#39;ll soon be needed again. With a humongous \n&#39;ready&#39; queue, chances are they&#39;ll be dropped from the cache before \nreused, so every URI crawled will require a read-in and write-out of \nthese related objects. (Lots more IO, lots more temporary-low-memory- \nconditions-forcing-soft-reference-clearing.)\n\n(2) small queues (eg &lt;5, &lt;20, &lt;100 URIs) won&#39;t get a chance to finish \nuntil *every* queue gets through that same number of items. That could \nmean a lot more nonempty queues in total -- and even with only the queue \nname in memory, could explain your problem in a long crawl. (A finished \nqueue has no memory footprint, but even the smallest filled queue has at \nleast its name in memory).\n\nSo for large/broad crawls, &#39;hold-queues&#39; should definitely be true.\n\nThere are other ways to get the desired &#39;broader rotation&#39; or &#39;less \nintense crawling&#39; effect you want, without going to round-robining all \nqueues. Increasing politeness delays is an obvious route, but another is \nto increase the &#39;target-ready-backlog&#39; value.\n\nThe crawler aims to always keep this many queues in the &#39;ready&#39; queue, \neven if all threads are busy. So it will activate queues from &#39;inactive&#39; \nwhenever the backlog falls below this number. Making the number larger \nmeans more queues are in &#39;active&#39; rotation -- so even with fast \npolitness settings, they may take a while to be hit again, while other \nqueues are in front of them. As these finish (or deplete their &#39;budget&#39;, \nsee below), others will come from &#39;inactive&#39; to replace them.\n\nThe other useful intensity-affecting settings are the &#39;cost&#39; and \n&#39;budget&#39; related values. A queue gets a &#39;balance-replenish-amount&#39; \nsession-budget whenever it first becomes &#39;ready&#39;. Each URI crawled \ndepletes that budget in accordance with the &#39;cost-policy&#39;. When the \nsession-budget reaches 0, the queue goes to the back of the &#39;inactive&#39; \nqueues to give others a chance to crawl.\n\nAssuming a UnitCostAssignmentPolicy (every URI costs 1), a \n&#39;balance-replenish-amount&#39; of 1 would be a degenerate case much like \n&#39;hold-queues&#39; being false: each queue would contribute one URI to be \ncrawled before getting to the back of one global line. With a \n&#39;balance-replenish-amount&#39; of 100, a queue would give out up to 100 URIs \nbefore deactivating. This has the nice property of letting small sites \nfinish the first time they become &#39;active, while larger sites rotate in \nand out of active crawling to let newly-discovered small sites a chance \nto finish.\n\nSo:\n  - &#39;target-ready-backlog&#39; and politeness settings affect how intensely \na queue is crawled while it is &#39;active&#39;\n  - &#39;balance-replenish-amount&#39; and &#39;cost-policy&#39; affect how long a large \nqueue stays &#39;active&#39; before making way for other queues\n  - &#39;hold-queues&#39; at false makes all queues round-robin in the &#39;ready&#39; \nstate; a minimum &#39;balance-replenish-amount&#39; (with a nonzero cost policy) \nmakes all queues round-robin through the &#39;inactive&#39; state. In either \ncase, the queue load (and memory load) can get much higher because \nqueues neither finish nor get crawled frequently enough to skip \nroundtrips to disk.\n\n&gt; Do you want to look the OOME problem further? What information should \n&gt; I send?\n\nIf it recurs after adjusting your queue behaviors, good info is always:\n\n- progress-statistics.log shortly before and after OOME\n\n- &#39;jmap -histo&#39; of heap, at times as close (before and after) OOME as \npossible\n\n- if reproduceable in JDK 1.6, the better error stack dump at time of \nOOME (to confirm if it is in fact heap-related), and if necessary the \nHeapDumpOnOutOfMemoryError automatic full heap dump, for later analysis \nof what&#39;s overgrown\n\n- Gordon @ IA\n\n\n\n\n"}}