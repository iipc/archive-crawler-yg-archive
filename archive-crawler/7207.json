{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"gaYF1Rfz5qXO8X5q1xf_PWPX04bsfPAMfEztFLLpMfcDGKrnJp8MNcqwe7IYU2ylIpiUQFmCNS3kIXdR5E882CE7eCE4TyE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] settings to maximize number of unique domains crawled?","postDate":"1310601242","msgId":7207,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFMUUzMDFBLjkwMzA3MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGl2bDRtcSsxc2NjQGVHcm91cHMuY29tPg==","referencesHeader":"PGl2bDRtcSsxc2NjQGVHcm91cHMuY29tPg=="},"prevInTopic":7206,"nextInTopic":7208,"prevInTime":7206,"nextInTime":7208,"topicId":7206,"numMessagesInTopic":5,"msgSnippet":"The best adjustment you could make to encourage wandering to a wide number of different hosts is to: (1) make sure the budgeting for allocating frontier","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 42927 invoked from network); 13 Jul 2011 23:54:35 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m16.grp.re1.yahoo.com with QMQP; 13 Jul 2011 23:54:35 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta3.grp.re1.yahoo.com with SMTP; 13 Jul 2011 23:54:35 -0000\r\nX-Received: (qmail 71254 invoked by uid 0); 13 Jul 2011 23:54:03 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay01.pair.com with SMTP; 13 Jul 2011 23:54:03 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4E1E301A.9030707@...&gt;\r\nDate: Wed, 13 Jul 2011 16:54:02 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.18) Gecko/20110616 Thunderbird/3.1.11\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: helloitsmaxine &lt;itsmaxine@...&gt;\r\nReferences: &lt;ivl4mq+1scc@...&gt;\r\nIn-Reply-To: &lt;ivl4mq+1scc@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] settings to maximize number of unique domains\n crawled?\r\nX-Yahoo-Group-Post: member; u=137285340; y=zbkkF5BJgcq48I0f2EZIJ-vWa2wgXytPXeFgvnv-6T6f\r\nX-Yahoo-Profile: gojomo\r\n\r\nThe best adjustment you could make to encourage wandering to a wide \nnumber of different hosts is to:\n\n(1) make sure the &#39;budgeting&#39; for allocating frontier effort among \nqueues is in effect, by having a non-zero CostAssignmentPolicy. \n(UnitCostAssignmentPolicy, treating each URI as &#39;1&#39;, is fine.)\n\n(2) then, make the &#39;balance-replenish-amount&#39; very small. Each queue \ngets devoted frontier attention until this amount is &#39;spent&#39;, then that \nqueue goes to the back of all other queues. (Usually, we want to \nintensely crawl a site for a while, ideally even finishing small sites, \nso that all the archived captures are close together in time... thus the \n3000 default value here. But it could be 50, or 10, or even 1. Note that \nat &#39;1&#39; behavior between queues is completely round-robin, which spread \nattention across the widest number of sites but also generally keeps \nmemory caches from getting any benefit from lots of crawling of the same \nsite in rapid succession.)\n\nLots of other refinements are possible, with advanced configuration \ntweaks (including some, like custom &#39;queue precedence policies&#39;, that \ntheoretically should work but are so seldom used they&#39;re not \nbattle-tested). But just that change biases things a lot more towards \nhost diversity early, rather than eventually.\n\nSeparately, the &#39;BroadScope&#39; and other ____Scope classes (other than \nDecidingScope) are discouraged; if you make a DecidingScope which starts \nwith an AcceptDecideRule, then focus the other rules on knocking out \nthings you don&#39;t want, you&#39;ll have a more modern (and H3-ready) \nbroadly-scoped crawl.\n\n- Gordon @ IA\n\nOn 7/13/11 3:03 PM, helloitsmaxine wrote:\n&gt; I&#39;m doing a crawl and interested in maximum the number of unique\n&gt; domains crawled, ie. I&#39;d rather crawl one page from each of 5 unique\n&gt; domains than have 10 pages from one domain. Right now it&#39;s weird\n&gt; because I have pretty standard/default settings, ie. BroadScope and\n&gt; high max hops and such, but out of ~50gb I&#39;ve crawled, less than 1000\n&gt; unique domains have been produced. I&#39;m counting by counting the\n&gt; number of folders in the mirror folder (each of which seems to\n&gt; represent the content from one unique domain), as I&#39;m using the\n&gt; MirrorWriter.\n&gt;\n&gt; My question is if anyone knows what the problem could be and how to\n&gt; fix it? I was wondering if there were possible a maximum bytes per\n&gt; domain limit to set (I know there&#39;s an overall max bytes but that\n&gt; wouldn&#39;t seem to help) or some other settings that could help me get\n&gt; more domains per amount of space crawled?\n&gt;\n&gt; Thanks for any help!\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}