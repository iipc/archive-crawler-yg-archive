{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"k2uTOiBVbW4p2Z3AstXizkwb41zAAKPuR5dUk69RhZNXIU3VrCVBEmW2m7MBeCu2Lg6-blzqYPKKS-xZmzrhhovpYMMwAVY","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re-discover links","postDate":"1301607693","msgId":7076,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREOTRGNTBELjkwMDAwMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDI3MTM0NS43NjcxMC5xbUB3ZWIxNTkwNS5tYWlsLmNuYi55YWhvby5jb20+","referencesHeader":"PDI3MTM0NS43NjcxMC5xbUB3ZWIxNTkwNS5tYWlsLmNuYi55YWhvby5jb20+"},"prevInTopic":7071,"nextInTopic":7103,"prevInTime":7075,"nextInTime":7077,"topicId":7044,"numMessagesInTopic":11,"msgSnippet":"The Heritrix mechanisms for defining rules for which URIs are of interest or not are all part of the Scope mechanism and DecideRules . The rules are usually","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 55605 invoked from network); 31 Mar 2011 21:41:35 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m14.grp.re1.yahoo.com with QMQP; 31 Mar 2011 21:41:35 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta2.grp.re1.yahoo.com with SMTP; 31 Mar 2011 21:41:35 -0000\r\nX-Received: (qmail 98420 invoked by uid 0); 31 Mar 2011 21:41:34 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay00.pair.com with SMTP; 31 Mar 2011 21:41:34 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4D94F50D.9000002@...&gt;\r\nDate: Thu, 31 Mar 2011 14:41:33 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.15) Gecko/20110303 Thunderbird/3.1.9\r\nMIME-Version: 1.0\r\nTo: HONGYING YI &lt;y195322@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;271345.76710.qm@...&gt;\r\nIn-Reply-To: &lt;271345.76710.qm@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re-discover links\r\nX-Yahoo-Group-Post: member; u=137285340; y=oxkQ_qRSfvflCPxPtX2GhsxAwQDE8ZiLwkoa9KHaMZVR\r\nX-Yahoo-Profile: gojomo\r\n\r\nThe Heritrix mechanisms for defining rules for which URIs are of \ninterest or not are all part of the &quot;Scope&quot; mechanism and &quot;DecideRules&quot;.\n\nThe rules are usually applied only to determine whether a URI should be \noffered to the &quot;Frontier&quot; of URIs waiting to be crawled. (They can also \nbe applied in a few other places to enable/disable certain processing \nsteps.)\n\nEven when we set rules to prevent certain URIs from being crawled, we \nwant a log of their discovery in the WARC &#39;metadata&#39; record. (This is \nvaluable for later analysis, and often for determining what future \nexpansions of defined scope would be suitable.) So there is no \nmechanism, nor do I foresee one, that would limit which outlinks are \nsaved as metadata. I suggest you write your apply your own filters as a \nlater step outside Heritrix (perhaps reusing some of its code as model, \nif it&#39;s helpful).\n\n- Gordon @ IA\n\n\nOn 3/28/11 4:32 PM, HONGYING YI wrote:\n&gt; Hi Gordon,\n&gt; Thanks a lot! I was able to get all the outlinks for each page on some\n&gt; domians following your suggestions.\n&gt; For my analysis, I also want to:\n&gt; (1) exclude URLs which do not belong to specified domains, e.g.\n&gt; &#39;yahoo.com&#39;;\n&gt; (2) excluded URLs containing a &#39;?&#39;;\n&gt; (3) remove .js files or other URLs which do not represent hypertext\n&gt; content (e.g., images, postscript and other data types), as they\n&gt; represent leaf nodeswithout outlinks.\n&gt; I know I can do some parsing of the meta data to get rid of those URLs,\n&gt; but I&#39;m wondering whether there are any configuration settings to\n&gt; exclude those URLs in Heritrix?\n&gt; Thanks again!\n&gt;\n&gt;\n&gt; --- *11年3月19日，周六, Gordon Mohr /&lt;gojomo@...&gt;/* 写道：\n&gt;\n&gt;\n&gt;     发件人: Gordon Mohr &lt;gojomo@...&gt;\n&gt;     主题: Re: [archive-crawler] Re-discover links\n&gt;     收件人: &quot;HONGYING YI&quot; &lt;y195322@...&gt;\n&gt;     抄送: archive-crawler@yahoogroups.com\n&gt;     日期: 2011年3月19日,周六,上午10:14\n&gt;\n&gt;     Both Heritrix 1.14.4 and Heritrix 3.0.0 include the WARCWriterProcessor\n&gt;     as an alternative way to write crawled content.\n&gt;\n&gt;     in both versions, if you use the WARCWriterProcessor, then &#39;metadata&#39;\n&gt;     records that include outlinks will be written alongside the other\n&gt;     content.\n&gt;\n&gt;     So you don&#39;t need to upgrade just to get the outlinks written to a file\n&gt;     during crawling.\n&gt;\n&gt;     If having the outlinks listed in the WARC file is not enough, then you\n&gt;     can write your own code that processes the oulinks in some other way. I\n&gt;     think adding your own modules is a little easier in Heritrix 3, but is\n&gt;     just as possible in Heritrix 1.14.4.\n&gt;\n&gt;     - Gordon @ IA\n&gt;\n&gt;     On 3/9/11 1:51 PM, HONGYING YI wrote:\n&gt;      &gt; Hi Gordon,\n&gt;      &gt; Thanks for your informative reply!\n&gt;      &gt; I am using Heritrix 1.14.4 now. For WARCWriterProcessor, if I\n&gt;     still use\n&gt;      &gt; Heritrix 1.14.4, it seems that I will need to make some changes\n&gt;     in the\n&gt;      &gt; source codes. If I upgrade it to Heritrix 3, then I can get all the\n&gt;      &gt; outlinks in the data without making changes to the source codes.\n&gt;     Is that\n&gt;      &gt; correct?\n&gt;      &gt; Thanks!\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt; --- *11年3月3日，周四, Gordon Mohr /&lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;/*\n&gt;     写道：\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt; 发件人: Gordon Mohr &lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;\n&gt;      &gt; 主题: Re: [archive-crawler] Re-discover links\n&gt;      &gt; 收件人: &quot;HONGYING YI&quot; &lt;y195322@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;&gt;\n&gt;      &gt; 抄送: archive-crawler@yahoogroups.com\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt; 日期: 2011年3月3日,周四,上午9:35\n&gt;      &gt;\n&gt;      &gt; On 3/2/11 3:47 PM, HONGYING YI wrote:\n&gt;      &gt; &gt; Hi Gordon,\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; Thanks for your reply!\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; I&#39;m still a little confused. Are all the outlinks of a URI logged\n&gt;      &gt; in the\n&gt;      &gt; &gt; crawl.log whether or not those outlinks have already been\n&gt;     discovered?\n&gt;      &gt;\n&gt;      &gt; Outlinks (specifically outlinks) are not logged to crawl.log. The\n&gt;      &gt; crawl.log is a log of *completed* URIs. So to appear in crawl.log, a\n&gt;      &gt; URI has to:\n&gt;      &gt;\n&gt;      &gt; (1) be discovered (as a seed or outlink from elsewhere); then...\n&gt;      &gt; (2) pass scoping rules, so that it is enqueued to be fetched; then...\n&gt;      &gt; (3) either be successfully fetched, or suffer definitive failure\n&gt;      &gt; (certain errors from the server or no-success after many retries).\n&gt;      &gt;\n&gt;      &gt; This crawl.log logging happens arbitrarily long after the URI was\n&gt;      &gt; first discovered. (The URI might be waiting in a queue for its\n&gt;      &gt; chance to be crawled for hours, days, weeks, months.)\n&gt;      &gt;\n&gt;      &gt; In a typical snapshot crawl, most normal URIs are only fetched once,\n&gt;      &gt; so they only appear in the crawl.log once. (The &#39;via&#39; URI that is\n&gt;      &gt; shown later on the crawl.log line is the one other URI from which it\n&gt;      &gt; was specifically discovered first, even though it may have later\n&gt;      &gt; been found many times.)\n&gt;      &gt;\n&gt;      &gt; (URIs which must be retried to refresh our knowledge, like DNS and\n&gt;      &gt; /robots.txt, will be fetched and appear in crawl.log multiple times.\n&gt;      &gt; Also, there are certain non-standard ways you can force a URI to be\n&gt;      &gt; refetched. But you wouldn&#39;t use those mechanisms to achieve what I\n&gt;      &gt; believe is your goal, discovering all the links to a page.)\n&gt;      &gt;\n&gt;      &gt; &gt; I&#39;m not familiar with WARCWriterProcessor, could you explain more\n&gt;      &gt; on that?\n&gt;      &gt;\n&gt;      &gt; In Heritrix 1.x releases, the default way to write web content was\n&gt;      &gt; the ARCWriterProcessor, which wrote HTTP responses to long\n&gt;      &gt; transcript files called ARCs (with extension .arc.gz). Since only\n&gt;      &gt; the raw content was written, to get the outlinks again at a later\n&gt;      &gt; time, you&#39;d need to parse the HTML again.\n&gt;      &gt;\n&gt;      &gt; In Heritrix 3, a new &#39;WARC&#39; format is preferred, via the\n&gt;      &gt; WARCWriterProcessor. While similar to the ARC format it has more\n&gt;      &gt; places for extra metadata. By default, it writes not just the HTTP\n&gt;      &gt; response (like in ARC) but also the exact HTTP request and an extra\n&gt;      &gt; &#39;metadata&#39; record with a list of the outlinks that were discovered.\n&gt;      &gt; Outlinks are listed here even if they did not pass scope-testing (so\n&gt;      &gt; would not be enqueued/fetched and thus would never appear in the\n&gt;      &gt; crawl.log).\n&gt;      &gt;\n&gt;      &gt; You can use WARCWriterProcessor in H1, also, by replacing the\n&gt;      &gt; reference to ARCWriterProcessor with WARCWriterProcessor.\n&gt;      &gt;\n&gt;      &gt; Even if you don&#39;t want to use WARCWriterProcessor, you could look at\n&gt;      &gt; its source code to see how it looks at the list of discovered\n&gt;      &gt; outlinks, and log them to someplace of your choosing, by writing\n&gt;      &gt; your own custom Processor code to plug into Heritrix.\n&gt;      &gt;\n&gt;      &gt; Hope this helps,\n&gt;      &gt;\n&gt;      &gt; - Gordon @ IA\n&gt;      &gt;\n&gt;      &gt; &gt; Thanks!\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; --- *11年3月1日，周二, Gordon Mohr /&lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;/*\n&gt;      &gt; 写道：\n&gt;      &gt; &gt;\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; 发件人: Gordon Mohr &lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;\n&gt;      &gt; &gt; 主题: Re: [archive-crawler] Re-discover links\n&gt;      &gt; &gt; 收件人: &quot;HONGYING YI&quot; &lt;y195322@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;&gt;\n&gt;      &gt; &gt; 抄送: archive-crawler@yahoogroups.com\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt;\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt; &gt; 日期: 2011年3月1日,周二,上午7:40\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; All the outlinks discovered from a URI are inside the CrawlURI\n&gt;     during\n&gt;      &gt; &gt; its processing, until it is finished (and logged in the crawl.log).\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; If you use the WARCWriterProcessor, they are part of the data\n&gt;      &gt; &gt; written in\n&gt;      &gt; &gt; the &#39;metadata&#39; record associated with the URI.\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; Alternatively, you could insert your own custom processor to\n&gt;     log this\n&gt;      &gt; &gt; data somewhere else (using the WARCWriterProcessor&#39;s writeMetadata\n&gt;      &gt; &gt; method as a model).\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; In this way, you can get the link data you want, without\n&gt;      &gt; revisiting the\n&gt;      &gt; &gt; same URLs many times (indeed infinitely via link-loops!).\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; - Gordon @ IA\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; On 2/28/11 3:16 PM, HONGYING YI wrote:\n&gt;      &gt; &gt; &gt; Hi Gordon,\n&gt;      &gt; &gt; &gt; We want to know all the out-going links and in-coming links for\n&gt;      &gt; each\n&gt;      &gt; &gt; &gt; page, and in the example, for aol.com, aolhealth.com is an\n&gt;      &gt; out-going\n&gt;      &gt; &gt; &gt; link for aol.com, and drugchecker.aolhealth.com is an in-coming\n&gt;      &gt; &gt; link for\n&gt;      &gt; &gt; &gt; aol.com. If the link from drugchecker.aolhealth.com to aol.com\n&gt;      &gt; &gt; does not\n&gt;      &gt; &gt; &gt; show up, we will not get all the in-coming links for aol.com. Is\n&gt;      &gt; &gt; there\n&gt;      &gt; &gt; &gt; any way to retrieve that link? Can any configuration setting\n&gt;     in the\n&gt;      &gt; &gt; &gt; crawl get round that?\n&gt;      &gt; &gt; &gt; Thanks!\n&gt;      &gt; &gt; &gt; Yang\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; --- *11年3月1日，周二, Gordon Mohr /&lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &gt;\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;/*\n&gt;      &gt; &gt; 写道：\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; 发件人: Gordon Mohr &lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;\n&gt;      &gt; &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;\n&gt;      &gt; &gt; &gt; 主题: Re: [archive-crawler] Re-discover links\n&gt;      &gt; &gt; &gt; 收件人: archive-crawler@yahoogroups.com\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt;\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt; &gt;\n&gt;      &gt;\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt; &gt; &gt; 抄送: &quot;y195322&quot; &lt;y195322@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;\n&gt;      &gt; &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;\n&gt;      &gt; &gt;\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;&gt;\n&gt;      &gt; &gt; &gt; 日期: 2011年3月1日,周二,上午6:33\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; On 2/28/11 10:20 AM, y195322 wrote:\n&gt;      &gt; &gt; &gt; &gt; Hi!We are doing web crawling using Heritrix, and found one\n&gt;     issue\n&gt;      &gt; &gt; &gt; in that once a link has been &#39;discovered,&#39; it won&#39;t be\n&gt;      &gt; &gt; &gt; &#39;rediscovered.&#39; This means that if we have something like:\n&gt;      &gt; &gt; &gt; &gt; aol.com-&gt;aolhealth.com\n&gt;      &gt; &gt; &gt; &gt; aolhealth.com-&gt;drugchecker.aolhealth.com\n&gt;      &gt; &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; &gt; and if drugchecker.aolhealth.com links to aol.com, that\n&gt;     link will\n&gt;      &gt; &gt; &gt; not show up in the file.\n&gt;      &gt; &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; &gt; Is there any way to fix this problem? Thanks!\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; Usually a single snapshot crawl only wants to retrieve each URL\n&gt;      &gt; &gt; &gt; once, so\n&gt;      &gt; &gt; &gt; this is by-design, rather than a problem.\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; Why do you want to fetch aol.com twice in a single crawl? (Or\n&gt;     am I\n&gt;      &gt; &gt; &gt; misunderstanding?)\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt; - Gordon @ IA\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt; &gt;\n&gt;      &gt; &gt;\n&gt;      &gt; &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt;\n\n"}}