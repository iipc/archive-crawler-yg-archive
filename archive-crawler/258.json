{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"G5howMQ62-HOIqfNTUlmzGRbKn7KnXXoiQtoXUviC698VkCBipd2ggsI-KzyGiwESk9Q7iHEWWbLYj7BLtnvV-ef0YnNWik","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix Checkpointing High-Level Design","postDate":"1074887467","msgId":258,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwMTE3QjJCLjUwNDAwQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDQwMTA4NTY5LjYwNjAxQGFyY2hpdmUub3JnPg==","referencesHeader":"PDQwMERBMjRELjMwMjA0MDNAYXJjaGl2ZS5vcmc+IDw0MDEwODU2OS42MDYwMUBhcmNoaXZlLm9yZz4="},"prevInTopic":257,"nextInTopic":259,"prevInTime":257,"nextInTime":259,"topicId":244,"numMessagesInTopic":13,"msgSnippet":"... I think at worst, it s the number of URIs in process at each checkpoint-request moment. Consider a worst-case quick and sloppy checkpoint strategy: at","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 23412 invoked from network); 23 Jan 2004 19:51:09 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m9.grp.scd.yahoo.com with QMQP; 23 Jan 2004 19:51:09 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 23 Jan 2004 19:51:12 -0000\r\nReceived: (qmail 31382 invoked by uid 100); 23 Jan 2004 19:48:52 -0000\r\nReceived: from b116-dyn-43.archive.org (HELO archive.org) (gojomo@...@209.237.240.43)\n  by ia14404.archive.org with SMTP; 23 Jan 2004 19:48:52 -0000\r\nMessage-ID: &lt;40117B2B.50400@...&gt;\r\nDate: Fri, 23 Jan 2004 11:51:07 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;400DA24D.3020403@...&gt; &lt;40108569.60601@...&gt;\r\nIn-Reply-To: &lt;40108569.60601@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-4.3 required=6.0 tests=AWL,BAYES_00 autolearn=ham \n\tversion=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix Checkpointing High-Level Design\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nMichael Stack wrote:\n &gt;&gt; - Erring slightly on the side of duplication/revisitation,\n &gt;&gt;   in logs or ARC files, is acceptable for our purposes,\n &gt;&gt;   if it otherwise simplifies or accelerates the process.\n &gt;\n &gt; Can we quantify what would be acceptable duplication?\n\nI think at worst, it&#39;s the number of URIs in process at\neach checkpoint-request moment.\n\nConsider a worst-case &quot;quick and sloppy&quot; checkpoint strategy:\nat the moment a checkpoint is requested, it stops any URIs from\nentering or leaving the Frontier. A number of URIs, up to the\nnumber of ToeThreads, would in process -- but we wouldn&#39;t wait\nfor them. We&#39;d checkpoint the Frontier, and include the in-process\nURIs in the checkpointed Frontier as if they&#39;d never started.\nThen we&#39;d checkpoint each other component as soon as it is\nindividually possible to do so. While we&#39;re doing this, the\nin-process URIs are still progressing, so each component may\nbe checkpointed with URIs at a different stage.\n\nIf we then resume from that checkpoint, all of those\nin-process URIs will be processed again. They might then appear\ntwice in some output/statistics. But in a crawl of millions of\nURIs, where each checkpoint could (at worst) cause a few\nhundred URIs to be double-processed, the overall effect remains\nnegligible.\n\nWe&#39;re not in fact planning to be that sloppy, but if in\na difficult situation (like hung threads), we have to\ncompromise in that direction, I think that&#39;s &#39;acceptable&#39;\nduplication.\n\n &gt;&gt; - When an individual worker ToeThread hangs or\n &gt;&gt;   spins indefinitely in an uninterruptible manner,\n &gt;&gt;   we will accept whatever small inconsistency\n &gt;&gt;   might be introduced by ignoring its status and\n &gt;&gt;   going forward with a checkpoint. (For example,\n &gt;&gt;   if a buggy Processor at any point in the chain\n &gt;&gt;   never completes, checkpointing should still\n &gt;&gt;   be possible, the problem-causing URI should be\n &gt;&gt;   noted, and our data may suffer side effects of its\n &gt;&gt;   partial processing.)\n &gt;\n &gt; Do we know how to identify a spinning uninterruptable thread\n &gt; programmatically or is this something a human does?  Will checkpointing\n &gt; be able to proceed though a thread is unreachable (spinning,\n &gt; uninterruptable)?  How will it know to skip the problematic thread?\n\nI think it can be automatic or manual: sutomatic, after a certain\nconfigurable amount of time has passed, or manual, when an operator\nloses patience and says, &quot;force it&quot;.\n\nFor example, once a checkpoint is requested it may be possible\nto checkpoint immediately (for example if the crawler was already\n&#39;paused&#39; previously). Usually, though, some processing will need\nto complete before the crawler is in a consistent state, and so\nthe checkpoint process may wait up to (say) 5 minutes to get the\ncrawl-consistency exclusive lock.\n\nIf the lock is still unavailable at that point, the checkpoint\nprocess might try more aggressive -- but still consistency-preserving --\nmeasures to shake the lock free from any remaining threads. This\ncould include sending those threads interrupt() calls. However,\neven that might not help, so after (say) another 10 minutes, the\ndecision could be made (by software or operator) to force a checkpoint\nwhich may not be consistent. The problematic in-process URIs would be\nspecially noted, and everything that can be checkpointed will. (There\nstill would be a chance of deadlock if one of the &#39;hung threads&#39; holds\nsynchronization locks that are needed by any of the prepare()/commit()\nCheckpointable methods, but such situations can be minimized or\neliminated if we&#39;re willing to accept the risk of inconsistency.)\n\nThe above implies the SharedExclusiveLock.acquireExclusive() method\nhas some form allowing timeouts, which I have not shown previously.\n\nThis will lead to checkpointing an inconsistent state, but I this\ninconsistency may be inconsequential (or manually fixable) and\nthus preferable to the alternative of losing all work since the\nlast checkpoint.\n\n &gt;&gt; - Logs begin in new files upon each checkpoint; to\n &gt;&gt;   see the log over the whole crawl, the files must\n &gt;&gt;   be concatenated. (This makes checkpointing easier\n &gt;&gt;   but some of the log-viewing admin UI harder.) The\n &gt;&gt;   Mercator approach is to name the current logs\n &gt;&gt;   LOGNAME.tmp, then rename them LOGNAME.00001, etc.,\n &gt;&gt;   after each checkpoint, and we will adopt this\n &gt;&gt;   same convention.\n &gt;\n &gt; So all logs for a crawl will be available under the logging directory?\n &gt; We don&#39;t want to rotate them out on a period?  Will the UI be expected\n &gt; to open massive log files (or rather should the UI just show reporting\n &gt; generated off data logged)?\n\nYes, I believe all the logs should collect in the logging directory\nuntil there is a pressing need to move them. Then, we might want to\nimplement -- inside or outside the crawler -- a rotate-away capacity.\n\nSuch rotation out to another volume shouldn&#39;t have much effect\non checkpointing. The UI&#39;s facilities for live-log browsing would\nhave to be made tolerant of such log disappearances.\n\n &gt;&gt;When user wants to resume from a checkpoint, they can\n &gt;&gt;browse a list of all known checkpoints (or point crawler\n &gt;&gt;to a previously unknown checkpoint). Checkpoint is\n &gt;&gt;first loaded in &#39;paused&#39; mode, allowing state to be\n &gt;&gt;viewed and paramters to be tuned. Then, crawl can be\n &gt;&gt;resumed on request.\n &gt;\n &gt; At what points during crawling can this above operation be done?  At any\n &gt; point?  Or just at pause after startup?  Will it be possible for\n &gt; operator to load a checkpoint while crawler is running (And if so, how\n &gt; does this work)?\n\nThe software should be in a &quot;clear&quot; state before a resume-\nfrom-checkpoint is attempted: no active crawling, no\npaused crawl state (or at least no paused crawl state you\ncare to save). This could be immediately after launch, or\nit could be after some other crawling is paused/saved/cleared.\n\nLoading a checkpoint should usually bring the crawler to a\n&quot;crawl paused&quot; state that is effectively a replica of the\nstate at the time of the checkpoint. Ideally, you could load\na checkpoint, use the software admin UI to examine it to see\nif it really is the one you&#39;d like to resume, perhaps repeat\nthis several times, tinker with some settings, and then hit\n&#39;begin&#39; to pick up where it left off.\n\n &gt;&gt;The checkpoint begins: each notable component of\n &gt;&gt;the system -- implementers of the Checkpointable\n &gt;&gt;interface -- are sent the prepare(checkpointNumber,\n &gt;&gt;storeDirectory) message. As necessary, they pass\n &gt;&gt;this to their subcomponents.\n &gt;\n &gt; How will you find all implementers of the Checkpointable interface?  Can\n &gt; any old POJO implement Checkpointable or is it only processors?\n\nThe CrawlController (which implements Checkpointable) is sent the\nrelevant messages. It propagates these to all components it\nbelieves needs checkpointing. Those may further propagate the\nmessages. Any POJO can implement the interface, but that&#39;s no\nguarantee it will be called; there&#39;s got to be a chain of\nintentional calls from the CrawlController down.\n\n &gt; (Is there a facilty for pausing a crawl or stopping a crawl?  If so, how\n &gt; is that done?)\n\nThe existing pause/terminate facility will be touched up as necessary;\nI believe it currently sets a flag indicating that a pause or termination\nhas been requested, and lets the CrawlController control thread react.\n\n &gt; Is it completely up to the Checkpointable implementer how they checkpoint?\n\nYes, though by convention, they should ensure all their state goes\ninto the designated checkpoint directory.\n\n &gt;&gt;Generally, the checkpointing of an object involves:\n &gt;&gt; (1) Writing its important in-memory state to\n &gt;&gt;     one or more files.\n &gt;&gt;\n &gt;&gt; (2) Duplicating any on-disk state to the checkpoint\n &gt;&gt;     directory. (In some cases, this may be possible\n &gt;&gt;     with filesystem hard-links rather than actual\n &gt;&gt;     copies.)\n &gt;&gt;\n &gt;\n &gt; Do you have examples of the above to illustrate how it would work?\n\nA trivial example of (1) would be the ARC writer knowing what\nsequence-number to assign the next ARC file to begin. When asked\nto checkpoint itself, it would write that bit of state to a file.\nSimilarly, any module collecting a in-memory histogram of\ninteresting resource features would dump its current data in\na recoverable fashion to a file.\n\nFor (2), an example would be the Frontier&#39;s overall pending queue\nor per-host queues. These might already substantially be on disk,\nwith a small amount in memory. I believe our existing disk-backed\nQueues can be quickly checkpointed into three files with a minimum\nof disk writing by:\n    (a) creating hard links to the up-to-2 constituent backing\n        disk files (&quot;flip files&quot;)\n    (b) writing a third file which contains the current lengths of\n        those backing files, the current pointer to the &#39;head&#39; entry\n        in one of those files, and the portions of the queue which\n        live in memory.\nAs activity continues after a checkpoint, the &quot;flip files&quot; grow\nbut are never overwritten, just discarded when their contents are\nno longer needed. Thus, the hard links will keep the checkpoint\ndata (as well as some extra cruft) alive under a different filename.\nWhen a resume becomes necessary the relevant excerpts of the files\nwill be restored.\n\n &gt;&gt;To resume from a checkpoint, the CrawlController would\n &gt;&gt;receive a resume-request with an origin directory. It\n &gt;&gt;would reconstitute the parts of the crawl, primarily by\n &gt;&gt;constructing new instances which read their state from\n &gt;&gt;the origin directory, copying data as necessary to the\n &gt;&gt;&quot;running&quot; disk space. (A resume should not alter the\n &gt;&gt;stored checkpoint in any way.)\n &gt;\n &gt; The origin directory points at the checkpoint we want to resume from?\n\nYes.\n\n &gt;&gt;CHECKPOINT FRAMEWORK\n &gt;&gt;\n &gt;&gt;Each ToeThread wraps its per-URI processing with:\n &gt;&gt;\n &gt;&gt;   crawlLock.acquireShared(); // crawlLock is a shared-exclusive\n &gt;&gt;                              // (AKA &#39;readwrite&#39;) lock\n &gt;&gt;   // all processing\n &gt;&gt;   crawlLock.releaseShared();\n &gt;&gt;\n &gt;&gt;(This lock may be refined later to leave out early\n &gt;&gt;processing stages, possibly up through fetching,\n &gt;&gt;which can be harmlessly considered to never have\n &gt;&gt;begun.)\n &gt;&gt;\n &gt;&gt;The CrawlController controlThread, when it detects\n &gt;&gt;a checkpoint has been requested, runs a checkpoint\n &gt;&gt;rountine which is roughly:\n &gt;&gt;\n &gt;&gt;   crawlLock.acquireExclusive();\n &gt;&gt;   versionId++;\n &gt;&gt;   prepare(versionId, checkpointDirectory); // actually does the checkpointing, passing\n &gt;&gt;                                            // prepare() calls to subcomponents\n &gt;&gt;   commit(versionId, checkpointDirectory);  // marks the checkpoint as complete, cleans up\n &gt;&gt;\n &gt;&gt;\n &gt;&gt;   crawlLock.releaseExclusive();\n &gt;\n &gt; Are you missing a resume here?\n\nMissing a resume() example, yes -- but in the checkpointing itself, no\nresume() is necessary: the prepare() and commit() do not destroy any\nof the crawl state.\n\nResuming is actually much like starting a crawl for the first time,\na close analogue to CrawlController.initialize() and\nCrawlController.startCrawl(), but I&#39;m not sure yet how the details will\nwork out.\n\nGreat questions! Keep them coming.\n\n- Gordon\n\n"}}