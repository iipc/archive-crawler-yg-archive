{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"david_pane1","from":"&quot;david_pane1&quot; &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"IERAywuyjnoRahISQ1m5yyakPnlFV5ltoGd8SZAncB9pjx5IUoRRTNQa9yjRGHUSl6po-fjsWqxp5pZqXOvOgKrJFCCLNFE","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: questions before we restart the crawl","postDate":"1327602677","msgId":7573,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpmczY1bCs5cm43QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRGMUQxNjVBLjgwNzA1MDFAZ21haWwuY29tPg=="},"prevInTopic":7569,"nextInTopic":7574,"prevInTime":7572,"nextInTime":7574,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"Kenji, Are you saying that you can get 25M pages per day on 100 threads and 1 instance or 25M URIs/day?  Do you capture all images, pdfs, and supporting page","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 71982 invoked from network); 26 Jan 2012 18:31:19 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m13.grp.sp2.yahoo.com with QMQP; 26 Jan 2012 18:31:19 -0000\r\nX-Received: from unknown (HELO ng19-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.168)\n  by mta4.grp.sp2.yahoo.com with SMTP; 26 Jan 2012 18:31:19 -0000\r\nX-Received: from [98.139.164.126] by ng19.bullet.mail.bf1.yahoo.com with NNFMP; 26 Jan 2012 18:31:18 -0000\r\nX-Received: from [69.147.65.151] by tg7.bullet.mail.bf1.yahoo.com with NNFMP; 26 Jan 2012 18:31:18 -0000\r\nX-Received: from [98.137.34.73] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 26 Jan 2012 18:31:18 -0000\r\nDate: Thu, 26 Jan 2012 18:31:17 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jfs65l+9rn7@...&gt;\r\nIn-Reply-To: &lt;4F1D165A.8070501@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;david_pane1&quot; &lt;dpane@...&gt;\r\nSubject: Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=500983475; y=kc3tVJgUaTP2yXvWYlUyYnMcZPeXV2BU9KFQt15Sv0pFH9PIrbyYVQ\r\nX-Yahoo-Profile: david_pane1\r\n\r\nKenji,\n\nAre you saying that you can get 25M pages per day on 100 threads an=\r\nd 1 instance or 25M URIs/day?  Do you capture all images, pdfs, and support=\r\ning page documents or are you just capturing html pages?\n\nMy test crawls be=\r\nfore running our large crawl showed significant increase in the number of p=\r\nages captured when we increased the number of threads.\n\n--David\n\n--- In arc=\r\nhive-crawler@yahoogroups.com, Kenji Nagahashi &lt;knagahashi@...&gt; wrote:\n&gt;\n&gt; H=\r\ni,\n&gt; \n&gt; May be a bit off-topic, but 25M/day with 5 machine is average 58/s =\r\nper \n&gt; machine. Since I know Heritrix-3 can crawl at this speed with just 1=\r\n00 \n&gt; ToeThreads, I wonder if most of your 1200 ToeThreads are idle.\n&gt; \n&gt; W=\r\nhile why you don&#39;t get much higher speed with 1200 threads is a big \n&gt; ques=\r\ntion, it may make sense to cut down the number of ToeThreads if \n&gt; you&#39;re o=\r\nkay with current crawl speed. Less threads will make H3 less \n&gt; susceptible=\r\n to memory problems... Just a thought.\n&gt; \n&gt; --Kenji\n&gt; \n&gt; (1/20/12 9:54 PM),=\r\n David Pane wrote:\n&gt; &gt; Gordon,\n&gt; &gt;\n&gt; &gt; Thank you for your response. And I a=\r\nm sorry for the overwhelming amount\n&gt; &gt; of information...I think I am a lit=\r\ntle overwhelmed.... and feeling the\n&gt; &gt; pressure.\n&gt; &gt;\n&gt; &gt; 1) Our Bloom filt=\r\ner configuration:\n&gt; &gt;\n&gt; &gt; &lt;bean id=3D&quot;uriUniqFilter&quot;\n&gt; &gt; class=3D&quot;org.archi=\r\nve.crawler.util.BloomUriUniqFilter&quot;&gt;\n&gt; &gt; &lt;property name=3D&quot;bloomFilter&quot;&gt;\n&gt; =\r\n&gt; &lt;bean class=3D&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&gt; &gt; &lt;constructor-arg v=\r\nalue=3D&quot;400000000&quot;/&gt;\n&gt; &gt; &lt;constructor-arg value=3D&quot;30&quot;/&gt;\n&gt; &gt; &lt;/bean&gt;\n&gt; &gt; &lt;/=\r\nproperty&gt;\n&gt; &gt; &lt;/bean&gt;\n&gt; &gt;\n&gt; &gt; 2) We are writing the crawl data to a NAS con=\r\nfigured with RAID 6. We did\n&gt; &gt; see some problems with disk errors on the N=\r\nAS earlier in the crawl (late\n&gt; &gt; Dec ). I recently found this out. We were=\r\n/are running in a degraded\n&gt; &gt; raid state - a few of the disks have been re=\r\nplaced and the RAID is being\n&gt; &gt; rebuilt. We didn&#39;t see any block device er=\r\nrors in the logs on the NAS\n&gt; &gt; so the write failures we saw are probably n=\r\not related to the rebuild. We\n&gt; &gt; did see some network hiccups (no outright=\r\n failures) in the logs.\n&gt; &gt; So, this may be the culprit for some of the\n&gt; &gt;=\r\n\n&gt; &gt; 3) Yes, we have been cross-feeding URIs.\n&gt; &gt;\n&gt; &gt; --David\n&gt; &gt;\n&gt; &gt; On 1/=\r\n21/12 12:22 AM, Gordon Mohr wrote:\n&gt; &gt;  &gt; You&#39;ve provided an overwhelming a=\r\nmount of information and we may be\n&gt; &gt;  &gt; dealing with multiple issues, som=\r\ne of which have roots going back\n&gt; &gt;  &gt; earlier than the diagnostic data we=\r\n now have available.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; A few key points of emphasis:\n&gt; &gt;  &gt;\n&gt; &gt;=\r\n  &gt; - we&#39;ve not run crawls with 1200 threads before, or on hardware\n&gt; &gt;  &gt; =\r\nsimilar to yours, so our experience is only vaguely suggestive\n&gt; &gt;  &gt;\n&gt; &gt;  =\r\n&gt; - it&#39;s not the lower thread counts that are the real source of\n&gt; &gt;  &gt; con=\r\ncern; you can even adjust the number of threads mid-crawl. It&#39;s\n&gt; &gt;  &gt; that=\r\n the error that killed the threads almost certainly left a queue\n&gt; &gt;  &gt; in =\r\na &#39;phantom&#39; state where no progress would be made crawling its\n&gt; &gt;  &gt; URIs,=\r\n each time it happened, on each resume leading to the current state.\n&gt; &gt;  &gt;=\r\n\n&gt; &gt;  &gt; - without having understood and fixed whatever software or system\n&gt;=\r\n &gt;  &gt; problems caused the earliest/most-foundational errors in your crawl,\n=\r\n&gt; &gt;  &gt; it&#39;s impossible to say how likely they are to recur.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; W=\r\nith that in mind, I&#39;ll try to provide quick answers to your other\n&gt; &gt;  &gt; qu=\r\nestions...\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; On 1/20/12 4:20 PM, David Pane wrote:\n&gt; &gt;  &gt;&gt;\n&gt; &gt; =\r\n &gt;&gt; We have collected about 550 million pages along with the images and\n&gt; &gt;=\r\n  &gt;&gt; supporting documents on our 5 instance crawl that was started Dec. 23r=\r\nd.\n&gt; &gt;  &gt;&gt; Although we are please with the amount of data we captured to da=\r\nte, we\n&gt; &gt;  &gt;&gt; are very concerned about the state of the Heritrix instances=\r\n. If fact,\n&gt; &gt;  &gt;&gt; we aren&#39;t very confident that the instances will last un=\r\ntil the end of\n&gt; &gt;  &gt;&gt; February. We are now running on a total of over 500 =\r\nless threads than\n&gt; &gt;  &gt;&gt; the configured 1200 threads/instance.\n&gt; &gt;  &gt;&gt;\n&gt; &gt;=\r\n  &gt;&gt; 0 - not running right now.\n&gt; &gt;  &gt;&gt; 1 - running on 1198 ( 2 less)\n&gt; &gt;  =\r\n&gt;&gt; 2 - running on 931 (269 less)\n&gt; &gt;  &gt;&gt; 3 - running on 987 (213 less)\n&gt; &gt; =\r\n &gt;&gt; 4 - running on 1170 (30 less)\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; Since we are seriously co=\r\nnsidering throwing away this past month&#39;s work\n&gt; &gt;  &gt;&gt; and starting over, w=\r\ne would like to pick your brain on some strategies\n&gt; &gt;  &gt;&gt; that will help u=\r\ns avoid getting into this situation again. We were\n&gt; &gt;  &gt;&gt; hoping to be don=\r\ne crawling by the end of February so this restart will\n&gt; &gt;  &gt;&gt; put us behin=\r\nd schedule.\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; 1) Can we continue from here but with &quot;clean&quot; H=\r\neritrix instances?\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; Is there a way that we can continue from=\r\n the this point forward, but\n&gt; &gt;  &gt;&gt; start with Heritrix instances that wil=\r\nl not be corrupt due to sever\n&gt; &gt;  &gt;&gt; error? (e.g. using the\n&gt; &gt;  &gt;&gt; https:=\r\n//webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &gt; &lt;https://web=\r\narchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt; ) If\n&gt; &gt;  &gt;&gt; so, wou=\r\nld you recommend doing this? You mentioned that this could be\n&gt; &gt;  &gt;&gt; time =\r\nconsuming. Each of our instances has downloaded around 170M URIs,\n&gt; &gt;  &gt;&gt; t=\r\nhey have over 700M queued URIs, what is your time estimate for\n&gt; &gt;  &gt;&gt; some=\r\nthing this large?\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; We are willing to sacrifice a few days to=\r\n get our crawler to a clean\n&gt; &gt;  &gt;&gt; state again so we can crawl for another=\r\n 30 days at the pace we have been\n&gt; &gt;  &gt;&gt; crawling.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; You can d=\r\no a big &#39;frontier-recover&#39; log replay to avoid recrawling the\n&gt; &gt;  &gt; same U=\r\nRIs, and approximate the earlier queue state. Splitting/filters\n&gt; &gt;  &gt; the =\r\nlogs manually beforehand as alluded to in the wiki page can speed\n&gt; &gt;  &gt; th=\r\nis process somewhat... but given the size of all your log-segments\n&gt; &gt;  &gt; t=\r\nhat log grooming beforehand is itself likely to be a lengthy process.\n&gt; &gt;  =\r\n&gt;\n&gt; &gt;  &gt; I don&#39;t think we&#39;ve ever done it with logs of 170M crawled / 870M\n=\r\n&gt; &gt;  &gt; discovered before, nor on any hardware comparable to yours. So it&#39;s\n=\r\n&gt; &gt;  &gt; impossible to project its duration in your environment. It&#39;s taken 2=\r\n-3\n&gt; &gt;  &gt; days for us on smaller crawls, slower hardware.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; An =\r\nadded complication is that this older frontier-recover-log replay\n&gt; &gt;  &gt; te=\r\nchnique happens in its own thread separate from the checkpointing\n&gt; &gt;  &gt; pr=\r\nocess, so it is not, itself, accurately checkpointed during the long\n&gt; &gt;  &gt;=\r\n reload process.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; At nearly 1B discovered URIs per node, even =\r\nif you are using the\n&gt; &gt;  &gt; alternate BloomUriUniqFilter, if you are using =\r\nit at its default size\n&gt; &gt;  &gt; (~500MB) it will now be heavily saturated and=\r\n thus returning many\n&gt; &gt;  &gt; false-positives causing truly unique URIs to be=\r\n rejected as\n&gt; &gt;  &gt; duplicates. (If you&#39;re using a significantly larger fil=\r\nter, you may\n&gt; &gt;  &gt; not yet be at a high false-positive rate: you&#39;d have to=\r\n do the bloom\n&gt; &gt;  &gt; filter math. If you&#39;re still using BdbUriUniqFilter, y=\r\nou&#39;re way way\n&gt; &gt;  &gt; past the point where its disk seeks have usually made =\r\nit too slow for\n&gt; &gt;  &gt; our purposes.)\n&gt; &gt;  &gt;\n&gt; &gt;  &gt;&gt; 2) What can be done to=\r\n avoid corrupting the Heritrix instances?\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; - What kind of st=\r\nrategies might we take to keep the crawl error free?\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; - Do y=\r\nou think the SEVER errors that we have seen are deterministic or\n&gt; &gt;  &gt;&gt; ra=\r\nndom (e.g., triggered by occasional flaky network conditions, disks,\n&gt; &gt;  &gt;=\r\n&gt; race conditions, or whatever)?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Hard to say. The main thing =\r\nI could suggest is watch very closely and\n&gt; &gt;  &gt; when a SEVERE error occurs=\r\n, prioritize diagnosing and resolving the\n&gt; &gt;  &gt; cause while the info is fr=\r\nesh.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt;&gt; - Do you believe that we can reliably backup to the pre=\r\nvious checkpoint\n&gt; &gt;  &gt;&gt; if we watch the logs and stop as soon as we see th=\r\ne first SEVER error?\n&gt; &gt;  &gt;&gt; If we do this, do you speculate that the same =\r\nSEVER will occur again?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Resuming from the latest checkpoint b=\r\nefore an error believed to\n&gt; &gt;  &gt; corrupt the on-disk state will be the bes=\r\nt strategy.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; If we never figure out the real cause, but run th=\r\ne same software on\n&gt; &gt;  &gt; the same machine, yes, I expect the same problem =\r\nwill recur!\n&gt; &gt;  &gt;\n&gt; &gt;  &gt;&gt; - Is there any reason why a Heritrix instance th=\r\nat is run while binded\n&gt; &gt;  &gt;&gt; to one ip address can&#39;t be resumed binded to=\r\n a different ip address?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Only the web UI to my knowledge bind=\r\ns to a chosen address, and it is\n&gt; &gt;  &gt; common to have it bind to all. I do=\r\nn&#39;t expect the outbound requests\n&gt; &gt;  &gt; would be hurt by a machine changing=\r\n its IP address while the crawl was\n&gt; &gt;  &gt; running, but I would run a test =\r\nto be sure if that was an important,\n&gt; &gt;  &gt; expected transition.\n&gt; &gt;  &gt;\n&gt; &gt;=\r\n  &gt;&gt; 3) Should we configure the crawler with more instances and switch\n&gt; &gt; =\r\n &gt;&gt; between them?\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; We have seen that we can run a single ins=\r\ntance to 100M pages +\n&gt; &gt;  &gt;&gt; supporting images and documents. Perhaps this=\r\n means that we need 10 or\n&gt; &gt;  &gt;&gt; more instances instead of 5. That raises =\r\nthe possibility of running 2\n&gt; &gt;  &gt;&gt; instances per machine. If we could run=\r\n 2, or even 4, instances on a\n&gt; &gt;  &gt;&gt; single machine, they would each run h=\r\nalf as long.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; I don&#39;t think the problems as reported are speci=\r\nfically due to one\n&gt; &gt;  &gt; node&#39;s progress growing beyond a certain size, bu=\r\nt it might be the\n&gt; &gt;  &gt; case that giant instances are more likely to suffe=\r\nr from, and harder\n&gt; &gt;  &gt; to recover from, single glitches (eg a single dis=\r\nk error). On the\n&gt; &gt;  &gt; other hand, many instances introduce more redundant=\r\n overhead costs\n&gt; &gt;  &gt; (certain data structures, cross-feeding URIs if you&#39;=\r\nre doing that, etc.).\n&gt; &gt;  &gt;\n&gt; &gt;  &gt;&gt; - Can you suggest a way to start/stop =\r\ninstances from a script so we can\n&gt; &gt;  &gt;&gt; change between instances automati=\r\ncally?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Not a mode I&#39;ve thought much about.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt;&gt; - H=\r\nave you seen frequent starting / stopping of instances introduce\n&gt; &gt;  &gt;&gt; in=\r\nstability?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; No... but it might make you notice latent issues s=\r\nooner.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt;&gt; 4) Crawl slows but restarting seems to improve the sp=\r\need again.\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; We noticed that the all of our instances would i=\r\nnitially run at a fast\n&gt; &gt;  &gt;&gt; pace. We would collect an average of 25M + p=\r\nages/day for 2-3 days and\n&gt; &gt;  &gt;&gt; then the crawl would slow down to 10M pag=\r\nes/day over the next few days.\n&gt; &gt;  &gt;&gt; (these numbers are totals of all 5 i=\r\nnstances combined). When we\n&gt; &gt;  &gt;&gt; restarted the instances, the average pa=\r\nges would improve back to 25M +\n&gt; &gt;  &gt;&gt; pages/day. The total crawled number=\r\ns (TiB) also reflected the slow down.\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; - Is this something t=\r\nhat others have experienced as well?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; I don&#39;t recall hearing o=\r\nther reports of speed boosts after\n&gt; &gt;  &gt; checkpoint-resumes but others may=\r\n have more experience.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt;&gt; 5) We are capturing tweets from twitt=\r\ner, harvesting the urls and want to\n&gt; &gt;  &gt;&gt; crawl those urls within 1 day o=\r\nf receiving the tweet. Can you recommend\n&gt; &gt;  &gt;&gt; a strategy for doing this =\r\nwith the 5 instances we are running?\n&gt; &gt;  &gt;&gt;\n&gt; &gt;  &gt;&gt; - Do we need to run a =\r\nseparate crawler dedicated to this? If so, can you\n&gt; &gt;  &gt;&gt; suggest a way to=\r\n crawl out from the tweeted urls but when we get\n&gt; &gt;  &gt;&gt; additional urls fr=\r\nom the tweets, quickly change focus to these urls\n&gt; &gt;  &gt;&gt; instead of the on=\r\nes branching out. When adding urls as seeds, can you\n&gt; &gt;  &gt;&gt; set a high pri=\r\nority to crawl those before the discovered urls? Do you\n&gt; &gt;  &gt;&gt; recommend m=\r\naybe setting up a specific crawl for these urls and then only\n&gt; &gt;  &gt;&gt; crawl=\r\n a few hopes from the seeds - injecting the urls from the tweets as\n&gt; &gt;  &gt;&gt;=\r\n seeds?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Dedicating a special script or crawler to URIs that c=\r\nome from such a\n&gt; &gt;  &gt; constrained source (Twitter feeds), or that need to =\r\nbe crawled in a\n&gt; &gt;  &gt; special timeframe, or according to other special lim=\r\nits (fewer hops),\n&gt; &gt;  &gt; could make sense.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; It would take some=\r\n customization of the queueing-policy or\n&gt; &gt;  &gt; &#39;precedence&#39; features of He=\r\nritrix to allow URIs added mid-crawl to be\n&gt; &gt;  &gt; prioritized above those a=\r\nlready discovered and queued. The most simple\n&gt; &gt;  &gt; possible customization=\r\n might be a UriPrecedencePolicy that takes all\n&gt; &gt;  &gt; zero-hop URIs (which =\r\nall seeds and most direct-fed URIs would be) and\n&gt; &gt;  &gt; gives them a higher=\r\n precedence (lower precedence number) than all\n&gt; &gt;  &gt; other URIs.\n&gt; &gt;  &gt;\n&gt; =\r\n&gt;  &gt;&gt; 6) I think the answer is no for this question, but I will ask it anyw=\r\nay.\n&gt; &gt;  &gt;&gt; If you have a Heritrix instance that is configured for 1200 thr=\r\neads on\n&gt; &gt;  &gt;&gt; one machine, can you recover from a checkpoint from that 12=\r\n00 thread\n&gt; &gt;  &gt;&gt; configuration on a different machine with an Heritrix ins=\r\ntance that is\n&gt; &gt;  &gt;&gt; configured for less threads (e.g. the default 25 thre=\r\nads)?\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Yes - there&#39;s no need to keep the thread count the same=\r\n after a\n&gt; &gt;  &gt; resume. None of the checkpoint structures (or usual disk st=\r\nructures)\n&gt; &gt;  &gt; are based on the number of worker threads (&#39;ToeThreads&#39;)..=\r\n. as\n&gt; &gt;  &gt; mentioned above you can even vary the number of threads in a ru=\r\nnning\n&gt; &gt;  &gt; crawl.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; - Gordon\n&gt; &gt;  &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}