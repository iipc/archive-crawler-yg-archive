{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"fd18Oq9pe8M82D8C3Z0dbeeXMruX7I20XG126vY4WPB4n4czFM1CVQZGs8O6zzlVxHORbi01T0MD1xcrcnrWbDBU_naD2xjV","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Multimachine crawl","postDate":"1147805617","msgId":2867,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0NkExRkIxLjUwNTA3MDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGI4MTI1OWEwMDYwNTE2MTExMnNlNTM3NzI4dmU3YmU2NDQ2ZDI2NDI5MzJAbWFpbC5nbWFpbC5jb20+","referencesHeader":"PGI4MTI1OWEwMDYwNTE2MTExMnNlNTM3NzI4dmU3YmU2NDQ2ZDI2NDI5MzJAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":2866,"nextInTopic":2872,"prevInTime":2866,"nextInTime":2870,"topicId":2866,"numMessagesInTopic":3,"msgSnippet":"... Hey Greg. We re not there yet.   Still a ways to go. The way we currently do multimachine crawls is still effectively as outlined here: ","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60630 invoked from network); 16 May 2006 18:53:01 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m2.grp.scd.yahoo.com with QMQP; 16 May 2006 18:53:00 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta5.grp.scd.yahoo.com with SMTP; 16 May 2006 18:52:51 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k4GHdlw13132;\n\tTue, 16 May 2006 10:39:48 -0700\r\nMessage-ID: &lt;446A1FB1.5050701@...&gt;\r\nDate: Tue, 16 May 2006 11:53:37 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8.0.1) Gecko/20060127 SeaMonkey/1.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;b81259a00605161112se537728ve7be6446d2642932@...&gt;\r\nIn-Reply-To: &lt;b81259a00605161112se537728ve7be6446d2642932@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Multimachine crawl\r\nX-Yahoo-Group-Post: member; u=168599281; y=Gcz2-drIk7-qy3H7I3U_MV_ARvf4obmBN0TsMCEeCvJemNLfObboIFFN\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nGreg Kempe wrote:\n&gt; Hi\n&gt;\n&gt; Are there any updates on the progress of Heritrix&#39;s multi-machine\n&gt; crawl support? The release notes for 1.8.0 suggest that support has\n&gt; been improved, as do the comments on the mailing list regarding hcc.\n&gt;\n&gt; I haven&#39;t managed to find information on actually running a\n&gt; multimachine crawl, though. Does anyone have any links or suggestions?\n\nHey Greg.\n\nWe&#39;re not there yet.   Still a ways to go.\n\nThe way we currently do multimachine crawls is still effectively as \noutlined here: \nhttp://groups.yahoo.com/group/archive-crawler/message/2402 (See also \nmessages 2438 and 2543). \n\nMinimally, we need to automate the currently manual URL exchange across \ncrawlers and we need to build out our cluster controller.\n\n(Perhaps you do not need the former facility?  A list member recently \ntalked of how he was doing broad crawling and was uninterested in URL \nexchange between crawlers because the manual process is currently too \npainful and because the crawler was discovering URLs at such a rate \nthat, he figured, the popular pages of each web segment would eventually \nbe found by the dedicated crawler.)\n\nRegards a console onto a coordinated cluster, the current state of \nthings is that you can programmatically instantiate an instance of hcc \nand through it, instantiate Heritrix instances in all remote &#39;Heritrix \ncontainers&#39;  that are registered with the common JNDI registry (where \n&#39;Heritrix container&#39; is a grand moniker for a JVM with Heritrix \nrunning).  Then via the hcc instance, you can do coarse manipulations \nsubmitting jobs, pausing, stopping and monitoring the remote instances.  \nThere is no UI for the hcc yet (would be a sweet project for someone to \ntake on).  Wouldn&#39;t take much adding a webapp with a few jsp pages to \nthe hcc subproject to instantiate a hcc instance to manipulate a cluster \nvia a UI.  hcc is currently being used in house as part of a specialized \napplication (archive-it) so we know it basically works.\n\nOther things to do are making Heritrix even more remotely configurable \nso you can do anything remotely via hcc that you can do via the local UI \n(except with hcc you can have it apply across the cluster rather  than \nto single instance) -- and experimenting with different techniques for \ndividing up the web over the cluster.\n\nSt.Ack\n\n\n\n\n\n\n\n"}}