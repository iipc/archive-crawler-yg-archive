{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","replyTo":"LIST","senderId":"3hgEsZASXLQX0WeSxpo3WFEr_Jf92mdYK_8IZUYRuOCp8rM2ETeTJmfgnuFCV7TNVlyNAMfTBQIiS8R8mpdF6A","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Proxy","postDate":"1089223467","msgId":591,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwRUMzQjJCLjYwNTA3MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEwODkxMjU4MjQuMjE2NDEuMzguY2FtZWxAbHh3czM2NTE1LmJsLnVrPg==","referencesHeader":"PDM0MzU0MENGODlDMjc3NDI4OUU3RjNDOUVFNEY1REYxMDU1M0E1OTlAbnQtbG9uZXgxLmJsLnVrPgkgPDQwRUE2NTAwLjYwNzA5MDBAYXJjaGl2ZS5vcmc+IDwxMDg5MTI1ODI0LjIxNjQxLjM4LmNhbWVsQGx4d3MzNjUxNS5ibC51az4="},"prevInTopic":590,"nextInTopic":594,"prevInTime":590,"nextInTime":592,"topicId":568,"numMessagesInTopic":20,"msgSnippet":"Your a good man Mark. The patch is a little ugly for sure (smile).  I went through it and extracted the attached (I made proxy an expert setting).  Does it","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 78691 invoked from network); 7 Jul 2004 18:12:58 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m6.grp.scd.yahoo.com with QMQP; 7 Jul 2004 18:12:58 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 7 Jul 2004 18:12:57 -0000\r\nReceived: (qmail 13471 invoked by uid 100); 7 Jul 2004 18:02:11 -0000\r\nReceived: from unknown (HELO ?209.237.240.13?) (stack@...@209.237.240.13)\n  by mail-dev.archive.org with SMTP; 7 Jul 2004 18:02:11 -0000\r\nMessage-ID: &lt;40EC3B2B.6050706@...&gt;\r\nDate: Wed, 07 Jul 2004 11:04:27 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7) Gecko/20040624 Debian/1.7-2\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;343540CF89C2774289E7F3C9EE4F5DF10553A599@...&gt;\t &lt;40EA6500.6070900@...&gt; &lt;1089125824.21641.38.camel@...&gt;\r\nIn-Reply-To: &lt;1089125824.21641.38.camel@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.5 required=6.0 tests=AWL,HTML_MESSAGE,\n\tHTML_TAG_BALANCE_A autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Proxy\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\nYour a good man Mark.\n\nThe patch is a little ugly for sure (smile).  I went through it and \nextracted the attached (I made proxy an expert setting).  Does it look \nright to you?  I tried it and all seems to work properly.  DNS doesn&#39;t \ngo via the proxy but I figure thats probably ok?\n\nOk if I add you as a contributor to heritrix?\n\nGood stuff,\nSt.Ack\n\n\nmark williamson wrote:\n\n&gt;Hi, \n&gt;\n&gt;here is a patch to add the a proxy server to the crawl\n&gt;settings. Patch wise its awful because I auto formated \n&gt;the code in Eclipse and so the format of the code doesn&#39;t \n&gt;fit properly and the patch then removes the whole file and \n&gt;adds a new one :-( \n&gt;\n&gt;The way I solved problems like this in my last company was to \n&gt;require *everyone* to run the code autoformatter in Eclipse \n&gt;prior to check in. That way everyones coding idiosyncrasies where \n&gt;smoothed away and diffs etc. all worked very nicely.\n&gt;\n&gt;The proxy and host can be set in the job settings using this \n&gt;patch. \n&gt;\n&gt;cheers \n&gt;\n&gt;mark \n&gt;\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;  \n&gt;\n&gt;------------------------------------------------------------------------\n&gt;\n&gt;diff -aur AOCBase/src/java/org/archive/crawler/fetcher/FetchHTTP.java ArchiveOpenCrawler/src/java/org/archive/crawler/fetcher/FetchHTTP.java\n&gt;--- AOCBase/src/java/org/archive/crawler/fetcher/FetchHTTP.java\t2004-07-06 12:04:33.000000000 +0100\n&gt;+++ ArchiveOpenCrawler/src/java/org/archive/crawler/fetcher/FetchHTTP.java\t2004-07-06 16:47:57.000000000 +0100\n&gt;@@ -81,858 +81,922 @@\n&gt; /**\n&gt;  * HTTP fetcher that uses &lt;a\n&gt;  * href=&quot;http://jakarta.apache.org/commons/httpclient/&quot;&gt;Apache Jakarta Commons\n&gt;- * HttpClient&lt;/a&gt; library.\n&gt;- *\n&gt;+ * HttpClient &lt;/a&gt; library.\n&gt;+ * \n&gt;  * @author Gordon Mohr\n&gt;  * @author Igor Ranitovic\n&gt;  * @author others\n&gt;  * @version $Id: FetchHTTP.java,v 1.51 2004/06/24 00:11:24 gojomo Exp $\n&gt;  */\n&gt; public class FetchHTTP extends Processor\n&gt;-    \timplements CoreAttributeConstants, FetchStatusCodes {\n&gt;-    // be robust against trivial implementation changes\n&gt;-    private static final long serialVersionUID = ArchiveUtils.classnameBasedUID(FetchHTTP.class,1);\n&gt;-    \n&gt;-    private static Logger logger = Logger.getLogger(FetchHTTP.class.getName());\n&gt;-\n&gt;-    public static final String ATTR_TIMEOUT_SECONDS = &quot;timeout-seconds&quot;;\n&gt;-    public static final String ATTR_SOTIMEOUT_MS = &quot;sotimeout-ms&quot;;\n&gt;-    public static final String ATTR_MAX_LENGTH_BYTES = &quot;max-length-bytes&quot;;\n&gt;-    public static final String ATTR_LOAD_COOKIES = &quot;load-cookies-from-file&quot;;\n&gt;-    public static final String ATTR_SAVE_COOKIES = &quot;save-cookies-to-file&quot;;\n&gt;-\n&gt;-    private static Integer DEFAULT_TIMEOUT_SECONDS = new Integer(1200);\n&gt;-    private static Integer DEFAULT_SOTIMEOUT_MS = new Integer(20000);\n&gt;-    private static Long DEFAULT_MAX_LENGTH_BYTES = new Long(Long.MAX_VALUE);\n&gt;-\n&gt;-   /**\n&gt;-     * Default setting for HttpClient&#39;s &quot;strict mode&quot;.\n&gt;-     * In strict mode, Cookies are served on a single header.\n&gt;-     */\n&gt;-    private static final boolean DEFAULT_HTTPCLIENT_STRICT = true;\n&gt;-\n&gt;-    /**\n&gt;-     * SSL trust level setting attribute name.\n&gt;-     */\n&gt;-    public static final String ATTR_TRUST = &quot;trust-level&quot;;\n&gt;-\n&gt;-    transient PatchedHttpClient http = null;\n&gt;-\n&gt;-    private int soTimeout;\n&gt;-\n&gt;-    /**\n&gt;-     * How many &#39;instant retries&#39; of HttpRecoverableExceptions have occurred\n&gt;-     */\n&gt;-    // Would like to be &#39;long&#39;, but longs aren&#39;t atomic\n&gt;-    private int recoveryRetries = 0;\n&gt;-\n&gt;-    // Would like to be &#39;long&#39;, but longs aren&#39;t atomic\n&gt;-    private int curisHandled = 0;\n&gt;-\n&gt;-    /**\n&gt;-     * Constructor.\n&gt;-     *\n&gt;-     * @param name Name of this processor.\n&gt;-     */\n&gt;-    public FetchHTTP(String name) {\n&gt;-        super(name, &quot;HTTP Fetcher&quot;);\n&gt;-        Type e;\n&gt;-        addElementToDefinition(new SimpleType(ATTR_TIMEOUT_SECONDS,\n&gt;-            &quot;If the fetch is not completed in this number of seconds,&quot;\n&gt;-            + &quot; give up&quot;, DEFAULT_TIMEOUT_SECONDS));\n&gt;-        e = addElementToDefinition(new SimpleType(ATTR_SOTIMEOUT_MS,\n&gt;-            &quot;If the socket is unresponsive for this number of milliseconds, &quot;\n&gt;-            + &quot;give up (and retry later)&quot;, DEFAULT_SOTIMEOUT_MS));\n&gt;-        e.setExpertSetting(true);\n&gt;-        addElementToDefinition(new SimpleType(ATTR_MAX_LENGTH_BYTES,\n&gt;-            &quot;Max length in bytes to fetch (truncate at this length)&quot;,\n&gt;-            DEFAULT_MAX_LENGTH_BYTES));\n&gt;-        e = addElementToDefinition(new SimpleType(ATTR_LOAD_COOKIES,\n&gt;-            &quot;File to preload cookies from&quot;, &quot;&quot;));\n&gt;-        e.setExpertSetting(true);\n&gt;-        e = addElementToDefinition(new SimpleType(ATTR_SAVE_COOKIES,\n&gt;-            &quot;When crawl finishes save cookies to this file&quot;, &quot;&quot;));\n&gt;-        e.setExpertSetting(true);\n&gt;-        e = addElementToDefinition(new SimpleType(ATTR_TRUST,\n&gt;-            &quot;SSL certificate trust level.  Range is from the default &#39;open&#39;&quot;\n&gt;-            + &quot; (trust all certs including expired, selfsigned, and those for&quot;\n&gt;-            + &quot; which we do not have a CA) through &#39;loose&#39; (trust all valid&quot;\n&gt;-            + &quot; certificates including selfsigned), &#39;normal&#39; (all valid&quot;\n&gt;-            + &quot; certificates not including selfsigned) to &#39;strict&#39; (Cert is&quot;\n&gt;-            + &quot; valid and DN must match servername)&quot;,\n&gt;-            ConfigurableX509TrustManager.DEFAULT,\n&gt;-            ConfigurableX509TrustManager.LEVELS_AS_ARRAY));\n&gt;-        e.setOverrideable(false);\n&gt;-        e.setExpertSetting(true);\n&gt;-    }\n&gt;-\n&gt;-    protected void innerProcess(CrawlURI curi) throws InterruptedException {\n&gt;-        if (!canFetch(curi)) {\n&gt;-            // Cannot fetch this, due to protocol, retries, or other problems\n&gt;-            return;\n&gt;-        }\n&gt;-\n&gt;-        this.curisHandled++;\n&gt;-\n&gt;-        // Note begin time\n&gt;-        curi.getAList().putLong(A_FETCH_BEGAN_TIME, System.currentTimeMillis());\n&gt;-\n&gt;-        // Get a reference to the HttpRecorder that is set into this ToeThread.\n&gt;-        HttpRecorder rec = HttpRecorder.getHttpRecorder();\n&gt;-        HttpMethod method = curi.isPost()?\n&gt;-            (HttpMethod)new HttpRecorderPostMethod(\n&gt;-                curi.getUURI().toString(), rec):\n&gt;-            (HttpMethod)new HttpRecorderGetMethod(\n&gt;-                curi.getUURI().toString(), rec);\n&gt;-        configureMethod(curi, method);\n&gt;-        boolean addedCredentials = populateCredentials(curi, method);\n&gt;-        int immediateRetries = 0;\n&gt;-        while (true) {\n&gt;-            // Retry until success (break) or unrecoverable exception\n&gt;-            // (early return)\n&gt;-            try {\n&gt;-                // TODO: make this initial reading subject to the same\n&gt;-                // length/timeout limits; currently only the soTimeout\n&gt;-                // is effective here, once the connection succeeds\n&gt;-                this.http.executeMethod(method);\n&gt;-                break;\n&gt;-            } catch (HttpRecoverableException e) {\n&gt;-                checkForInterrupt();\n&gt;-                if (immediateRetries &lt; getMaxImmediateRetries()) {\n&gt;-                    // See &quot;[ 910219 ] [httpclient] unable...starting with&quot;\n&gt;-                    // http://sourceforge.net/tracker/?group_id=73833&atid=539099&func=detail&aid=910219\n&gt;-                    // for the justification for this loop.\n&gt;-                    this.recoveryRetries++;\n&gt;-                    immediateRetries++;\n&gt;-                    continue;\n&gt;-                } else {\n&gt;-                    // Treat as connect failed\n&gt;-                    failedExecuteCleanup(method, curi, e);\n&gt;-                    return;\n&gt;-                }\n&gt;-            } catch (IOException e) {\n&gt;-                failedExecuteCleanup(method, curi, e);\n&gt;-                return;\n&gt;-            } catch (ArrayIndexOutOfBoundsException e) {\n&gt;-                // For weird windows-only ArrayIndex exceptions in native\n&gt;-                // code... see\n&gt;-                // http://forum.java.sun.com/thread.jsp?forum=11&thread=378356\n&gt;-                // treating as if it were an IOException\n&gt;-                failedExecuteCleanup(method, curi, e);\n&gt;-                return;\n&gt;-            }\n&gt;-        }\n&gt;-\n&gt;-        try {\n&gt;-            // Force read-to-end, so that any socket hangs occur here,\n&gt;-            // not in later modules\n&gt;-            rec.getRecordedInput().readFullyOrUntil(getMaxLength(curi),\n&gt;-                1000 * getTimeout(curi));\n&gt;-        } catch (RecorderTimeoutException ex) {\n&gt;-            curi.addAnnotation(&quot;timeTrunc&quot;);\n&gt;-        } catch (RecorderLengthExceededException ex) {\n&gt;-            curi.addAnnotation(&quot;lengthTrunc&quot;);\n&gt;-        } catch (IOException e) {\n&gt;-            cleanup(method, curi, e, &quot;readFully&quot;, S_CONNECT_LOST);\n&gt;-            return;\n&gt;-        } catch (ArrayIndexOutOfBoundsException e) {\n&gt;-            // For weird windows-only ArrayIndex exceptions from native code\n&gt;-            // see http://forum.java.sun.com/thread.jsp?forum=11&thread=378356\n&gt;-            // treating as if it were an IOException\n&gt;-            cleanup(method, curi, e, &quot;readFully&quot;, S_CONNECT_LOST);\n&gt;-            return;\n&gt;-        } finally {\n&gt;-            method.releaseConnection();\n&gt;-        }\n&gt;-\n&gt;-        // Note completion time\n&gt;-        curi.getAList().putLong(A_FETCH_COMPLETED_TIME,\n&gt;-            System.currentTimeMillis());\n&gt;-\n&gt;-        // Set the response charset into the HttpRecord if available.\n&gt;-        rec.setCharacterEncoding(((HttpMethodBase)method).getResponseCharSet());\n&gt;-\n&gt;-        // Set httpRecorder into curi for convenience of subsequent processors.\n&gt;-        curi.setHttpRecorder(rec);\n&gt;-\n&gt;-        int statusCode = method.getStatusCode();\n&gt;-        long contentSize = curi.getHttpRecorder().getRecordedInput().getSize();\n&gt;-        curi.setContentSize(contentSize);\n&gt;-        curi.setFetchStatus(statusCode);\n&gt;-        Header ct = method.getResponseHeader(&quot;content-type&quot;);\n&gt;-        curi.setContentType((ct == null)? null: ct.getValue());\n&gt;-        if (logger.isLoggable(Level.FINE)) {\n&gt;-            logger.fine((curi.isPost()? &quot;POST&quot;: &quot;GET&quot;) + &quot; &quot; +\n&gt;-            \t\tcuri.getUURI().toString() + &quot; &quot; + statusCode + &quot; &quot; +\n&gt;-                contentSize + &quot; &quot; + curi.getContentType());\n&gt;-        }\n&gt;-\n&gt;-        if (curi.isSuccess() && addedCredentials) {\n&gt;-            // Promote the credentials from the CrawlURI to the CrawlServer\n&gt;-            // so they are available for all subsequent CrawlURIs on this\n&gt;-            // server.\n&gt;-            promoteCredentials(curi);\n&gt;-            if (logger.isLoggable(Level.FINE)) {\n&gt;-                // Print out the cookie.  Might help with the debugging.\n&gt;-                Header setCookie = method.getResponseHeader(&quot;set-cookie&quot;);\n&gt;-                if (setCookie != null) {\n&gt;-                    logger.fine(setCookie.toString().trim());\n&gt;-                }\n&gt;-            }\n&gt;-        } else if (statusCode == HttpStatus.SC_UNAUTHORIZED) {\n&gt;-            // 401 is not &#39;success&#39;.\n&gt;-            handle401(method, curi);\n&gt;-        }\n&gt;-\n&gt;-        // Save off the GetMethod just in case needed by subsequent processors.\n&gt;-        curi.getAList().putObject(A_HTTP_TRANSACTION, method);\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * Cleanup after a failed method execute.\n&gt;-     * @param curi CrawlURI we failed on.\n&gt;-     * @param method Method we failed on.\n&gt;-     * @param exception Exception we failed with.\n&gt;-     */\n&gt;-    private void failedExecuteCleanup(final HttpMethod method,\n&gt;-            final CrawlURI curi, final Exception exception) {\n&gt;-        cleanup(method, curi, exception, &quot;executeMethod&quot;, S_CONNECT_FAILED);\n&gt;-    }\n&gt;-    /**\n&gt;-     * Cleanup after a failed method execute.\n&gt;-     * @param curi CrawlURI we failed on.\n&gt;-     * @param method Method we failed on.\n&gt;-     * @param exception Exception we failed with.\n&gt;-     * @param message Message to log with failure.\n&gt;-     * @param status Status to set on the fetch.\n&gt;-     */\n&gt;-    private void cleanup(final HttpMethod method, final CrawlURI curi,\n&gt;-            final Exception exception, final String message, final int status) {\n&gt;-        curi.addLocalizedError(this.getName(), exception, message);\n&gt;-        curi.setFetchStatus(status);\n&gt;-\n&gt;-        // Its ok if releaseConnection is called multiple times: i.e. here and\n&gt;-        // in the finally that is at end of one of the innerProcess blocks\n&gt;-        // above.\n&gt;-        method.releaseConnection();\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * @return maximum immediate retures.\n&gt;-     */\n&gt;-    private int getMaxImmediateRetries() {\n&gt;-        // TODO make configurable\n&gt;-        return 5;\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * Can this processor fetch the given CrawlURI. May set a fetch\n&gt;-     * status if this processor would usually handle the CrawlURI,\n&gt;-     * but cannot in this instance.\n&gt;-     *\n&gt;-     * @param curi\n&gt;-     * @return True if processor can fetch.\n&gt;-     */\n&gt;-    private boolean canFetch(CrawlURI curi) {\n&gt;-        String scheme = curi.getUURI().getScheme();\n&gt;-         if (!(scheme.equals(&quot;http&quot;) || scheme.equals(&quot;https&quot;))) {\n&gt;-             // handles only plain http and https\n&gt;-             return false;\n&gt;-         }\n&gt;-\n&gt;-//         System.out.println(curi.toString() + &quot; : &quot; + curi.getFetchAttempts());\n&gt;-//         if (curi.getFetchAttempts() &gt;= getMaxFetchAttempts(curi)) {\n&gt;-//             curi.setFetchStatus(S_TOO_MANY_RETRIES);\n&gt;-//             return false;\n&gt;-//         }\n&gt;-\n&gt;-         // make sure the dns lookup succeeded\n&gt;-         if (curi.getServer().getHost().getIP() == null\n&gt;-             && curi.getServer().getHost().hasBeenLookedUp()) {\n&gt;-             curi.setFetchStatus(S_DOMAIN_UNRESOLVABLE);\n&gt;-             return false;\n&gt;-         }\n&gt;-        return true;\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * Configure the HttpMethod setting options and headers.\n&gt;-     *\n&gt;-     * @param curi CrawlURI from which we pull configuration.\n&gt;-     * @param get The GetMethod to configure.\n&gt;-     */\n&gt;-    private void configureMethod(CrawlURI curi, HttpMethod method)\n&gt;-    {\n&gt;-        // Don&#39;t auto-follow redirects\n&gt;-        method.setFollowRedirects(false);\n&gt;-\n&gt;-        // Set strict on the client; whatever the client&#39;s mode overrides\n&gt;-        // the methods mode inside in the depths of executeMethod.\n&gt;-        this.http.setStrictMode(DEFAULT_HTTPCLIENT_STRICT);\n&gt;-\n&gt;-        // Use only HTTP/1.0 (to avoid receiving chunked responses)\n&gt;-        ((HttpMethodBase)method).setHttp11(false);\n&gt;-\n&gt;-        CrawlOrder order = getSettingsHandler().getOrder();\n&gt;-        String userAgent = curi.getUserAgent();\n&gt;-        if (userAgent == null) {\n&gt;-            userAgent = order.getUserAgent(curi);\n&gt;-        }\n&gt;-        method.setRequestHeader(&quot;User-Agent&quot;, userAgent);\n&gt;-        method.setRequestHeader(&quot;From&quot;, order.getFrom(curi));\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * Add credentials if any to passed &lt;code&gt;method&lt;/code&gt;.\n&gt;-     *\n&gt;-     * Do credential handling.  Credentials are in two places.  1. Credentials\n&gt;-     * that succeeded are added to the CrawlServer (Or rather, avatars for\n&gt;-     * credentials are whats added because its not safe to keep around\n&gt;-     * references to credentials).  2. Credentials to be tried are in the curi.\n&gt;-     * Returns true if found credentials to be tried.\n&gt;-     *\n&gt;-     * @param curi Current CrawlURI.\n&gt;-     * @param method The method to add to.\n&gt;-     * @return True if prepopulated &lt;code&gt;method&lt;/code&gt; with credentials AND the\n&gt;-     * credentials came from the &lt;code&gt;curi&lt;/code&gt;, not from the CrawlServer.\n&gt;-     * The former is  special in that if the &lt;code&gt;curi&lt;/curi&gt; credentials\n&gt;-     * succeed, then the caller needs to promote them from the CrawlURI to the\n&gt;-     * CrawlServer so they are available for all subsequent CrawlURIs on this\n&gt;-     * server.\n&gt;-     */\n&gt;-    private boolean populateCredentials(CrawlURI curi, HttpMethod method) {\n&gt;-\n&gt;-        // First look at the server avatars. Add any that are to be volunteered\n&gt;-        // on every request (e.g. RFC2617 credentials).  Every time creds will\n&gt;-        // return true when we call &#39;isEveryTime().\n&gt;-        if (curi.getServer().hasCredentialAvatars()) {\n&gt;-            Set avatars = curi.getServer().getCredentialAvatars();\n&gt;-            for (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt;-                CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt;-                Credential c = ca.getCredential(getSettingsHandler(), curi);\n&gt;-                if (c.isEveryTime()) {\n&gt;-                    c.populate(curi, this.http, method, ca.getPayload());\n&gt;-                }\n&gt;-            }\n&gt;-        }\n&gt;-\n&gt;-        boolean result = false;\n&gt;-\n&gt;-        // Now look in the curi.  The Curi will have credentials loaded either\n&gt;-        // by the handle401 method if its a rfc2617 or it&#39;ll have been set into\n&gt;-        // the curi by the preconditionenforcer as this login uri came through.\n&gt;-        if (curi.hasCredentialAvatars()) {\n&gt;-            Set avatars = curi.getCredentialAvatars();\n&gt;-            for (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt;-                CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt;-                Credential c = ca.getCredential(getSettingsHandler(), curi);\n&gt;-                if (c.populate(curi, this.http, method, ca.getPayload())) {\n&gt;-                    result = true;\n&gt;-                }\n&gt;-            }\n&gt;-        }\n&gt;-\n&gt;-        return result;\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * Promote successful credential to the server.\n&gt;-     *\n&gt;-     * @param curi CrawlURI whose credentials we are to promote.\n&gt;-     * @param method Method used.\n&gt;-     */\n&gt;-    private void promoteCredentials(final CrawlURI curi) {\n&gt;-        if (!curi.hasCredentialAvatars()) {\n&gt;-            logger.severe(&quot;No credentials to promote when there should be &quot; +\n&gt;-                curi);\n&gt;-        } else {\n&gt;-            Set avatars = curi.getCredentialAvatars();\n&gt;-            for (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt;-                CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt;-                curi.removeCredentialAvatar(ca);\n&gt;-                // The server to attach too may not be the server that hosts\n&gt;-                // this passed curi.  It might be of another subdomain.\n&gt;-                // The avatar needs to be added to the server that is dependent\n&gt;-                // on this precondition.  Find it by name.  Get the name from\n&gt;-                // the credential this avatar represents.\n&gt;-                Credential c = ca.getCredential(getSettingsHandler(), curi);\n&gt;-                String cd = null;\n&gt;-                try {\n&gt;-                    cd = c.getCredentialDomain(curi);\n&gt;-                }\n&gt;-                catch (AttributeNotFoundException e) {\n&gt;-                    logger.severe(&quot;Failed to get cred domain for &quot; + curi +\n&gt;-                        &quot; for &quot; + ca + &quot;: &quot; + e.getMessage());\n&gt;-                }\n&gt;-                if (cd != null) {\n&gt;-                    CrawlServer cs\n&gt;-                        = getController().getServerCache().getServerFor(cd);\n&gt;-                    if (cs != null) {\n&gt;-                        cs.addCredentialAvatar(ca);\n&gt;-                    }\n&gt;-                }\n&gt;-            }\n&gt;-        }\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * Server is looking for basic/digest auth credentials (RFC2617). If we have\n&gt;-     * any, put them into the CrawlURI and have it come around again. Presence\n&gt;-     * of the credential serves as flag to frontier to requeue promptly. If we\n&gt;-     * already tried this domain and still got a 401, then our credentials are\n&gt;-     * bad. Remove them and let this curi die.\n&gt;-     *\n&gt;-     * @param get Method that got a 401.\n&gt;-     * @param curi CrawlURI that got a 401.\n&gt;-     */\n&gt;-    private void handle401(final HttpMethod method, final CrawlURI curi) {\n&gt;-\n&gt;-        AuthScheme authscheme = getAuthScheme(method, curi);\n&gt;-        if (authscheme == null) {\n&gt;-            return;\n&gt;-        }\n&gt;-\n&gt;-        String realm = authscheme.getRealm();\n&gt;-        if (realm == null) {\n&gt;-            return;\n&gt;-        }\n&gt;-\n&gt;-        // Look to see if this curi had rfc2617 avatars loaded.  If so, are\n&gt;-        // any of them for this realm?  If so, then the credential failed if\n&gt;-        // we got a 401 and it should be let die a natural 401 death.\n&gt;-        Set curiRfc2617Credentials =\n&gt;-            getCredentials(getSettingsHandler(), curi, Rfc2617Credential.class);\n&gt;-        Rfc2617Credential extant = Rfc2617Credential.\n&gt;-            getByRealm(curiRfc2617Credentials, realm, curi);\n&gt;-        if (extant != null) {\n&gt;-            // Then, already tried this credential.  Remove ANY rfc2617\n&gt;-            // credential since presence of a rfc2617 credential serves\n&gt;-            // as flag to frontier to requeue this curi and let the curi\n&gt;-            // die a natural death.\n&gt;-            extant.detachAll(curi);\n&gt;-            logger.fine(&quot;Auth failed (401) though supplied realm &quot; +\n&gt;-                realm + &quot; to &quot; + curi.toString());\n&gt;-        } else {\n&gt;-            // Look see if we have a credential that corresponds to this realm\n&gt;-            // in credential store.  Filter by type and credential domain.  If\n&gt;-            // not, let this curi die. Else, add it to the curi and let it come\n&gt;-            // around again. Add in the AuthScheme we got too.  Its needed when\n&gt;-            // we go to run the Auth on second time around.\n&gt;-            CredentialStore cs =\n&gt;-                CredentialStore.getCredentialStore(getSettingsHandler());\n&gt;-            if (cs == null) {\n&gt;-                logger.severe(&quot;No credential store for &quot; + curi);\n&gt;-            } else {\n&gt;-                Set storeRfc2617Credentials = cs.subset(curi,\n&gt;-                    Rfc2617Credential.class, curi.getServer().getName());\n&gt;-                if (storeRfc2617Credentials == null ||\n&gt;-                    storeRfc2617Credentials.size() &lt;= 0) {\n&gt;-                    logger.fine(&quot;No rfc2617 credentials for &quot; + curi);\n&gt;-                } else {\n&gt;-                    Rfc2617Credential found = Rfc2617Credential.\n&gt;-                    \t\tgetByRealm(storeRfc2617Credentials, realm, curi);\n&gt;-                    if (found == null) {\n&gt;-                        logger.fine(&quot;No rfc2617 credentials for realm &quot; +\n&gt;-                            realm + &quot; in &quot; + curi);\n&gt;-                    } else {\n&gt;-                        found.attach(curi, authscheme);\n&gt;-                        logger.fine(&quot;Found credential for realm &quot; + realm +\n&gt;-                            &quot; in store for &quot; + curi.toString());\n&gt;-                    }\n&gt;-                }\n&gt;-            }\n&gt;-        }\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * @param handler Settings Handler.\n&gt;-     * @param curi CrawlURI that got a 401.\n&gt;-     * @param type Class of credential to get from curi.\n&gt;-     * @return Set of credentials attached to this curi.\n&gt;-     */\n&gt;-    private Set getCredentials(SettingsHandler handler, CrawlURI curi,\n&gt;-            Class type) {\n&gt;-        Set result = null;\n&gt;-\n&gt;-        if (curi.hasCredentialAvatars()) {\n&gt;-            for (Iterator i = curi.getCredentialAvatars().iterator();\n&gt;-                    i.hasNext();) {\n&gt;-                CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt;-                if (ca.match(type)) {\n&gt;-                    if (result == null) {\n&gt;-                        result = new HashSet();\n&gt;-                    }\n&gt;-                    result.add(ca.getCredential(handler, curi));\n&gt;-                }\n&gt;-            }\n&gt;-        }\n&gt;-        return result;\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * @param get Method that got a 401.\n&gt;-     * @param curi CrawlURI that got a 401.\n&gt;-     * @return Authscheme made from the authenticate header or null if failed to\n&gt;-     * get it.\n&gt;-     */\n&gt;-    private AuthScheme getAuthScheme(final HttpMethod method,\n&gt;-            final CrawlURI curi) {\n&gt;-        AuthScheme result = null;\n&gt;-        Header header = method.getResponseHeader(HttpAuthenticator.WWW_AUTH);\n&gt;-        if (header == null) {\n&gt;-            logger.info(&quot;No &quot; + HttpAuthenticator.WWW_AUTH + &quot; headers though&quot; +\n&gt;-                &quot; we got a 401: &quot; + curi);\n&gt;-        } else {\n&gt;-            try {\n&gt;-                result =\n&gt;-                    HttpAuthenticator.selectAuthScheme(new Header[] {header});\n&gt;-            } catch (MalformedChallengeException e) {\n&gt;-                logger.severe(&quot;Failed to get auth headers: &quot; + e.toString() +\n&gt;-                    &quot; &quot; + curi.toString());\n&gt;-            } catch (UnsupportedOperationException uoe) {\n&gt;-                // This is probably a message like this:\n&gt;-                // Authentication scheme(s) not supported:\n&gt;-                // {negotiate,=Negotiate, NTLM}\n&gt;-                // Log it as a warning.  Not much we can do about it.  Return\n&gt;-                // null.  We&#39;ll get the 401 in the arcs and a page that says\n&gt;-                // something like &#39;Access denied&#39;.\n&gt;-                logger.warning(curi + &quot;: &quot; + uoe);\n&gt;-            }\n&gt;-        }\n&gt;-        return result;\n&gt;-    }\n&gt;-\n&gt;-    public void initialTasks() {\n&gt;-        this.soTimeout = getSoTimeout(null);\n&gt;-        setupHttp();\n&gt;-\n&gt;-        // load cookies from a file if specified in the order file.\n&gt;-        loadCookies();\n&gt;-    }\n&gt;+\t\timplements\n&gt;+\t\t\tCoreAttributeConstants,\n&gt;+\t\t\tFetchStatusCodes {\n&gt;+\t// be robust against trivial implementation changes\n&gt;+\tprivate static final long serialVersionUID = ArchiveUtils\n&gt;+\t\t\t.classnameBasedUID(FetchHTTP.class, 1);\n&gt;+\n&gt;+\tprivate static Logger logger = Logger.getLogger(FetchHTTP.class.getName());\n&gt;+\n&gt;+\tpublic static final String ATTR_HTTP_PROXY_HOST = &quot;http-proxy-host&quot;;\n&gt;+\tpublic static final String ATTR_HTTP_PROXY_PORT = &quot;http-proxy-port&quot;;\n&gt;+\tpublic static final String ATTR_TIMEOUT_SECONDS = &quot;timeout-seconds&quot;;\n&gt;+\tpublic static final String ATTR_SOTIMEOUT_MS = &quot;sotimeout-ms&quot;;\n&gt;+\tpublic static final String ATTR_MAX_LENGTH_BYTES = &quot;max-length-bytes&quot;;\n&gt;+\tpublic static final String ATTR_LOAD_COOKIES = &quot;load-cookies-from-file&quot;;\n&gt;+\tpublic static final String ATTR_SAVE_COOKIES = &quot;save-cookies-to-file&quot;;\n&gt;+\n&gt;+\tprivate static Integer DEFAULT_TIMEOUT_SECONDS = new Integer(1200);\n&gt;+\tprivate static Integer DEFAULT_SOTIMEOUT_MS = new Integer(20000);\n&gt;+\tprivate static Long DEFAULT_MAX_LENGTH_BYTES = new Long(Long.MAX_VALUE);\n&gt;+\n&gt;+\t/**\n&gt;+\t * Default setting for HttpClient&#39;s &quot;strict mode&quot;. In strict mode, Cookies\n&gt;+\t * are served on a single header.\n&gt;+\t */\n&gt;+\tprivate static final boolean DEFAULT_HTTPCLIENT_STRICT = true;\n&gt;+\n&gt;+\t/**\n&gt;+\t * SSL trust level setting attribute name.\n&gt;+\t */\n&gt;+\tpublic static final String ATTR_TRUST = &quot;trust-level&quot;;\n&gt;+\n&gt;+\ttransient PatchedHttpClient http = null;\n&gt;+\n&gt;+\tprivate int soTimeout;\n&gt;+\n&gt;+\t/**\n&gt;+\t * How many &#39;instant retries&#39; of HttpRecoverableExceptions have occurred\n&gt;+\t */\n&gt;+\t// Would like to be &#39;long&#39;, but longs aren&#39;t atomic\n&gt;+\tprivate int recoveryRetries = 0;\n&gt;+\n&gt;+\t// Would like to be &#39;long&#39;, but longs aren&#39;t atomic\n&gt;+\tprivate int curisHandled = 0;\n&gt;+\n&gt;+\t/**\n&gt;+\t * Constructor.\n&gt;+\t * \n&gt;+\t * @param name\n&gt;+\t *                    Name of this processor.\n&gt;+\t */\n&gt;+\tpublic FetchHTTP(String name) {\n&gt;+\t\tsuper(name, &quot;HTTP Fetcher&quot;);\n&gt;+\t\tType e;\n&gt;+\t\taddElementToDefinition(new SimpleType(ATTR_HTTP_PROXY_HOST,\n&gt;+\t\t\t\t&quot;Proxy hostname (set only if needed)&quot;, &quot;&quot;));\n&gt;+\t\taddElementToDefinition(new SimpleType(ATTR_HTTP_PROXY_PORT,\n&gt;+\t\t\t\t&quot;Proxy port (set only if needed)&quot;, &quot;&quot;));\n&gt;+\t\taddElementToDefinition(new SimpleType(ATTR_TIMEOUT_SECONDS,\n&gt;+\t\t\t\t&quot;If the fetch is not completed in this number of seconds,&quot;\n&gt;+\t\t\t\t\t\t+ &quot; give up&quot;, DEFAULT_TIMEOUT_SECONDS));\n&gt;+\t\te = addElementToDefinition(new SimpleType(ATTR_SOTIMEOUT_MS,\n&gt;+\t\t\t\t&quot;If the socket is unresponsive for this number of milliseconds, &quot;\n&gt;+\t\t\t\t\t\t+ &quot;give up (and retry later)&quot;, DEFAULT_SOTIMEOUT_MS));\n&gt;+\t\te.setExpertSetting(true);\n&gt;+\t\taddElementToDefinition(new SimpleType(ATTR_MAX_LENGTH_BYTES,\n&gt;+\t\t\t\t&quot;Max length in bytes to fetch (truncate at this length)&quot;,\n&gt;+\t\t\t\tDEFAULT_MAX_LENGTH_BYTES));\n&gt;+\t\te = addElementToDefinition(new SimpleType(ATTR_LOAD_COOKIES,\n&gt;+\t\t\t\t&quot;File to preload cookies from&quot;, &quot;&quot;));\n&gt;+\t\te.setExpertSetting(true);\n&gt;+\t\te = addElementToDefinition(new SimpleType(ATTR_SAVE_COOKIES,\n&gt;+\t\t\t\t&quot;When crawl finishes save cookies to this file&quot;, &quot;&quot;));\n&gt;+\t\te.setExpertSetting(true);\n&gt;+\t\te = addElementToDefinition(new SimpleType(\n&gt;+\t\t\t\tATTR_TRUST,\n&gt;+\t\t\t\t&quot;SSL certificate trust level.  Range is from the default &#39;open&#39;&quot;\n&gt;+\t\t\t\t\t\t+ &quot; (trust all certs including expired, selfsigned, and those for&quot;\n&gt;+\t\t\t\t\t\t+ &quot; which we do not have a CA) through &#39;loose&#39; (trust all valid&quot;\n&gt;+\t\t\t\t\t\t+ &quot; certificates including selfsigned), &#39;normal&#39; (all valid&quot;\n&gt;+\t\t\t\t\t\t+ &quot; certificates not including selfsigned) to &#39;strict&#39; (Cert is&quot;\n&gt;+\t\t\t\t\t\t+ &quot; valid and DN must match servername)&quot;,\n&gt;+\t\t\t\tConfigurableX509TrustManager.DEFAULT,\n&gt;+\t\t\t\tConfigurableX509TrustManager.LEVELS_AS_ARRAY));\n&gt;+\t\te.setOverrideable(false);\n&gt;+\t\te.setExpertSetting(true);\n&gt;+\t}\n&gt;+\n&gt;+\tprotected void innerProcess(CrawlURI curi) throws InterruptedException {\n&gt;+\t\tif (!canFetch(curi)) {\n&gt;+\t\t\t// Cannot fetch this, due to protocol, retries, or other problems\n&gt;+\t\t\treturn;\n&gt;+\t\t}\n&gt;+\n&gt;+\t\tthis.curisHandled++;\n&gt;+\n&gt;+\t\t// Note begin time\n&gt;+\t\tcuri.getAList().putLong(A_FETCH_BEGAN_TIME, System.currentTimeMillis());\n&gt;+\n&gt;+\t\t// Get a reference to the HttpRecorder that is set into this ToeThread.\n&gt;+\t\tHttpRecorder rec = HttpRecorder.getHttpRecorder();\n&gt;+\t\tHttpMethod method = curi.isPost()\n&gt;+\t\t\t\t? (HttpMethod) new HttpRecorderPostMethod(curi.getUURI()\n&gt;+\t\t\t\t\t\t.toString(), rec)\n&gt;+\t\t\t\t: (HttpMethod) new HttpRecorderGetMethod(curi.getUURI()\n&gt;+\t\t\t\t\t\t.toString(), rec);\n&gt;+\t\tconfigureMethod(curi, method);\n&gt;+\t\tboolean addedCredentials = populateCredentials(curi, method);\n&gt;+\t\tint immediateRetries = 0;\n&gt;+\t\twhile (true) {\n&gt;+\t\t\t// Retry until success (break) or unrecoverable exception\n&gt;+\t\t\t// (early return)\n&gt;+\t\t\ttry {\n&gt;+\t\t\t\t// TODO: make this initial reading subject to the same\n&gt;+\t\t\t\t// length/timeout limits; currently only the soTimeout\n&gt;+\t\t\t\t// is effective here, once the connection succeeds\n&gt;+\t\t\t\tthis.http.executeMethod(method);\n&gt;+\t\t\t\tbreak;\n&gt;+\t\t\t} catch (HttpRecoverableException e) {\n&gt;+\t\t\t\tcheckForInterrupt();\n&gt;+\t\t\t\tif (immediateRetries &lt; getMaxImmediateRetries()) {\n&gt;+\t\t\t\t\t// See &quot;[ 910219 ] [httpclient] unable...starting with&quot;\n&gt;+\t\t\t\t\t// http://sourceforge.net/tracker/?group_id=73833&atid=539099&func=detail&aid=910219\n&gt;+\t\t\t\t\t// for the justification for this loop.\n&gt;+\t\t\t\t\tthis.recoveryRetries++;\n&gt;+\t\t\t\t\timmediateRetries++;\n&gt;+\t\t\t\t\tcontinue;\n&gt;+\t\t\t\t} else {\n&gt;+\t\t\t\t\t// Treat as connect failed\n&gt;+\t\t\t\t\tfailedExecuteCleanup(method, curi, e);\n&gt;+\t\t\t\t\treturn;\n&gt;+\t\t\t\t}\n&gt;+\t\t\t} catch (IOException e) {\n&gt;+\t\t\t\tfailedExecuteCleanup(method, curi, e);\n&gt;+\t\t\t\treturn;\n&gt;+\t\t\t} catch (ArrayIndexOutOfBoundsException e) {\n&gt;+\t\t\t\t// For weird windows-only ArrayIndex exceptions in native\n&gt;+\t\t\t\t// code... see\n&gt;+\t\t\t\t// http://forum.java.sun.com/thread.jsp?forum=11&thread=378356\n&gt;+\t\t\t\t// treating as if it were an IOException\n&gt;+\t\t\t\tfailedExecuteCleanup(method, curi, e);\n&gt;+\t\t\t\treturn;\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt;+\n&gt;+\t\ttry {\n&gt;+\t\t\t// Force read-to-end, so that any socket hangs occur here,\n&gt;+\t\t\t// not in later modules\n&gt;+\t\t\trec.getRecordedInput().readFullyOrUntil(getMaxLength(curi),\n&gt;+\t\t\t\t\t1000 * getTimeout(curi));\n&gt;+\t\t} catch (RecorderTimeoutException ex) {\n&gt;+\t\t\tcuri.addAnnotation(&quot;timeTrunc&quot;);\n&gt;+\t\t} catch (RecorderLengthExceededException ex) {\n&gt;+\t\t\tcuri.addAnnotation(&quot;lengthTrunc&quot;);\n&gt;+\t\t} catch (IOException e) {\n&gt;+\t\t\tcleanup(method, curi, e, &quot;readFully&quot;, S_CONNECT_LOST);\n&gt;+\t\t\treturn;\n&gt;+\t\t} catch (ArrayIndexOutOfBoundsException e) {\n&gt;+\t\t\t// For weird windows-only ArrayIndex exceptions from native code\n&gt;+\t\t\t// see http://forum.java.sun.com/thread.jsp?forum=11&thread=378356\n&gt;+\t\t\t// treating as if it were an IOException\n&gt;+\t\t\tcleanup(method, curi, e, &quot;readFully&quot;, S_CONNECT_LOST);\n&gt;+\t\t\treturn;\n&gt;+\t\t} finally {\n&gt;+\t\t\tmethod.releaseConnection();\n&gt;+\t\t}\n&gt;+\n&gt;+\t\t// Note completion time\n&gt;+\t\tcuri.getAList().putLong(A_FETCH_COMPLETED_TIME,\n&gt;+\t\t\t\tSystem.currentTimeMillis());\n&gt;+\n&gt;+\t\t// Set the response charset into the HttpRecord if available.\n&gt;+\t\trec\n&gt;+\t\t\t\t.setCharacterEncoding(((HttpMethodBase) method)\n&gt;+\t\t\t\t\t\t.getResponseCharSet());\n&gt;+\n&gt;+\t\t// Set httpRecorder into curi for convenience of subsequent processors.\n&gt;+\t\tcuri.setHttpRecorder(rec);\n&gt;+\n&gt;+\t\tint statusCode = method.getStatusCode();\n&gt;+\t\tlong contentSize = curi.getHttpRecorder().getRecordedInput().getSize();\n&gt;+\t\tcuri.setContentSize(contentSize);\n&gt;+\t\tcuri.setFetchStatus(statusCode);\n&gt;+\t\tHeader ct = method.getResponseHeader(&quot;content-type&quot;);\n&gt;+\t\tcuri.setContentType((ct == null) ? null : ct.getValue());\n&gt;+\t\tif (logger.isLoggable(Level.FINE)) {\n&gt;+\t\t\tlogger.fine((curi.isPost() ? &quot;POST&quot; : &quot;GET&quot;) + &quot; &quot;\n&gt;+\t\t\t\t\t+ curi.getUURI().toString() + &quot; &quot; + statusCode + &quot; &quot;\n&gt;+\t\t\t\t\t+ contentSize + &quot; &quot; + curi.getContentType());\n&gt;+\t\t}\n&gt;+\n&gt;+\t\tif (curi.isSuccess() && addedCredentials) {\n&gt;+\t\t\t// Promote the credentials from the CrawlURI to the CrawlServer\n&gt;+\t\t\t// so they are available for all subsequent CrawlURIs on this\n&gt;+\t\t\t// server.\n&gt;+\t\t\tpromoteCredentials(curi);\n&gt;+\t\t\tif (logger.isLoggable(Level.FINE)) {\n&gt;+\t\t\t\t// Print out the cookie. Might help with the debugging.\n&gt;+\t\t\t\tHeader setCookie = method.getResponseHeader(&quot;set-cookie&quot;);\n&gt;+\t\t\t\tif (setCookie != null) {\n&gt;+\t\t\t\t\tlogger.fine(setCookie.toString().trim());\n&gt;+\t\t\t\t}\n&gt;+\t\t\t}\n&gt;+\t\t} else if (statusCode == HttpStatus.SC_UNAUTHORIZED) {\n&gt;+\t\t\t// 401 is not &#39;success&#39;.\n&gt;+\t\t\thandle401(method, curi);\n&gt;+\t\t}\n&gt;+\n&gt;+\t\t// Save off the GetMethod just in case needed by subsequent processors.\n&gt;+\t\tcuri.getAList().putObject(A_HTTP_TRANSACTION, method);\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Cleanup after a failed method execute.\n&gt;+\t * \n&gt;+\t * @param curi\n&gt;+\t *                    CrawlURI we failed on.\n&gt;+\t * @param method\n&gt;+\t *                    Method we failed on.\n&gt;+\t * @param exception\n&gt;+\t *                    Exception we failed with.\n&gt;+\t */\n&gt;+\tprivate void failedExecuteCleanup(final HttpMethod method,\n&gt;+\t\t\tfinal CrawlURI curi, final Exception exception) {\n&gt;+\t\tcleanup(method, curi, exception, &quot;executeMethod&quot;, S_CONNECT_FAILED);\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Cleanup after a failed method execute.\n&gt;+\t * \n&gt;+\t * @param curi\n&gt;+\t *                    CrawlURI we failed on.\n&gt;+\t * @param method\n&gt;+\t *                    Method we failed on.\n&gt;+\t * @param exception\n&gt;+\t *                    Exception we failed with.\n&gt;+\t * @param message\n&gt;+\t *                    Message to log with failure.\n&gt;+\t * @param status\n&gt;+\t *                    Status to set on the fetch.\n&gt;+\t */\n&gt;+\tprivate void cleanup(final HttpMethod method, final CrawlURI curi,\n&gt;+\t\t\tfinal Exception exception, final String message, final int status) {\n&gt;+\t\tcuri.addLocalizedError(this.getName(), exception, message);\n&gt;+\t\tcuri.setFetchStatus(status);\n&gt;+\n&gt;+\t\t// Its ok if releaseConnection is called multiple times: i.e. here and\n&gt;+\t\t// in the finally that is at end of one of the innerProcess blocks\n&gt;+\t\t// above.\n&gt;+\t\tmethod.releaseConnection();\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * @return maximum immediate retures.\n&gt;+\t */\n&gt;+\tprivate int getMaxImmediateRetries() {\n&gt;+\t\t// TODO make configurable\n&gt;+\t\treturn 5;\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Can this processor fetch the given CrawlURI. May set a fetch status if\n&gt;+\t * this processor would usually handle the CrawlURI, but cannot in this\n&gt;+\t * instance.\n&gt;+\t * \n&gt;+\t * @param curi\n&gt;+\t * @return True if processor can fetch.\n&gt;+\t */\n&gt;+\tprivate boolean canFetch(CrawlURI curi) {\n&gt;+\t\tString scheme = curi.getUURI().getScheme();\n&gt;+\t\tif (!(scheme.equals(&quot;http&quot;) || scheme.equals(&quot;https&quot;))) {\n&gt;+\t\t\t// handles only plain http and https\n&gt;+\t\t\treturn false;\n&gt;+\t\t}\n&gt;+\n&gt;+\t\t//         System.out.println(curi.toString() + &quot; : &quot; +\n&gt;+\t\t// curi.getFetchAttempts());\n&gt;+\t\t//         if (curi.getFetchAttempts() &gt;= getMaxFetchAttempts(curi)) {\n&gt;+\t\t//             curi.setFetchStatus(S_TOO_MANY_RETRIES);\n&gt;+\t\t//             return false;\n&gt;+\t\t//         }\n&gt;+\n&gt;+\t\t// make sure the dns lookup succeeded\n&gt;+\t\tif (curi.getServer().getHost().getIP() == null\n&gt;+\t\t\t\t&& curi.getServer().getHost().hasBeenLookedUp()) {\n&gt;+\t\t\tcuri.setFetchStatus(S_DOMAIN_UNRESOLVABLE);\n&gt;+\t\t\treturn false;\n&gt;+\t\t}\n&gt;+\t\treturn true;\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Configure the HttpMethod setting options and headers.\n&gt;+\t * \n&gt;+\t * @param curi\n&gt;+\t *                    CrawlURI from which we pull configuration.\n&gt;+\t * @param get\n&gt;+\t *                    The GetMethod to configure.\n&gt;+\t */\n&gt;+\tprivate void configureMethod(CrawlURI curi, HttpMethod method) {\n&gt;+\t\t// Don&#39;t auto-follow redirects\n&gt;+\t\tmethod.setFollowRedirects(false);\n&gt;+\n&gt;+\t\t// Set strict on the client; whatever the client&#39;s mode overrides\n&gt;+\t\t// the methods mode inside in the depths of executeMethod.\n&gt;+\t\tthis.http.setStrictMode(DEFAULT_HTTPCLIENT_STRICT);\n&gt;+\n&gt;+\t\ttry {\n&gt;+\t\t\tString proxy = (String) getAttribute(ATTR_HTTP_PROXY_HOST);\n&gt;+\t\t\tif (proxy.equals(&quot;&quot;) != true) {\n&gt;+\t\t\t\tthis.http.setHttpProxy(proxy);\n&gt;+\t\t\t\tthis.http\n&gt;+\t\t\t\t\t\t.setHttpProxyport(Integer\n&gt;+\t\t\t\t\t\t\t\t.parseInt(((String) getAttribute(ATTR_HTTP_PROXY_PORT))));\n&gt;+\t\t\t}\n&gt;+\t\t} catch (AttributeNotFoundException e) {\n&gt;+\t\t\t// TODO Auto-generated catch block\n&gt;+\t\t\te.printStackTrace();\n&gt;+\t\t} catch (MBeanException e) {\n&gt;+\t\t\t// TODO Auto-generated catch block\n&gt;+\t\t\te.printStackTrace();\n&gt;+\t\t} catch (ReflectionException e) {\n&gt;+\t\t\t// TODO Auto-generated catch block\n&gt;+\t\t\te.printStackTrace();\n&gt;+\t\t}\n&gt;+\n&gt;+\t\t// Use only HTTP/1.0 (to avoid receiving chunked responses)\n&gt;+\t\t((HttpMethodBase) method).setHttp11(false);\n&gt;+\n&gt;+\t\tCrawlOrder order = getSettingsHandler().getOrder();\n&gt;+\t\tString userAgent = curi.getUserAgent();\n&gt;+\t\tif (userAgent == null) {\n&gt;+\t\t\tuserAgent = order.getUserAgent(curi);\n&gt;+\t\t}\n&gt;+\t\tmethod.setRequestHeader(&quot;User-Agent&quot;, userAgent);\n&gt;+\t\tmethod.setRequestHeader(&quot;From&quot;, order.getFrom(curi));\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Add credentials if any to passed &lt;code&gt;method&lt;/code&gt;.\n&gt;+\t * \n&gt;+\t * Do credential handling. Credentials are in two places. 1. Credentials\n&gt;+\t * that succeeded are added to the CrawlServer (Or rather, avatars for\n&gt;+\t * credentials are whats added because its not safe to keep around\n&gt;+\t * references to credentials). 2. Credentials to be tried are in the curi.\n&gt;+\t * Returns true if found credentials to be tried.\n&gt;+\t * \n&gt;+\t * @param curi\n&gt;+\t *                    Current CrawlURI.\n&gt;+\t * @param method\n&gt;+\t *                    The method to add to.\n&gt;+\t * @return True if prepopulated &lt;code&gt;method&lt;/code&gt; with credentials AND\n&gt;+\t *               the credentials came from the &lt;code&gt;curi&lt;/code&gt;, not from the\n&gt;+\t *               CrawlServer. The former is special in that if the\n&gt;+\t *               &lt;code&gt;curi&lt;/curi&gt; credentials\n&gt;+\t * succeed, then the caller needs to promote them from the CrawlURI to the\n&gt;+\t * CrawlServer so they are available for all subsequent CrawlURIs on this\n&gt;+\t * server.\n&gt;+\t */\n&gt;+\tprivate boolean populateCredentials(CrawlURI curi, HttpMethod method) {\n&gt;+\n&gt;+\t\t// First look at the server avatars. Add any that are to be volunteered\n&gt;+\t\t// on every request (e.g. RFC2617 credentials). Every time creds will\n&gt;+\t\t// return true when we call &#39;isEveryTime().\n&gt;+\t\tif (curi.getServer().hasCredentialAvatars()) {\n&gt;+\t\t\tSet avatars = curi.getServer().getCredentialAvatars();\n&gt;+\t\t\tfor (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt;+\t\t\t\tCredentialAvatar ca = (CredentialAvatar) i.next();\n&gt;+\t\t\t\tCredential c = ca.getCredential(getSettingsHandler(), curi);\n&gt;+\t\t\t\tif (c.isEveryTime()) {\n&gt;+\t\t\t\t\tc.populate(curi, this.http, method, ca.getPayload());\n&gt;+\t\t\t\t}\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt;+\n&gt;+\t\tboolean result = false;\n&gt;+\n&gt;+\t\t// Now look in the curi. The Curi will have credentials loaded either\n&gt;+\t\t// by the handle401 method if its a rfc2617 or it&#39;ll have been set into\n&gt;+\t\t// the curi by the preconditionenforcer as this login uri came through.\n&gt;+\t\tif (curi.hasCredentialAvatars()) {\n&gt;+\t\t\tSet avatars = curi.getCredentialAvatars();\n&gt;+\t\t\tfor (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt;+\t\t\t\tCredentialAvatar ca = (CredentialAvatar) i.next();\n&gt;+\t\t\t\tCredential c = ca.getCredential(getSettingsHandler(), curi);\n&gt;+\t\t\t\tif (c.populate(curi, this.http, method, ca.getPayload())) {\n&gt;+\t\t\t\t\tresult = true;\n&gt;+\t\t\t\t}\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt; \n&gt;-    void setupHttp() throws RuntimeException {\n&gt;+\t\treturn result;\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Promote successful credential to the server.\n&gt;+\t * \n&gt;+\t * @param curi\n&gt;+\t *                    CrawlURI whose credentials we are to promote.\n&gt;+\t * @param method\n&gt;+\t *                    Method used.\n&gt;+\t */\n&gt;+\tprivate void promoteCredentials(final CrawlURI curi) {\n&gt;+\t\tif (!curi.hasCredentialAvatars()) {\n&gt;+\t\t\tlogger.severe(&quot;No credentials to promote when there should be &quot;\n&gt;+\t\t\t\t\t+ curi);\n&gt;+\t\t} else {\n&gt;+\t\t\tSet avatars = curi.getCredentialAvatars();\n&gt;+\t\t\tfor (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt;+\t\t\t\tCredentialAvatar ca = (CredentialAvatar) i.next();\n&gt;+\t\t\t\tcuri.removeCredentialAvatar(ca);\n&gt;+\t\t\t\t// The server to attach too may not be the server that hosts\n&gt;+\t\t\t\t// this passed curi. It might be of another subdomain.\n&gt;+\t\t\t\t// The avatar needs to be added to the server that is dependent\n&gt;+\t\t\t\t// on this precondition. Find it by name. Get the name from\n&gt;+\t\t\t\t// the credential this avatar represents.\n&gt;+\t\t\t\tCredential c = ca.getCredential(getSettingsHandler(), curi);\n&gt;+\t\t\t\tString cd = null;\n&gt;+\t\t\t\ttry {\n&gt;+\t\t\t\t\tcd = c.getCredentialDomain(curi);\n&gt;+\t\t\t\t} catch (AttributeNotFoundException e) {\n&gt;+\t\t\t\t\tlogger.severe(&quot;Failed to get cred domain for &quot; + curi\n&gt;+\t\t\t\t\t\t\t+ &quot; for &quot; + ca + &quot;: &quot; + e.getMessage());\n&gt;+\t\t\t\t}\n&gt;+\t\t\t\tif (cd != null) {\n&gt;+\t\t\t\t\tCrawlServer cs = getController().getServerCache()\n&gt;+\t\t\t\t\t\t\t.getServerFor(cd);\n&gt;+\t\t\t\t\tif (cs != null) {\n&gt;+\t\t\t\t\t\tcs.addCredentialAvatar(ca);\n&gt;+\t\t\t\t\t}\n&gt;+\t\t\t\t}\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Server is looking for basic/digest auth credentials (RFC2617). If we have\n&gt;+\t * any, put them into the CrawlURI and have it come around again. Presence\n&gt;+\t * of the credential serves as flag to frontier to requeue promptly. If we\n&gt;+\t * already tried this domain and still got a 401, then our credentials are\n&gt;+\t * bad. Remove them and let this curi die.\n&gt;+\t * \n&gt;+\t * @param get\n&gt;+\t *                    Method that got a 401.\n&gt;+\t * @param curi\n&gt;+\t *                    CrawlURI that got a 401.\n&gt;+\t */\n&gt;+\tprivate void handle401(final HttpMethod method, final CrawlURI curi) {\n&gt;+\n&gt;+\t\tAuthScheme authscheme = getAuthScheme(method, curi);\n&gt;+\t\tif (authscheme == null) {\n&gt;+\t\t\treturn;\n&gt;+\t\t}\n&gt;+\n&gt;+\t\tString realm = authscheme.getRealm();\n&gt;+\t\tif (realm == null) {\n&gt;+\t\t\treturn;\n&gt;+\t\t}\n&gt;+\n&gt;+\t\t// Look to see if this curi had rfc2617 avatars loaded. If so, are\n&gt;+\t\t// any of them for this realm? If so, then the credential failed if\n&gt;+\t\t// we got a 401 and it should be let die a natural 401 death.\n&gt;+\t\tSet curiRfc2617Credentials = getCredentials(getSettingsHandler(), curi,\n&gt;+\t\t\t\tRfc2617Credential.class);\n&gt;+\t\tRfc2617Credential extant = Rfc2617Credential.getByRealm(\n&gt;+\t\t\t\tcuriRfc2617Credentials, realm, curi);\n&gt;+\t\tif (extant != null) {\n&gt;+\t\t\t// Then, already tried this credential. Remove ANY rfc2617\n&gt;+\t\t\t// credential since presence of a rfc2617 credential serves\n&gt;+\t\t\t// as flag to frontier to requeue this curi and let the curi\n&gt;+\t\t\t// die a natural death.\n&gt;+\t\t\textant.detachAll(curi);\n&gt;+\t\t\tlogger.fine(&quot;Auth failed (401) though supplied realm &quot; + realm\n&gt;+\t\t\t\t\t+ &quot; to &quot; + curi.toString());\n&gt;+\t\t} else {\n&gt;+\t\t\t// Look see if we have a credential that corresponds to this realm\n&gt;+\t\t\t// in credential store. Filter by type and credential domain. If\n&gt;+\t\t\t// not, let this curi die. Else, add it to the curi and let it come\n&gt;+\t\t\t// around again. Add in the AuthScheme we got too. Its needed when\n&gt;+\t\t\t// we go to run the Auth on second time around.\n&gt;+\t\t\tCredentialStore cs = CredentialStore\n&gt;+\t\t\t\t\t.getCredentialStore(getSettingsHandler());\n&gt;+\t\t\tif (cs == null) {\n&gt;+\t\t\t\tlogger.severe(&quot;No credential store for &quot; + curi);\n&gt;+\t\t\t} else {\n&gt;+\t\t\t\tSet storeRfc2617Credentials = cs.subset(curi,\n&gt;+\t\t\t\t\t\tRfc2617Credential.class, curi.getServer().getName());\n&gt;+\t\t\t\tif (storeRfc2617Credentials == null\n&gt;+\t\t\t\t\t\t|| storeRfc2617Credentials.size() &lt;= 0) {\n&gt;+\t\t\t\t\tlogger.fine(&quot;No rfc2617 credentials for &quot; + curi);\n&gt;+\t\t\t\t} else {\n&gt;+\t\t\t\t\tRfc2617Credential found = Rfc2617Credential.getByRealm(\n&gt;+\t\t\t\t\t\t\tstoreRfc2617Credentials, realm, curi);\n&gt;+\t\t\t\t\tif (found == null) {\n&gt;+\t\t\t\t\t\tlogger.fine(&quot;No rfc2617 credentials for realm &quot; + realm\n&gt;+\t\t\t\t\t\t\t\t+ &quot; in &quot; + curi);\n&gt;+\t\t\t\t\t} else {\n&gt;+\t\t\t\t\t\tfound.attach(curi, authscheme);\n&gt;+\t\t\t\t\t\tlogger.fine(&quot;Found credential for realm &quot; + realm\n&gt;+\t\t\t\t\t\t\t\t+ &quot; in store for &quot; + curi.toString());\n&gt;+\t\t\t\t\t}\n&gt;+\t\t\t\t}\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * @param handler\n&gt;+\t *                    Settings Handler.\n&gt;+\t * @param curi\n&gt;+\t *                    CrawlURI that got a 401.\n&gt;+\t * @param type\n&gt;+\t *                    Class of credential to get from curi.\n&gt;+\t * @return Set of credentials attached to this curi.\n&gt;+\t */\n&gt;+\tprivate Set getCredentials(SettingsHandler handler, CrawlURI curi,\n&gt;+\t\t\tClass type) {\n&gt;+\t\tSet result = null;\n&gt;+\n&gt;+\t\tif (curi.hasCredentialAvatars()) {\n&gt;+\t\t\tfor (Iterator i = curi.getCredentialAvatars().iterator(); i\n&gt;+\t\t\t\t\t.hasNext();) {\n&gt;+\t\t\t\tCredentialAvatar ca = (CredentialAvatar) i.next();\n&gt;+\t\t\t\tif (ca.match(type)) {\n&gt;+\t\t\t\t\tif (result == null) {\n&gt;+\t\t\t\t\t\tresult = new HashSet();\n&gt;+\t\t\t\t\t}\n&gt;+\t\t\t\t\tresult.add(ca.getCredential(handler, curi));\n&gt;+\t\t\t\t}\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt;+\t\treturn result;\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * @param get\n&gt;+\t *                    Method that got a 401.\n&gt;+\t * @param curi\n&gt;+\t *                    CrawlURI that got a 401.\n&gt;+\t * @return Authscheme made from the authenticate header or null if failed to\n&gt;+\t *               get it.\n&gt;+\t */\n&gt;+\tprivate AuthScheme getAuthScheme(final HttpMethod method,\n&gt;+\t\t\tfinal CrawlURI curi) {\n&gt;+\t\tAuthScheme result = null;\n&gt;+\t\tHeader header = method.getResponseHeader(HttpAuthenticator.WWW_AUTH);\n&gt;+\t\tif (header == null) {\n&gt;+\t\t\tlogger.info(&quot;No &quot; + HttpAuthenticator.WWW_AUTH + &quot; headers though&quot;\n&gt;+\t\t\t\t\t+ &quot; we got a 401: &quot; + curi);\n&gt;+\t\t} else {\n&gt;+\t\t\ttry {\n&gt;+\t\t\t\tresult = HttpAuthenticator\n&gt;+\t\t\t\t\t\t.selectAuthScheme(new Header[]{header});\n&gt;+\t\t\t} catch (MalformedChallengeException e) {\n&gt;+\t\t\t\tlogger.severe(&quot;Failed to get auth headers: &quot; + e.toString()\n&gt;+\t\t\t\t\t\t+ &quot; &quot; + curi.toString());\n&gt;+\t\t\t} catch (UnsupportedOperationException uoe) {\n&gt;+\t\t\t\t// This is probably a message like this:\n&gt;+\t\t\t\t// Authentication scheme(s) not supported:\n&gt;+\t\t\t\t// {negotiate,=Negotiate, NTLM}\n&gt;+\t\t\t\t// Log it as a warning. Not much we can do about it. Return\n&gt;+\t\t\t\t// null. We&#39;ll get the 401 in the arcs and a page that says\n&gt;+\t\t\t\t// something like &#39;Access denied&#39;.\n&gt;+\t\t\t\tlogger.warning(curi + &quot;: &quot; + uoe);\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt;+\t\treturn result;\n&gt;+\t}\n&gt;+\n&gt;+\tpublic void initialTasks() {\n&gt;+\t\tthis.soTimeout = getSoTimeout(null);\n&gt;+\t\tsetupHttp();\n&gt;+\n&gt;+\t\t// load cookies from a file if specified in the order file.\n&gt;+\t\tloadCookies();\n&gt;+\t}\n&gt;+\n&gt;+\tvoid setupHttp() throws RuntimeException {\n&gt; \t\tCookiePolicy.setDefaultPolicy(CookiePolicy.COMPATIBILITY);\n&gt;-        SingleHttpConnectionManager connectionManager =\n&gt;-            new SingleHttpConnectionManager();\n&gt;-        this.http = new PatchedHttpClient(connectionManager);\n&gt;-\n&gt;-        try {\n&gt;-            String trustLevel = (String) getAttribute(ATTR_TRUST);\n&gt;-            Protocol.registerProtocol(&quot;https&quot;, new Protocol(&quot;https&quot;,\n&gt;-                    new ConfigurableTrustManagerProtocolSocketFactory(\n&gt;-                            trustLevel), 443));\n&gt;-        }\n&gt;-\n&gt;-        catch (Exception e) {\n&gt;-            // Convert all to RuntimeException so get an exception out if\n&gt;-            // initialization fails.\n&gt;-            throw new RuntimeException(\n&gt;-                    &quot;Failed initialization getting attributes: &quot;\n&gt;-                            + e.getMessage());\n&gt;-        }\n&gt;-\n&gt;-        // Considered same as overall timeout, for now.\n&gt;-        // TODO: When HTTPClient stops using a monitor &#39;waitingThread&#39;\n&gt;-        // thread to watch over the getting of the socket from socket\n&gt;-        // factory and instead supports the java.net.Socket#connect timeout.\n&gt;-        // http.setConnectionTimeout((int)timeout);\n&gt;-        // set per-read() timeout: overall timeout will be checked at least\n&gt;-        // this\n&gt;-        // frequently\n&gt;-        this.http.setTimeout(this.soTimeout);\n&gt;-\t}\n&gt;-\n&gt;-    private int getSoTimeout(CrawlURI curi) {\n&gt;-        Integer res;\n&gt;-        try {\n&gt;-            res = (Integer) getAttribute(ATTR_SOTIMEOUT_MS, curi);\n&gt;-        } catch (Exception e) {\n&gt;-            res = DEFAULT_SOTIMEOUT_MS;\n&gt;-        }\n&gt;-        return res.intValue();\n&gt;-    }\n&gt;-\n&gt;-    private int getTimeout(CrawlURI curi) {\n&gt;-        Integer res;\n&gt;-        try {\n&gt;-            res = (Integer) getAttribute(ATTR_TIMEOUT_SECONDS, curi);\n&gt;-        } catch (Exception e) {\n&gt;-            res = DEFAULT_TIMEOUT_SECONDS;\n&gt;-        }\n&gt;-        return res.intValue();\n&gt;-    }\n&gt;-\n&gt;-    private long getMaxLength(CrawlURI curi) {\n&gt;-        Long res;\n&gt;-        try {\n&gt;-            res = (Long) getAttribute(ATTR_MAX_LENGTH_BYTES, curi);\n&gt;-        } catch (Exception e) {\n&gt;-            res = DEFAULT_MAX_LENGTH_BYTES;\n&gt;-        }\n&gt;-        return res.longValue();\n&gt;-    }\n&gt;-\n&gt;-    /**\n&gt;-     * Load cookies from a file before the first fetch.\n&gt;-     * &lt;p&gt;\n&gt;-     * The file is a text file in the Netscape&#39;s &#39;cookies.txt&#39; file format.&lt;br&gt;\n&gt;-     * Example entry of cookies.txt file:&lt;br&gt;\n&gt;-     * &lt;br&gt;\n&gt;-     * www.archive.org FALSE / FALSE 1074567117 details-visit texts-cralond&lt;br&gt;\n&gt;-     * &lt;br&gt;\n&gt;-     * Each line has 7 tab-separated fields:&lt;br&gt;\n&gt;-     * &lt;li&gt;1. DOMAIN: The domain that created and have access to the cookie\n&gt;-     * value.\n&gt;-     * &lt;li&gt;2. FLAG: A TRUE or FALSE value indicating if hosts within the given\n&gt;-     * domain can access the cookie value.\n&gt;-     * &lt;li&gt;3. PATH: The path within the domain that the cookie value is valid\n&gt;-     * for.\n&gt;-     * &lt;li&gt;4. SECURE: A TRUE or FALSE value indicating if to use a secure\n&gt;-     * connection to access the cookie value.\n&gt;-     * &lt;li&gt;5. EXPIRATION: The expiration time of the cookie value (unix style.)\n&gt;-     * &lt;li&gt;6. NAME: The name of the cookie value\n&gt;-     * &lt;li&gt;7. VALUE: The cookie value\n&gt;-     *\n&gt;-     * @param cookiesFile file in the Netscape&#39;s &#39;cookies.txt&#39; format.\n&gt;-     */\n&gt;-    public void loadCookies(String cookiesFile) {\n&gt;-        // Do nothing if cookiesFile is not specified.\n&gt;-        if (cookiesFile == null || cookiesFile.length() &lt;= 0) {\n&gt;-            return;\n&gt;-        }\n&gt;-        RandomAccessFile raf = null;\n&gt;-        try {\n&gt;-            raf = new RandomAccessFile(cookiesFile, &quot;r&quot;);\n&gt;-            String[] cookieParts;\n&gt;-            String line;\n&gt;-            Cookie cookie = null;\n&gt;-            while ((line = raf.readLine()) != null) {\n&gt;-                // Line that starts with # is commented line, therefore skip it.\n&gt;-                if (!line.startsWith(&quot;#&quot;)) {\n&gt;-                    cookieParts = line.split(&quot;&#92;&#92;t&quot;);\n&gt;-                    if (cookieParts.length == 7) {\n&gt;-                        // Create cookie with not expiration date (-1 value).\n&gt;-                        // TODO: add this as an option.\n&gt;-                        cookie =\n&gt;-                            new Cookie(cookieParts[0], cookieParts[5],\n&gt;-                                cookieParts[6], cookieParts[2], -1,\n&gt;-                                Boolean.valueOf(cookieParts[3]).booleanValue());\n&gt;-\n&gt;-                        if (cookieParts[1].toLowerCase().equals(&quot;true&quot;)) {\n&gt;-                            cookie.setDomainAttributeSpecified(true);\n&gt;-                        } else {\n&gt;-                            cookie.setDomainAttributeSpecified(false);\n&gt;-                        }\n&gt;-                        this.http.getState().addCookie(cookie);\n&gt;-                        logger.fine(\n&gt;-                            &quot;Adding cookie: &quot; + cookie.toExternalForm());\n&gt;-                    }\n&gt;-                }\n&gt;-            }\n&gt;-        } catch (FileNotFoundException e) {\n&gt;-            // We should probably throw FatalConfigurationException.\n&gt;-            System.out.println(&quot;Could not find file: &quot; + cookiesFile\n&gt;-                    + &quot; (Element: &quot; + ATTR_LOAD_COOKIES + &quot;)&quot;);\n&gt;-\n&gt;-        } catch (IOException e) {\n&gt;-            // We should probably throw FatalConfigurationException.\n&gt;-            e.printStackTrace();\n&gt;-        } finally {\n&gt;-            try {\n&gt;-                if (raf != null) {\n&gt;-                    raf.close();\n&gt;-                }\n&gt;-            } catch (IOException e) {\n&gt;-                e.printStackTrace();\n&gt;-            }\n&gt;-        }\n&gt;-    }\n&gt;-\n&gt;-    /* (non-Javadoc)\n&gt;-     * @see org.archive.crawler.framework.Processor#report()\n&gt;-     */\n&gt;-    public String report() {\n&gt;-        StringBuffer ret = new StringBuffer();\n&gt;-        ret.append(&quot;Processor: org.archive.crawler.fetcher.FetchHTTP&#92;n&quot;);\n&gt;-        ret.append(&quot;  Function:          Fetch HTTP URIs&#92;n&quot;);\n&gt;-        ret.append(&quot;  CrawlURIs handled: &quot; + this.curisHandled + &quot;&#92;n&quot;);\n&gt;-        ret.append(&quot;  Recovery retries:   &quot; + this.recoveryRetries + &quot;&#92;n&#92;n&quot;);\n&gt;-\n&gt;-        return ret.toString();\n&gt;-    }\n&gt;-\n&gt;-\n&gt;-    /**\n&gt;-     * Load cookies from the file specified in the order file.\n&gt;-     *\n&gt;-     * &lt;p&gt;\n&gt;-     * The file is a text file in the Netscape&#39;s &#39;cookies.txt&#39; file format.&lt;br&gt;\n&gt;-     * Example entry of cookies.txt file:&lt;br&gt;\n&gt;-     * &lt;br&gt;\n&gt;-     * www.archive.org FALSE / FALSE 1074567117 details-visit texts-cralond&lt;br&gt;\n&gt;-     * &lt;br&gt;\n&gt;-     * Each line has 7 tab-separated fields:&lt;br&gt;\n&gt;-     * &lt;li&gt;1. DOMAIN: The domain that created and have access to the cookie\n&gt;-     * value.\n&gt;-     * &lt;li&gt;2. FLAG: A TRUE or FALSE value indicating if hosts within the given\n&gt;-     * domain can access the cookie value.\n&gt;-     * &lt;li&gt;3. PATH: The path within the domain that the cookie value is valid\n&gt;-     * for.\n&gt;-     * &lt;li&gt;4. SECURE: A TRUE or FALSE value indicating if to use a secure\n&gt;-     * connection to access the cookie value.\n&gt;-     * &lt;li&gt;5. EXPIRATION: The expiration time of the cookie value (unix style.)\n&gt;-     * &lt;li&gt;6. NAME: The name of the cookie value\n&gt;-     * &lt;li&gt;7. VALUE: The cookie value\n&gt;-     */\n&gt;-    public void loadCookies() {\n&gt;-        try {\n&gt;-            loadCookies((String) getAttribute(ATTR_LOAD_COOKIES));\n&gt;-        } catch (MBeanException e) {\n&gt;-            logger.warning(e.getLocalizedMessage());\n&gt;-        } catch (ReflectionException e) {\n&gt;-            logger.warning(e.getLocalizedMessage());\n&gt;-        } catch (AttributeNotFoundException e) {\n&gt;-            logger.warning(e.getLocalizedMessage());\n&gt;-        }\n&gt;-    }\n&gt;-    /**\n&gt;-     * Saves cookies to the file specified in the order file.\n&gt;-     *\n&gt;-     * Output file is in the Netscape &#39;cookies.txt&#39; format.\n&gt;-     *\n&gt;-     */\n&gt;-    public void saveCookies() {\n&gt;-        try {\n&gt;-            saveCookies((String) getAttribute(ATTR_SAVE_COOKIES));\n&gt;-        } catch (MBeanException e) {\n&gt;-            logger.warning(e.getLocalizedMessage());\n&gt;-        } catch (ReflectionException e) {\n&gt;-            logger.warning(e.getLocalizedMessage());\n&gt;-        } catch (AttributeNotFoundException e) {\n&gt;-            logger.warning(e.getLocalizedMessage());\n&gt;-        }\n&gt;-    }\n&gt;-    /**\n&gt;-     * Saves cookies to a file.\n&gt;-     *\n&gt;-     * Output file is in the Netscape &#39;cookies.txt&#39; format.\n&gt;-     *\n&gt;-     * @param saveCookiesFile output file.\n&gt;-     */\n&gt;-    public void saveCookies(String saveCookiesFile) {\n&gt;-        // Do nothing if cookiesFile is not specified.\n&gt;-        if (saveCookiesFile == null || saveCookiesFile.length() &lt;= 0) {\n&gt;-            return;\n&gt;-        }\n&gt;-\n&gt;-        FileOutputStream out = null;\n&gt;-        try {\n&gt;-            out = new FileOutputStream(new File(saveCookiesFile));\n&gt;-            Cookie cookies[] = this.http.getState().getCookies();\n&gt;-            String tab =&quot;&#92;t&quot;;\n&gt;-            out.write(&quot;# Heritrix Cookie File&#92;n&quot;.getBytes());\n&gt;-            out.write(\n&gt;-                &quot;# This file is the Netscape cookies.txt format&#92;n&#92;n&quot;.getBytes());\n&gt;-            for (int i = 0; i &lt; cookies.length; i++) {\n&gt;-                StringBuffer line = new StringBuffer();\n&gt;-                line.append(cookies[i].getDomain());\n&gt;-                line.append(tab);\n&gt;-                line.append(\n&gt;-                    cookies[i].isDomainAttributeSpecified() == true\n&gt;-                        ? &quot;TRUE&quot;\n&gt;-                        : &quot;FALSE&quot;);\n&gt;-                line.append(tab);\n&gt;-                line.append(cookies[i].getPath());\n&gt;-                line.append(tab);\n&gt;-                line.append(\n&gt;-                    cookies[i].getSecure() == true ? &quot;TRUE&quot; : &quot;FALSE&quot;);\n&gt;-                line.append(tab);\n&gt;-                line.append(cookies[i].getName());\n&gt;-                line.append(tab);\n&gt;-                line.append(cookies[i].getValue());\n&gt;-                line.append(&quot;&#92;n&quot;);\n&gt;-                out.write(line.toString().getBytes());\n&gt;-            }\n&gt;-        } catch (FileNotFoundException e) {\n&gt;-            // We should probably throw FatalConfigurationException.\n&gt;-            System.out.println(&quot;Could not find file: &quot; + saveCookiesFile\n&gt;-                    + &quot; (Element: &quot; + ATTR_SAVE_COOKIES + &quot;)&quot;);\n&gt;-        } catch (IOException e) {\n&gt;-            e.printStackTrace();\n&gt;-        } finally {\n&gt;-            try {\n&gt;-                if (out != null) {\n&gt;-                    out.close();\n&gt;-                }\n&gt;-            } catch (IOException e) {\n&gt;-                e.printStackTrace();\n&gt;-            }\n&gt;-        }\n&gt;-    }\n&gt;-    /**\n&gt;-     * At the end save cookies to the file specified in the order file.\n&gt;-     *\n&gt;-     * @see org.archive.crawler.framework.Processor#finalTasks()\n&gt;-     */\n&gt;-    public void finalTasks() {\n&gt;-        saveCookies();\n&gt;-    }\n&gt;-\n&gt;-    /* (non-Javadoc)\n&gt;-     * @see org.archive.crawler.settings.ModuleType#listUsedFiles(java.util.List)\n&gt;-     */\n&gt;-    protected void listUsedFiles(List list) {\n&gt;-        // List the cookies files\n&gt;-        // Add seed file\n&gt;-        try {\n&gt;-            String tmp = (String)getAttribute(ATTR_LOAD_COOKIES);\n&gt;-            if(tmp != null && tmp.length() &gt; 0){\n&gt;-                File file = getSettingsHandler().\n&gt;-                        getPathRelativeToWorkingDirectory(tmp);\n&gt;-                list.add(file.getAbsolutePath());\n&gt;-            }\n&gt;-            tmp = (String)getAttribute(ATTR_SAVE_COOKIES);\n&gt;-            if(tmp != null && tmp.length() &gt; 0){\n&gt;-                File file = getSettingsHandler().\n&gt;-                        getPathRelativeToWorkingDirectory(tmp);\n&gt;-                list.add(file.getAbsolutePath());\n&gt;-            }\n&gt;-        } catch (AttributeNotFoundException e) {\n&gt;-            // TODO Auto-generated catch block\n&gt;-            e.printStackTrace();\n&gt;-        } catch (MBeanException e) {\n&gt;-            // TODO Auto-generated catch block\n&gt;-            e.printStackTrace();\n&gt;-        } catch (ReflectionException e) {\n&gt;-            // TODO Auto-generated catch block\n&gt;-            e.printStackTrace();\n&gt;-        }\n&gt;-    }\n&gt;-    \n&gt;-    // custom serialization\n&gt;-    private void writeObject(ObjectOutputStream stream) throws IOException {\n&gt;-        stream.defaultWriteObject();\n&gt;-        // save cookies\n&gt;-        stream.writeObject(http.getState().getCookies());\n&gt;-    }\n&gt;-    \n&gt;-    private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException {\n&gt;-        stream.defaultReadObject();\n&gt;-        Cookie cookies[] = (Cookie[]) stream.readObject();\n&gt;-        ObjectPlusFilesInputStream coistream = (ObjectPlusFilesInputStream)stream;\n&gt;-        coistream.registerFinishTask( new PostRestore(cookies) );\n&gt;-    }\n&gt;-    \n&gt;-    class PostRestore implements Runnable {\n&gt;-        Cookie cookies[];\n&gt;-        public PostRestore(Cookie cookies[]) {\n&gt;-            this.cookies = cookies;\n&gt;-        }\n&gt;-    \tpublic void run() {\n&gt;-            setupHttp();\n&gt;-            for(int i = 0; i &lt; cookies.length; i++) {\n&gt;-                FetchHTTP.this.http.getState().addCookie(cookies[i]);\n&gt;-            }\n&gt;-        }\n&gt;-    }\n&gt;-}\n&gt;+\t\tSingleHttpConnectionManager connectionManager = new SingleHttpConnectionManager();\n&gt;+\t\tthis.http = new PatchedHttpClient(connectionManager);\n&gt;+\n&gt;+\t\ttry {\n&gt;+\t\t\tString trustLevel = (String) getAttribute(ATTR_TRUST);\n&gt;+\t\t\tProtocol.registerProtocol(&quot;https&quot;, new Protocol(&quot;https&quot;,\n&gt;+\t\t\t\t\tnew ConfigurableTrustManagerProtocolSocketFactory(\n&gt;+\t\t\t\t\t\t\ttrustLevel), 443));\n&gt;+\t\t}\n&gt;+\n&gt;+\t\tcatch (Exception e) {\n&gt;+\t\t\t// Convert all to RuntimeException so get an exception out if\n&gt;+\t\t\t// initialization fails.\n&gt;+\t\t\tthrow new RuntimeException(\n&gt;+\t\t\t\t\t&quot;Failed initialization getting attributes: &quot;\n&gt;+\t\t\t\t\t\t\t+ e.getMessage());\n&gt;+\t\t}\n&gt;+\n&gt;+\t\t// Considered same as overall timeout, for now.\n&gt;+\t\t// TODO: When HTTPClient stops using a monitor &#39;waitingThread&#39;\n&gt;+\t\t// thread to watch over the getting of the socket from socket\n&gt;+\t\t// factory and instead supports the java.net.Socket#connect timeout.\n&gt;+\t\t// http.setConnectionTimeout((int)timeout);\n&gt;+\t\t// set per-read() timeout: overall timeout will be checked at least\n&gt;+\t\t// this\n&gt;+\t\t// frequently\n&gt;+\t\tthis.http.setTimeout(this.soTimeout);\n&gt;+\t}\n&gt;+\n&gt;+\tprivate int getSoTimeout(CrawlURI curi) {\n&gt;+\t\tInteger res;\n&gt;+\t\ttry {\n&gt;+\t\t\tres = (Integer) getAttribute(ATTR_SOTIMEOUT_MS, curi);\n&gt;+\t\t} catch (Exception e) {\n&gt;+\t\t\tres = DEFAULT_SOTIMEOUT_MS;\n&gt;+\t\t}\n&gt;+\t\treturn res.intValue();\n&gt;+\t}\n&gt;+\n&gt;+\tprivate int getTimeout(CrawlURI curi) {\n&gt;+\t\tInteger res;\n&gt;+\t\ttry {\n&gt;+\t\t\tres = (Integer) getAttribute(ATTR_TIMEOUT_SECONDS, curi);\n&gt;+\t\t} catch (Exception e) {\n&gt;+\t\t\tres = DEFAULT_TIMEOUT_SECONDS;\n&gt;+\t\t}\n&gt;+\t\treturn res.intValue();\n&gt;+\t}\n&gt;+\n&gt;+\tprivate long getMaxLength(CrawlURI curi) {\n&gt;+\t\tLong res;\n&gt;+\t\ttry {\n&gt;+\t\t\tres = (Long) getAttribute(ATTR_MAX_LENGTH_BYTES, curi);\n&gt;+\t\t} catch (Exception e) {\n&gt;+\t\t\tres = DEFAULT_MAX_LENGTH_BYTES;\n&gt;+\t\t}\n&gt;+\t\treturn res.longValue();\n&gt;+\t}\n&gt;+\n&gt;+\t/**\n&gt;+\t * Load cookies from a file before the first fetch.\n&gt;+\t * &lt;p&gt;\n&gt;+\t * The file is a text file in the Netscape&#39;s &#39;cookies.txt&#39; file format. &lt;br&gt;\n&gt;+\t * Example entry of cookies.txt file: &lt;br&gt;\n&gt;+\t * &lt;br&gt;\n&gt;+\t * www.archive.org FALSE / FALSE 1074567117 details-visit texts-cralond &lt;br&gt;\n&gt;+\t * &lt;br&gt;\n&gt;+\t * Each line has 7 tab-separated fields: &lt;br&gt;\n&gt;+\t * &lt;li&gt;1. DOMAIN: The domain that created and have access to the cookie\n&gt;+\t * value.\n&gt;+\t * &lt;li&gt;2. FLAG: A TRUE or FALSE value indicating if hosts within the given\n&gt;+\t * domain can access the cookie value.\n&gt;+\t * &lt;li&gt;3. PATH: The path within the domain that the cookie value is valid\n&gt;+\t * for.\n&gt;+\t * &lt;li&gt;4. SECURE: A TRUE or FALSE value indicating if to use a secure\n&gt;+\t * connection to access the cookie value.\n&gt;+\t * &lt;li&gt;5. EXPIRATION: The expiration time of the cookie value (unix style.)\n&gt;+\t * &lt;li&gt;6. NAME: The name of the cookie value\n&gt;+\t * &lt;li&gt;7. VALUE: The cookie value\n&gt;+\t * \n&gt;+\t * @param cookiesFile\n&gt;+\t *                    file in the Netscape&#39;s &#39;cookies.txt&#39; format.\n&gt;+\t */\n&gt;+\tpublic void loadCookies(String cookiesFile) {\n&gt;+\t\t// Do nothing if cookiesFile is not specified.\n&gt;+\t\tif (cookiesFile == null || cookiesFile.length() &lt;= 0) {\n&gt;+\t\t\treturn;\n&gt;+\t\t}\n&gt;+\t\tRandomAccessFile raf = null;\n&gt;+\t\ttry {\n&gt;+\t\t\traf = new RandomAccessFile(cookiesFile, &quot;r&quot;);\n&gt;+\t\t\tString[] cookieParts;\n&gt;+\t\t\tString line;\n&gt;+\t\t\tCookie cookie = null;\n&gt;+\t\t\twhile ((line = raf.readLine()) != null) {\n&gt;+\t\t\t\t// Line that starts with # is commented line, therefore skip it.\n&gt;+\t\t\t\tif (!line.startsWith(&quot;#&quot;)) {\n&gt;+\t\t\t\t\tcookieParts = line.split(&quot;&#92;&#92;t&quot;);\n&gt;+\t\t\t\t\tif (cookieParts.length == 7) {\n&gt;+\t\t\t\t\t\t// Create cookie with not expiration date (-1 value).\n&gt;+\t\t\t\t\t\t// TODO: add this as an option.\n&gt;+\t\t\t\t\t\tcookie = new Cookie(cookieParts[0], cookieParts[5],\n&gt;+\t\t\t\t\t\t\t\tcookieParts[6], cookieParts[2], -1, Boolean\n&gt;+\t\t\t\t\t\t\t\t\t\t.valueOf(cookieParts[3]).booleanValue());\n&gt;+\n&gt;+\t\t\t\t\t\tif (cookieParts[1].toLowerCase().equals(&quot;true&quot;)) {\n&gt;+\t\t\t\t\t\t\tcookie.setDomainAttributeSpecified(true);\n&gt;+\t\t\t\t\t\t} else {\n&gt;+\t\t\t\t\t\t\tcookie.setDomainAttributeSpecified(false);\n&gt;+\t\t\t\t\t\t}\n&gt;+\t\t\t\t\t\tthis.http.getState().addCookie(cookie);\n&gt;+\t\t\t\t\t\tlogger\n&gt;+\t\t\t\t\t\t\t\t.fine(&quot;Adding cookie: &quot;\n&gt;+\t\t\t\t\t\t\t\t\t\t+ cookie.toExternalForm());\n&gt;+\t\t\t\t\t}\n&gt;+\t\t\t\t}\n&gt;+\t\t\t}\n&gt;+\t\t} catch (FileNotFoundException e) {\n&gt;+\t\t\t// We should probably throw FatalConfigurationException.\n&gt;+\t\t\tSystem.out.println(&quot;Could not find file: &quot; + cookiesFile\n&gt;+\t\t\t\t\t+ &quot; (Element: &quot; + ATTR_LOAD_COOKIES + &quot;)&quot;);\n&gt;+\n&gt;+\t\t} catch (IOException e) {\n&gt;+\t\t\t// We should probably throw FatalConfigurationException.\n&gt;+\t\t\te.printStackTrace();\n&gt;+\t\t} finally {\n&gt;+\t\t\ttry {\n&gt;+\t\t\t\tif (raf != null) {\n&gt;+\t\t\t\t\traf.close();\n&gt;+\t\t\t\t}\n&gt;+\t\t\t} catch (IOException e) {\n&gt;+\t\t\t\te.printStackTrace();\n&gt;+\t\t\t}\n&gt;+\t\t}\n&gt;+\t}\n&gt;+\n&gt;+\t/*\n&gt;+\t * (non-Javadoc)\n&gt;+\t * \n&gt;+\t * @see org.archive.crawler.framework.Processor#report()\n&gt;+\t */\n&gt;+\tpublic String report() {\n&gt;+\t\tStringBuffer ret = new StringBuffer();\n&gt;+\t\tret.append(&quot;Processor: org.archive.crawler.fetcher.FetchHTTP&#92;n&quot;);\n&gt;+\t\tret.append(&quot;  Function:     \n(Message over 64 KB, truncated)"}}