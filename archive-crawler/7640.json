{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":509984195,"authorName":"thomas.zeithaml","from":"&quot;thomas.zeithaml&quot; &lt;thomas.zeithaml@...&gt;","profile":"thomas.zeithaml","replyTo":"LIST","senderId":"WD86yCx9CWLSuJ9vMuyYiyhTnwC7G5cI2JHoVAk0_nyxhCdxKi-Dvb6hSGJloJG95lZJxLBJ0FByvxUe4ylH3utOe4qg94ccxucjc1qeVZdeJgM","spamInfo":{"isSpam":false,"reason":"12"},"subject":"robots.txt","postDate":"1332344406","msgId":7640,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGprY3NvbStuaGM4QGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":7673,"prevInTime":7639,"nextInTime":7641,"topicId":7640,"numMessagesInTopic":2,"msgSnippet":"Hi all, is there a possibility to get all sites from a crawl which was forbidden by the robots.txt ? Is there a information in the WARC File - or is it needed","rawEmail":"Return-Path: &lt;thomas.zeithaml@...&gt;\r\nX-Sender: thomas.zeithaml@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 64477 invoked from network); 21 Mar 2012 15:40:10 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m11.grp.sp2.yahoo.com with QMQP; 21 Mar 2012 15:40:10 -0000\r\nX-Received: from unknown (HELO ng5-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.57)\n  by mta5.grp.sp2.yahoo.com with SMTP; 21 Mar 2012 15:40:10 -0000\r\nX-Received: from [98.137.0.81] by ng5.bullet.mail.gq1.yahoo.com with NNFMP; 21 Mar 2012 15:40:07 -0000\r\nX-Received: from [69.147.65.151] by tg10.bullet.mail.gq1.yahoo.com with NNFMP; 21 Mar 2012 15:40:07 -0000\r\nX-Received: from [98.137.35.12] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 21 Mar 2012 15:40:07 -0000\r\nDate: Wed, 21 Mar 2012 15:40:06 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jkcsom+nhc8@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;thomas.zeithaml&quot; &lt;thomas.zeithaml@...&gt;\r\nSubject: robots.txt\r\nX-Yahoo-Group-Post: member; u=509984195; y=uDs1onkJMoq0VAc7mOkbnpei0wZcjBNoR7LEe63EFq_Kx6oF9M8yYBjT\r\nX-Yahoo-Profile: thomas.zeithaml\r\n\r\nHi all,\nis there a possibility to get all sites from a crawl which was forb=\r\nidden by the robots.txt ?\n\nIs there a information in the WARC File - or is =\r\nit needed to parse the crawl.log ?\n\nbest wishes \nTom\n\n\n"}}