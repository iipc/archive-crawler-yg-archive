{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":280593181,"authorName":"Erik Hetzner","from":"Erik Hetzner &lt;erik.hetzner@...&gt;","profile":"e_hetzner","replyTo":"LIST","senderId":"7n7V70qAIKuuQ94sp58DA81NfIYuTvVDqeUlZEKVrcJIkpCJurdwM1d-CXHuuALQxVBtBLLAxSrvXOP-BOf672SuxjawprAZEWGp","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] rebuild crawl.log","postDate":"1280521887","msgId":6629,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFAtSVJDLUVYQkUwMUdadU5ZMVMwMDAwMDZmMkBFWC5VQ09QLkVEVT4=","inReplyToHeader":"PEE1NDZGQkRGN0QyNUIzNDY4NTI3RTlERDAxMThDNTYyMDI5NUJDMDFAU0JFTTM1RVhDMTAxMS5lZmQuaW50cmEuYWRtaW4uY2g+","referencesHeader":"PEE1NDZGQkRGN0QyNUIzNDY4NTI3RTlERDAxMThDNTYyMDI5NUJDMDFAU0JFTTM1RVhDMTAxMS5lZmQuaW50cmEuYWRtaW4uY2g+"},"prevInTopic":6628,"nextInTopic":6637,"prevInTime":6628,"nextInTime":6630,"topicId":6626,"numMessagesInTopic":4,"msgSnippet":"At Fri, 30 Jul 2010 09:03:12 +0200, ... No, generally, because the crawl.log contains information not present in the ARC files. For WARC files it may be a","rawEmail":"Return-Path: &lt;Erik.Hetzner@...&gt;\r\nX-Sender: Erik.Hetzner@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 23259 invoked from network); 30 Jul 2010 20:31:28 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m12.grp.re1.yahoo.com with QMQP; 30 Jul 2010 20:31:28 -0000\r\nX-Received: from unknown (HELO mailgate-02.ucop.edu) (128.48.123.91)\n  by mta2.grp.sp2.yahoo.com with SMTP; 30 Jul 2010 20:31:28 -0000\r\nX-WSS-ID: 0L6E10D-04-16K-02\r\nX-M-MSG: \r\nX-Received: from EX.UCOP.EDU (p-irc-exbe01.ucop.edu [128.48.122.86])\n\tby mailgate-02.ucop.edu (Tumbleweed MailGate 3.7.2) with ESMTP id 23E26ABE40\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 30 Jul 2010 13:31:24 -0700 (PDT)\r\nX-Received: from gales.cdlib.org ([128.48.204.228]) by EX.UCOP.EDU with Microsoft SMTPSVC(6.0.3790.4675);\n\t Fri, 30 Jul 2010 13:31:27 -0700\r\nDate: Fri, 30 Jul 2010 13:31:27 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;A546FBDF7D25B3468527E9DD0118C5620295BC01@...&gt;\r\nReferences: &lt;A546FBDF7D25B3468527E9DD0118C5620295BC01@...&gt;\r\nUser-Agent: Wanderlust/2.15.9 (Almost Unreal) SEMI/1.14.6 (Maruoka)\n FLIM/1.14.9 (=?UTF-8?B?R29qxY0=?=) APEL/10.8 Emacs/23.1\n (x86_64-pc-linux-gnu) MULE/6.0 (HANACHIRUSATO)\r\nMIME-Version: 1.0 (generated by SEMI 1.14.6 - &quot;Maruoka&quot;)\r\nContent-Type: multipart/signed;\n boundary=&quot;pgp-sign-Multipart_Fri_Jul_30_13:31:27_2010-1&quot;; micalg=pgp-sha1;\n protocol=&quot;application/pgp-signature&quot;\r\nContent-Transfer-Encoding: 7bit\r\nReturn-Path: erik.hetzner@...\r\nMessage-ID: &lt;P-IRC-EXBE01GZuNY1S000006f2@...&gt;\r\nX-OriginalArrivalTime: 30 Jul 2010 20:31:27.0041 (UTC) FILETIME=[2F493B10:01CB3026]\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Erik Hetzner &lt;erik.hetzner@...&gt;\r\nSubject: Re: [archive-crawler] rebuild crawl.log\r\nX-Yahoo-Group-Post: member; u=280593181; y=YEqmk9E-5O-wWgxf2GoqEwytuTrrIWumiXuYjtjtjTLJbpVT\r\nX-Yahoo-Profile: e_hetzner\r\n\r\n\r\n--pgp-sign-Multipart_Fri_Jul_30_13:31:27_2010-1\r\nContent-Type: multipart/mixed;\n boundary=&quot;Multipart_Fri_Jul_30_13:31:27_2010-1&quot;\r\n\r\n\r\n--Multipart_Fri_Jul_30_13:31:27_2010-1\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nAt Fri, 30 Jul 2010 09:03:12 +0200,\n&lt;mac.kobus@...&gt; wrote:\n&gt; \n&gt; Hi =\r\neverybody,\n&gt; \n&gt; usually we use Heritrix to harvest single websites, so they=\r\n come each with their own crawl.log.\n&gt; \n&gt; In our Ingestprocess the crawl.lo=\r\ng is used to verify the content of the delivered ARC-files.\n&gt; \n&gt;  \n&gt; \n&gt; Now=\r\n I want to ingest (some externally crawled) ARC-files, that actually don=C2=\r\n=B4t have a crawl.log.\n&gt; \n&gt; Here are my questions:\n&gt; \n&gt; 1.       Is it poss=\r\nible to rebuild a crawl.log from existing ARC-files?\n\nNo, generally, becaus=\r\ne the crawl.log contains information not present\nin the ARC files. For WARC=\r\n files it may be a different story.\n\nHowever you could build a mostly equiv=\r\nalent version of a crawl.log\nfrom an existing set of ARC files, with timest=\r\namps, URLs, response\ncodes, etc. You would be missing at least (if I recall=\r\n correctly) the\ntime the fetch took, the discovery path (the LXXE looking s=\r\ntring) as\nwell as some entries which do not generate an ARC entry.\n\nProbabl=\r\ny the Heritrix devs have more accurate information.\n\nbest, Erik Hetzner\n\r\n--Multipart_Fri_Jul_30_13:31:27_2010-1\r\nContent-Type: text/plain; charset=US-ASCII\r\n\r\nSent from my free software system &lt;http://fsf.org/&gt;.\n\r\n--Multipart_Fri_Jul_30_13:31:27_2010-1--\r\n\n\r\n--pgp-sign-Multipart_Fri_Jul_30_13:31:27_2010-1\r\nContent-Type: application/pgp-signature\r\n\r\n[ Attachment content not displayed ]\r\n--pgp-sign-Multipart_Fri_Jul_30_13:31:27_2010-1--\r\n\n"}}