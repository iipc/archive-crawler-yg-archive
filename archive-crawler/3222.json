{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":139911393,"authorName":"thiru_sundaram","from":"&quot;thiru_sundaram&quot; &lt;thiru_sundaram@...&gt;","profile":"thiru_sundaram","replyTo":"LIST","senderId":"tA-PE6aRBLuBHbBIbCJ5Bo2YipWgBGzAZthwAUZ2H9Lj-Qe6C4zMNs6S6enMkuiN2qM_oxknlzcIUCeXuSiG1qydL75wPj2wxomdAH4ofah0iQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Crawl using Proxy","postDate":"1157018427","msgId":3222,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVkNmJ2cis5dTloQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0RjMxMDdCLjUwMTAzMDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":3221,"nextInTopic":0,"prevInTime":3221,"nextInTime":3223,"topicId":3220,"numMessagesInTopic":3,"msgSnippet":"Hi, Thanks for the help. I had left the configuration to default time [30min] and i could not figure it out. I reduced the timings and its working fine now. ","rawEmail":"Return-Path: &lt;thiru_sundaram@...&gt;\r\nX-Sender: thiru_sundaram@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 90912 invoked from network); 31 Aug 2006 10:01:51 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m34.grp.scd.yahoo.com with QMQP; 31 Aug 2006 10:01:51 -0000\r\nReceived: from unknown (HELO n8c.bullet.sc5.yahoo.com) (66.163.187.199)\n  by mta6.grp.scd.yahoo.com with SMTP; 31 Aug 2006 10:01:50 -0000\r\nReceived: from [66.163.187.120] by n8.bullet.sc5.yahoo.com with NNFMP; 31 Aug 2006 10:00:28 -0000\r\nReceived: from [66.218.69.5] by t1.bullet.sc5.yahoo.com with NNFMP; 31 Aug 2006 10:00:28 -0000\r\nReceived: from [66.218.66.80] by t5.bullet.scd.yahoo.com with NNFMP; 31 Aug 2006 10:00:28 -0000\r\nDate: Thu, 31 Aug 2006 10:00:27 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;ed6bvr+9u9h@...&gt;\r\nIn-Reply-To: &lt;44F3107B.5010302@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;thiru_sundaram&quot; &lt;thiru_sundaram@...&gt;\r\nSubject: Re: Crawl using Proxy\r\nX-Yahoo-Group-Post: member; u=139911393; y=KFZBlj_lMzmwB57l-MsGNhW5jCEdCTwZ7vNZviPJlVOXNVZdCZLVPAw\r\nX-Yahoo-Profile: thiru_sundaram\r\n\r\n\nHi,\nThanks for the help. I had left the configuration to default time\n[30m=\r\nin] and i could not figure it out. I reduced the timings and its\nworking fi=\r\nne now.\n\n\n--- In archive-crawler@yahoogroups.com, Igor Ranitovic &lt;igor@...&gt;=\r\n wrote:\n&gt;\n&gt; Hello,\n&gt; \n&gt; The error that you are seeing is a retry-able fetch=\r\n error. By default \n&gt; there are 30 retires with a 15 minute wait between th=\r\nem. Both of these \n&gt; settings are configurable.\n&gt; \n&gt; If you are having only=\r\n one host (one queue), then the crawler will be \n&gt; idle for 15 minutes afte=\r\nr each of these errors happen. This is why you \n&gt; might think that the craw=\r\nl has stopped.\n&gt; \n&gt; Now, I am pretty sure that pausing and resuming the job=\r\n should not \n&gt; change this behavior, but if it does it is a bug.\n&gt; \n&gt; What =\r\nversion of Heritrix are you using?\n&gt; \n&gt; Igor\n&gt; \n&gt; thiru_sundaram wrote:\n&gt; &gt;=\r\n Hi all,\n&gt; &gt; I setup configuration to crawl a website using proxy. But some=\r\n time i\n&gt; &gt; get the below exception [ in local-errors.log]. Heritrix stops\n=\r\n&gt; &gt; crawling after that. If i pause and resume the job through UI, it\n&gt; &gt; a=\r\ngain starts from the point it left out. Any idea why socket time out\n&gt; &gt; ex=\r\nception for one url makes heritrix stop crawling ?\n&gt; &gt; \n&gt; &gt; I have given be=\r\nlow the error pattern\n&gt; &gt; 2006-08-28T12:30:23.174Z    -2          - &quot;url1&quot; =\r\nL &quot;url2&quot; no-type #022\n&gt; &gt; - - - le:SocketTimeoutException@HTTP\n&gt; &gt;         =\r\nat java.net.SocketTimeoutException: Read timed out\n&gt; &gt;         at java.net.=\r\nSocketInputStream.read(SocketInputStream.java:129)\n&gt; &gt;         at\njava.io.B=\r\nufferedInputStream.fill(BufferedInputStream.java:218)\n&gt; &gt;         at\njava.i=\r\no.BufferedInputStream.read(BufferedInputStream.java:235)\n&gt; &gt;         at\n&gt; &gt;=\r\n org.archive.io.RecordingInputStream.read(RecordingInputStream.java:96)\n&gt; &gt;=\r\n         at\n&gt; &gt;\norg.apache.commons.httpclient.HttpParser.readRawLine(HttpPa=\r\nrser.java:77)\n&gt; &gt;         at\n&gt; &gt; org.apache.commons.httpclient.HttpParser.r=\r\neadLine(HttpParser.java:105)\n&gt; &gt;         at\n&gt; &gt;\norg.apache.commons.httpclie=\r\nnt.HttpConnection.readLine(HttpConnection.java:1129)\n&gt; &gt;         at\n&gt; &gt;\norg=\r\n.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.jav=\r\na:1850)\n&gt; &gt;         at\n&gt; &gt;\norg.apache.commons.httpclient.HttpMethodBase.rea=\r\ndResponse(HttpMethodBase.java:1610)\n&gt; &gt;         at\n&gt; &gt;\norg.apache.commons.h=\r\nttpclient.HttpMethodBase.execute(HttpMethodBase.java:1000)\n&gt; &gt;         at\n&gt;=\r\n &gt;\norg.archive.httpclient.HttpRecorderGetMethod.execute(HttpRecorderGetMeth=\r\nod.java:117)\n&gt; &gt;         at\n&gt; &gt;\norg.apache.commons.httpclient.HttpMethodDir=\r\nector.executeWithRetry(HttpMethodDirector.java:393)\n&gt; &gt;         at\n&gt; &gt;\norg.=\r\napache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirect=\r\nor.java:168)\n&gt; &gt;         at\n&gt; &gt;\norg.apache.commons.httpclient.HttpClient.ex=\r\necuteMethod(HttpClient.java:396)\n&gt; &gt;         at\n&gt; &gt;\norg.apache.commons.http=\r\nclient.HttpClient.executeMethod(HttpClient.java:324)\n&gt; &gt;         at\n&gt; &gt; org=\r\n.archive.crawler.fetcher.FetchHTTP.innerProcess(FetchHTTP.java:408)\n&gt; &gt;    =\r\n     at\n&gt; &gt; org.archive.crawler.framework.Processor.process(Processor.java:=\r\n103)\n&gt; &gt;         at\n&gt; &gt;\norg.archive.crawler.framework.ToeThread.processCraw=\r\nlUri(ToeThread.java:306)\n&gt; &gt;         at\norg.archive.crawler.framework.ToeTh=\r\nread.run(ToeThread.java:153)\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; Y=\r\nahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n\n\n\n"}}