{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":308470903,"authorName":"ivlcic","from":"&quot;ivlcic&quot; &lt;ivlcic@...&gt;","profile":"ivlcic","replyTo":"LIST","senderId":"rd0JUYRC9DEQaNLf9iTs2vX3rI3CjoD0lB5GJQIKYfkm8Ow4Q5IFIQFAnuVBbVtvcXsk10Q-I0pwLuHmlCgP6Irn","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: New duplication-reduction features in 1.12 and later versions","postDate":"1177488149","msgId":4177,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYwbjF1bCt1a2s1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ2MmM0OWY5LjE3NjYwMzAwLjFjOGMuZmZmZmU0YWZAbXguZ29vZ2xlLmNvbT4="},"prevInTopic":4166,"nextInTopic":4190,"prevInTime":4176,"nextInTime":4178,"topicId":4166,"numMessagesInTopic":4,"msgSnippet":"I ve managed to make some tests and here is how I see it. Please note that I m just a Heritrix beginner and I had the same wiki page as reference. 1.) Setting","rawEmail":"Return-Path: &lt;ivlcic@...&gt;\r\nX-Sender: ivlcic@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 82219 invoked from network); 25 Apr 2007 08:05:09 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m40.grp.scd.yahoo.com with QMQP; 25 Apr 2007 08:05:09 -0000\r\nReceived: from unknown (HELO n12c.bullet.sp1.yahoo.com) (69.147.64.111)\n  by mta6.grp.scd.yahoo.com with SMTP; 25 Apr 2007 08:05:09 -0000\r\nReceived: from [216.252.122.217] by n12.bullet.sp1.yahoo.com with NNFMP; 25 Apr 2007 08:02:29 -0000\r\nReceived: from [66.218.69.4] by t2.bullet.sp1.yahoo.com with NNFMP; 25 Apr 2007 08:02:29 -0000\r\nReceived: from [66.218.66.87] by t4.bullet.scd.yahoo.com with NNFMP; 25 Apr 2007 08:02:29 -0000\r\nDate: Wed, 25 Apr 2007 08:02:29 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;f0n1ul+ukk5@...&gt;\r\nIn-Reply-To: &lt;462c49f9.17660300.1c8c.ffffe4af@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;3-4501875438-0119401832=:4&quot;\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;ivlcic&quot; &lt;ivlcic@...&gt;\r\nSubject: Re: New duplication-reduction features in 1.12 and later versions\r\nX-Yahoo-Group-Post: member; u=308470903; y=EMtCW5zk-fGeHlSefyv2VHOzdpT_mKye6Ma2ObATfW3r\r\nX-Yahoo-Profile: ivlcic\r\n\r\n\r\n--3-4501875438-0119401832=:4\r\nContent-Type: text/plain; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nI&#39;ve managed to make some tests and here is how I see it.\nPlease note that =\r\nI&#39;m just a Heritrix beginner and I had the same wiki\npage as reference.\n\n1.=\r\n) Setting up the processors\n1.1.) I&#39;ve put on top of the fetch-processors l=\r\nist the:\norg.archive.crawler.processor.recrawl.PersistLoadProcessor\nThis pr=\r\nocessor loads all previous CrawlURL&#39;s pressistent AList data from\nthe crawl=\r\n state directory. The most important is the content digest from\nprevious cr=\r\nawls associated with CrawlURL\n\n1.2.) On the bottom of the fetch-processors =\r\nI&#39;ve put the:\norg.archive.crawler.processor.recrawl.FetchHistoryProcessor\nT=\r\nhis processor is responsible form maintaining the history list of\nCrawlURLs=\r\n from previous crawls including the current CrawlURL.\nThats why we are supp=\r\nosed to have the minimum of two. At least one\nslot for previous and one slo=\r\nt for current.\n\n1.3.) At the end of whole chain (post-processors) I&#39;ve put =\r\nthe:\norg.archive.crawler.processor.recrawl.PersistStoreProcessor\nThis proce=\r\nssor actually stores the CrawlURLs to Berkely DB in to state\ndirectory of t=\r\nhe crawl.\n\n\n2.) Using the processors\n\n2.1.) In order to use the current set=\r\nup (As far as I get it) we have to\nmaintain the same state directory for ea=\r\nch crawl since the BDB files in\nit have also a history of previous crawls.\n=\r\nEnable the view of expert settings in web UI and set up the crawl order\nsta=\r\nte-path attribute to always point\nto the same directory for all the jobs.\n\n=\r\n2.2.) To disable writing the duplicate pages I&#39;ve set the attribute\nskip-id=\r\nentical-digests=3Dtrue on the\norg.archive.crawler.writer.ARCWriterProcessor=\r\n\n\nTo achive the same thing on processors that do not have such attribute\nI&#39;=\r\nve used the\norg.archive.crawler.deciderules.recrawl.IdenticalDigestDecideRu=\r\nle filter\nsetting it to REJECT.\n(org.archive.crawler.writer.MirrorWriterPro=\r\ncessor is such processor)\n\nYou should also maintain the same &quot;mirror&quot; or &quot;a=\r\nrcs&quot; store path between\ncrawls.\nIf you don&#39;t you will get only the &quot;changed=\r\n pages&quot;\n(changed pages are those with different content digests and/or etag=\r\n,\nmodified since headers)\n\n2.3.) To prevent the fetching of the same CrawlU=\r\nRLs for the following\ncrawls I&#39;ve set up\nsend-if-modified-since=3Dtrue\nsend=\r\n-if-none-match=3Dtrue\non the org.archive.crawler.fetcher.FetchHTTP.\nThis on=\r\nly works for static pages / files (send-if-modified-since) or\nwith dynamic =\r\npages that support the\netag HTTP response header (send-if-none-match). Thos=\r\ne are the most\ncommon options that you can use to decide\nprior to fetching =\r\nthe whole content.\n\n2.4.) To minimize the content processing and link extra=\r\nction you can use\nTrapSuppressExtractor.\nIf you crawled URL [A] with conten=\r\nt digest of [123] and just fetched the\nURL [B] with content digest of [123]=\r\n,\nthen this processor disables link extraction so no further processing is\n=\r\ndone. (the links from URL [B] are never promoted back to frontier)\n(I haven=\r\n&#39;t tested this one yet)\n\n\n3.) alternatives\nInstead of org.archive.crawler.p=\r\nrocessor.recrawl.PersistLoadProcessor\nyou can use the\norg.archive.crawler.p=\r\nrocessor.recrawl.PersistLogProcessor that uses txt\nfile for history\n(don&#39;t =\r\nuse both)\n\nIf you want to minimize the fetch duplication between crawls the=\r\n only\nthing you can do is mimic the\nthe org.archive.crawler.fetcher.FetchHT=\r\nTP.send-if-none-match and handle\nsome specific\nHTTP headers that target hos=\r\nts return.\nYou can store specific data to CrawlURLs persistent AList.\n(I gu=\r\ness Heritrix people didn&#39;t do it because there is no standard. I\nhaven&#39;t te=\r\nsted this one yet also)\n\nNikola\n\n--- In archive-crawler@yahoogroups.com, &quot;C=\r\netin Sert&quot; &lt;cetin.sert@...&gt;\nwrote:\n&gt;\n&gt; Hi,\n&gt;\n&gt;\n&gt;\n&gt; I would love to know how=\r\n to use the new duplicate-reduction features.\nThe\n&gt; wiki page about them is=\r\n too short to understand and there are many new\n&gt; processors etc., which ma=\r\nkes it difficult for me to understand how\nbest to\n&gt; combine them.\n&gt;\n&gt; B. Ab=\r\nbrieviate retrieval of unchanged content.\n&gt;\n&gt; Based on headers of previous =\r\nretrieval of same URI, adjust current\nretrieval\n&gt; (via conditional-GET requ=\r\nests) so that if content is unchanged,\nretrieval\n&gt; ends quickly and cheaply=\r\n and no duplicate content is stored. Motivated\nby\n&gt; KB-Denmark study (Claus=\r\nen 2004); reduces bandwidth used as well as\nstorage.\n&gt;\n&gt; This is accomplish=\r\ned in Heritrix 1.12.0 by:\n&gt;\n&gt; * Using the FetchHistoryProcessor and either =\r\nPersistLogProcessor or\n&gt; PersistStoreProcessor on an initial crawl1\n&gt; * Car=\r\nrying forward the initial crawl info in a form accessible to a\n&gt; later craw=\r\nl, perhaps by using utility functionality on the\nPersistProcessor\n&gt; class2\n=\r\n&gt; * Using the PersistLoadProcessor and FetchHistoryProcessor in a\n&gt; followi=\r\nng crawl3 so that relevant history information is available at\n&gt; store-deci=\r\nsion time3\n&gt; * Using new capabilities of the FetchHTTP processor to issue\n&gt;=\r\n conditional-GETs where appropriate\n&gt; * Using new options on any writer pro=\r\ncessors to skip or abbrieviate\n&gt; storage as desired4\n&gt;\n&gt; C. Note and discar=\r\nd duplicate &#39;trap&#39; content from same crawl.\n&gt;\n&gt; For one simple kind of craw=\r\nler-trap, where followup URIs return\nidentical\n&gt; content at different (exte=\r\nnded) URIs, disable link-extraction.\n&gt;\n&gt; The TrapSuppressExtractor in 1.12.=\r\n0 demonstrates this strategy, and\ncould be\n&gt; the basis for other similar tr=\r\nap/duplicate suppression processors.5\n&gt;\n&gt; 1: where exactly? Is this a prepr=\r\nocessor, a postprocessor or a writer\n&gt; module?\n&gt;\n&gt; 2: where and how?\n&gt;\n&gt; 3:=\r\n where and how?\n&gt;\n&gt; 4: are these options readily implemented or should we i=\r\nmplement them\n&gt; ourselves? If they are already there, then how do we use th=\r\nem?\n&gt;\n&gt; 5: how?\n&gt;\n&gt;\n&gt;\n&gt; A configuration xml file (order.xml) with all the s=\r\nettings enabled, or\na few\n&gt; screenshots of the web interface with the proce=\r\nssors at place might be\nof\n&gt; great help.\n&gt;\n&gt;\n&gt;\n&gt; I have had experience with=\r\n implementing custom decision rules and\nwriters\n&gt; but this seems to require=\r\n some explanation.\n&gt;\n&gt;\n&gt;\n&gt; Thanks in advance to anyone who takes time to an=\r\nswer my question.\n&gt;\n&gt;\n&gt;\n&gt; Yours Sincerely,\n&gt;\n&gt; Cetin Sert\n&gt;\n\n\r\n--3-4501875438-0119401832=:4\r\nContent-Type: text/html; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nI&#39;ve managed to make some tests and here is how I see it.&lt;br&gt;Please note th=\r\nat I&#39;m just a Heritrix beginner and I had the same wiki page as reference.&lt;=\r\nbr&gt;&lt;br&gt;1.) Setting up the processors&lt;br&gt;1.1.) I&#39;ve put on top of the fetch-=\r\nprocessors list the:&lt;br&gt;&lt;font face=3D&quot;courier new&quot;&gt;org.archive.crawler.proc=\r\nessor.recrawl.PersistLoadProcessor&lt;/font&gt;&lt;br&gt;This processor loads all previ=\r\nous &lt;font face=3D&quot;courier new&quot;&gt;CrawlURL&#39;s&lt;/font&gt; pressistent &lt;font face=3D&quot;=\r\ncourier new&quot;&gt;AList&lt;/font&gt; data from the crawl &lt;font face=3D&quot;courier new&quot;&gt;st=\r\nate&lt;/font&gt; directory. The most important is the content digest from previou=\r\ns crawls associated with &lt;font face=3D&quot;courier new&quot;&gt;CrawlURL&lt;/font&gt;&lt;br&gt;&lt;br&gt;=\r\n1.2.) On the bottom of the fetch-processors I&#39;ve put the:&lt;br&gt;&lt;font face=3D&quot;=\r\ncourier new&quot;&gt;org.archive.crawler.processor.recrawl.FetchHistoryProcessor&lt;/f=\r\nont&gt;&lt;br&gt;This processor is responsible form maintaining the history list of&lt;=\r\nbr&gt;&lt;font face=3D&quot;courier new&quot;&gt;CrawlURL&lt;/font&gt;s from previous crawls includi=\r\nng the current &lt;font face=3D&quot;courier new&quot;&gt;CrawlURL&lt;/font&gt;.&lt;br&gt;Thats why we =\r\nare supposed to have the minimum of two. At least one&lt;br&gt;slot for previous =\r\nand one slot for current.&lt;br&gt;&lt;br&gt;1.3.) At the end of whole chain (&lt;font fac=\r\ne=3D&quot;courier new&quot;&gt;post-processors&lt;/font&gt;) I&#39;ve put the:&lt;br&gt;&lt;font face=3D&quot;co=\r\nurier new&quot;&gt;org.archive.crawler.processor.recrawl.PersistStoreProcessor&lt;/fon=\r\nt&gt;&lt;br&gt;This processor actually stores the &lt;font face=3D&quot;courier new&quot;&gt;CrawlUR=\r\nL&lt;/font&gt;s to Berkely DB in to &lt;font face=3D&quot;courier new&quot;&gt;state&lt;/font&gt; direc=\r\ntory of the crawl.&lt;br&gt;&lt;br&gt;&lt;br&gt;2.) Using the processors&lt;br&gt;&lt;br&gt;2.1.) In orde=\r\nr to use the current setup (As far as I get it) we have to &lt;br&gt;maintain the=\r\n same state directory for each crawl since the BDB files in it have also a =\r\nhistory of previous crawls.&lt;br&gt;Enable the view of &lt;font face=3D&quot;courier new=\r\n&quot;&gt;expert settings&lt;/font&gt; in web UI and set up the crawl &lt;font face=3D&quot;couri=\r\ner new&quot;&gt;order state-path&lt;/font&gt; attribute to always point &lt;br&gt;to the same d=\r\nirectory for all the jobs.&lt;br&gt;&lt;br&gt;2.2.) To disable writing the duplicate pa=\r\nges I&#39;ve set the attribute &lt;font face=3D&quot;courier new&quot;&gt;skip-identical-digest=\r\ns=3Dtrue &lt;/font&gt;on the&lt;br&gt;&lt;font face=3D&quot;courier new&quot;&gt;org.archive.crawler.wr=\r\niter.ARCWriterProcessor&lt;/font&gt;&lt;br&gt;&lt;br&gt;To achive the same thing on processor=\r\ns that do not have such attribute I&#39;ve used the&lt;br&gt;&lt;font face=3D&quot;courier ne=\r\nw&quot;&gt;org.archive.crawler.deciderules.recrawl.IdenticalDigestDecideRule&lt;/font&gt;=\r\n filter setting it to &lt;font face=3D&quot;courier new&quot;&gt;REJECT&lt;/font&gt;.&lt;br&gt;(&lt;font f=\r\nace=3D&quot;courier new&quot;&gt;org.archive.crawler.writer.MirrorWriterProcessor&lt;/font&gt;=\r\n is such processor)&lt;br&gt;&lt;br&gt;You should also maintain the same &quot;mirror&quot; or &quot;a=\r\nrcs&quot; store path between crawls. &lt;br&gt;If you don&#39;t you will get only the &quot;cha=\r\nnged pages&quot;&lt;br&gt;(changed pages are those with different content digests and/=\r\nor etag, modified since headers)&lt;br&gt;&lt;br&gt;2.3.) To prevent the fetching of th=\r\ne same &lt;font face=3D&quot;courier new&quot;&gt;CrawlURL&lt;/font&gt;s for the following crawls=\r\n I&#39;ve set up&lt;br&gt;&lt;font face=3D&quot;courier new&quot;&gt;send-if-modified-since=3Dtrue&lt;/f=\r\nont&gt;&lt;br&gt;&lt;font face=3D&quot;courier new&quot;&gt;send-if-none-match=3Dtrue&lt;/font&gt;&lt;br&gt;on t=\r\nhe &lt;font face=3D&quot;courier new&quot;&gt;org.archive.crawler.fetcher.FetchHTTP&lt;/font&gt;.=\r\n&lt;br&gt;This only works for static pages / files (&lt;font face=3D&quot;courier new&quot;&gt;se=\r\nnd-if-modified-since&lt;/font&gt;) or with dynamic pages that support the&lt;br&gt;etag=\r\n HTTP response header (&lt;font face=3D&quot;courier new&quot;&gt;send-if-none-match&lt;/font&gt;=\r\n). Those are the most common options that you can use to decide&lt;br&gt;prior to=\r\n fetching the whole content.&lt;br&gt;&lt;br&gt;2.4.) To minimize the content processin=\r\ng and link extraction you can use&lt;br&gt;&lt;font face=3D&quot;courier new&quot;&gt;TrapSuppres=\r\nsExtractor&lt;/font&gt;.&lt;br&gt;If you crawled URL [A] with content digest of [123] a=\r\nnd just fetched the URL [B] with content digest of [123],&lt;br&gt;then\nthis proc=\r\nessor disables link extraction so no further processing is\ndone. (the links=\r\n from URL [B] are never promoted back to &lt;font face=3D&quot;courier new&quot;&gt;frontie=\r\nr&lt;/font&gt;)&lt;br&gt;(I haven&#39;t tested this one yet)&lt;br&gt;&lt;br&gt;&lt;br&gt;3.) alternatives&lt;br=\r\n&gt;Instead of &lt;font face=3D&quot;courier new&quot;&gt;org.archive.crawler.processor.recraw=\r\nl.PersistLoadProcessor &lt;/font&gt;you can use the &lt;br&gt;&lt;font face=3D&quot;courier new=\r\n&quot;&gt;org.archive.crawler.processor.recrawl.PersistLogProcessor&lt;/font&gt; that use=\r\ns txt file for history&lt;br&gt;(don&#39;t use both)&lt;br&gt;&lt;br&gt;If you want to minimize t=\r\nhe fetch duplication between crawls the only thing you can do is mimic the =\r\n&lt;br&gt;the &lt;font face=3D&quot;courier new&quot;&gt;org.archive.crawler.fetcher.FetchHTTP.se=\r\nnd-if-none-match&lt;/font&gt; and handle some specific &lt;br&gt;\nHTTP headers that tar=\r\nget hosts return.&lt;br&gt;You can store specific data to &lt;font face=3D&quot;courier n=\r\new&quot;&gt;CrawlURL&lt;/font&gt;s persistent &lt;font face=3D&quot;courier new&quot;&gt;AList&lt;/font&gt;.&lt;br=\r\n&gt;(I guess Heritrix people didn&#39;t do it because there is no standard. I have=\r\nn&#39;t tested this one yet also)&lt;br&gt;&lt;br&gt;Nikola&lt;br&gt;&lt;br&gt;--- In archive-crawler@y=\r\nahoogroups.com, &quot;Cetin Sert&quot; &lt;cetin.sert@...&gt; wrote:&lt;br&gt;&gt;&lt;br&gt;&gt; =\r\nHi,&lt;br&gt;&gt; &lt;br&gt;&gt;  &lt;br&gt;&gt; &lt;br&gt;&gt; I would love to know how to use the=\r\n new duplicate-reduction features. The&lt;br&gt;&gt; wiki page about them is too =\r\nshort to understand and there are many new&lt;br&gt;&gt; processors etc., which m=\r\nakes it difficult for me to understand how best to&lt;br&gt;&gt; combine them.&lt;br=\r\n&gt;&gt; &lt;br&gt;&gt; B. Abbrieviate retrieval of unchanged content. &lt;br&gt;&gt; &lt;br&gt;=\r\n&gt; Based on headers of previous retrieval of same URI, adjust current ret=\r\nrieval&lt;br&gt;&gt; (via conditional-GET requests) so that if content is unchang=\r\ned, retrieval&lt;br&gt;&gt; ends quickly and cheaply and no duplicate content is =\r\nstored. Motivated by&lt;br&gt;&gt; KB-Denmark study (Clausen 2004); reduces bandw=\r\nidth used as well as storage. &lt;br&gt;&gt; &lt;br&gt;&gt; This is accomplished in Her=\r\nitrix 1.12.0 by:&lt;br&gt;&gt; &lt;br&gt;&gt; *\tUsing the FetchHistoryProcessor and eit=\r\nher PersistLogProcessor or&lt;br&gt;&gt; PersistStoreProcessor on an initial craw=\r\nl1 &lt;br&gt;&gt; *\tCarrying forward the initial crawl info in a form accessible =\r\nto a&lt;br&gt;&gt; later crawl, perhaps by using utility functionality on the Per=\r\nsistProcessor&lt;br&gt;&gt; class2&lt;br&gt;&gt; *\tUsing the PersistLoadProcessor and F=\r\netchHistoryProcessor in a&lt;br&gt;&gt; following crawl3 so that relevant history=\r\n information is available at&lt;br&gt;&gt; store-decision time3&lt;br&gt;&gt; *\tUsing n=\r\new capabilities of the FetchHTTP processor to issue&lt;br&gt;&gt; conditional-GET=\r\ns where appropriate &lt;br&gt;&gt; *\tUsing new options on any writer processors t=\r\no skip or abbrieviate&lt;br&gt;&gt; storage as desired4 &lt;br&gt;&gt; &lt;br&gt;&gt; C. Note=\r\n and discard duplicate &#39;trap&#39; content from same crawl.&lt;br&gt;&gt; &lt;br&gt;&gt; For=\r\n one simple kind of crawler-trap, where followup URIs return identical&lt;br&gt;&=\r\ngt; content at different (extended) URIs, disable link-extraction. &lt;br&gt;&gt;=\r\n &lt;br&gt;&gt; The TrapSuppressExtractor in 1.12.0 demonstrates this strategy, a=\r\nnd could be&lt;br&gt;&gt; the basis for other similar trap/duplicate suppression =\r\nprocessors.5 &lt;br&gt;&gt; &lt;br&gt;&gt; 1: where exactly? Is this a preprocessor, a =\r\npostprocessor or a writer&lt;br&gt;&gt; module?&lt;br&gt;&gt; &lt;br&gt;&gt; 2: where and how=\r\n?&lt;br&gt;&gt; &lt;br&gt;&gt; 3: where and how?&lt;br&gt;&gt; &lt;br&gt;&gt; 4: are these options =\r\nreadily implemented or should we implement them&lt;br&gt;&gt; ourselves? If they =\r\nare already there, then how do we use them?&lt;br&gt;&gt; &lt;br&gt;&gt; 5: how?&lt;br&gt;&gt=\r\n; &lt;br&gt;&gt;  &lt;br&gt;&gt; &lt;br&gt;&gt; A configuration xml file (order.xml) with all=\r\n the settings enabled, or a few&lt;br&gt;&gt; screenshots of the web interface wi=\r\nth the processors at place might be of&lt;br&gt;&gt; great help.&lt;br&gt;&gt; &lt;br&gt;&gt;=\r\n  &lt;br&gt;&gt; &lt;br&gt;&gt; I have had experience with implementing custom decision=\r\n rules and writers&lt;br&gt;&gt; but this seems to require some explanation.&lt;br&gt;&=\r\ngt; &lt;br&gt;&gt;  &lt;br&gt;&gt; &lt;br&gt;&gt; Thanks in advance to anyone who takes time =\r\nto answer my question.&lt;br&gt;&gt; &lt;br&gt;&gt;  &lt;br&gt;&gt; &lt;br&gt;&gt; Yours Sincerely,=\r\n&lt;br&gt;&gt; &lt;br&gt;&gt; Cetin Sert&lt;br&gt;&gt;&lt;br&gt;\n\n\r\n--3-4501875438-0119401832=:4--\r\n\n"}}