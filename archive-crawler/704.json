{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (@Internet Archive)","from":"&quot;Gordon Mohr (@Internet Archive)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"l-qIhxRXeHTfdndIm3c0AMf9S7Hy5VvV8f3Vy4J4CVOgmU0xfHva2eef12SuUOOSnjkCu_tW4RFRkIY-Bs-i2fvj2yZq9KkEZGp8qm7vSIo4B2Bq46p8XxNGFc1u","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Robots bug?","postDate":"1090598599","msgId":704,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxMDEzNkM3LjEwMDA3MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEwOTA1ODU3NDkuMjU1OC41LmNhbWVsQGx4d3MzNjUxNS5ibC51az4=","referencesHeader":"PGNkcHM0ZCtlZHJiQGVHcm91cHMuY29tPiA8MTA5MDU4NTc0OS4yNTU4LjUuY2FtZWxAbHh3czM2NTE1LmJsLnVrPg=="},"prevInTopic":703,"nextInTopic":707,"prevInTime":703,"nextInTime":705,"topicId":701,"numMessagesInTopic":6,"msgSnippet":"... If that s the exact robots.txt, it has a syntax problem: no : after User-agent . That may be the problem; there are a number of robots.txt validators on","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 54300 invoked from network); 23 Jul 2004 16:03:28 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m22.grp.scd.yahoo.com with QMQP; 23 Jul 2004 16:03:28 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 23 Jul 2004 16:03:28 -0000\r\nReceived: (qmail 18632 invoked by uid 100); 23 Jul 2004 15:54:10 -0000\r\nReceived: from adsl-67-124-240-18.dsl.snfc21.pacbell.net (HELO ?10.0.10.13?) (gojomo@...@67.124.240.18)\n  by mail-dev.archive.org with SMTP; 23 Jul 2004 15:54:10 -0000\r\nMessage-ID: &lt;410136C7.1000706@...&gt;\r\nDate: Fri, 23 Jul 2004 09:03:19 -0700\r\nUser-Agent: Mozilla Thunderbird 0.6 (X11/20040502)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;cdps4d+edrb@...&gt; &lt;1090585749.2558.5.camel@...&gt;\r\nIn-Reply-To: &lt;1090585749.2558.5.camel@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: **\r\nX-Spam-Status: No, hits=2.7 required=7.0 tests=AWL,RCVD_IN_DYNABLOCK,\n\tRCVD_IN_SORBS autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: &quot;Gordon Mohr (@Internet Archive)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Robots bug?\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nmark williamson wrote:\n&gt; Hi, \n&gt; \n&gt; just thought I&#39;d ask about this one before digging through source code.\n&gt; I&#39;ve been crawling a huge multi sub- domain site *.main.domain on one \n&gt; of the sub-domains library.main.domain is the robots.txt:\n&gt; \n&gt; User-agent *\n&gt; Disallow: /\n&gt; \n&gt; The main domain also has a robots.txt. The crawl robots policy is the default\n&gt; out of the box setting. \n&gt; \n&gt; The crawl essentially ignored the library.main.domain/robots.txt (which in\n&gt; addition to our proxy server removing all of the nice header info with \n&gt; my email address it in it caused all sorts of terrible problems but we \n&gt; wont go into that - oops).\n&gt; \n&gt; is this a known problem or have I got set something wrong? \n\nIf that&#39;s the exact robots.txt, it has a syntax problem: no &#39;:&#39; after\n&#39;User-agent&#39;. That may be the problem; there are a number of robots.txt\nvalidators on the net which can be helpful for highlighting such things.\n\n(Perhaps we should be tolerant of such sloppiness, but this hasn&#39;t been\na problem before and I don&#39;t know what the big search engine crawlers\ndo.)\n\nIf that&#39;s not it, not sure what the problem would be. There&#39;s no inheritance\nof robots.txt rules to subdomains, and unless you&#39;ve explicitly configured\nthe crawler to ignore robots rules, Heritrix should always insist on\nretrieving and consulting them before getting anything else from a particular\nhost.\n\nSeparately, regarding the headers issue: did your proxy also strip the\nUser-Agent? In general, you can&#39;t count on website admins having access\nto the &#39;From&#39; header even if it does reach their server. Usually only the\nUser-Agent is logged and thus it&#39;s the most important place to put an\nURL with more info about your crawling and contact info.\n\n- Gordon\n\n"}}