{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":324458274,"authorName":"Kris Carpenter Negulescu","from":"Kris Carpenter Negulescu &lt;kcarpenter@...&gt;","profile":"kris_carpenter2002","replyTo":"LIST","senderId":"PvGkIrgiECj1Dr8-kIxQx5fCCHlJlUDngqBLxcEz-CkgUP8wddZLIOWdJ03m9F4S0ovE_lV88kqfR07okB2ICDh-nIGS9T9Vx7jhvdGEEy6jmAnv0NBntw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Slow (?) loading millions of seeds","postDate":"1334182939","msgId":7650,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEM2MzE3NDZBLUFCQkMtNDU3RS04Q0NELURFNTE0MDJGODg0NkBhcmNoaXZlLm9yZz4=","inReplyToHeader":"PDdEMzE2MDdFLTI2RDgtNEZFQS1BMTQ2LTkxRDQ2MEFEREQ4QUBhcmNoaXZlLm9yZz4=","referencesHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MEVBNDVCQ0VAdzJrMy1ic3BleDE+IDw3RDMxNjA3RS0yNkQ4LTRGRUEtQTE0Ni05MUQ0NjBBREREOEFAYXJjaGl2ZS5vcmc+"},"prevInTopic":7649,"nextInTopic":7651,"prevInTime":7649,"nextInTime":7651,"topicId":7645,"numMessagesInTopic":9,"msgSnippet":"You might also want to look at leveraging the Action Directory and splitting the seeds into multiple files that get loaded incrementally vs as a single","rawEmail":"Return-Path: &lt;kcarpenter@...&gt;\r\nX-Sender: kcarpenter@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 85559 invoked from network); 11 Apr 2012 22:22:23 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m13.grp.sp2.yahoo.com with QMQP; 11 Apr 2012 22:22:23 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.224.6)\n  by mta1.grp.sp2.yahoo.com with SMTP; 11 Apr 2012 22:22:23 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 641966841042;\n\tWed, 11 Apr 2012 15:22:23 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id uR8xZjMsBeA9; Wed, 11 Apr 2012 15:22:21 -0700 (PDT)\r\nX-Received: from [192.168.1.134] (adsl-75-35-73-78.dsl.pltn13.sbcglobal.net [75.35.73.78])\n\tby mail.archive.org (Postfix) with ESMTPSA id 731EF684103D;\n\tWed, 11 Apr 2012 15:22:20 -0700 (PDT)\r\nMime-Version: 1.0 (Apple Message framework v1084)\r\nContent-Type: multipart/alternative; boundary=Apple-Mail-12--606545667\r\nIn-Reply-To: &lt;7D31607E-26D8-4FEA-A146-91D460ADDD8A@...&gt;\r\nDate: Wed, 11 Apr 2012 15:22:19 -0700\r\nCc: Kris Carpenter Negulescu &lt;kcarpenter@...&gt;\r\nMessage-Id: &lt;C631746A-ABBC-457E-8CCD-DE51402F8846@...&gt;\r\nReferences: &lt;74C97E7DF5A7784D997217FF75D121660EA45BCE@w2k3-bspex1&gt; &lt;7D31607E-26D8-4FEA-A146-91D460ADDD8A@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nX-Mailer: Apple Mail (2.1084)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Kris Carpenter Negulescu &lt;kcarpenter@...&gt;\r\nSubject: Re: [archive-crawler] Slow (?) loading millions of seeds\r\nX-Yahoo-Group-Post: member; u=324458274; y=LDA1i34dyrKvHbkYk8lqUiFE9Iy-q8zPxraiHT0c1RG_iSksMRKS2QHjq5Q9\r\nX-Yahoo-Profile: kris_carpenter2002\r\n\r\n\r\n--Apple-Mail-12--606545667\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/plain;\n\tcharset=windows-1252\r\n\r\nYou might also want to look at leveraging the Action Directory and splittin=\r\ng the seeds into multiple files that get loaded incrementally vs as a singl=\r\ne monolithic file....\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/H3=\r\n+Dev+Notes+for+Crawl+Operators#H3DevNotesforCrawlOperators-MillionsofseedsO=\r\nK\nActionDirectory for post-launch URI-loading\nA bean class ActionDirectory,=\r\n if present in a crawl configuration (and it is recommended to become part =\r\nof all standard configurations), watches a configured &#39;action&#39; directory fo=\r\nr any files which appear (rechecking a configurable interval, default 30 se=\r\nconds). For each file, an action is taken in accordance with the file&#39;s suf=\r\nfix, then the file is moved to a &#39;done&#39; directory.\n\nA file ending &#39;.seeds&#39; =\r\nwill trigger the addition of more seeds. A file ending &#39;.recover&#39; will be t=\r\nreated as a traditional recovery log -- with all &#39;Fs&#39; lines considered incl=\r\nuded (to suppress recrawling) then all &#39;F+&#39; lines rescheduled. A file endin=\r\ng &#39;.include&#39;, &#39;.schedule&#39;, or &#39;.force&#39; respectively will be treated as if a=\r\n recovery-log format (with 3-character prefix tag per line), but all URIs l=\r\nisted (regardless of prefix-tag) will be considered-included, scheduled, or=\r\n force-scheduled respectively.\n\nAny of these files may be gzip-compressed (=\r\nwith a &#39;.gz&#39; extension), and those in recovery-log-format may have a &#39;.s.&#39; =\r\ninserted prior to the functional suffix (eg &#39;frontier.s.recover.gz&#39;) to ind=\r\nicate that prior to other steps, scoping should be attempted against the in=\r\ncluded URIs.\n\nDropping the proper files (possibly filtered) into this direc=\r\ntory will likely be the recommended way to recover prior crawl frontier sta=\r\nte, or perform other bulk adds to a running crawler.\n\n\nOn Apr 11, 2012, at =\r\n3:15 PM, Kris Carpenter Negulescu wrote:\n\n&gt; For very large crawls you will =\r\nwant to consider using the HashCrawlMapper to spread the crawl over multipl=\r\ne crawl instances.\n&gt; \n&gt; \n&gt; https://webarchive.jira.com/wiki/display/Heritri=\r\nx/Multiple+Machine+Crawling\n&gt; \n&gt; For the volume of seeds you describe, you =\r\nwill want to distribute across 10 or more crawl instances.\n&gt; \n&gt; Let us know=\r\n if you need more detail/help in getting started.\n&gt; \n&gt; Kris\n&gt; \n&gt; Kris Carpe=\r\nnter Negulescu\n&gt; Director, Web Group\n&gt; Internet Archive\n&gt; kcarpenter@archiv=\r\ne.org\n&gt; skypeid: kris.carpenter\n&gt; \n&gt; On Apr 11, 2012, at 12:55 AM, Coram, R=\r\noger wrote:\n&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; We=92re trying to begin a slash-page crawl of 10=\r\n,000,000+ seeds using Heritrix 3.0.0. While the initial few million load re=\r\nlatively quickly after a few hours the rate slows to a (pardon the pun) cra=\r\nwl. The time take thus far is now measurable in days.\n&gt;&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; We=92=\r\nve tried doubling the memory allocated to the JVM, switching to the latest =\r\nversion of Java and tried launching a crawl with Heritrix 3.1.0 for compari=\r\nson but the results don=92t seem noticeably improved. Is this to be expecte=\r\nd? How long should 10 or even 20 million seeds optimally take to read? Is t=\r\nhere anything obvious we can do to improve things?\n&gt;&gt; \n&gt;&gt;  \n&gt;&gt; \n&gt;&gt; Many tha=\r\nnks,\n&gt;&gt; \n&gt;&gt; Roger\n&gt;&gt; \n&gt;&gt; \n&gt; \n&gt; \n&gt; \n\n\r\n--Apple-Mail-12--606545667\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html;\n\tcharset=windows-1252\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body style=3D&quot;word-wrap: break-word; -webkit-nbsp-mode:=\r\n space; -webkit-line-break: after-white-space; &quot;&gt;You might also want to loo=\r\nk at leveraging the Action Directory and splitting the seeds into multiple =\r\nfiles that get loaded incrementally vs as a single monolithic file....&lt;div&gt;=\r\n&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;a href=3D&quot;https://webarchive.jira.com/wiki/display/Heritrix=\r\n/H3+Dev+Notes+for+Crawl+Operators#H3DevNotesforCrawlOperators-Millionsofsee=\r\ndsOK&quot;&gt;https://webarchive.jira.com/wiki/display/Heritrix/H3+Dev+Notes+for+Cr=\r\nawl+Operators#H3DevNotesforCrawlOperators-MillionsofseedsOK&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;=\r\nh3 id=3D&quot;H3DevNotesforCrawlOperators-ActionDirectoryforpost-launchURI-loadi=\r\nng&quot; style=3D&quot;line-height: normal; font-weight: bold; padding-top: 0px; padd=\r\ning-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 14pt; ma=\r\nrgin-top: 21px; margin-right: 0px; margin-bottom: 4px; margin-left: 0px; co=\r\nlor: rgb(0, 51, 102); font-family: Arial, Helvetica, FreeSans, sans-serif; =\r\n&quot;&gt;ActionDirectory for post-launch URI-loading&lt;/h3&gt;&lt;p style=3D&quot;font-size: 10=\r\npt; line-height: 13pt; color: rgb(0, 0, 0); font-weight: normal; padding-to=\r\np: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; margin-=\r\nbottom: 10px; font-family: Arial, Helvetica, FreeSans, sans-serif; &quot;&gt;A bean=\r\n class ActionDirectory, if present in a crawl configuration (and it is reco=\r\nmmended to become part of all standard configurations), watches a configure=\r\nd &#39;action&#39; directory for any files which appear (rechecking a configurable =\r\ninterval, default 30 seconds). For each file, an action is taken in accorda=\r\nnce with the file&#39;s suffix, then the file is moved to a &#39;done&#39; directory.&lt;/=\r\np&gt;&lt;p style=3D&quot;font-size: 10pt; line-height: 13pt; color: rgb(0, 0, 0); font=\r\n-weight: normal; padding-top: 0px; padding-right: 0px; padding-bottom: 0px;=\r\n padding-left: 0px; margin-bottom: 10px; font-family: Arial, Helvetica, Fre=\r\neSans, sans-serif; &quot;&gt;A file ending &#39;.seeds&#39; will trigger the addition of mo=\r\nre seeds. A file ending &#39;.recover&#39; will be treated as a traditional recover=\r\ny log -- with all &#39;Fs&#39; lines considered included (to suppress recrawling) t=\r\nhen all &#39;F+&#39; lines rescheduled. A file ending &#39;.include&#39;, &#39;.schedule&#39;, or &#39;=\r\n.force&#39; respectively will be treated as if a recovery-log format (with 3-ch=\r\naracter prefix tag per line), but all URIs listed (regardless of prefix-tag=\r\n) will be considered-included, scheduled, or force-scheduled respectively.&lt;=\r\n/p&gt;&lt;p style=3D&quot;font-size: 10pt; line-height: 13pt; color: rgb(0, 0, 0); fon=\r\nt-weight: normal; padding-top: 0px; padding-right: 0px; padding-bottom: 0px=\r\n; padding-left: 0px; margin-bottom: 10px; font-family: Arial, Helvetica, Fr=\r\neeSans, sans-serif; &quot;&gt;Any of these files may be gzip-compressed (with a &#39;.g=\r\nz&#39; extension), and those in recovery-log-format may have a &#39;.s.&#39; inserted p=\r\nrior to the functional suffix (eg &#39;frontier.s.recover.gz&#39;) to indicate that=\r\n prior to other steps, scoping should be attempted against the included URI=\r\ns.&lt;/p&gt;&lt;p style=3D&quot;font-size: 10pt; line-height: 13pt; color: rgb(0, 0, 0); =\r\nfont-weight: normal; padding-top: 0px; padding-right: 0px; padding-bottom: =\r\n0px; padding-left: 0px; margin-bottom: 10px; font-family: Arial, Helvetica,=\r\n FreeSans, sans-serif; &quot;&gt;Dropping the proper files (possibly filtered) into=\r\n this directory will likely be the recommended way to recover prior crawl f=\r\nrontier state, or perform other bulk adds to a running crawler.&lt;/p&gt;\n&lt;br&gt;&lt;di=\r\nv&gt;&lt;div&gt;On Apr 11, 2012, at 3:15 PM, Kris Carpenter Negulescu wrote:&lt;/div&gt;&lt;b=\r\nr class=3D&quot;Apple-interchange-newline&quot;&gt;&lt;blockquote type=3D&quot;cite&quot;&gt;\n\n\n\n\n\n\n\n\n\n\n=\r\n\n\n\n&lt;div style=3D&quot;background-color: #fff;&quot;&gt;\n&lt;span style=3D&quot;display:none&quot;&gt;&nb=\r\nsp;&lt;/span&gt;\n\n\n\n    &lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;p&gt;For very large crawls you will w=\r\nant to consider using the HashCrawlMapper to spread the crawl over multiple=\r\n crawl instances.&lt;/p&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;a href=3D&quot;https://webarchive.jira=\r\n.com/wiki/display/Heritrix/Multiple+Machine+Crawling&quot;&gt;https://webarchive.ji=\r\nra.com/wiki/display/Heritrix/Multiple+Machine+Crawling&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/=\r\ndiv&gt;&lt;div&gt;For the volume of seeds you describe, you will want to distribute =\r\nacross 10 or more crawl instances.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Let us know if =\r\nyou need more detail/help in getting started.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Kris=\r\n&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;br class=3D&quot;webkit-block-placeholder&quot;&gt;&lt;/div&gt;&lt;div&gt;\nKris Car=\r\npenter Negulescu&lt;br&gt;Director, Web Group&lt;br&gt;Internet Archive&lt;br&gt;&lt;a href=3D&quot;m=\r\nailto:kcarpenter@...&quot;&gt;kcarpenter@...&lt;/a&gt;&lt;br&gt;skypeid: kris.c=\r\narpenter\n&lt;/div&gt;\n&lt;br&gt;&lt;div&gt;&lt;div&gt;On Apr 11, 2012, at 12:55 AM, Coram, Roger wr=\r\note:&lt;/div&gt;&lt;br class=3D&quot;Apple-interchange-newline&quot;&gt;&lt;blockquote type=3D&quot;cite&quot;=\r\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background-color: #fff;&quot;&gt;\n&lt;span&gt;&nbsp;&lt;/span&gt;\n=\r\n\n\n\n    &lt;div id=3D&quot;ygrp-text&quot;&gt;&lt;div&gt;&lt;br class=3D&quot;webkit-block-placeholder&quot;&gt;&lt;/=\r\ndiv&gt;&lt;div class=3D&quot;WordSection1&quot;&gt;&lt;p class=3D&quot;MsoNormal&quot;&gt;We=92re trying to be=\r\ngin a slash-page crawl of 10,000,000+ seeds using Heritrix 3.0.0. While the=\r\n initial few million load relatively quickly after a few hours the rate slo=\r\nws to a (pardon the pun) crawl. The time take thus far is now measurable in=\r\n days. &lt;o&gt;&lt;/o&gt;&lt;/p&gt;&lt;p class=3D&quot;MsoNormal&quot;&gt;&lt;o&gt;&nbsp;&lt;/o&gt;&lt;/p&gt;&lt;p class=3D&quot;MsoNo=\r\nrmal&quot;&gt;We=92ve tried doubling the memory allocated to the JVM, switching to =\r\nthe latest version of Java and tried launching a crawl with Heritrix 3.1.0 =\r\nfor comparison but the results don=92t seem noticeably improved. Is this to=\r\n be expected? How long should 10 or even 20 million seeds optimally take to=\r\n read? Is there anything obvious we can do to improve things?&lt;o&gt;&lt;/o&gt;&lt;/p&gt;&lt;p =\r\nclass=3D&quot;MsoNormal&quot;&gt;&lt;o&gt;&nbsp;&lt;/o&gt;&lt;/p&gt;&lt;p class=3D&quot;MsoNormal&quot;&gt;Many thanks,&lt;o&gt;=\r\n&lt;/o&gt;&lt;/p&gt;&lt;p class=3D&quot;MsoNormal&quot;&gt;Roger&lt;o&gt;&lt;/o&gt;&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;br class=3D&quot;webk=\r\nit-block-placeholder&quot;&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n\n    \n\n&lt;/div&gt;\n\n\n\n&lt;!-- end gr=\r\noup email --&gt;\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br class=3D&quot;webkit-block-=\r\nplaceholder&quot;&gt;&lt;/div&gt;\n\n    &lt;/div&gt;\n     \n\n    \n\n&lt;/div&gt;\n\n\n\n&lt;!-- end group email=\r\n --&gt;\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\r\n--Apple-Mail-12--606545667--\r\n\n"}}