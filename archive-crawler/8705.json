{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":319565083,"authorName":"Shriphani Palakodety","from":"Shriphani Palakodety &lt;shriphanip@...&gt;","profile":"shriphanip","replyTo":"LIST","senderId":"4BjsEfBt0rlz008Krmoe-7UUvCwKvAJCrziCplvEl6Zx4PNQQjX3Ne_8LMzRXAy9jeFILLER1N6PPEl5UOzFFwKYNfu-JbXxXZ_KhweEKyxpUg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Focused crawling","postDate":"1433527278","msgId":8705,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PENBR01lR3lINUItaWtoQkR6QTZKRWUtcFpUYnRGOE1ERXFfZ3JFZnRkLVpNOGN1WitrUUBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PG1rbmM5Zys1ZXFtYTRAWWFob29Hcm91cHMuY29tPg==","referencesHeader":"PG1rbmM5Zys1ZXFtYTRAWWFob29Hcm91cHMuY29tPg=="},"prevInTopic":8704,"nextInTopic":0,"prevInTime":8704,"nextInTime":8706,"topicId":8704,"numMessagesInTopic":2,"msgSnippet":"Helen, It would be better to roll out your own extractors for this. Extend the Extractor class. You can do file IO from there. I have a simple tutorial here: ","rawEmail":"Return-Path: &lt;shriphanip@...&gt;\r\nX-Sender: shriphanip@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 968 invoked by uid 102); 5 Jun 2015 18:01:29 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m8.grp.bf1.yahoo.com with SMTP; 5 Jun 2015 18:01:29 -0000\r\nX-Received: (qmail 3535 invoked from network); 5 Jun 2015 18:01:29 -0000\r\nX-Received: from unknown (HELO mail-ob0-f180.google.com) (98.139.245.165)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 5 Jun 2015 18:01:29 -0000\r\nX-Received: by obbqz1 with SMTP id qz1so41878720obb.3\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 05 Jun 2015 11:01:29 -0700 (PDT)\r\nX-Received: by 10.60.228.67 with SMTP id sg3mr4107145oec.32.1433527289135;\n Fri, 05 Jun 2015 11:01:29 -0700 (PDT)\r\nMIME-Version: 1.0\r\nReferences: &lt;mknc9g+5eqma4@...&gt;\r\nIn-Reply-To: &lt;mknc9g+5eqma4@...&gt;\r\nDate: Fri, 05 Jun 2015 18:01:18 +0000\r\nMessage-ID: &lt;CAGMeGyH5B-ikhBDzA6JEe-pZTbtF8MDEq_grEftd-ZM8cuZ+kQ@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001a11363046d846f10517c914a4\r\nSubject: Re: [archive-crawler] Focused crawling\r\nX-Yahoo-Group-Post: member; u=319565083; y=_lu5ZLc73VO3Qzdx_llsJyyledKg6M3Jao3Kz_DeQmL-z89zHA\r\nX-Yahoo-Profile: shriphanip\r\nFrom: Shriphani Palakodety &lt;shriphanip@...&gt;\r\n\r\n\r\n--001a11363046d846f10517c914a4\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHelen,\n\nIt would be better to roll out your own extractors for this.\n\nExten=\r\nd the Extractor class. You can do file IO from there. I have a simple\ntutor=\r\nial here:\nhttp://blog.shriphani.com/2014/03/13/modifying-the-heritrix-web-c=\r\nrawler/\n\nOn Wed, Jun 3, 2015 at 11:40 PM helenchenpoon@... [archive-c=\r\nrawler] &lt;\narchive-crawler@yahoogroups.com&gt; wrote:\n\n&gt;\n&gt;\n&gt; I am investigating=\r\n various open source including Heritrix to do the\n&gt; following.\n&gt;\n&gt; Crawling=\r\n products from a list of know websites like amazon.\n&gt;\n&gt; To specify a crawli=\r\nng like amazon, I&#39;d specify the seed URL, set up the\n&gt; crawling rule at dif=\r\nferent level (following certain patterns or URLs\n&gt; contain certain text), t=\r\nhen extraction rule on the page that should contain\n&gt; the product (here, I =\r\nshould have the web page, so I can extract the\n&gt; specific fields or links o=\r\nr images). The extracted products are then output\n&gt; to file/files.\n&gt;\n&gt; Can =\r\nanyone give me any points as to if Heritrix is appropriate for my use\n&gt; cas=\r\ne. If anyone has done this in the past, would you share your experiences\n&gt; =\r\nwith me?\n&gt;  \n&gt;\n\r\n--001a11363046d846f10517c914a4\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div dir=3D&quot;ltr&quot;&gt;Helen,&lt;br&gt;&lt;br&gt;&lt;div&gt;It would be better to roll out your own=\r\n extractors for this.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Extend the Extractor class. =\r\nYou can do file IO from there. I have a simple tutorial here:=C2=A0&lt;a href=\r\n=3D&quot;http://blog.shriphani.com/2014/03/13/modifying-the-heritrix-web-crawler=\r\n/&quot;&gt;http://blog.shriphani.com/2014/03/13/modifying-the-heritrix-web-crawler/=\r\n&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;div class=3D&quot;gmail_quote&quot;&gt;&lt;div dir=3D&quot;ltr&quot;&gt;On Wed, Jun=\r\n 3, 2015 at 11:40 PM &lt;a href=3D&quot;mailto:helenchenpoon@...&quot;&gt;helenchenpo=\r\non@...&lt;/a&gt; [archive-crawler] &lt;&lt;a href=3D&quot;mailto:archive-crawler@ya=\r\nhoogroups.com&quot;&gt;archive-crawler@yahoogroups.com&lt;/a&gt;&gt; wrote:&lt;br&gt;&lt;/div&gt;&lt;blo=\r\nckquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin:0 0 0 .8ex;border-left:1px #c=\r\ncc solid;padding-left:1ex&quot;&gt;\n\n\n&lt;u&gt;&lt;/u&gt;\n\n\n\n\n\n\n\n\n\n \n&lt;div style=3D&quot;background-c=\r\nolor:#fff&quot;&gt;\n&lt;span&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n =\r\n     &lt;p&gt;I am investigating various open source including Heritrix to do the=\r\n following.&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Crawling products from a list of know websit=\r\nes like amazon.=C2=A0&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;To specify a crawling like a=\r\nmazon, I&#39;d specify the seed URL, set up the crawling rule at different =\r\nlevel (following certain patterns or URLs contain certain text), then extra=\r\nction rule on the page that should contain the product (here, I should have=\r\n the web page, so I can extract the specific fields or links or images). Th=\r\ne extracted products are then output to file/files.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;di=\r\nv&gt;Can anyone give me any points as to if Heritrix is appropriate for my use=\r\n case. If anyone has done this in the past, would you share your experience=\r\ns with me?&lt;/div&gt;&lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n    \n    &lt;div style=3D&quot;color:#fff;m=\r\nin-height:0&quot;&gt;&lt;/div&gt;\n\n\n&lt;/div&gt;\n\n\n\n  \n\n\n\n\n\n\n&lt;/blockquote&gt;&lt;/div&gt;\n\r\n--001a11363046d846f10517c914a4--\r\n\n"}}