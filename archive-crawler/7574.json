{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264138474,"authorName":"Kenji Nagahashi","from":"Kenji Nagahashi &lt;knagahashi@...&gt;","profile":"kenznag","replyTo":"LIST","senderId":"yZUpYFtSzOSOAbpTFI_qQIpRuUVOVrlnWJTS9WINFn_D5_7P67VfNnDbYoc_tnsfYwrSXJYqhbTCfyPkBu0uv3sbBwTqem8DydnakLY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: questions before we restart the crawl","postDate":"1327644585","msgId":7574,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMjIzRkE5LjUwNjA4MDBAZ21haWwuY29tPg==","inReplyToHeader":"PGpmczY1bCs5cm43QGVHcm91cHMuY29tPg==","referencesHeader":"PGpmczY1bCs5cm43QGVHcm91cHMuY29tPg=="},"prevInTopic":7573,"nextInTopic":7578,"prevInTime":7573,"nextInTime":7575,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"David, I understood 25M + page/day was total for 5 machines, as you wrote (these numbers are totals of all 5 instances combined) . Did you mean 25M+","rawEmail":"Return-Path: &lt;knagahashi@...&gt;\r\nX-Sender: knagahashi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 34776 invoked from network); 27 Jan 2012 06:10:01 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m13.grp.sp2.yahoo.com with QMQP; 27 Jan 2012 06:10:01 -0000\r\nX-Received: from unknown (HELO mail-iy0-f180.google.com) (209.85.210.180)\n  by mta2.grp.sp2.yahoo.com with SMTP; 27 Jan 2012 06:10:01 -0000\r\nX-Received: by iabz7 with SMTP id z7so2374590iab.11\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 26 Jan 2012 22:10:00 -0800 (PST)\r\nX-Received: by 10.50.173.98 with SMTP id bj2mr5550996igc.27.1327644600614;\n        Thu, 26 Jan 2012 22:10:00 -0800 (PST)\r\nReturn-Path: &lt;knagahashi@...&gt;\r\nX-Received: from kenji-mbp.local (adsl-71-135-163-49.dsl.pltn13.pacbell.net. [71.135.163.49])\n        by mx.google.com with ESMTPS id vg9sm3164546igb.4.2012.01.26.22.09.54\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Thu, 26 Jan 2012 22:09:59 -0800 (PST)\r\nMessage-ID: &lt;4F223FA9.5060800@...&gt;\r\nDate: Thu, 26 Jan 2012 22:09:45 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;jfs65l+9rn7@...&gt;\r\nIn-Reply-To: &lt;jfs65l+9rn7@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nSubject: Re: [archive-crawler] Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=264138474; y=v-w-PP48ZZVIsdWQNwyyJikcaIKIZNz2lBtLTy15ii1wPw\r\nX-Yahoo-Profile: kenznag\r\n\r\nDavid,\n\nI understood &quot;25M + page/day&quot; was total for 5 machines, as you wrote \n&quot;(these numbers are totals of all 5 instances combined)&quot;. Did you mean \n25M+ page/day/instance? If so, my &quot;100 threads&quot; comment is pointless. \nPlease disregard it.\n\nI&#39;m running broad crawl that captures everything linked: images, script, \nCSS, PDF, Excel, ... even mpeg4 videos. Our Heritrix 3 runs on 8GB \nmemory + 4 core virtual machine (KVM), with 100 threads. it goes \n~60URI/s on average (per instance). Probably we could go as high as 150 \nthreads to get higher crawl speed, but it comes with higher risk of \ndying of OutOfMemoryError, empirically. Crawl speed is also limited by \nlower disk I/O performance of VMs. 100 seems to be a good number for us.\n\nYes, increasing threads brings significant increase in crawl speed, to \ncertain extent. If you don&#39;t have enough &quot;active queues,&quot; threads are \njust wasted. There are other bottleneck, too, and it can change over time.\n\nAt least we&#39;re getting sustained 60URI/s level of speed with 100 \nthreads. With 1,200 threads and enough active queues, you should be \ngetting crawl speed much much higher than that (I&#39;ve never been able to \nrun my crawler with 1200 threads, though!)\n\n--Kenji\n\n(1/26/12 10:31 AM), david_pane1 wrote:\n&gt; Kenji,\n&gt;\n&gt; Are you saying that you can get 25M pages per day on 100 threads and 1\n&gt; instance or 25M URIs/day? Do you capture all images, pdfs, and\n&gt; supporting page documents or are you just capturing html pages?\n&gt;\n&gt; My test crawls before running our large crawl showed significant\n&gt; increase in the number of pages captured when we increased the number of\n&gt; threads.\n&gt;\n&gt; --David\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt; &lt;knagahashi@...&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; Hi,\n&gt;  &gt;\n&gt;  &gt; May be a bit off-topic, but 25M/day with 5 machine is average 58/s per\n&gt;  &gt; machine. Since I know Heritrix-3 can crawl at this speed with just 100\n&gt;  &gt; ToeThreads, I wonder if most of your 1200 ToeThreads are idle.\n&gt;  &gt;\n&gt;  &gt; While why you don&#39;t get much higher speed with 1200 threads is a big\n&gt;  &gt; question, it may make sense to cut down the number of ToeThreads if\n&gt;  &gt; you&#39;re okay with current crawl speed. Less threads will make H3 less\n&gt;  &gt; susceptible to memory problems... Just a thought.\n&gt;  &gt;\n&gt;  &gt; --Kenji\n&gt;  &gt;\n&gt;  &gt; (1/20/12 9:54 PM), David Pane wrote:\n&gt;  &gt; &gt; Gordon,\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Thank you for your response. And I am sorry for the overwhelming amount\n&gt;  &gt; &gt; of information...I think I am a little overwhelmed.... and feeling the\n&gt;  &gt; &gt; pressure.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; 1) Our Bloom filter configuration:\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; &lt;bean id=&quot;uriUniqFilter&quot;\n&gt;  &gt; &gt; class=&quot;org.archive.crawler.util.BloomUriUniqFilter&quot;&gt;\n&gt;  &gt; &gt; &lt;property name=&quot;bloomFilter&quot;&gt;\n&gt;  &gt; &gt; &lt;bean class=&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&gt;  &gt; &gt; &lt;constructor-arg value=&quot;400000000&quot;/&gt;\n&gt;  &gt; &gt; &lt;constructor-arg value=&quot;30&quot;/&gt;\n&gt;  &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &lt;/property&gt;\n&gt;  &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; 2) We are writing the crawl data to a NAS configured with RAID 6.\n&gt; We did\n&gt;  &gt; &gt; see some problems with disk errors on the NAS earlier in the crawl\n&gt; (late\n&gt;  &gt; &gt; Dec ). I recently found this out. We were/are running in a degraded\n&gt;  &gt; &gt; raid state - a few of the disks have been replaced and the RAID is\n&gt; being\n&gt;  &gt; &gt; rebuilt. We didn&#39;t see any block device errors in the logs on the NAS\n&gt;  &gt; &gt; so the write failures we saw are probably not related to the\n&gt; rebuild. We\n&gt;  &gt; &gt; did see some network hiccups (no outright failures) in the logs.\n&gt;  &gt; &gt; So, this may be the culprit for some of the\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; 3) Yes, we have been cross-feeding URIs.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; --David\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; On 1/21/12 12:22 AM, Gordon Mohr wrote:\n&gt;  &gt; &gt; &gt; You&#39;ve provided an overwhelming amount of information and we may be\n&gt;  &gt; &gt; &gt; dealing with multiple issues, some of which have roots going back\n&gt;  &gt; &gt; &gt; earlier than the diagnostic data we now have available.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; A few key points of emphasis:\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; - we&#39;ve not run crawls with 1200 threads before, or on hardware\n&gt;  &gt; &gt; &gt; similar to yours, so our experience is only vaguely suggestive\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; - it&#39;s not the lower thread counts that are the real source of\n&gt;  &gt; &gt; &gt; concern; you can even adjust the number of threads mid-crawl. It&#39;s\n&gt;  &gt; &gt; &gt; that the error that killed the threads almost certainly left a queue\n&gt;  &gt; &gt; &gt; in a &#39;phantom&#39; state where no progress would be made crawling its\n&gt;  &gt; &gt; &gt; URIs, each time it happened, on each resume leading to the\n&gt; current state.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; - without having understood and fixed whatever software or system\n&gt;  &gt; &gt; &gt; problems caused the earliest/most-foundational errors in your crawl,\n&gt;  &gt; &gt; &gt; it&#39;s impossible to say how likely they are to recur.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; With that in mind, I&#39;ll try to provide quick answers to your other\n&gt;  &gt; &gt; &gt; questions...\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; On 1/20/12 4:20 PM, David Pane wrote:\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; We have collected about 550 million pages along with the images and\n&gt;  &gt; &gt; &gt;&gt; supporting documents on our 5 instance crawl that was started\n&gt; Dec. 23rd.\n&gt;  &gt; &gt; &gt;&gt; Although we are please with the amount of data we captured to\n&gt; date, we\n&gt;  &gt; &gt; &gt;&gt; are very concerned about the state of the Heritrix instances. If\n&gt; fact,\n&gt;  &gt; &gt; &gt;&gt; we aren&#39;t very confident that the instances will last until the\n&gt; end of\n&gt;  &gt; &gt; &gt;&gt; February. We are now running on a total of over 500 less threads\n&gt; than\n&gt;  &gt; &gt; &gt;&gt; the configured 1200 threads/instance.\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; 0 - not running right now.\n&gt;  &gt; &gt; &gt;&gt; 1 - running on 1198 ( 2 less)\n&gt;  &gt; &gt; &gt;&gt; 2 - running on 931 (269 less)\n&gt;  &gt; &gt; &gt;&gt; 3 - running on 987 (213 less)\n&gt;  &gt; &gt; &gt;&gt; 4 - running on 1170 (30 less)\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; Since we are seriously considering throwing away this past\n&gt; month&#39;s work\n&gt;  &gt; &gt; &gt;&gt; and starting over, we would like to pick your brain on some\n&gt; strategies\n&gt;  &gt; &gt; &gt;&gt; that will help us avoid getting into this situation again. We were\n&gt;  &gt; &gt; &gt;&gt; hoping to be done crawling by the end of February so this\n&gt; restart will\n&gt;  &gt; &gt; &gt;&gt; put us behind schedule.\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; 1) Can we continue from here but with &quot;clean&quot; Heritrix instances?\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; Is there a way that we can continue from the this point forward, but\n&gt;  &gt; &gt; &gt;&gt; start with Heritrix instances that will not be corrupt due to sever\n&gt;  &gt; &gt; &gt;&gt; error? (e.g. using the\n&gt;  &gt; &gt; &gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt; ) If\n&gt;  &gt; &gt; &gt;&gt; so, would you recommend doing this? You mentioned that this could be\n&gt;  &gt; &gt; &gt;&gt; time consuming. Each of our instances has downloaded around 170M\n&gt; URIs,\n&gt;  &gt; &gt; &gt;&gt; they have over 700M queued URIs, what is your time estimate for\n&gt;  &gt; &gt; &gt;&gt; something this large?\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; We are willing to sacrifice a few days to get our crawler to a clean\n&gt;  &gt; &gt; &gt;&gt; state again so we can crawl for another 30 days at the pace we\n&gt; have been\n&gt;  &gt; &gt; &gt;&gt; crawling.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; You can do a big &#39;frontier-recover&#39; log replay to avoid\n&gt; recrawling the\n&gt;  &gt; &gt; &gt; same URIs, and approximate the earlier queue state. Splitting/filters\n&gt;  &gt; &gt; &gt; the logs manually beforehand as alluded to in the wiki page can speed\n&gt;  &gt; &gt; &gt; this process somewhat... but given the size of all your log-segments\n&gt;  &gt; &gt; &gt; that log grooming beforehand is itself likely to be a lengthy\n&gt; process.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I don&#39;t think we&#39;ve ever done it with logs of 170M crawled / 870M\n&gt;  &gt; &gt; &gt; discovered before, nor on any hardware comparable to yours. So it&#39;s\n&gt;  &gt; &gt; &gt; impossible to project its duration in your environment. It&#39;s\n&gt; taken 2-3\n&gt;  &gt; &gt; &gt; days for us on smaller crawls, slower hardware.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; An added complication is that this older frontier-recover-log replay\n&gt;  &gt; &gt; &gt; technique happens in its own thread separate from the checkpointing\n&gt;  &gt; &gt; &gt; process, so it is not, itself, accurately checkpointed during the\n&gt; long\n&gt;  &gt; &gt; &gt; reload process.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; At nearly 1B discovered URIs per node, even if you are using the\n&gt;  &gt; &gt; &gt; alternate BloomUriUniqFilter, if you are using it at its default size\n&gt;  &gt; &gt; &gt; (~500MB) it will now be heavily saturated and thus returning many\n&gt;  &gt; &gt; &gt; false-positives causing truly unique URIs to be rejected as\n&gt;  &gt; &gt; &gt; duplicates. (If you&#39;re using a significantly larger filter, you may\n&gt;  &gt; &gt; &gt; not yet be at a high false-positive rate: you&#39;d have to do the bloom\n&gt;  &gt; &gt; &gt; filter math. If you&#39;re still using BdbUriUniqFilter, you&#39;re way way\n&gt;  &gt; &gt; &gt; past the point where its disk seeks have usually made it too slow for\n&gt;  &gt; &gt; &gt; our purposes.)\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; 2) What can be done to avoid corrupting the Heritrix instances?\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; - What kind of strategies might we take to keep the crawl error\n&gt; free?\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; - Do you think the SEVER errors that we have seen are\n&gt; deterministic or\n&gt;  &gt; &gt; &gt;&gt; random (e.g., triggered by occasional flaky network conditions,\n&gt; disks,\n&gt;  &gt; &gt; &gt;&gt; race conditions, or whatever)?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Hard to say. The main thing I could suggest is watch very closely and\n&gt;  &gt; &gt; &gt; when a SEVERE error occurs, prioritize diagnosing and resolving the\n&gt;  &gt; &gt; &gt; cause while the info is fresh.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; - Do you believe that we can reliably backup to the previous\n&gt; checkpoint\n&gt;  &gt; &gt; &gt;&gt; if we watch the logs and stop as soon as we see the first SEVER\n&gt; error?\n&gt;  &gt; &gt; &gt;&gt; If we do this, do you speculate that the same SEVER will occur\n&gt; again?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Resuming from the latest checkpoint before an error believed to\n&gt;  &gt; &gt; &gt; corrupt the on-disk state will be the best strategy.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; If we never figure out the real cause, but run the same software on\n&gt;  &gt; &gt; &gt; the same machine, yes, I expect the same problem will recur!\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; - Is there any reason why a Heritrix instance that is run while\n&gt; binded\n&gt;  &gt; &gt; &gt;&gt; to one ip address can&#39;t be resumed binded to a different ip address?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Only the web UI to my knowledge binds to a chosen address, and it is\n&gt;  &gt; &gt; &gt; common to have it bind to all. I don&#39;t expect the outbound requests\n&gt;  &gt; &gt; &gt; would be hurt by a machine changing its IP address while the\n&gt; crawl was\n&gt;  &gt; &gt; &gt; running, but I would run a test to be sure if that was an important,\n&gt;  &gt; &gt; &gt; expected transition.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; 3) Should we configure the crawler with more instances and switch\n&gt;  &gt; &gt; &gt;&gt; between them?\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; We have seen that we can run a single instance to 100M pages +\n&gt;  &gt; &gt; &gt;&gt; supporting images and documents. Perhaps this means that we need\n&gt; 10 or\n&gt;  &gt; &gt; &gt;&gt; more instances instead of 5. That raises the possibility of\n&gt; running 2\n&gt;  &gt; &gt; &gt;&gt; instances per machine. If we could run 2, or even 4, instances on a\n&gt;  &gt; &gt; &gt;&gt; single machine, they would each run half as long.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I don&#39;t think the problems as reported are specifically due to one\n&gt;  &gt; &gt; &gt; node&#39;s progress growing beyond a certain size, but it might be the\n&gt;  &gt; &gt; &gt; case that giant instances are more likely to suffer from, and harder\n&gt;  &gt; &gt; &gt; to recover from, single glitches (eg a single disk error). On the\n&gt;  &gt; &gt; &gt; other hand, many instances introduce more redundant overhead costs\n&gt;  &gt; &gt; &gt; (certain data structures, cross-feeding URIs if you&#39;re doing\n&gt; that, etc.).\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; - Can you suggest a way to start/stop instances from a script so\n&gt; we can\n&gt;  &gt; &gt; &gt;&gt; change between instances automatically?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Not a mode I&#39;ve thought much about.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; - Have you seen frequent starting / stopping of instances introduce\n&gt;  &gt; &gt; &gt;&gt; instability?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; No... but it might make you notice latent issues sooner.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; 4) Crawl slows but restarting seems to improve the speed again.\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; We noticed that the all of our instances would initially run at\n&gt; a fast\n&gt;  &gt; &gt; &gt;&gt; pace. We would collect an average of 25M + pages/day for 2-3\n&gt; days and\n&gt;  &gt; &gt; &gt;&gt; then the crawl would slow down to 10M pages/day over the next\n&gt; few days.\n&gt;  &gt; &gt; &gt;&gt; (these numbers are totals of all 5 instances combined). When we\n&gt;  &gt; &gt; &gt;&gt; restarted the instances, the average pages would improve back to\n&gt; 25M +\n&gt;  &gt; &gt; &gt;&gt; pages/day. The total crawled numbers (TiB) also reflected the\n&gt; slow down.\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; - Is this something that others have experienced as well?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I don&#39;t recall hearing other reports of speed boosts after\n&gt;  &gt; &gt; &gt; checkpoint-resumes but others may have more experience.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; 5) We are capturing tweets from twitter, harvesting the urls and\n&gt; want to\n&gt;  &gt; &gt; &gt;&gt; crawl those urls within 1 day of receiving the tweet. Can you\n&gt; recommend\n&gt;  &gt; &gt; &gt;&gt; a strategy for doing this with the 5 instances we are running?\n&gt;  &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt;&gt; - Do we need to run a separate crawler dedicated to this? If so,\n&gt; can you\n&gt;  &gt; &gt; &gt;&gt; suggest a way to crawl out from the tweeted urls but when we get\n&gt;  &gt; &gt; &gt;&gt; additional urls from the tweets, quickly change focus to these urls\n&gt;  &gt; &gt; &gt;&gt; instead of the ones branching out. When adding urls as seeds,\n&gt; can you\n&gt;  &gt; &gt; &gt;&gt; set a high priority to crawl those before the discovered urls?\n&gt; Do you\n&gt;  &gt; &gt; &gt;&gt; recommend maybe setting up a specific crawl for these urls and\n&gt; then only\n&gt;  &gt; &gt; &gt;&gt; crawl a few hopes from the seeds - injecting the urls from the\n&gt; tweets as\n&gt;  &gt; &gt; &gt;&gt; seeds?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Dedicating a special script or crawler to URIs that come from such a\n&gt;  &gt; &gt; &gt; constrained source (Twitter feeds), or that need to be crawled in a\n&gt;  &gt; &gt; &gt; special timeframe, or according to other special limits (fewer hops),\n&gt;  &gt; &gt; &gt; could make sense.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; It would take some customization of the queueing-policy or\n&gt;  &gt; &gt; &gt; &#39;precedence&#39; features of Heritrix to allow URIs added mid-crawl to be\n&gt;  &gt; &gt; &gt; prioritized above those already discovered and queued. The most\n&gt; simple\n&gt;  &gt; &gt; &gt; possible customization might be a UriPrecedencePolicy that takes all\n&gt;  &gt; &gt; &gt; zero-hop URIs (which all seeds and most direct-fed URIs would be) and\n&gt;  &gt; &gt; &gt; gives them a higher precedence (lower precedence number) than all\n&gt;  &gt; &gt; &gt; other URIs.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;&gt; 6) I think the answer is no for this question, but I will ask it\n&gt; anyway.\n&gt;  &gt; &gt; &gt;&gt; If you have a Heritrix instance that is configured for 1200\n&gt; threads on\n&gt;  &gt; &gt; &gt;&gt; one machine, can you recover from a checkpoint from that 1200 thread\n&gt;  &gt; &gt; &gt;&gt; configuration on a different machine with an Heritrix instance\n&gt; that is\n&gt;  &gt; &gt; &gt;&gt; configured for less threads (e.g. the default 25 threads)?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Yes - there&#39;s no need to keep the thread count the same after a\n&gt;  &gt; &gt; &gt; resume. None of the checkpoint structures (or usual disk structures)\n&gt;  &gt; &gt; &gt; are based on the number of worker threads (&#39;ToeThreads&#39;)... as\n&gt;  &gt; &gt; &gt; mentioned above you can even vary the number of threads in a running\n&gt;  &gt; &gt; &gt; crawl.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; - Gordon\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt;\n&gt;\n&gt; \n\n\n"}}