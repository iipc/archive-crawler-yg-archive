{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"TpUopWR2NK3Bn8DXcdSC6j6qSctOtFIfNmS8z6G_42ycJczGdr83IV5WzNE-LvijilSaUju4K5HWCJsb517KUmMKhBhJ3x8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] HTML meta tags based filter","postDate":"1317189516","msgId":7337,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFODJCNzhDLjcwNDA4MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEzMTcwNjk3MTIuODkxNjAuWWFob29NYWlsTmVvQHdlYjQzMTQyLm1haWwuc3AxLnlhaG9vLmNvbT4=","referencesHeader":"PDEzMTcwNjk3MTIuODkxNjAuWWFob29NYWlsTmVvQHdlYjQzMTQyLm1haWwuc3AxLnlhaG9vLmNvbT4="},"prevInTopic":7330,"nextInTopic":7340,"prevInTime":7336,"nextInTime":7338,"topicId":7330,"numMessagesInTopic":3,"msgSnippet":"... Since you need the URI contents to check the META tag, in one important sense it s already been crawled (or at least fetched ). So, you can t really","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 86860 invoked from network); 28 Sep 2011 05:58:38 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m2.grp.sp2.yahoo.com with QMQP; 28 Sep 2011 05:58:38 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta5.grp.sp2.yahoo.com with SMTP; 28 Sep 2011 05:58:38 -0000\r\nX-Received: (qmail 35330 invoked by uid 0); 28 Sep 2011 05:58:37 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay01.pair.com with SMTP; 28 Sep 2011 05:58:37 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4E82B78C.7040802@...&gt;\r\nDate: Tue, 27 Sep 2011 22:58:36 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:6.0.2) Gecko/20110902 Thunderbird/6.0.2\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;1317069712.89160.YahooMailNeo@...&gt;\r\nIn-Reply-To: &lt;1317069712.89160.YahooMailNeo@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] HTML meta tags based filter\r\nX-Yahoo-Group-Post: member; u=137285340; y=b1BX0pteF7Z1RajepUbk4yKSu25-lzeLP0QnizTvDVp8\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 9/26/11 1:41 PM, Pranay Pandey wrote:\n&gt; I need to exclude a URL from crawling if it has a specific value for a\n&gt; specific meta tag.\n&gt;\n&gt; I was hoping that ExtractorHTML bean might have a property that could\n&gt; let me do a regex match for the meta tags and accept/reject the url\n&gt; accordingly.\n&gt;\n&gt; Is there an off the shelf solution that could serve my purpose or\n&gt; patching extractorHTML with a custom meta-handling property, the only way?\n\nSince you need the URI contents to check the META tag, in one important \nsense it&#39;s already been &#39;crawled&#39; (or at least &#39;fetched&#39;). So, you can&#39;t \nreally reject it in the scoping sense... but you could cancel its \npermanent storage and/or discard its outlinks.\n\nFor that purpose, I&#39;d recommend creating your own special-purpose \nProcessor that scans for the relevant META tag values. (Combining your \nscan for the META tag with ExtractorHTML might be a little more \nefficient but mixing that functionality into the general extractor would \nbe more complicated and error-prone.)\n\nIn that special-purpose Processor, when appropriate, you could mark-up \nthe CrawlURI instance to change its subsequent handling.\n\nFor example, there are at least two ways you could suppress a later \nWARCWriterProcessor from writing that fetch:\n\n(1) Forcing a negative status-code (other than the few that are \nauto-retried), instead of whatever the real network response code was. \nThis should cause HTTP-related-WARC-record-writing to be skipped. (See \nWriterPoolProcessor.shouldWrite()&#39;s HTTP case.)\n\n(2) Add your own marker of some sort to the CrawlURI (for example in its \ngetData() map). Then, add your own set of processor-local DecideRules to \nWARCWriterProcessor that check for that marker and return the REJECT \ndecision if that processor should be skipped.\n\nThe second option involves a bit more code and configuration but avoids \nclobbering the original network-response code for reference in the \ncrawl.log.\n\nNote that the custom Processor that scans for the META tag and marks-up \nthe CrawlURI, and the custom DecideRule, could both potentially be \nimplemented via the ScriptedProcessor/ScriptedDecideRule, and thus be a \nmatter of a script in your crawl configuration rather than adding Java \nclasses to your installation.\n\nSimilarly, a custom Processor could strip out any outlinks discovered by \nan preceding ExtractorHTML (after extraction but before the \nCandidatesProcessor checks scope and submits to the frontier). Such a \nProcessor could thus change whether those URIs are ever \nenqueued/fetched, based on the presence/absence of a META tag value \nwhere they were discovered.\n\nHope this helps,\n\n- Gordon\n\n\n\n\n"}}