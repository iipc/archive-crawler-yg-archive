{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"WOHnAwJn-PXHmVkEfC2CNFBfLXXWMGGGsj5NhcT48L3FqbUV0SIgk6U1bjcCIi6q-ldPKNRqAxLuXhngiNv-IJ2bCHzOfp-dVkToA0xY","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Is heritrix a good framework for crawling the web for images?","postDate":"1250103104","msgId":5972,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg1djMwMCtnNXZ2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGg1ZTN1Yitob2diQGVHcm91cHMuY29tPg=="},"prevInTopic":5961,"nextInTopic":5993,"prevInTime":5971,"nextInTime":5973,"topicId":5945,"numMessagesInTopic":9,"msgSnippet":"We have a requirement that we DO NOT want to store URLs with known image/audio/video content-types . Here is our implementation in the write-processor to","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 31786 invoked from network); 12 Aug 2009 18:51:58 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m5.grp.sp2.yahoo.com with QMQP; 12 Aug 2009 18:51:58 -0000\r\nX-Received: from unknown (HELO n41b.bullet.mail.sp1.yahoo.com) (66.163.168.155)\n  by mta3.grp.sp2.yahoo.com with SMTP; 12 Aug 2009 18:51:58 -0000\r\nX-Received: from [69.147.65.148] by n41.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2009 18:51:44 -0000\r\nX-Received: from [98.137.35.14] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 12 Aug 2009 18:51:44 -0000\r\nDate: Wed, 12 Aug 2009 18:51:44 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h5v300+g5vv@...&gt;\r\nIn-Reply-To: &lt;h5e3ub+hogb@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: Is heritrix a good framework for crawling the web for images?\r\nX-Yahoo-Group-Post: member; u=132996324; y=Ly1T0ofjGTvV3RRMorRju-O7-l88_FPB96dqKEiqKy4qO-ty2Q\r\nX-Yahoo-Profile: joehung302\r\n\r\nWe have a requirement that we DO NOT want to store URLs with known image/au=\r\ndio/video &quot;content-types&quot;.\n\nHere is our implementation in the write-process=\r\nor to filter out known &quot;content-type&quot;. You can certainly invert the conditi=\r\non so that the writer-processor only to write known image &quot;content-types&quot;.\n=\r\n\n=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=\r\n=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D\n\n    &lt;map name=3D&quot;write-processors&quot;&gt;\n    =\r\n  &lt;newObject name=3D&quot;Archiver&quot; class=3D&quot;org.archive.crawler.writer.ARCWrite=\r\nrProcessor&quot;&gt;\n        &lt;boolean name=3D&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n        &lt;newO=\r\nbject name=3D&quot;Archiver#decide-rules&quot; class=3D&quot;org.archive.crawler.deciderul=\r\nes.DecideRuleSequence&quot;&gt;\n          &lt;map name=3D&quot;rules&quot;&gt;\n            &lt;newObje=\r\nct name=3D&quot;noimg&quot; class=3D&quot;org.archive.crawler.deciderules.FilterDecideRule=\r\n&quot;&gt;\n              &lt;map name=3D&quot;filters&quot;&gt;\n                &lt;newObject name=3D&quot;=\r\nimgcnt&quot; class=3D&quot;org.archive.crawler.filter.ContentTypeRegExpFilter&quot;&gt;\n     =\r\n             &lt;boolean name=3D&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n                  &lt;bo=\r\nolean name=3D&quot;if-match-return&quot;&gt;false&lt;/boolean&gt;\n                  &lt;string na=\r\nme=3D&quot;regexp&quot;&gt;(.*/(css|x-javascript|javascript|octet-stream|x-shockwave-fla=\r\nsh)|(i|I)mage/.*|video/.*|audio/.*)&lt;/string&gt;\n                &lt;/newObject&gt;\n =\r\n             &lt;/map&gt;\n              &lt;string name=3D&quot;true-decision&quot;&gt;ACCEPT&lt;/st=\r\nring&gt;\n              &lt;string name=3D&quot;false-decision&quot;&gt;REJECT&lt;/string&gt;\n       =\r\n     &lt;/newObject&gt;\n          &lt;/map&gt;\n        &lt;/newObject&gt;\n        &lt;boolean na=\r\nme=3D&quot;compress&quot;&gt;true&lt;/boolean&gt;\n        &lt;string name=3D&quot;prefix&quot;&gt;ACC&lt;/string&gt;=\r\n\n        &lt;string name=3D&quot;suffix&quot;&gt;${HOSTNAME}&lt;/string&gt;\n        &lt;long name=3D=\r\n&quot;max-size-bytes&quot;&gt;100000000&lt;/long&gt;\n        &lt;stringList name=3D&quot;path&quot;&gt;\n      =\r\n    &lt;string&gt;arcs&lt;/string&gt;\n        &lt;/stringList&gt;\n        &lt;integer name=3D&quot;po=\r\nol-max-active&quot;&gt;5&lt;/integer&gt;\n        &lt;integer name=3D&quot;pool-max-wait&quot;&gt;300000&lt;/=\r\ninteger&gt;\n        &lt;long name=3D&quot;total-bytes-to-write&quot;&gt;0&lt;/long&gt;\n        &lt;bool=\r\nean name=3D&quot;skip-identical-digests&quot;&gt;false&lt;/boolean&gt;\n      &lt;/newObject&gt;\n    =\r\n&lt;/map&gt;\n\n\n\n\n\n\n\n\n\n--- In archive-crawler@yahoogroups.com, &quot;progre55&quot; &lt;ikromch=\r\nik@...&gt; wrote:\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, &quot;thiagofmam&quot; &lt;th=\r\niagofmam@&gt; wrote:\n&gt; &gt;\n&gt; &gt; I would like know if exists some tutorial of the =\r\nhow get links of images.\n&gt; &gt; \n&gt; &gt; Thank you\n&gt; &gt; \n&gt; \n&gt; Hi!\n&gt; \n&gt; You would mo=\r\nst likely be interested in configuring a DecideRule, particularly MatchesFi=\r\nlePatternDecideRule (Heritrix 3, and there might be smth similar in other H=\r\n versions). It has a preset called &quot;IMAGES&quot; with a default pattern (&quot;.*(?i)=\r\n(&#92;&#92;.(bmp|gif|jpe?g|png|svg|tiff?))$&quot;)\n&gt; \n&gt; However, dont have it on your sc=\r\nope level, as you would not be able to crawl any links if you filter all no=\r\nn-image pages. Try to configure it for your writer-processor (e.g. ARCWrite=\r\nrProcessor) if you want to store images only.\n&gt; Try to search the group-mes=\r\nsages for setting up DecideRules.\n&gt; \n&gt; Cheers,\n&gt; Ikrom\n&gt; \n&gt; \n&gt; &gt; --- In arc=\r\nhive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; Heri=\r\ntrix is a reasonable choice for your goals, because it does not have \n&gt; &gt; &gt;=\r\n the &#39;text-only&#39; focus of some other bulk crawlers, and provides many \n&gt; &gt; =\r\n&gt; configuration options for controlling what is fetched, what is stored, \n&gt;=\r\n &gt; &gt; and inserting extra steps at any point of the process.\n&gt; &gt; &gt; \n&gt; &gt; &gt; It=\r\n will take some custom configuration and careful oversight to run \n&gt; &gt; &gt; mu=\r\nltiple machines on a web-scale collection effort; there are previous \n&gt; &gt; &gt;=\r\n threads here that may help.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Of course to discover images you&#39;=\r\nll have to fetch and follow a lot of \n&gt; &gt; &gt; non-image links -- but it&#39;s up =\r\nto your configuration as to whether those \n&gt; &gt; &gt; are stored or just link-ex=\r\ntracted and discarded.\n&gt; &gt; &gt; \n&gt; &gt; &gt; - Gordon @ IA\n&gt; &gt; &gt; \n&gt; &gt; &gt; Quilby wrote=\r\n:\n&gt; &gt; &gt; &gt; Hi-\n&gt; &gt; &gt; &gt; I posted this question to stackoverflow.com, but I th=\r\nought that it\n&gt; &gt; &gt; &gt; would also be a good idea to ask you guys-\n&gt; &gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; &gt; We are in the starting phase of a project, and we are currently\n&gt; &gt; =\r\n&gt; &gt; wondering whether which crawler is the best choice for us.\n&gt; &gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; &gt; Our project:\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Basically, we&#39;re going to set up Hadoop =\r\nand crawl the web for images.\n&gt; &gt; &gt; &gt; We will then run our own indexing sof=\r\ntware on the images stored in\n&gt; &gt; &gt; &gt; HDFS based on the Map/Reduce facility=\r\n in Hadoop. We will not use other\n&gt; &gt; &gt; &gt; indexing than our own.\n&gt; &gt; &gt; &gt; \n&gt;=\r\n &gt; &gt; &gt; Some particular questions:\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;     * Which crawler will=\r\n handle crawling for images best?\n&gt; &gt; &gt; &gt;     * Which crawler will best ada=\r\npt to a distributed crawling system,\n&gt; &gt; &gt; &gt; in which we use many servers c=\r\nonducting crawling together?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Right now these look like the=\r\n 3 best options-\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;     * Nutch: Known to scale. Doesn&#39;t look=\r\n like the best option because\n&gt; &gt; &gt; &gt; it seems that is it tied closely to t=\r\nheir text searching software.\n&gt; &gt; &gt; &gt;     * Heritrix: Also scales. This one=\r\n currently looks like the best\n&gt; &gt; &gt; &gt; option.\n&gt; &gt; &gt; &gt;     * Scrapy: Has no=\r\nt been used on a large scale (not sure though). I\n&gt; &gt; &gt; &gt; dont know if it h=\r\nas the basic stuff like URL canonicalization. I would\n&gt; &gt; &gt; &gt; like to use t=\r\nhis one because it is a python framework (I like python\n&gt; &gt; &gt; &gt; more than j=\r\nava), but I don&#39;t know if they have implemented the\n&gt; &gt; &gt; &gt; advanced featur=\r\nes of a web crawler.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Summary:\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; We need to =\r\nget as many images as possible from the web. Which existing\n&gt; &gt; &gt; &gt; crawlin=\r\ng framework is both scalable and efficient , but also the one\n&gt; &gt; &gt; &gt; which=\r\n will be the easiest to modify to get only images?\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Thanks!=\r\n\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; ------------------------------------\n&gt; &gt; &gt; &gt; \n&gt; =\r\n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}