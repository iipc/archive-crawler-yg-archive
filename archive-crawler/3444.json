{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":284768837,"authorName":"Maximilian Schoefmann","from":"&quot;Maximilian Schoefmann&quot; &lt;schoefma@...&gt;","replyTo":"LIST","senderId":"nQvpDMW_eKM7Gv16BLVagCMiYl9FEUNRb7fnNWL878jg4KbQvt37imhU0ppU9t20zD1FJZOCudA3ewhJOHNv0h58pNyQUyi9STTzKFyLXdovQPRGo83tYg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: JMX password file is missing or      permissions not set correctly","postDate":"1160732308","msgId":3444,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU4Njc1LjE5My4xODAuMTg5LjE1OC4xMTYwNzMyMzA4LnNxdWlycmVsQHdlYm1haWwuaWZpLmxtdS5kZT4=","inReplyToHeader":"PGVnbjlubCtuMjdzQGVHcm91cHMuY29tPg==","referencesHeader":"PDYyMTY1LjgzLjIyNy4xMTYuMTkwLjExNjA2OTA0MDAuc3F1aXJyZWxAd2VibWFpbC5pZmkubG11LmRlPiAgICA8ZWduOW5sK24yN3NAZUdyb3Vwcy5jb20+"},"prevInTopic":3441,"nextInTopic":3445,"prevInTime":3443,"nextInTime":3445,"topicId":3393,"numMessagesInTopic":8,"msgSnippet":"... I got it working surprisingly fast on Linux with the data of a single crawl, but was stuck then as incremental indexing is somehow broken at the moment","rawEmail":"Return-Path: &lt;schoefma@...&gt;\r\nX-Sender: schoefma@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 69451 invoked from network); 13 Oct 2006 09:39:32 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m25.grp.scd.yahoo.com with QMQP; 13 Oct 2006 09:39:32 -0000\r\nReceived: from unknown (HELO kokytos.rz.ifi.lmu.de) (141.84.214.13)\n  by mta2.grp.scd.yahoo.com with SMTP; 13 Oct 2006 09:39:32 -0000\r\nReceived: from minotaurus.cip.informatik.uni-muenchen.de (news.ifi.lmu.de [141.84.220.21])\n\tby kokytos.rz.ifi.lmu.de (Postfix) with ESMTP id 294B743CB0\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 13 Oct 2006 11:38:28 +0200 (CEST)\r\nReceived: from webmail.ifi.lmu.de (localhost [127.0.0.1])\n\tby minotaurus.cip.informatik.uni-muenchen.de (Postfix) with ESMTP id F2D423B9DCD\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 13 Oct 2006 11:38:27 +0200 (CEST)\r\nReceived: from 193.180.189.158\n        (SquirrelMail authenticated user schoefma)\n        by webmail.ifi.lmu.de with HTTP;\n        Fri, 13 Oct 2006 11:38:28 +0200 (CEST)\r\nMessage-ID: &lt;58675.193.180.189.158.1160732308.squirrel@...&gt;\r\nIn-Reply-To: &lt;egn9nl+n27s@...&gt;\r\nReferences: &lt;62165.83.227.116.190.1160690400.squirrel@...&gt;\n    &lt;egn9nl+n27s@...&gt;\r\nDate: Fri, 13 Oct 2006 11:38:28 +0200 (CEST)\r\nTo: archive-crawler@yahoogroups.com\r\nUser-Agent: SquirrelMail/1.4.4\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nX-Priority: 3 (Normal)\r\nImportance: Normal\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;Maximilian Schoefmann&quot; &lt;schoefma@...&gt;\r\nSubject: Re: [archive-crawler] Re: JMX password file is missing or \n     permissions not set correctly\r\nX-Yahoo-Group-Post: member; u=284768837\r\n\r\n&gt;&gt; NutchWAX is next.... this might be harder :-)\n&gt;\n&gt; NutchWAX has me stumped even on linux. Happy I&#39;m not building an index\n&gt; of my crawled sites yet!\n\nI got it working surprisingly fast on Linux with the data of a single\ncrawl, but was stuck then as incremental indexing is somehow broken at the\nmoment (see\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1518431&group_id=118427&atid=681137\nand\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1477183&group_id=118427&atid=681137)\n\n&gt; Care to share how you plan to use your crawled data?\n\nBasically a wayback with search function for a fairly big website. I also\nthought about mirroring and using a different (file system) indexer. But I\ndidn&#39;t like the idea of having hundreds of thousands small files,\nespecially if I would extend the crawl to more sites and make more\nfrequent crawls. I also found the end-user experience of WERA and the\nother tools nice and don&#39;t really want to reimplement this.\n\n&gt; For my use I crawl in mirror, as arc is just a pain on windows. We then\n&gt; do some regex fun as searching comments inside HTML is more fun for our\n&gt; use.\n\n\n\n"}}