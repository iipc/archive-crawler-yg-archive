{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":27855065,"authorName":"bergmark_d","from":"&quot;bergmark_d&quot; &lt;bergmark@...&gt;","profile":"bergmark_d","replyTo":"LIST","senderId":"RZuOapOSMwrlOL38HlwuwS2UMF7PdP3VHthcdapJstoCuUlW7HBlM9MnYQchAPy9nZ-Ftvf1Jr0WqL33H4NU6Ah9q0N_ZqIC_xekxoI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Newbie question -- I should be more specific","postDate":"1108996703","msgId":1596,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGN2Y3JvditlZ2xpQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQyMTlEQUY2LjkwNzAzMDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":1595,"nextInTopic":1601,"prevInTime":1595,"nextInTime":1597,"topicId":1588,"numMessagesInTopic":7,"msgSnippet":"Not necessary to finish the crawl first and then examine the pages; you can write a processor that examines the pages on the fly, and continue to crawl only","rawEmail":"Return-Path: &lt;bergmark@...&gt;\r\nX-Sender: bergmark@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 13077 invoked from network); 21 Feb 2005 14:39:18 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m5.grp.scd.yahoo.com with QMQP; 21 Feb 2005 14:39:18 -0000\r\nReceived: from unknown (HELO n18a.bulk.scd.yahoo.com) (66.94.237.47)\n  by mta6.grp.scd.yahoo.com with SMTP; 21 Feb 2005 14:39:17 -0000\r\nReceived: from [66.218.69.1] by n18.bulk.scd.yahoo.com with NNFMP; 21 Feb 2005 14:38:26 -0000\r\nReceived: from [66.218.66.69] by mailer1.bulk.scd.yahoo.com with NNFMP; 21 Feb 2005 14:38:26 -0000\r\nDate: Mon, 21 Feb 2005 14:38:23 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;cvcrov+egli@...&gt;\r\nIn-Reply-To: &lt;4219DAF6.9070303@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 5379\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.47\r\nFrom: &quot;bergmark_d&quot; &lt;bergmark@...&gt;\r\nSubject: Re: Newbie question -- I should be more specific\r\nX-Yahoo-Group-Post: member; u=27855065\r\nX-Yahoo-Profile: bergmark_d\r\n\r\n\nNot necessary to finish the crawl first and then examine the pages;\nyou can write a processor that examines the pages on the fly, and\ncontinue to crawl only from pages of interest, or only write out \npages of interest.  To do this, you must parse the HTML, PDF, etc.\non the fly.  (I have code examples if you want to see how; maybe \nthis should be a section in the user manual for Heritrix).  bergmark_d\n\n--- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; billo_ga wrote:\n&gt; \n&gt; &gt;\n&gt; &gt; --- In archive-crawler@yahoogroups.com, &quot;billo_ga&quot; &lt;billo@r...&gt;\nwrote:\n&gt; &gt; &gt; I am looking for a tool that will allow me to focus on a domain\nand\n&gt; &gt; &gt; only grab a few pages based on content - for example, simple\ntext\n&gt; &gt; &gt; searching criteria.  For instance, I may be only looking for\npages\n&gt; &gt; &gt; that contain the word &quot;acre&quot; or &quot;juniper,&quot; or only pages that\nhave\n&gt; &gt; &gt; links to more than two .mpeg movies. Of course, the more useful\nthings\n&gt; &gt; &gt; are more complex.\n&gt; \n&gt; In the above, you need to explain more what you mean by &#39;...grab a\nfew \n&gt; pages based on content...&#39;  When you say grab, do you mean crawl\nonly \n&gt; certain pages?  Or, do you mean that pages with &#39;acre&#39; or &#39;juniper&#39;\nare \n&gt; the pages that you&#39;ll subsequently be interested in after all has\nbeen \n&gt; crawled?  (You probably mean the latter because you understand that\nthe \n&gt; crawler has to download all pages first before it can see which\npages \n&gt; have &#39;acre&#39; or &#39;juniper&#39; in them).\n&gt; \n&gt; See more below.\n&gt; \n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt; I should probably be more specific here.  I am a forensic\npathologist,\n&gt; &gt; and am interested in searching various sites for information and\n&gt; &gt; imagery involving patterned injury of the skin.  I am, for\ninstance,\n&gt; &gt; writing an atlas of patterned injury in order to help people\ndetermine\n&gt; &gt; what object was used in an assault (hammer, crowbar, etc.) from\nthe\n&gt; &gt; patterns those objects leave on the skin.\n&gt; \n&gt; Interesting.  I&#39;m guessing it&#39;ll be a large volume.\n&gt; \n&gt; &gt;\n&gt; &gt; I am interested in searching hardware sites for specific kinds of\n&gt; &gt; implements, and searching medical sites for images of skin injury.\n&gt; &gt;\n&gt; &gt; I happen to have some formal training in Computer Science, and can\n&gt; &gt; write code in Java, C++, C, python, perl, and a few others (even\n--\n&gt; &gt; not to show my age, APL, God help me; I still have a keyboard\nlaying\n&gt; &gt; around for it).  So, I&#39;m happy to write a parser or whatever if I\ncan\n&gt; &gt; get a handle on how to fit it in here (or where to fit it in).\n&gt; \n&gt; Here&#39;s a sketch of one way in which I could imagine it working.\n&gt; \n&gt; Run the crawler against the sites you are interested in.  Then,\nafter \n&gt; the crawl has completed, feed the downloaded ARCs to a search\nengine so \n&gt; you can run your &#39;acre&#39; and &#39;juniper&#39; queries.  There is quite a\nbit of \n&gt; work involved here -- parsing ARC files, feeding each ARC record to\na \n&gt; mimetype-particular parser (i.e. an html parser for the text/html),\nthen \n&gt; passing the extracted text to a search engine indexer, etc. -- but\nthe \n&gt; good news here is that you should be able to leverage the work\nbegun \n&gt; here, \n&gt;\nhttp://cvs.sourceforge.net/viewcvs.py/*checkout*/a\nrchive-access/archive-access/projects/nutch/README.txt?rev=1,\n\n&gt; which has tools to feed ARC files to nutch (Whats there has been\ntried \n&gt; on 40million plus text/html pages.  The quality of the searches is \n&gt; lacking but is currently being worked on).\n&gt; \n&gt; Asking nutch to return pages with two or more mpegs may work.  If\nit \n&gt; doesn&#39;t, you&#39;d need to do your own purposed parse of the pages \n&gt; populating an index/db that you can run these types of queries\nagainst \n&gt; (You could write the parse in APL).  Dependent on the type of parse\nyou \n&gt; want to run, this would probably be a significant undertaking (See \n&gt; portions of nutch for an example).\n&gt; \n&gt; Are the domains you are interested in large?  If they are, running\na \n&gt; large crawl and running the downloaded crawl via an indexer is a \n&gt; significant adminstrative task requiring ample hardware (disk,\ncpu).  \n&gt; For example, feeding the 40million plus above mentioned pages to\nnutch \n&gt; to index took 4 machines running multiple days (Speed is also being \n&gt; worked on in the aforementioned archive-access nutch project).\n&gt; \n&gt; Yours,\n&gt; St.Ack\n&gt; \n&gt; &gt;\n&gt; &gt; billo\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; *Yahoo! Groups Sponsor*\n&gt; &gt;\n&lt;http://us.ard.yahoo.com/SIG=129qjc88h/M=324658.60\n70095.7083352.3001176/D=groups/S=1705004924:HM/EXP\n=1108918675/A=2343726/R=0/SIG=12ij8ddfl/*http://cl\nk.atdmt.com/VON/go/yhxxxvon01900091von/direct/01/&\ntime=1108832275327474&gt;\n\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Get unlimited calls to\n&gt; &gt;\n&gt; &gt; U.S./Canada\n&gt; &gt;\n&gt; &gt;\n&lt;http://us.ard.yahoo.com/SIG=129qjc88h/M=324658.60\n70095.7083352.3001176/D=groups/S=1705004924:HM/EXP\n=1108918675/A=2343726/R=1/SIG=12ij8ddfl/*http://cl\nk.atdmt.com/VON/go/yhxxxvon01900091von/direct/01/&\ntime=1108832275327474&gt;\n\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n--------------------------------------------------\n----------------------\n&gt; &gt; *Yahoo! Groups Links*\n&gt; &gt;\n&gt; &gt;     * To visit your group on the web, go to:\n&gt; &gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt; &gt;        \n&gt; &gt;     * To unsubscribe from this group, send an email to:\n&gt; &gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt; &gt;      \n&lt;mailto:archive-crawler-unsubscribe@...\nm?subject=Unsubscribe&gt;\n&gt; &gt;        \n&gt; &gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt; &gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; &gt;\n&gt; &gt;\n\n\n\n\n"}}