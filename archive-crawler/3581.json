{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"RXMhnDEWoeHgF_OURKlqWqWyWfnS5KAejs7NeqnKcAYuXvru8467UOQkw1SrBSadTbpKUhex5HG_pTRFXNpn_OCm_p3_Ruvg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] How to stop filtering of already seen URIs based on previous fetch status code?","postDate":"1165604868","msgId":3581,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1NzlCODA0LjcwMzA5MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGVsY2JyaytmN2g2QGVHcm91cHMuY29tPg==","referencesHeader":"PGVsY2JyaytmN2g2QGVHcm91cHMuY29tPg=="},"prevInTopic":3580,"nextInTopic":3582,"prevInTime":3580,"nextInTime":3582,"topicId":3580,"numMessagesInTopic":5,"msgSnippet":"... Thats odd.  With the force-fetch flag enabled, the URLs should be crawled whether they ve been seen or not.  Did the URLs show again in crawl.log after","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 21371 invoked from network); 8 Dec 2006 19:01:33 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m33.grp.scd.yahoo.com with QMQP; 8 Dec 2006 19:01:33 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta4.grp.scd.yahoo.com with SMTP; 8 Dec 2006 19:01:33 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 9AC221418E16B;\n\tFri,  8 Dec 2006 11:00:59 -0800 (PST)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 24574-03-43; Fri, 8 Dec 2006 11:00:57 -0800 (PST)\r\nReceived: from [192.168.1.204] (c-71-198-60-165.hsd1.ca.comcast.net [71.198.60.165])\n\tby mail.archive.org (Postfix) with ESMTP id C864C14182EB2;\n\tFri,  8 Dec 2006 11:00:56 -0800 (PST)\r\nMessage-ID: &lt;4579B804.7030902@...&gt;\r\nDate: Fri, 08 Dec 2006 11:07:48 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.2) Gecko/20060405 SeaMonkey/1.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;elcbrk+f7h6@...&gt;\r\nIn-Reply-To: &lt;elcbrk+f7h6@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] How to stop filtering of already seen URIs\n based on previous fetch status code?\r\nX-Yahoo-Group-Post: member; u=168599281; y=Fjasok-uuXqADkXFEd2ooFTkgieGTEGlJysUIAPzgqovlsOvXU4cyhUD\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nastar_t wrote:\n&gt;\n&gt; Hi, I&#39;m running a crawl where initially some URIs were rejected by a\n&gt; DecideRule that was added by mistake. So the URIs show up in the\n&gt; crawl.log as having a -5000 fetch code. However, I have realized that\n&gt; I would actually like to crawl a subset of these URIs that have already\n&gt; been rejected.\n&gt;\n&gt; So I tried to import the URIs back into the frontier with force fetch\n&gt; via JMX cmdline but they are not being fetched. I&#39;m assuming this is\n&gt; because they are &quot;already seen&quot; URIs so the BdbUriUniqFilter is\n&gt; ignoring them.\n&gt;\n\n\n\n\n\n\n\n\n\n\nThats odd.  With the &#39;force-fetch&#39; flag enabled, the URLs should be \ncrawled whether they&#39;ve been seen or not.  Did the URLs show again in \ncrawl.log after resubmission (or in any of the Heritrix logs)?\n\nSt.Ack\n\n\n\n\n&gt;\n&gt; Is there a way I can force Heritrx to retry a fetch of the URIs that\n&gt; previously had a -5000 code ?\n&gt;\n&gt; Thanks!\n&gt; Adam T.\n&gt;\n&gt;  \n\n\n"}}