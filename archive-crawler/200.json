{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"TMbzdFzryT9EfVtv7EB4hi1p9SzFHNIpRgts68-VvOjMSU7JYoRJJfQZOJQBvIMAibLqeFaxN03BTvrKWGfXezvlX4dBMG0","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Checkpointing","postDate":"1071171563","msgId":200,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNGRDhDN0VCLjUwOTAyMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEwNzExMDEwMDMuMTA5MDQuOTEuY2FtZWxAYjExNi1keW4tMzcuYXJjaGl2ZS5vcmc+","referencesHeader":"PDNGQkJGNjUzLjkwNjAxMDdAYXJjaGl2ZS5vcmc+IDwxMDcxMTAxMDAzLjEwOTA0LjkxLmNhbWVsQGIxMTYtZHluLTM3LmFyY2hpdmUub3JnPg=="},"prevInTopic":199,"nextInTopic":201,"prevInTime":199,"nextInTime":201,"topicId":178,"numMessagesInTopic":7,"msgSnippet":"... For recovery from major crawl problems, the checkpoint resolution is how much time/work will be lost when the crawl restarts. We d like to be able to do","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 29538 invoked from network); 11 Dec 2003 19:39:25 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m8.grp.scd.yahoo.com with QMQP; 11 Dec 2003 19:39:25 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 11 Dec 2003 19:39:25 -0000\r\nReceived: (qmail 26595 invoked by uid 100); 11 Dec 2003 19:38:41 -0000\r\nReceived: from b116-dyn-43.archive.org (HELO archive.org) (gojomo@...@209.237.240.43)\n  by mail-dev.archive.org with SMTP; 11 Dec 2003 19:38:41 -0000\r\nMessage-ID: &lt;3FD8C7EB.5090200@...&gt;\r\nDate: Thu, 11 Dec 2003 11:39:23 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nSubject: Re: [archive-crawler] Checkpointing\r\nReferences: &lt;3FBBF653.9060107@...&gt; &lt;1071101003.10904.91.camel@...&gt;\r\nIn-Reply-To: &lt;1071101003.10904.91.camel@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-5.9 required=6.0\n\ttests=AWL,BAYES_01,EMAIL_ATTRIBUTION,IN_REP_TO,QUOTED_EMAIL_TEXT,\n\t      REFERENCES,REPLY_WITH_QUOTES,USER_AGENT_MOZILLA_UA\n\tautolearn=ham version=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nJohn Erik Halse wrote:\n&gt; When thinking of different approaches for doing checkpoints I came up\n&gt; with a some questions that should be answered before we try to design\n&gt; it.\n&gt; \n&gt; * How often are we supposed to do a checkpoint (aka how costly is a\n&gt; checkpoint allowed to be).\n\nFor recovery from major crawl problems, the checkpoint resolution is\nhow much time/work will be lost when the crawl restarts. We&#39;d like\nto be able to do checkpoints every few hours on most focused crawls.\nWe might want as infrequent as once per day on broad crawls.\n\n&gt; If the checkpoints are very expensive, we could do a combination of\n&gt; checkpoint and recovery log. The recovery log should then be reset at\n&gt; every checkpoint.\n\n&gt; * Is it ok to pause the crawler for a checkpoint? It might take some\n&gt; time to wait for all the threads to finish. Is this acceptable?\n\nSome disruption/pause to full crawler throughput is inevitable.\n\nAn approach that has been recommended is to divide up the processing\nof a CrawlURI into a first phase (including the potentially lengthy\nnetwork fetch), which has no persistent effect on the checkpointed\nstate (of the Frontier or other modules), and then a second phase,\nwhich if started must be completed before a checkpoint occurs.\n\nSo the problem of waiting for all the threads to finish becomes in\nfact just waiting for them to finish their critical second phases,\nwhich are more likely to be guaranteed to finish in a bounded\namount of time. Also, progress can continue on first phase network\ndownload activity -- indeed new URIs can even be begun during a\ncheckpoint, they just can&#39;t proceed to the second phase processing.\n\nIn our design, this might entail making some subset of the Processors\nafter the fetchers, including the ARCWriter and others with lasting\neffect on in-memory structures and running statistics, into the critical\nsecond phase\n\n&gt; * Is checkpointing just for recovering from crashes?\n&gt; If not:\n&gt;   - Should it be possible to manipulate queues in a suspended state? For\n&gt; example adding or removing URIs in the pending queue.\n\nYes -- though perhaps this is just the same capability as we would want\nfor any paused crawl. (That is, the operator might not directly edit\nstate on disk, but rather (1) load checkpoint without restarting active\ncrawl; then (2) use other admin options to edit standing queues.)\n\n&gt;   - Should it be possible to change implementation of modules between\n&gt; suspend and resume? For example fixing bugs.\n\nDefinitely.\n\n&gt;   - Should it be possible to alter the configuration in suspended state\n\nYes -- though again this may just be the same capability as would\nbe wanted for any paused crawl, whether it has been completely\ncheckpointed or not.\n\n&gt; * Is it ok to insert a checkpoint mark in the working files or should\n&gt; everything be copied to a safe location to make sure that a crash would\n&gt; not corrupt files?\n\nTo be determined; I think some files which change in arbitrary ways\nbetween checkpoints would need to be duplicated in full. A &quot;safe location&quot;\nmight just be another filename in the same working directory.\n\n&gt; If we add the possibility to run a multiple machine crawl; Should the\n&gt; checkpoint span all the crawler instances or should the checkpoint be\n&gt; local to a single instance?\n\nIt must span all instances to the extent required to prevent any URIs\nfrom falling through the cracks or major discrepancies in expected\nbehavior between the original run and a subsequent resume-from-checkpoint.\n\nFor example, if cooperating crawler A sends a URI to B for crawling, then\nmakes checkpoint 0001, while B makes checkpoint 0001, then receives\nthe URI, there would be a problem upon resuming both from their respective\ncheckpoints 0001: A would think the URI was already handled, while B will\nnot have received it.\n\n- Gordon\n\n"}}