{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"eUVkn3lutKh6y_oPx-3V1m-ZYXsPgio--HkofAOaL5EzLZGfVm_yLi30Y9ePyoeVhDLH0vMCZhYX5esrbLyF6chR1ToK1V5K","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Large scale crawling with Heritrix","postDate":"1085422094","msgId":445,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwQjIzQTBFLjIwMTA2MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBDNzVAbG9raS5ib2suaGkuaXM+","referencesHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBDNzVAbG9raS5ib2suaGkuaXM+"},"prevInTopic":440,"nextInTopic":447,"prevInTime":444,"nextInTime":446,"topicId":432,"numMessagesInTopic":9,"msgSnippet":"Hi Kris, Kaisa, ... Pathological path should take care of both patterns (default pattern: .*/(.*/) 1{3}.*) Second pattern is a bit problematic since you can","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 96792 invoked from network); 24 May 2004 18:17:38 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m22.grp.scd.yahoo.com with QMQP; 24 May 2004 18:17:38 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 24 May 2004 18:17:38 -0000\r\nReceived: (qmail 12757 invoked by uid 100); 24 May 2004 18:10:31 -0000\r\nReceived: from b116-dyn-56.archive.org (HELO archive.org) (igor@...@209.237.240.56)\n  by mail-dev.archive.org with SMTP; 24 May 2004 18:10:31 -0000\r\nMessage-ID: &lt;40B23A0E.2010608@...&gt;\r\nDate: Mon, 24 May 2004 11:08:14 -0700\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;D958110B273CD511ACC100B0D079AA4E01960C75@...&gt;\r\nIn-Reply-To: &lt;D958110B273CD511ACC100B0D079AA4E01960C75@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: **\r\nX-Spam-Status: No, hits=2.0 required=6.0 tests=AWL,DOMAIN_BODY,\n\tDRASTIC_REDUCED autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Re: Large scale crawling with Heritrix\r\nX-Yahoo-Group-Post: member; u=137477665\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Kris, Kaisa,\n\n&gt; I did add the pathological path filter that does filter out paths like \n&gt; .../img/img/img/... but not repeating sequences like \n&gt; .../src/some/path/src/some/path/...\n\nPathological path should take care of both patterns (default pattern: .*/(.*/)&#92;&#92;1{3}.*)\nSecond pattern is a bit problematic since you can crawl a lot &#39;junk&#39; before the filter kicks in.\n.../src/some/path/src/some/path/src/some/path/...for default value of 3 repetitions.\n\n&gt; I did not apply the path depth filter but it with a sensible setting \n&gt; would limit such repeating patterns.\n\nOn all crawls I set both pathological and depthpath filters. It is a good practice to have them.\n\n&gt; Hopefully Igor will chime in on this, it�s really his field. Actually, \n&gt; Igor, how about writing a �tips and tricks� section in the user manual?\n\nI probably should but the problem is that there is a lot of corner cases. It would be hard to cover \neverything. Pathological and depthpath filters take care of a lot of &#39;traps.&#39;\n\n&gt; How do you handle dynamic urls (or better urls pointing to dynamic\n&gt; files). Even large search engines are very careful with them.\n&gt; \n&gt; *) Reject all urls which contain parameters longer than 10\n&gt; characters, because those parameters are probably session ids. Length\n&gt; 10 is arbitrary, could be 7, 8, 12 or something else? Filters are\n&gt; easy to construct if one only knew what &#39;number&#39; works in most cases.\n&gt; \n&gt; http://www�..net/?id=12hswjdufh27e6djjsu3778s \n&gt; &lt;http://www�..net/?id=12hswjdufh27e6djjsu3778s&gt;\n\nAt this point we let http client to take care of it via cookies. It is pretty common for session ids \nto be set via cookies.\nThe more serious issue is when session ids expire during crawls. In that case same URLs with \ndifferent session ids might be crawled again.\nOne solution to this is striping session ids before checking if a URL have been already crawled.\nFor example:\nhttp://www�..net/index.html?id=12hswjdufh27e6djjsu3778s (crawled)\nhttp://www�..net/index.html?id=aaaawjdufh27e6djjsu3778s (to be crawled)\nIf we strip session id it is obvious that we have already crawled this url.\nhttp://www�..net/index.html? (crawled)\nhttp://www�..net/index.html? (don&#39;t crawl again)\n\nStripping session ids is not trivial issue. There are some commonly used session ids such as \nPHPSESSID, JSESSIONID, CFID, CFTOKEN and etc. For these ids we can probably add default stripping \nrules. For others we will have to maintain stripping rules probably per host bases. This could be \nvery hard to manage.\n\nAnyway, Heritrix should probably have a facility for doing this.\n\n&gt; *)  Reject urls with too many parameters. Set maximum number of\n&gt; parameters to some value: are 4 parameters acceptable, but 5 too much\n&gt; (or some other limit)?\n&gt; \n&gt; http:/www�...net/?\n&gt; project_id=2&ykieli=fi&startfrom=0&stopto=6&commands=329&pic=\n\nI am not sure why would you want to do this?\n\nTake care.\ni.\n\n&gt; --- In archive-crawler@yahoogroups.com, &quot;Kristinn Sigurdsson&quot;\n&gt; &lt;kris@a...&gt; wrote:\n&gt;&gt;  Hi all,\n&gt;&gt;\n&gt;&gt;  This week I&#39;ve been experimenting with running a crawl over the\n&gt; entire .is\n&gt;&gt;  TLD. As expected I&#39;ve encountered several problems and limitations\n&gt; that I\n&gt;&gt;  thought might of interest to this community as this is (as far as I\n&gt; know)\n&gt;&gt;  the first attempt to run Heritrix on such a scale. Crawling has\n&gt; been done\n&gt;&gt;  with very recent development builds.\n&gt;&gt;\n&gt;&gt;  Many of the limits I&#39;ve encountered are already known but I&#39;m\n&gt; reiterating\n&gt;&gt;  them here for the sake of completeness.\n&gt;&gt;\n&gt;&gt;  The .is TLD has a little over 10000 registered second level\n&gt; domains. A list\n&gt;&gt;  of them with a www prefix was used as a seed list. Instead of one\n&gt; of the\n&gt;&gt;  scopes shipped with Heritrix I wrote a custom scope that limited\n&gt; the crawl\n&gt;&gt;  to hosts with the .is suffix (plus the usual transitive includes).\n&gt;&gt;\n&gt;&gt;  The first problem I encountered was Heritrix&#39;s excessive use of file\n&gt;&gt;  handlers. In addition to numerous other files used by Heritrix each\n&gt; one of\n&gt;&gt;  the 10000+ domains immediately needed its own host queue with two\n&gt; file\n&gt;&gt;  handlers each. This led me to (with the help of Gordon and Michael)\n&gt; to\n&gt;&gt;  redesign the so called DiskBackedQueues so that they only have open\n&gt; files\n&gt;&gt;  when they are large enough to warrant it. I.e. when items in them\n&gt; are too\n&gt;&gt;  many to fit into the memory &#39;head&#39; that they have (200 files for\n&gt; the host\n&gt;&gt;  queues).\n&gt;&gt;\n&gt;&gt;  This fix drastically reduced the number of open files and for the\n&gt; most part\n&gt;&gt;  eliminated this problem. With it out of the way it was possible to\n&gt; launch a\n&gt;&gt;  crawl with this many seeds.\n&gt;&gt;\n&gt;&gt;  Initial progress was quite impressive, running at over 50 documents\n&gt; per\n&gt;&gt;  second initially. Eventually it started to gradually drop and now\n&gt; several\n&gt;&gt;  days later it stands at around 17 documents per second. I&#39;m unsure\n&gt; of the\n&gt;&gt;  reason for this gradual decline. It may be related to increasing\n&gt; sizes of\n&gt;&gt;  various data structures.\n&gt;&gt;\n&gt;&gt;  Over the course of the past 3 days the crawl has downloaded nearly\n&gt; 4 million\n&gt;&gt;  documents totaling over 150 GB. In addition some 11 million plus\n&gt; documents\n&gt;&gt;  have been discovered and are waiting processing. While impressive\n&gt; this is\n&gt;&gt;  still only scratching the top of the .is domain. I estimate that\n&gt; currently\n&gt;&gt;  documents 3 link hops from the seeds are being processed.\n&gt;&gt;\n&gt;&gt;  Memory use was initially my main concern. The crawl is running on a\n&gt; machine\n&gt;&gt;  with 1.5 GB RAM and the JVM max heap size was set to 1.25 GB. To\n&gt; date the\n&gt;&gt;  JVM has only allocated itself 1 GB and garbage collection still\n&gt; drops the\n&gt;&gt;  memory being used to almost half that.\n&gt;&gt;\n&gt;&gt;  Disk use by the disk bound queues however has been much greater\n&gt; then I\n&gt;&gt;  anticipated. With said 11 million URLs waiting in the queues they\n&gt; now take\n&gt;&gt;  up about 16 GB. This comes out at about 1.6 KB per URI. This will\n&gt; turn out\n&gt;&gt;  to be the limiting factor for my current crawl since the disk in\n&gt; question\n&gt;&gt;  only has another 3 GB free so it will be exhausted soon.\n&gt;&gt;\n&gt;&gt;  Even with much larger disks, say 200 GB, crawls will be limited to\n&gt; having\n&gt;&gt;  120-130 million URIs waiting. This seems like a huge number until\n&gt; you\n&gt;&gt;  consider that the .is domain would seem to have at least that many\n&gt; documents\n&gt;&gt;  based on this crawl. Doing a crawl like this on an even larger\n&gt; scale would\n&gt;&gt;  seem to merit trying to reduce the size (on disk) of these URIs.\n&gt;&gt;\n&gt;&gt;  Of course a crawl of that scope is not possible until the list of\n&gt; already\n&gt;&gt;  seen URIs can be disk backed.  With the current method of using 4\n&gt; byte\n&gt;&gt;  fingerprints for each encountered URI 1 GB of memory can hold\n&gt; around 28\n&gt;&gt;  million URIs. Even with a machine with 4 GB RAM would not be able\n&gt; to scale\n&gt;&gt;  up to even a full .is TLD crawl.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  Some of my thoughts on dealing with the limits follow:\n&gt;&gt;\n&gt;&gt;  Moving the list of encountered URIs to disk seems to be imperative.\n&gt; But that\n&gt;&gt;  will pose even greater demands on disk space so I would suggest\n&gt; that we\n&gt;&gt;  remain aware of that issue and try (whenever possible) to limit the\n&gt; size of\n&gt;&gt;  the data being written to disk to that which is actually needed.\n&gt; That may\n&gt;&gt;  have the additional benefit of speeding I/O operations (even if only\n&gt;&gt;  marginally). Perhaps it should also be possible to split the data\n&gt; among many\n&gt;&gt;  disk (of course this can be done at the OS level to some extent).\n&gt;&gt;\n&gt;&gt;  Multi machine setup might alleviate some of this but for any sort\n&gt; of large\n&gt;&gt;  scale crawl (targeting 100 million +) each machine would probably be\n&gt;&gt;  handling tens of millions URIs at least. For truly large scale\n&gt; crawls (like\n&gt;&gt;  the ones I know IA wants to do) the demands grow even more.\n&gt;&gt;\n&gt;&gt;  Another approach which might limit the problem is the\n&gt; implementation of the\n&gt;&gt;  site first crawling strategy. In my crawl all .is domains are\n&gt; tackled in\n&gt;&gt;  parallel. This makes the crawl a true breadth first crawl. A site\n&gt; first\n&gt;&gt;  approach would let Heritrix focus it&#39;s efforts on a limited number\n&gt; of sites\n&gt;&gt;  at a time. Since the usual pattern for crawling a site is an initial\n&gt;&gt;  explosion of available URIs followed by a steady decline and\n&gt; eventual\n&gt;&gt;  exhaustion we would likely not wind up having as huge a number of\n&gt; URIs\n&gt;&gt;  waiting (at least not as quickly). This does nothing for the limits\n&gt; imposed\n&gt;&gt;  by the demands on RAM memory though.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  Kristinn Sigur�sson\n&gt;&gt;  Software engineer\n&gt;&gt;  National and University Library of Iceland\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; *Yahoo! Groups Sponsor*\n&gt; ADVERTISEMENT\n&gt; &lt;http://rd.yahoo.com/SIG=129nfh7uo/M=295196.4901138.6071305.3001176/D=groups/S=1705004924:HM/EXP=1085481210/A=2128215/R=0/SIG=10se96mf6/*http://companion.yahoo.com&gt;\n&gt; \n&gt; \n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt; \n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;. \n&gt; \n&gt; \n\n\n"}}