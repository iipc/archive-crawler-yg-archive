{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":289645082,"authorName":"helloitsmaxine","from":"&quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;","profile":"helloitsmaxine","replyTo":"LIST","senderId":"qlDQJk7qd6e0muvFzsAQ3KDHFyqPHUPCl0I6CuNW5r3DQfvGJKhfQVusE5l5Xgu_DuvV2LcMJ-C9_ekq98mNe0MMW-fwe-LIfXLPS1Y","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: settings to maximize number of unique domains crawled?","postDate":"1310687465","msgId":7210,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGl2bnZkOSszbzV0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRFMUUzODdGLjUwOTAzMDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":7209,"nextInTopic":0,"prevInTime":7209,"nextInTime":7211,"topicId":7206,"numMessagesInTopic":5,"msgSnippet":"Just wanted to follow up on some results I recently got. I m not sure if this improvement will scale up, but in my latest crawl, after I implemented all the","rawEmail":"Return-Path: &lt;itsmaxine@...&gt;\r\nX-Sender: itsmaxine@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 77928 invoked from network); 14 Jul 2011 23:51:07 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m7.grp.sp2.yahoo.com with QMQP; 14 Jul 2011 23:51:07 -0000\r\nX-Received: from unknown (HELO ng16-ip1.bullet.mail.ne1.yahoo.com) (98.138.215.243)\n  by mta2.grp.sp2.yahoo.com with SMTP; 14 Jul 2011 23:51:07 -0000\r\nX-Received: from [98.138.217.178] by ng16.bullet.mail.ne1.yahoo.com with NNFMP; 14 Jul 2011 23:51:07 -0000\r\nX-Received: from [69.147.65.150] by tg3.bullet.mail.ne1.yahoo.com with NNFMP; 14 Jul 2011 23:51:06 -0000\r\nX-Received: from [98.137.34.32] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 14 Jul 2011 23:51:06 -0000\r\nDate: Thu, 14 Jul 2011 23:51:05 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;ivnvd9+3o5t@...&gt;\r\nIn-Reply-To: &lt;4E1E387F.5090303@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;\r\nSubject: Re: settings to maximize number of unique domains crawled?\r\nX-Yahoo-Group-Post: member; u=289645082; y=CJu-zIquFP_zw4fJUW3jK3QDScCoNE4EuX0ftuugeYY_EVSLMoHyYcp1W6gN507DAiM0GeMGL6mGhWE\r\nX-Yahoo-Profile: helloitsmaxine\r\n\r\nJust wanted to follow up on some results I recently got. I&#39;m not sure if th=\r\nis improvement will scale up, but in my latest crawl, after I implemented a=\r\nll the suggestions you gave, for about 3gb crawled (about an hour) I have o=\r\nver 7000 unique domains, which is way, way better than what I was getting b=\r\nefore. I&#39;ll update a little further down the road with more stats, but this=\r\n improvement is really encouraging, especially since we are aiming eventual=\r\nly to crawl close to a million unique domains.\n\nSo thank you so much for yo=\r\nur help (also I&#39;ve been to the Setting tab so many times and never noticed =\r\nthe expert settings link). If anyone has other insights I&#39;d like to hear th=\r\nem too. :)\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...=\r\n&gt; wrote:\n&gt;\n&gt; In H1, on the &#39;Settings&#39; tab of crawl job configuration, if yo=\r\nu &#39;view\n&gt; expert settings&#39;, there should be a &#39;cost-policy&#39; setting on the\n=\r\n&gt; frontier with drop-down options.\n&gt; \n&gt; In H3, you&#39;d use the Spring XML syn=\r\ntax to specify a CostAssignmentPolicy\n&gt; bean of the desired class.\n&gt; \n&gt; - G=\r\nordon @ IA\n&gt; \n&gt; On 7/13/11 5:07 PM, helloitsmaxine wrote:\n&gt; &gt; Hi Gordon,\n&gt; =\r\n&gt;\n&gt; &gt; Thanks for your advice.\n&gt; &gt;\n&gt; &gt; I&#39;ve changed my scope now to Deciding=\r\n with the AcceptRule on. As for\n&gt; &gt; the first two suggestions, they make se=\r\nnse but I can&#39;t seem to figure\n&gt; &gt; out how to implement those configuration=\r\ns through the web UI.\n&gt; &gt; Searching about the CostAssignmentPolicy has turn=\r\ned up something to\n&gt; &gt; do with a WorkQueueFrontier, which doesn&#39;t seem to c=\r\nome as an option?\n&gt; &gt; Am I just looking in the wrong place or are there cer=\r\ntain features\n&gt; &gt; that need some additional work to set up?\n&gt; &gt;\n&gt; &gt; --- In =\r\narchive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@&gt;\n&gt; &gt; wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt; =\r\nThe best adjustment you could make to encourage wandering to a\n&gt; &gt;&gt; wide nu=\r\nmber of different hosts is to:\n&gt; &gt;&gt;\n&gt; &gt;&gt; (1) make sure the &#39;budgeting&#39; for =\r\nallocating frontier effort among\n&gt; &gt;&gt; queues is in effect, by having a non-=\r\nzero CostAssignmentPolicy.\n&gt; &gt;&gt; (UnitCostAssignmentPolicy, treating each UR=\r\nI as &#39;1&#39;, is fine.)\n&gt; &gt;&gt;\n&gt; &gt;&gt; (2) then, make the &#39;balance-replenish-amount&#39;=\r\n very small. Each\n&gt; &gt;&gt; queue gets devoted frontier attention until this amo=\r\nunt is &#39;spent&#39;,\n&gt; &gt;&gt; then that queue goes to the back of all other queues. =\r\n(Usually, we\n&gt; &gt;&gt; want to intensely crawl a site for a while, ideally even =\r\nfinishing\n&gt; &gt;&gt; small sites, so that all the archived captures are close tog=\r\nether\n&gt; &gt;&gt; in time... thus the 3000 default value here. But it could be 50,=\r\n or\n&gt; &gt;&gt; 10, or even 1. Note that at &#39;1&#39; behavior between queues is\n&gt; &gt;&gt; co=\r\nmpletely round-robin, which spread attention across the widest\n&gt; &gt;&gt; number =\r\nof sites but also generally keeps memory caches from getting\n&gt; &gt;&gt; any benef=\r\nit from lots of crawling of the same site in rapid\n&gt; &gt;&gt; succession.)\n&gt; &gt;&gt;\n&gt;=\r\n &gt;&gt; Lots of other refinements are possible, with advanced\n&gt; &gt;&gt; configuratio=\r\nn tweaks (including some, like custom &#39;queue precedence\n&gt; &gt;&gt; policies&#39;, tha=\r\nt theoretically should work but are so seldom used\n&gt; &gt;&gt; they&#39;re not battle-=\r\ntested). But just that change biases things a\n&gt; &gt;&gt; lot more towards host di=\r\nversity early, rather than eventually.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Separately, the &#39;BroadScop=\r\ne&#39; and other ____Scope classes (other\n&gt; &gt;&gt; than DecidingScope) are discoura=\r\nged; if you make a DecidingScope\n&gt; &gt;&gt; which starts with an AcceptDecideRule=\r\n, then focus the other rules\n&gt; &gt;&gt; on knocking out things you don&#39;t want, yo=\r\nu&#39;ll have a more modern\n&gt; &gt;&gt; (and H3-ready) broadly-scoped crawl.\n&gt; &gt;&gt;\n&gt; &gt;&gt;=\r\n - Gordon @ IA\n&gt; &gt;&gt;\n&gt; &gt;&gt; On 7/13/11 3:03 PM, helloitsmaxine wrote:\n&gt; &gt;&gt;&gt; I&#39;=\r\nm doing a crawl and interested in maximum the number of unique\n&gt; &gt;&gt;&gt; domain=\r\ns crawled, ie. I&#39;d rather crawl one page from each of 5\n&gt; &gt;&gt;&gt; unique domain=\r\ns than have 10 pages from one domain. Right now it&#39;s\n&gt; &gt;&gt;&gt; weird because I =\r\nhave pretty standard/default settings, ie.\n&gt; &gt;&gt;&gt; BroadScope and high max ho=\r\nps and such, but out of ~50gb I&#39;ve\n&gt; &gt;&gt;&gt; crawled, less than 1000 unique dom=\r\nains have been produced. I&#39;m\n&gt; &gt;&gt;&gt; counting by counting the number of folde=\r\nrs in the mirror folder\n&gt; &gt;&gt;&gt; (each of which seems to represent the content=\r\n from one unique\n&gt; &gt;&gt;&gt; domain), as I&#39;m using the MirrorWriter.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; =\r\nMy question is if anyone knows what the problem could be and how\n&gt; &gt;&gt;&gt; to f=\r\nix it? I was wondering if there were possible a maximum bytes\n&gt; &gt;&gt;&gt; per dom=\r\nain limit to set (I know there&#39;s an overall max bytes but\n&gt; &gt;&gt;&gt; that wouldn=\r\n&#39;t seem to help) or some other settings that could\n&gt; &gt;&gt;&gt; help me get more d=\r\nomains per amount of space crawled?\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Thanks for any help!\n&gt; &gt;&gt;&gt;\n=\r\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; ------------------------------------\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; Yahoo! G=\r\nroups Links\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --------------------=\r\n----------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}