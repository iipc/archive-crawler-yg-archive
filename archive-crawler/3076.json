{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225011788,"authorName":"Karl Wright","from":"Karl Wright &lt;kwright@...&gt;","profile":"daddywri","replyTo":"LIST","senderId":"1t9R9L9lt2g_p1rQn4MiGET1XEkQ2GfoKFWt_zyem9M76i7uRpqdPHKUjyL5w1gSjb8xXu3iLaZkCbtRGP5iVBx1gIi6khDLxHI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Advice needed on how to (properly) structure           new Heritrix modify and delete functionality","postDate":"1153224299","msgId":3076,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkNDRTZCLjMwMzA3MDJAbWV0YWNhcnRhLmNvbT4=","inReplyToHeader":"PDEyMTQuMTMwLjIwOC4xNTIuODAuMTE1MzIxMTQxMy5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg==","referencesHeader":"PDEyMTQuMTMwLjIwOC4xNTIuODAuMTE1MzIxMTQxMy5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg=="},"prevInTopic":3074,"nextInTopic":3078,"prevInTime":3075,"nextInTime":3077,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"Hi Kris, I ve added another round of comments below... ... This largely depends on our customer.  Our current maximum is 7.5 million urls.  The maximum is","rawEmail":"Return-Path: &lt;kwright@...&gt;\r\nX-Sender: kwright@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 37768 invoked from network); 18 Jul 2006 12:05:18 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m39.grp.scd.yahoo.com with QMQP; 18 Jul 2006 12:05:18 -0000\r\nReceived: from unknown (HELO silene.metacarta.com) (65.77.47.18)\n  by mta7.grp.scd.yahoo.com with SMTP; 18 Jul 2006 12:05:18 -0000\r\nReceived: from localhost (silene.metacarta.com [65.77.47.18])\n\tby silene.metacarta.com (Postfix) with ESMTP id 87E9E73A42\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Jul 2006 08:03:24 -0400 (EDT)\r\nReceived: from silene.metacarta.com ([65.77.47.18])\n\tby localhost (silene.metacarta.com [65.77.47.18]) (amavisd-new, port 10024)\n\twith ESMTP id 01524-06 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 18 Jul 2006 08:03:23 -0400 (EDT)\r\nX-Auth-Received: from [192.168.1.101] (146-115-66-62.c3-0.lex-ubr1.sbo-lex.ma.cable.rcn.com [146.115.66.62])\n\t(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))\n\t(No client certificate requested)\n\tby silene.metacarta.com (Postfix) with ESMTP id C832C73924\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 18 Jul 2006 08:03:22 -0400 (EDT)\r\nMessage-ID: &lt;44BCCE6B.3030702@...&gt;\r\nDate: Tue, 18 Jul 2006 08:04:59 -0400\r\nUser-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;1214.130.208.152.80.1153211413.squirrel@...&gt;\r\nIn-Reply-To: &lt;1214.130.208.152.80.1153211413.squirrel@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: by amavisd-new-20030616-p10 (Debian) at metacarta.com\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Karl Wright &lt;kwright@...&gt;\r\nSubject: Re: [archive-crawler] Advice needed on how to (properly) structure\n           new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=225011788; y=DJvb_U_7KR-x_COG1wus29wRc6PqiTX59mqU_JHHFg_B_bA\r\nX-Yahoo-Profile: daddywri\r\n\r\nHi Kris,\n\nI&#39;ve added another round of comments below...\n\nkris@... wrote:\n&gt; Hey Karl,\n&gt; \n&gt; Thanks for sharing your usage scenario. I think I now understand what your\n&gt; needs are and I may have a thought on how to help you.\n&gt; \n&gt; \n&gt; \n&gt;&gt;&gt;That is a very ambitious project. It would be very\n&gt;&gt;\n&gt;&gt;interesting if you\n&gt;&gt;\n&gt;&gt;&gt;could share why you want this functionality.\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;Ok. We build appliances here that index content for\n&gt;&gt;specialized searches. We use a number of tools for ingesting\n&gt;&gt;content into the system and removing it when it goes away.\n&gt;&gt;Our other tools all work incrementally; they only ingest\n&gt;&gt;stuff into the index that is new or has changed, and they\n&gt;&gt;delete content from the index when the content has gone away.\n&gt;&gt;We use Heritrix right now, but it effectively a one-shot for\n&gt;&gt;us; effectively we cannot detect changes without rerunning a\n&gt;&gt;job from scratch, and also we cannot detect deletions at all.\n&gt;&gt;Several of our clients have attempted to solve these problems\n&gt;&gt;by building their own re-fetcher, which looks at the Heritrix\n&gt;&gt;crawl logs to determine what to do, but basically this runs\n&gt;&gt;afoul of politeness issues and all that stuff. So we\n&gt;&gt;concluded that we really did want to build a version of\n&gt;&gt;Heritrix that fully understands how to work incrementally.\n&gt; \n&gt; \n&gt; Just to make sure we are on the same page I&#39;m going to reword your situation.\n&gt; \n&gt; You have an index of crawled content and you wish to update it in an\n&gt; incremental fashion.\n&gt; \n&gt; A question here; how long does it take to do a complete crawl? How big is it?\n&gt; \n\nThis largely depends on our customer.  Our current maximum is 7.5 \nmillion urls.  The maximum is expected to be 20 million or more.\n\n&gt; \n&gt;&gt;The adaptive frontier seemed like a possible way to go,\n&gt;&gt;except that I was told (by stack) that it did not use a\n&gt;&gt;database, and therefore would run out of memory readily. We\n&gt;&gt;already have problems even with the bdbfrontier on memory\n&gt;&gt;consumption, so I did not think that would be a viable way to proceed.\n&gt; \n&gt; \n&gt; Stack got it wrong (or you misunderstood him) the ARFrontier uses BDB just\n&gt; like the BdbFrontier to serialize data to disk. However, it was designed\n&gt; to work with a _limited_ number of hosts. The BdbFrontier uses only one\n&gt; database for all its queues while the ARF uses one database *per* queue.\n&gt; Each host queue has a some overhead and there are some issues with having\n&gt; multiple Bdb databases in the same environment like the ARF does. And\n&gt; since the ARFrontier has never seen significant use (mostly because of the\n&gt; change detection problem) it has never received much optimization.\n&gt; \n&gt; Incremental crawling on a truly large scale is bloody hard. As you note\n&gt; the BdbFrontier becomes very heavy during your crawls, yet it only needs\n&gt; to keep a small fingerprint of crawled URIs. An incremental frontier needs\n&gt; to keep rich objects for each URI (crawled or not) and is going to be\n&gt; accessing on-disk content far more randomly (read: higher access latency\n&gt; and decreased cache performance) then a snapshot frontier.\n&gt; \n\nI fully understand the tradeoffs.  However, in general, crawl *rate* has \nnever been a problem for our application; indeed we get dinged for \ncrawling too fast much of the time (so we have had to adopt much more \nstringent politeness parameters than the defaults in many cases, and \nintroduced burst throttling code as well).\n\nI can say this: Our customers are largely crawling intranets - very \nlarge intranets, but these *are* definitely constrained by domain.  As a \nballpark, I would say that the maximum number of domains we would care \nabout would be about 5,000.  That&#39;s still probably way too many, though, \nif you are keeping one queue per DB.  What do you think?\n\nBased on your description, maybe the best approach is to analyze the \nbehavior of ARF and try to optimize it for our particular usage pattern.\n\n&gt; \n&gt; \n&gt;&gt;We don&#39;t need to be too precise about detecting changes. If\n&gt;&gt;content changes only in formatting, say, the worst that can\n&gt;&gt;happen is we will reingest needlessly. It&#39;s still a lot\n&gt;&gt;better than reingesting *everything*, which would be the alternative.\n&gt; \n&gt; \n&gt; That is one problem down.\n&gt; \n&gt; I&#39;m going to suggest you take a look at the (still unreleased)\n&gt; DeDuplicator. A prerelease version can be found here:\n&gt; http://vefsofnun.bok.hi.is/deduplicator/. The DeDuplicator was developed\n&gt; by me and has been tested at netarkivet.dk and is nearing a formal\n&gt; release.\n&gt; \n&gt; The idea behind it is simple. Perform a regular snapshot. Once completed\n&gt; create a deduplication index (Lucene is used for this). On subsequent\n&gt; crawls the deduplication index is used to discard duplicate data\n&gt; (duplicate detection is done by comparing the SHA-1 content digests that\n&gt; Heritrix&#39;s FetchHTTP processor creates).\n&gt; \n\nSo, effectively, you are building a second index that you can query \nagainst for the SHA-1.\n\n&gt; The only thing I&#39;d be worried about is how well the DeDuplicator scales.\n&gt; Because duplicate detection of HTML documents is so unreliable it is\n&gt; typically used only against non-text documents (using mime types for\n&gt; discrimination). In that configuration I&#39;ve used (with only minimal\n&gt; performance hit) where I&#39;ve index ~7 million documents and used it to\n&gt; filter duplicates in a crawl of 25 million documents where ~7 million\n&gt; where non-text documents. I.e. the lookup was done on about one in three\n&gt; documents. As the index gets larger the performance will get worse. In\n&gt; particular the Lucene index and the BdbFrontier will begin to contend for\n&gt; I/O. Keeping the Bdb scratch space and the Lucene dedup index on separate\n&gt; disks would undoubtedly be wise for large crawls.\n&gt; \n\nThe only kinds of documents we are interested in are text, Word, PDF, \nand Excel at the moment.  But let me try to understand - it sounds like \nDeDuplicator doesn&#39;t actually do anything in the way of a &quot;smart&quot; \ncomparison - it&#39;s just behaving as a second index?\n\n&gt; \n&gt;&gt;&gt;&gt;These conditions would have to be signaled in some way to\n&gt;&gt;\n&gt;&gt;all Writers.\n&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;URIs always pass through all processor unless their processing is\n&gt;&gt;&gt;interrupted so once the determination has been made it is\n&gt;&gt;\n&gt;&gt;simply a matter\n&gt;&gt;\n&gt;&gt;&gt;of recording the meta-data in the CrawlURI and having the\n&gt;&gt;\n&gt;&gt;writers pick up\n&gt;&gt;\n&gt;&gt;&gt;on that. You&#39;d clearly need your own writers but that was a\n&gt;&gt;\n&gt;&gt;given in any\n&gt;&gt;\n&gt;&gt;&gt;case.\n&gt;&gt;\n&gt;&gt;Yes, we have our own writer. What I didn&#39;t know (or didn&#39;t think of)\n&gt;&gt;was that the writer would be called for a 404 error. If that&#39;s the\n&gt;&gt;case, the delete path issue is &quot;solved&quot;.\n&gt; \n&gt; \n&gt; By filtering repeat crawls with the DeDuplicator only changed URIs would\n&gt; reach your writers. A 404 will have a different content digest to the\n&gt; original document and will be signaled as a change.\n&gt; \n\nYes, I understand.\n\n&gt; \n&gt;&gt;&gt;&gt;Alternatively, since backwards compatibility might be a\n&gt;&gt;\n&gt;&gt;problem, it&#39;s\n&gt;&gt;\n&gt;&gt;&gt;&gt;potentially possible that a new kind of Writer-like module\n&gt;&gt;\n&gt;&gt;would need\n&gt;&gt;\n&gt;&gt;&gt;&gt;to be created. I was going to look at the adaptive frontier\n&gt;&gt;\n&gt;&gt;to see how\n&gt;&gt;\n&gt;&gt;&gt;&gt;exactly it handled changes, if indeed it does anything\n&gt;&gt;\n&gt;&gt;special at all.\n&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;It issues URIs, the URIs pass through the processing chain\n&gt;&gt;\n&gt;&gt;where stuff\n&gt;&gt;\n&gt;&gt;&gt;happens and then they return to the frontier with updated\n&gt;&gt;\n&gt;&gt;state and are\n&gt;&gt;\n&gt;&gt;&gt;reinserted. The ARFrontier doesn&#39;t worry to much about\n&gt;&gt;\n&gt;&gt;&#39;change&#39; as such.\n&gt;&gt;\n&gt;&gt;&gt;It relies on the fact that change can only occur when a URI is being\n&gt;&gt;&gt;processed. There are no external &#39;events&#39; that can trigger a change.\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;I understand that the change detecting comes only as a result of\n&gt;&gt;refetching a url. What I don&#39;t get is how I could detect a change\n&gt;&gt;without saving some indication somewhere of what the data looked like\n&gt;&gt;last time through.\n&gt; \n&gt; \n&gt; What the AR stuff did was that the SHA-1 fingerprint of a URI was stored\n&gt; in the CrawlURI during fetching. This information survives reentry into\n&gt; the ARFrontier and is still intact when the CrawlURI is reissues\n&gt; (recrawled) and can then be compared against the new fingerprint by the\n&gt; ChangeEvaluator.\n&gt; \n&gt; The DeDuplicator stores this information in a separate index that is\n&gt; created &#39;offline&#39; (between snapshot crawls) and is looked up as needed.\n&gt; \n\nIn our case it&#39;s perfectly acceptable to have the writer make the \ndecision as to whether the document has &quot;changed&quot;.  Is this possible? \nDoes the old SHA-1 signature and the new one both appear available to \nthe writer at that time?\n\nIt sounds to me like you&#39;d be using the DeDuplicator to prefilter \npotential duplicates, which could save time spent extracting urls and \nqueueing them and stuff, but may not be that significant if there were \nfew changes.\n\n&gt; \n&gt;&gt;One thing that would help in this regard is simply keeping a checksum\n&gt;&gt;around of the contents of the url as it was fetched last time\n&gt;&gt;round, and\n&gt;&gt;the checksum of the data for the current fetch. Then, if they differ,\n&gt;&gt;we&#39;d presume that the data had changed.\n&gt; \n&gt; \n&gt; Which is exactly what existing strategies do.\n&gt; \n&gt; \n&gt; \n&gt;&gt;I would definitely plan on doing this as an entirely new\n&gt;&gt;frontier, based\n&gt;&gt;on both the Bdb guy and the adaptive guy.\n&gt; \n&gt; \n&gt; A large scale incremental frontier would definitely be interesting, but\n&gt; would also be hard to achieve for reasons cited above.\n&gt; \n&gt; If you decide to go this route I wish you all the luck in the world but I\n&gt; do note the following:\n&gt; \n&gt; 1. If you do not use some type of adaptive strategy to optimize your\n&gt; crawling you are no better off then doing repeated snapshot crawls with\n&gt; the DeDuplicator\n\nIf the adaptive frontier has the ability to &quot;schedule&quot; recrawls of \nspecific URI&#39;s then I think it&#39;s still a win, simply because you would \nnot need to recrawl URI&#39;s very often that did not change frequently. \nDoes it have a notion of scheduling, or is it built simply like a queue?\n\n&gt; \n&gt; 2. If you do use some adaptive strategies the complexity of the frontier\n&gt; will go up making it harder to scale it to multimillion document crawls.\n&gt; \n\nWhere do you see the scaling issues come in?  Just due to random access \nvs. sequential?\n\n&gt; I&#39;m not trying to discourage you, just trying to point out the main\n&gt; problems you&#39;ll face. I&#39;ve been down this road before and I found my\n&gt; solution outside the Frontier (the DeDuplicator) but my goal was subtly\n&gt; different (reduce amount of duplicate data stored in an archive).\n&gt; \n\n&gt; \n&gt;&gt;I hear you. I will look further at both frontiers. In the meantime,\n&gt;&gt;have you considered how you might have proceeded if you\n&gt;&gt;needed to make\n&gt;&gt;the adaptive frontier store its queue on disk? That&#39;s\n&gt;&gt;basically what I\n&gt;&gt;am trying to build.\n&gt; \n&gt; \n&gt; I used BDB. The way I used it could be optimized, but I&#39;m unaware of a\n&gt; better choice then BDB for storing data on disk.\n&gt; \n&gt; If you could make the ARFrontier effectively scale to multi-million\n&gt; document crawls I believe you&#39;d have what you need. I also believe that\n&gt; that is no easy task.\n\n\nDo you see any reason that I shouldn&#39;t work on ARFrontier in-place \n(rather than creating a whole new frontier)?\n\n&gt; \n&gt; - Kris\n&gt; \n\nThanks very much for your advice.  I&#39;ll ponder this and see if I can \nconclude what the right path is.  Right now, I&#39;m looking forward to the \nchallenge - it sounds like fun - and it sounds like all the right moving \nparts are there.  Would you mind if I contact you every now and then as \nthings are proceeding?\n\nKarl\n\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}