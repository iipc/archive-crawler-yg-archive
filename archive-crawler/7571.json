{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":516443375,"authorName":"ngzikai92","from":"&quot;ngzikai92&quot; &lt;ngzikai92@...&gt;","profile":"ngzikai92","replyTo":"LIST","senderId":"c9_v7AH4rVEY6gV2rHA_UPNP7AhsOl9O0ZCXVH8SAYLJiW2Yu9kr1gSUajqu6K4o7BKrrxut9TG8B_VZu7sCvYkCO5nv_kug","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Crawling Layer by Layer","postDate":"1327561037","msgId":7571,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpmcXRnZCtzc2syQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRGMUEzNEZCLjQwNjAyMDBAYXJjaGl2ZS5vcmc+"},"prevInTopic":7558,"nextInTopic":7572,"prevInTime":7570,"nextInTime":7572,"topicId":7518,"numMessagesInTopic":6,"msgSnippet":"Hi, First of all, thanks alot for the replies. In order to facilitate the crawling of layer by layer, I have been playing with the hops filter for awhile now. ","rawEmail":"Return-Path: &lt;ngzikai92@...&gt;\r\nX-Sender: ngzikai92@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 3336 invoked from network); 26 Jan 2012 06:57:21 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m11.grp.sp2.yahoo.com with QMQP; 26 Jan 2012 06:57:21 -0000\r\nX-Received: from unknown (HELO ng16-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.138)\n  by mta6.grp.sp2.yahoo.com with SMTP; 26 Jan 2012 06:57:20 -0000\r\nX-Received: from [98.139.164.120] by ng16.bullet.mail.bf1.yahoo.com with NNFMP; 26 Jan 2012 06:57:20 -0000\r\nX-Received: from [69.147.65.150] by tg1.bullet.mail.bf1.yahoo.com with NNFMP; 26 Jan 2012 06:57:20 -0000\r\nX-Received: from [98.137.34.184] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 26 Jan 2012 06:57:20 -0000\r\nDate: Thu, 26 Jan 2012 06:57:17 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jfqtgd+ssk2@...&gt;\r\nIn-Reply-To: &lt;4F1A34FB.4060200@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;ngzikai92&quot; &lt;ngzikai92@...&gt;\r\nSubject: Re: Crawling Layer by Layer\r\nX-Yahoo-Group-Post: member; u=516443375; y=doKS_oT_u2rVaUzh8oTYWc2P82NHwYOYBQ9Jn3MANws_Eqkc\r\nX-Yahoo-Profile: ngzikai92\r\n\r\nHi,\n\nFirst of all, thanks alot for the replies. \n\nIn order to facilitate th=\r\ne crawling of layer by layer, I have been playing with the hops filter for =\r\nawhile now. \n\nBecause I wanted to crawl layer by layer, I have been configu=\r\nring my hop filter with a value of &#39;1&#39;. My idea is to start with a seed url=\r\n, and crawl it with a hop filter of 1 (meaning the uris will be 1 layer dow=\r\nn from the seed), extract the results of the crawl.log, and start a new cra=\r\nwl with those links I extracted from the first crawl as the seeds. \n\nI have=\r\n been using the TooManyHopsDecideRule. However, I something  interesting ha=\r\nppens when I tried it out. When I limit the hops to 1, my crawl.log has a l=\r\not of links with a status code of &#39;-63&#39; (which means a prerequisite could n=\r\not be resolved) even though they are 1 hop away from the link. I retried cr=\r\nawling the same seed, this time with a hop limit of 2 and interestingly, th=\r\ne uris which I was unable to crawl previously could be crawled now. This le=\r\nad me to believe (correct me if I am wrong) that DNS requests are also cons=\r\nidered a &#39;hop&#39;. My question is that is are there any configurations (or per=\r\nhaps some other decide rule?) that I can use so that such uris could also b=\r\ne crawled?\n\nOnce again, thanks for the help give.\n\nRegards,\nZi Kai\n--- In a=\r\nrchive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; The one=\r\n thing I would add is that the &#39;layers&#39; as encountered by the \n&gt; crawler ma=\r\ny not be the ones you are most interested in, because there \n&gt; are many lin=\r\nk-hop paths from your seed to each URI, and which path is \n&gt; followed first=\r\n by the crawler is affected by lots of factors.\n&gt; \n&gt; For example, you might=\r\n reach one URI via an &#39;LLLL&#39; path -- four \n&gt; navigational outinks in a row =\r\n-- as reported in the crawl.log.\n&gt; \n&gt; But, that doesn&#39;t mean there isn&#39;t al=\r\nso a shorter &#39;LL&#39; path that *could* \n&gt; have been followed. Only that for th=\r\ne crawler, with all of its various \n&gt; ordering tendencies and the delays it=\r\n encountered, happened to discover \n&gt; the &#39;LLLL&#39; path first. (If the first =\r\n&#39;L&#39; of that short &#39;LL&#39; path was on \n&gt; a slow host, or on a host that alread=\r\ny had hours or days worth of URIs \n&gt; queued up, then by the time it is craw=\r\nled, the other &#39;LLLL&#39; path has \n&gt; already finished. Thus the &#39;LL&#39; discovery=\r\n path is rejected as providing \n&gt; a non-unique URI, and never enters the cr=\r\nawler queues/crawl.log.)\n&gt; \n&gt; If you want a &#39;web graph&#39; of all paths (or al=\r\nl shortest paths) between \n&gt; URIs, that&#39;s usually calculated via a big post=\r\n-crawl analysis.\n&gt; \n&gt; - Gordon\n&gt; \n&gt; On 1/20/12 12:04 PM, Noah Levitt wrote:=\r\n\n&gt; &gt; Hello Ng Zi Kai,\n&gt; &gt;\n&gt; &gt; You can determine the number of hops from see=\r\nd by looking at the\n&gt; &gt; path-from-seed, which is the 5th column in crawl.lo=\r\ng. The number of\n&gt; &gt; letters in that field is the number of hops from seed =\r\n(unless it&#39;s\n&gt; &gt; more than 50 hops in which case it will be abbreviated).\n&gt;=\r\n &gt;\n&gt; &gt; It is possible to write separate crawl logs for the different number=\r\ns\n&gt; &gt; of hops, but it&#39;s not simple to configure. My suggestion would be to\n=\r\n&gt; &gt; split the crawl log up in a postprocessing step, by examining the\n&gt; &gt; p=\r\nath-from-seed for each crawled url.\n&gt; &gt;\n&gt; &gt; Noah\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; On 2012-01-12 =\r\n19:09 , ngzikai92 wrote:\n&gt; &gt;&gt; Hi,\n&gt; &gt;&gt;\n&gt; &gt;&gt; This might be a little confusin=\r\ng, and I am just starting out.\n&gt; &gt;&gt;\n&gt; &gt;&gt; If we think about Heritrix, we can=\r\n see it like a &#39;tree structure&#39;\n&gt; &gt;&gt; where the seed url is at the top layer=\r\n, the additional urls that\n&gt; &gt;&gt; are found from the root at the second layer=\r\n, and the urls that are\n&gt; &gt;&gt; found from the the additional urls at the thir=\r\nd layer, and so on.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Currently, based on what I can get from Herit=\r\nrix&#39;s crawl.log, I am\n&gt; &gt;&gt; only able to see the list of urls Heritrix found=\r\n/visits as a list,\n&gt; &gt;&gt; but I am not able to determine the layers of the &#39;t=\r\nree structure&#39;\n&gt; &gt;&gt; from there.\n&gt; &gt;&gt;\n&gt; &gt;&gt; I would like to know, if it is po=\r\nssible to customise Heritrix in\n&gt; &gt;&gt; such a way that I can crawl layer by l=\r\nayer and separating the urls\n&gt; &gt;&gt; obtained from each layer into separate lo=\r\ng files?\n&gt; &gt;&gt;\n&gt; &gt;&gt; Much thanks and regards, Ng Zi Kai\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; -=\r\n-----------------------------------\n&gt; &gt;&gt;\n&gt; &gt;&gt; Yahoo! Groups Links\n&gt; &gt;&gt;\n&gt; &gt;&gt;=\r\n\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ------------------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Group=\r\ns Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}