{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":360569707,"authorName":"happyxinglele","from":"&quot;happyxinglele&quot; &lt;happyxinglele@...&gt;","profile":"happyxinglele","replyTo":"LIST","senderId":"ajU-UlDXgAS1Aaszz8-wVU6qvqPaWZ2s6-Q-lSiGsO3g9XJhE3cNiI80UJfBjVQNjauKzIKW-_yt-H6rhB5bQfDXo6tCJfmVHoyLWOo9Se1LlTE","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to let Heritrix do not crawl robots.txt and DNS","postDate":"1223518297","msgId":5506,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdjanA4cCtpb2lmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGFmNDQzYTk3MDgxMDA4MDg1NXEyYjU4NWE5OGwxZmI2N2E5YzY0MTUzZTNkQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5505,"nextInTopic":5507,"prevInTime":5505,"nextInTime":5507,"topicId":5501,"numMessagesInTopic":7,"msgSnippet":"As you said, I realize robots.txt is a very importent file for limit illegal crawler. I ll start to learn it more. I prefer to let it remain. Thanks a lot. ...","rawEmail":"Return-Path: &lt;happyxinglele@...&gt;\r\nX-Sender: happyxinglele@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 95453 invoked from network); 9 Oct 2008 02:11:40 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m42.grp.scd.yahoo.com with QMQP; 9 Oct 2008 02:11:40 -0000\r\nX-Received: from unknown (HELO n32a.bullet.sp1.yahoo.com) (209.131.38.211)\n  by mta17.grp.scd.yahoo.com with SMTP; 9 Oct 2008 02:11:40 -0000\r\nX-Received: from [69.147.65.172] by n32.bullet.sp1.yahoo.com with NNFMP; 09 Oct 2008 02:11:40 -0000\r\nX-Received: from [209.73.164.86] by t14.bullet.mail.sp1.yahoo.com with NNFMP; 09 Oct 2008 02:11:40 -0000\r\nX-Received: from [69.147.65.174] by t8.bullet.scd.yahoo.com with NNFMP; 09 Oct 2008 02:11:40 -0000\r\nX-Received: from [66.218.67.197] by t12.bullet.mail.sp1.yahoo.com with NNFMP; 09 Oct 2008 02:11:40 -0000\r\nDate: Thu, 09 Oct 2008 02:11:37 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gcjp8p+ioif@...&gt;\r\nIn-Reply-To: &lt;af443a970810080855q2b585a98l1fb67a9c64153e3d@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;happyxinglele&quot; &lt;happyxinglele@...&gt;\r\nSubject: Re: How to let Heritrix do not crawl robots.txt and DNS\r\nX-Yahoo-Group-Post: member; u=360569707; y=JayF7oML6Kg4vy0RQL13FjC1Fsb2ayG_ju4Bt9amvKfEPf8yqa8o4A\r\nX-Yahoo-Profile: happyxinglele\r\n\r\nAs you said, I realize robots.txt is a very importent file for limit \nilleg=\r\nal crawler. I&#39;ll start to learn it more. I prefer to let it \nremain. Thanks=\r\n a lot. \n\n--- In archive-crawler@yahoogroups.com, &quot;Bart Kiers&quot; &lt;bkiers@...&gt;=\r\n \nwrote:\n&gt;\n&gt; I also don&#39;t know the reason for not downloading the robots.tx=\r\nt, \nbut if it&#39;s\n&gt; because of not wanting to follow the &quot;advice&quot; in them, th=\r\nen there&#39;s \na simple\n&gt; \n&gt; option in Heritrix to ignore robots.txt (there ma=\r\ny be good reasons \nto do\n&gt; so).\n&gt; So, there&#39;s no need to &quot;not download&quot; rob=\r\nots.txt at all for that.\n&gt; \n&gt; But, what the OP is really after can be anyon=\r\ne&#39;s best guess. In \nhis/her last\n&gt; \n&gt; reply s/he mentioned wanting to &quot;stri=\r\np the robots.txt&quot;... I have no \nidea\n&gt; what that\n&gt; means.\n&gt; \n&gt; Regards,\n&gt; \n=\r\n&gt; Bart Kiers\n&gt; \n&gt; \n&gt; On Wed, Oct 8, 2008 at 5:47 PM, John Lekashman &lt;lekash=\r\n@...&gt; wrote:\n&gt; \n&gt; &gt;   Hi,\n&gt; &gt; Could we please not publish this?\n&gt; &gt;\n&gt; &gt; I d=\r\non&#39;t know if the original writer is real, or not.\n&gt; &gt;\n&gt; &gt; But it is suspici=\r\nous.\n&gt; &gt;\n&gt; &gt; Eliminating one text file from a crawl to save time is \nquesti=\r\nonable.\n&gt; &gt;\n&gt; &gt; And even if they really need it, posting a cookbook how to,=\r\n can \nreach\n&gt; &gt; out to all sorts of places we don&#39;t want.\n&gt; &gt;\n&gt; &gt; Its not s=\r\nomething we need, heritrix becoming a script kiddie tool \nto\n&gt; &gt; get around=\r\n robots.txt, and having the name on lists of bad robots.\n&gt; &gt;\n&gt; &gt; Yes, I kno=\r\nw, robots.txt is just advisory, but so are locks on the \nfront\n&gt; &gt; door of =\r\na house.\n&gt; &gt; Any idiot with 5 minutes of will power can break them, but\n&gt; &gt;=\r\n it is amazing how effective advisory locks are.\n&gt; &gt;\n&gt; &gt; John\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; h=\r\nappyxinglele wrote:\n&gt; &gt;\n&gt; &gt; &gt; Oh, I know.\n&gt; &gt; &gt; But could anyone tell me ho=\r\nw to strip the robots.txt\n&gt; &gt; &gt;\n&gt; &gt; &gt; -Thanks\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In archive-cr=\r\nawler@yahoogroups.com&lt;archive-crawler%\n40yahoogroups.com&gt;\n&gt; &gt; &gt; &lt;mailto:arc=\r\nhive-crawler%40yahoogroups.com&lt;archive-crawler%\n2540yahoogroups.com&gt;&gt;,\n&gt; &gt; =\r\n&quot;Jean-No=EBl Rivasseau&quot;\n&gt; &gt; &gt; &lt;elvanor@&gt; wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I dont know=\r\n for the robots.txt part, but you will still be \nforced to\n&gt; &gt; &gt; &gt; &quot;crawl&quot; =\r\n(eg, contact) the DNS to obtain the IP address of the\n&gt; &gt; &gt; server.\n&gt; &gt; &gt; &gt;=\r\n This is mandatory.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; On Wed, Oct 8, 2008 at 10:29 AM=\r\n, happyxinglele\n&gt; &gt; &gt; &gt; &lt;happyxinglele@&gt; wrote:\n&gt; &gt; &gt; &gt; &gt; I only crawl test=\r\n URLs of myself. And Heritrix need to crawl\n&gt; &gt; &gt; robots and\n&gt; &gt; &gt; &gt; &gt; DNS =\r\nfirstly, which is cost lots of time. I donot need the\n&gt; &gt; &gt; Heritrix to\n&gt; &gt;=\r\n &gt; &gt; &gt; crawl the robots file and the DNS.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt; Could you tel=\r\nl me how can I let it work?\n&gt; &gt; &gt; &gt; &gt; p.s. I use Heritrix-2.0.0\n&gt; &gt; &gt; &gt; &gt;\n&gt;=\r\n &gt; &gt; &gt; &gt; Thanks a lot!\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; &gt;  \n&gt; =\r\n&gt;\n&gt;\n\n\n\n"}}