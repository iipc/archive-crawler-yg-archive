{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256346715,"authorName":"Adam Fisk","from":"&quot;Adam Fisk&quot; &lt;adamfisk@...&gt;","profile":"afisk3","replyTo":"LIST","senderId":"fNffYFDeMW-w3HpIGa6cBLfpV2QINVTliM82sjpWpQXXNWKiDY6U6JeAjPoYqt94ALCxPALOsu5JhhEKg-LDP8yFX2IZCqU","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: OutOfMemoryError on small crawl","postDate":"1141141563","msgId":2718,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR1MXI3cis0MWJkQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGR0bGY1citkaDhpQGVHcm91cHMuY29tPg=="},"prevInTopic":2714,"nextInTopic":2719,"prevInTime":2717,"nextInTime":2719,"topicId":2709,"numMessagesInTopic":7,"msgSnippet":"This is running well now, just for anyone else searching through the forum.  I first increased the mx to 256MB and ran out of memory again. I then bumped it up","rawEmail":"Return-Path: &lt;adamfisk@...&gt;\r\nX-Sender: adamfisk@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 10195 invoked from network); 28 Feb 2006 15:46:11 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m34.grp.scd.yahoo.com with QMQP; 28 Feb 2006 15:46:11 -0000\r\nReceived: from unknown (HELO n7a.bullet.scd.yahoo.com) (66.94.237.41)\n  by mta5.grp.scd.yahoo.com with SMTP; 28 Feb 2006 15:46:11 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.6] by n7.bullet.scd.yahoo.com with NNFMP; 28 Feb 2006 15:46:04 -0000\r\nReceived: from [66.218.66.83] by t6.bullet.scd.yahoo.com with NNFMP; 28 Feb 2006 15:46:04 -0000\r\nDate: Tue, 28 Feb 2006 15:46:03 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;du1r7r+41bd@...&gt;\r\nIn-Reply-To: &lt;dtlf5r+dh8i@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Adam Fisk&quot; &lt;adamfisk@...&gt;\r\nSubject: Re: OutOfMemoryError on small crawl\r\nX-Yahoo-Group-Post: member; u=256346715; y=gZNhIwnucS4IDU2s3YZ4Qy0cQSSEYZY-uzbnrsKjml6A\r\nX-Yahoo-Profile: afisk3\r\n\r\nThis is running well now, just for anyone else searching through the \nforum=\r\n.  I first increased the mx to 256MB and ran out of memory again. \n I then =\r\nbumped it up to 600MB, and the crawl has been running for \nabout 3 days wit=\r\nh about 800,000 html pages downloaded.\n\nDoes my volume sound pretty typical=\r\n to other folks out there?  I&#39;m \nonly crawling 20 sites, but they include s=\r\nites like nytimes.com.  \nThat&#39;s about 40,000 html pages per site, and the c=\r\nrawl&#39;s still not \ndone.  Is that a pretty typical size for larger sites out=\r\n there?\n\nThanks very much.\n\n-Adam\n\n\n--- In archive-crawler@yahoogroups.com,=\r\n &quot;Adam Fisk&quot; &lt;adamfisk@...&gt; \nwrote:\n&gt;\n&gt; Thanks Gordon-\n&gt; \n&gt; Yes -- our heap=\r\n size is definitely capped at the Java default.  I\n&gt; can&#39;t go much higher t=\r\nhan 256 on this machine, but I&#39;ll see what I \nget\n&gt; with that.  I just didn=\r\n&#39;t want to mask another problem by going \nahead\n&gt; and cranking the memory, =\r\nespecially for such a small crawl.  Those \n20\n&gt; were bringing in upwards of=\r\n 40,000 raw text/html pages, though, so I\n&gt; guess memory issues are to be e=\r\nxpected.\n&gt; \n&gt; Where does most of the memory go?  I assume the ToeThreads \na=\r\nccumulate\n&gt; a lot of data over time?  We&#39;re going to be doing much larger c=\r\nrawls\n&gt; soon (hundreds of sites), and we&#39;d prefer not to dedicate a full\n&gt; =\r\nmachine to this task, but it looks like we might have to.\n&gt; \n&gt; Thanks again=\r\n.\n&gt; \n&gt; -Adam\n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;g=\r\nojomo@&gt; wrote:\n&gt; &gt;\n&gt; &gt; It looks like your heap is capped at a maximum size =\r\nof 64MB -- \n&gt; &gt; that&#39;s the Java default in the absence of any -Xmx setting =\r\nin Java \n&gt; &gt; 1.4 and previous -- so your 2GB of RAM isn&#39;t doing the crawler=\r\n any \n&gt; &gt; good.\n&gt; &gt; \n&gt; &gt; Use of -Xmx is definitely indicated; if the machin=\r\ne is dedicated \n&gt; &gt; to crawling, and you want the crawler to be able to use=\r\n all the \n&gt; &gt; RAM, -Xmx1500m would be justified.\n&gt; &gt; \n&gt; &gt; (It&#39;s probably po=\r\nssible to crawl 20 hosts in 64MB, if there isn&#39;t \n&gt; &gt; an explosion of subdo=\r\nmains, and you use only a small number of \n&gt; &gt; threads -- but I doubt that&#39;=\r\ns a real constraint you want to try to \n&gt; &gt; live within.)\n&gt; &gt; \n&gt; &gt; The &#39;max=\r\n-depth&#39; and &#39;average-depth&#39; readings on that status line \n&gt; &gt; (and in the c=\r\nrawler console) refer to the size of queues: \n&gt; &gt; &#39;max-depth&#39; is the longes=\r\nt queue in the frontier, &#39;average-depth&#39; \n&gt; &gt; is the average of all queue s=\r\nizes.\n&gt; &gt; \n&gt; &gt; - Gordon @ IA\n&gt;\n\n\n\n\n\n"}}