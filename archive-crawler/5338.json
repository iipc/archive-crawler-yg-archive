{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"uCe-RymEWXuEpLiXPDDa1znvGuTe77C2cltJpXj5BogKwg049a6O5EUz6BLZYd1Q7jXTKcglUoeheZYxLlv7AnIqfrFSKDI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Need help--How to get rid of Out Of Memory","postDate":"1214680741","msgId":5338,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NjY4RUE1LjMwMjA5MDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIxMTg3Ny4yMjg3MC5xbUB3ZWI2NTUxMy5tYWlsLmFjNC55YWhvby5jb20+","referencesHeader":"PDIxMTg3Ny4yMjg3MC5xbUB3ZWI2NTUxMy5tYWlsLmFjNC55YWhvby5jb20+"},"prevInTopic":5337,"nextInTopic":0,"prevInTime":5337,"nextInTime":5339,"topicId":5336,"numMessagesInTopic":3,"msgSnippet":"I see you have switched away from the default BdbUriUniqFilter to BloomUriUUniqFilter. BloomUriUniqFilter alone uses 512MB of space. The default allocation for","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 3511 invoked from network); 28 Jun 2008 19:19:08 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m42.grp.scd.yahoo.com with QMQP; 28 Jun 2008 19:19:08 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta16.grp.scd.yahoo.com with SMTP; 28 Jun 2008 19:19:08 -0000\r\nX-Received: (qmail 85402 invoked from network); 28 Jun 2008 19:19:00 -0000\r\nX-Received: from unknown (HELO ?10.0.10.7?) (unknown)\n  by unknown with SMTP; 28 Jun 2008 19:19:00 -0000\r\nX-pair-Authenticated: 70.137.132.156\r\nMessage-ID: &lt;48668EA5.3020900@...&gt;\r\nDate: Sat, 28 Jun 2008 12:19:01 -0700\r\nUser-Agent: Thunderbird 2.0.0.14 (Windows/20080421)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;211877.22870.qm@...&gt;\r\nIn-Reply-To: &lt;211877.22870.qm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Need help--How to get rid of Out Of Memory\r\nX-Yahoo-Group-Post: member; u=137285340; y=o8IzHuW7_KZ_IDJZ_gYqaaOfq0LlpTIVWdx9v21denM8\r\nX-Yahoo-Profile: gojomo\r\n\r\nI see you have switched away from the default BdbUriUniqFilter to \nBloomUriUUniqFilter.\n\nBloomUriUniqFilter alone uses 512MB of space. The default allocation for \nBerkeleyDB&#39;s working-space (cache) is 60% of heap -- 780MB of your \n1300MB heap.\n\n512MB+780MB = 1292MB, not leaving a lot of other heap space for anything \nelse!\n\nSince you&#39;re not using BdbUriUniqFilter, and you have a sizable heap, \nyou can make do with less than 60% heap allocated to BDB cache. 30% \nmight be a reasonable number.\n\nI suspect this is your problem. If not, supply other details about which \nversion of Heritrix you are using, what OS/JVM, what non-default \nconfiguration, etc.\n\n- Gordon @ IA\n\nhijbul alam wrote:\n&gt; \n&gt; \n&gt; Hi\n&gt; \n&gt;  \n&gt; \n&gt; I am getting OOME error continously. At first i tried with Broadscope, \n&gt; then Surtprefixscope.\n&gt; \n&gt; Now I am trying with deciding in scope. I tried about 10 times tuning \n&gt; different parameter. But all time i failed to run the crawler more \n&gt; than to 2 or 3 hour where as I am planning to do large crawl(100 million \n&gt; pages).\n&gt; \n&gt;  \n&gt; \n&gt; My System is Intel Core 2 Quad 2.4 Ghz. 2.00 GB of RAM. and i found \n&gt; that more than 1 GB physical  is not used or  free during crawling. i \n&gt; allocate 1300m java heap allocation. i use\n&gt; \n&gt;  \n&gt; \n&gt; DecidingScope with the following set of DecideRules:\n&gt; \n&gt; 1. RejectDecideRule\n&gt; \n&gt; 2. SurtPrefixedDecideRule--------------------- 40000 URI as seed and \n&gt; seeds as-surt-prefixes\n&gt; \n&gt; 3. TooManyHopsDecideRule----------------10\n&gt; \n&gt; 4. PathologicalPathDecideRule\n&gt; \n&gt; 5. TooManyPathSegmentsDecideRule-----10\n&gt; \n&gt; 6. NotMatchesFilePatternDecideRule---          regexp: .*(/|&#92;.html)$\n&gt; \n&gt; 7. PrerequisiteAcceptDecideRule\n&gt; \n&gt;  \n&gt; \n&gt; and BloomUriUniqfilter(default settings) and 100 toethread\n&gt; \n&gt;  \n&gt; \n&gt; with above setting it crawls 30-40 URI/sec and I would like to  continue \n&gt; crawl to about 30-40 URI/sec. I have  2MB Bandwith of interenet. but the \n&gt; crawler runs with avg 700 KB. IF i use 10-20 seeds it crawls 3 URI/Sec.\n&gt; \n&gt;  \n&gt; \n&gt; According to this website \n&gt; http://java.sun.com/javase/6/webnotes/trouble/TSG-VM/html/gbywc.html#gbyvh there \n&gt; are 3 reseaon for java.lang.OutOfMemoryError: Java heap space.\n&gt; \n&gt;  \n&gt; \n&gt; Though BloomUriUniq filter is like to support 125 miilion URI to check \n&gt; already seen. but my crawler stuck discovering 1038937 URI and \n&gt; downloading 268187 pages after before running 2 hour.\n&gt; \n&gt;  \n&gt; \n&gt; Please tell me the neccessary step to get rid of Out of memory error.\n&gt; \n&gt;  \n&gt; \n&gt; Thanks in Advance\n&gt; \n&gt;  \n&gt; \n&gt; HIJBUL\n&gt; \n&gt;  \n&gt; \n&gt; N.B\n&gt; \n&gt; caused by: java.lang.OutOfMemoryError: Java heap space\n&gt; \n&gt; at com.sleepycat.je.log.LogUtils.readByteArray( LogUtils.java:204 )\n&gt; \n&gt; at com.sleepycat.je.tree.IN.readFromLog( IN.java:2952 )\n&gt; \n&gt; at com.sleepycat.je.log.entry.INLogEntry.readEntry( INLogEntry.java:91 )\n&gt; \n&gt; at com.sleepycat.je.log.LogManager.getLogEntryFromLogSource( \n&gt; LogManager.java:678 )\n&gt; \n&gt; at com.sleepycat.je.log.LogManager.getLogEntry( LogManager.java:595 )\n&gt; \n&gt; at com.sleepycat.je.tree.IN.fetchTarget( IN.java:958 )\n&gt; \n&gt; at com.sleepycat.je.tree.Tree.searchSubTreeInternal( Tree.java:1917 )\n&gt; \n&gt; at com.sleepycat.je.tree.Tree.searchSubTree( Tree.java:1754 )\n&gt; \n&gt; at com.sleepycat.je.tree.Tree.search( Tree.java:1619 )\n&gt; \n&gt; at com.sleepycat.je.dbi.CursorImpl.searchAndPosition( \n&gt; CursorImpl.java:1912 )\n&gt; \n&gt; at com.sleepycat.je.Cursor.searchInternal( Cursor.java:1188 )\n&gt; \n&gt; at com.sleepycat.je.Cursor.searchAllowPhantoms( Cursor.java:1158 )\n&gt; \n&gt; at com.sleepycat.je.Cursor.search( Cursor.java:1024 )\n&gt; \n&gt; at com.sleepycat.je.Cursor.getSearchKey( Cursor.java:566 )\n&gt; \n&gt; at com.sleepycat.util.keyrange.RangeCursor.doGetSearchKey( \n&gt; RangeCursor.java:964 )\n&gt; \n&gt; at com.sleepycat.util.keyrange.RangeCursor.getSearchKey( \n&gt; RangeCursor.java:591 )\n&gt; \n&gt; at com.sleepycat.collections.DataCursor.doGetSearchKey( \n&gt; DataCursor.java:577 )\n&gt; \n&gt; at com.sleepycat.collections.DataCursor.initForPut( DataCursor.java:818 )\n&gt; \n&gt; at com.sleepycat.collections.DataCursor.put( DataCursor.java:758 )\n&gt; \n&gt; at com.sleepycat.collections.StoredContainer.put( \n&gt; StoredContainer.java:302 )\n&gt; \n&gt; at com.sleepycat.collections.StoredMap.put( StoredMap.java:248 )\n&gt; \n&gt; at org.archive.util.CachedBdbMap.expungeStaleEntry( CachedBdbMap.java:562 )\n&gt; \n&gt; at org.archive.util.CachedBdbMap.expungeStaleEntries( \n&gt; CachedBdbMap.java:533 )\n&gt; \n&gt; at org.archive.util.CachedBdbMap.get( CachedBdbMap.java:358 )\n&gt; \n&gt; at org.archive.crawler.datamodel.ServerCache.getHostFor( \n&gt; ServerCache.java:146 )\n&gt; \n&gt; at org.archive.crawler.datamodel.ServerCache.getHostFor( \n&gt; ServerCache.java:175 )\n&gt; \n&gt; at org.archive.crawler.fetcher.FetchHTTP.canFetch( FetchHTTP.java:731 )\n&gt; \n&gt; at org.archive.crawler.fetcher.FetchHTTP.innerProcess( FetchHTTP.java:419 )\n&gt; \n&gt; at org.archive.crawler.framework.Processor.process( Processor.java:112 )\n&gt; \n&gt; at org.archive.crawler.framework.ToeThread.processCrawlUri( \n&gt; ToeThread.java:302 )\n&gt; \n&gt; at org.archive.crawler.framework.ToeThread.run( ToeThread.java:151 )\n&gt; \n&gt; Environment invalid because of previous exception: \n&gt; com.sleepycat.je.RunRecoveryException\n&gt; \n&gt; at com.sleepycat.je.dbi.EnvironmentImpl.checkIfInvalid( \n&gt; EnvironmentImpl.java:976 )\n&gt; \n&gt; at com.sleepycat.je.Database.checkEnv( Database.java:1106 )\n&gt; \n&gt; at com.sleepycat.je.Database.delete( Database.java:404 )\n&gt; \n&gt; at org.archive.crawler.frontier.BdbMultipleWorkQueues.delete( \n&gt; BdbMultipleWorkQueues.java:438 )\n&gt; \n&gt; at org.archive.crawler.frontier.BdbWorkQueue.deleteItem( \n&gt; BdbWorkQueue.java:91 )\n&gt; \n&gt; at org.archive.crawler.frontier.WorkQueue.dequeue( WorkQueue.java:161 )\n&gt; \n&gt; at org.archive.crawler.frontier.WorkQueueFrontier.finished( \n&gt; WorkQueueFrontier.java:883 )\n&gt; \n&gt; at org.archive.crawler.framework.ToeThread.run( ToeThread.java:157 )\n&gt; \n&gt;  \n&gt; \n&gt;  \n&gt; \n&gt;  \n&gt; \n&gt; *Crawler Status: CRAWLING JOBS* | Hold \n&gt; &lt;http://127.0.0.1:8080/console/action.jsp?action=stop&gt;\n&gt; *Memory*\n&gt; 61262 KB used\n&gt; 65088 KB current heap\n&gt; 65088 KB max heap\n&gt; *Jobs*\n&gt; Pausing: /General/\n&gt; 0 pending, 6 completed\n&gt; *Alerts:* 143 (143 new) &lt;http://127.0.0.1:8080/console/alerts.jsp&gt;\n&gt; \n&gt; *Job Status: Pausing *| Resume \n&gt; &lt;http://127.0.0.1:8080/console/action.jsp?action=resume&gt; | Checkpoint \n&gt; &lt;http://127.0.0.1:8080/console/action.jsp?action=checkpoint&gt; | Terminate\n&gt; *Load*\n&gt; 0 active of 0 threads\n&gt; 9,331.4 congestion ratio\n&gt; 27636 deepest queue\n&gt; 22 average depth\n&gt; *Rates*\n&gt; 0 URIs/sec (33.22 avg)\n&gt; 0 KB/sec (671 avg)\n&gt; *Time*\n&gt; 2h35m41s elapsed\n&gt; 10h3m9s remaining (estimated)\n&gt; *Totals*\n&gt; downloaded 268187  \t\t *20*% \t 1038937 queued\n&gt; \n&gt; 1307124 total downloaded and queued\n&gt; 5.2 GB crawled (5.2 GB novel)\n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n"}}