{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":185567631,"authorName":"penguinoamante2","from":"&quot;penguinoamante2&quot; &lt;penguinoamante2@...&gt;","profile":"penguinoamante2","replyTo":"LIST","senderId":"NdEyMfW1x4_1L4mK28EVtM3VMs-Tb7A07ZKIcj_XZJZUjREQ23Ala3-T6v5jcav_LV1aTZWFi3y03rv0Rk-pwRoLDk0PIMAC6mrKbPrzFxQj_R8q","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: submit batch jobs","postDate":"1084461034","msgId":386,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM4MDM1YSsxMGJnYkBlR3JvdXBzLmNvbT4=","inReplyToHeader":"PDQwQTExNjMxLjMwNjA2MDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":381,"nextInTopic":387,"prevInTime":385,"nextInTime":387,"topicId":372,"numMessagesInTopic":10,"msgSnippet":"How is it more manageable crawling through all the customers URIs and\nthen dealing with all their data mixed up in one file?  What programs\nexist for querying","rawEmail":"Return-Path: &lt;penguinoamante2@...&gt;\r\nX-Sender: penguinoamante2@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 34296 invoked from network); 13 May 2004 15:10:46 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m17.grp.scd.yahoo.com with QMQP; 13 May 2004 15:10:46 -0000\r\nReceived: from unknown (HELO n4.grp.scd.yahoo.com) (66.218.66.88)\n  by mta5.grp.scd.yahoo.com with SMTP; 13 May 2004 15:10:46 -0000\r\nReceived: from [66.218.67.162] by n4.grp.scd.yahoo.com with NNFMP; 13 May 2004 15:10:35 -0000\r\nDate: Thu, 13 May 2004 15:10:34 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;c8035a+10bgb@...&gt;\r\nIn-Reply-To: &lt;40A11631.3060608@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Length: 6618\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.88\r\nFrom: &quot;penguinoamante2&quot; &lt;penguinoamante2@...&gt;\r\nSubject: Re: submit batch jobs\r\nX-Yahoo-Group-Post: member; u=185567631\r\nX-Yahoo-Profile: penguinoamante2\r\n\r\nHow is it more manageable crawling through all the customers URIs and\nthen =\r\ndealing with all their data mixed up in one file?  What programs\nexist for =\r\nquerying the ARC file. A common query is show all pages with\ncontact info i=\r\nn the domain of customerX.  \n\nDoesn&#39;t the Way Back Machine use heritrix? Wh=\r\nat tools do they use for\nquerying the ARC?\n\nSo far Heritrix has worked for =\r\nme so I&#39;m confident about the crawling\nside.  I&#39;m not so confident about qu=\r\nerying ARC files.\n\nThanks \n\n--- In archive-crawler@yahoogroups.com, Michael=\r\n Stack &lt;stack@a...&gt; wrote:\n&gt; penguinoamante2 wrote:\n&gt; \n&gt; &gt;Yes you guys are =\r\nright.  When I restart heritrix the pending jobs on\n&gt; &gt;disk get loaded into=\r\n the crawler.\n&gt; &gt;\n&gt; &gt;Thanks for the tips.  \n&gt; &gt;\n&gt; &gt;I should be asking weath=\r\ner this is the feature to have or if there is\n&gt; &gt;a better way to do what I =\r\nwant to do.  \n&gt; &gt;\n&gt; &gt;I want to watch a list of customers sites to make sure=\r\n that they\n&gt; &gt;continue to sell what they say they are currently selling, th=\r\nat their\n&gt; &gt;contact info is in the USA, and that they aren&#39;t doing anything=\r\n\nillegal.\n&gt; &gt;\n&gt; &gt;It seems like launching a batch of jobs one for each custo=\r\nmer would do\n&gt; &gt;the job. However heritrix could just crawl all the customer=\r\n URIs in\n&gt; &gt;one job and then another program could split up the data from t=\r\nhe ARC\n&gt; &gt;file into individual customers.\n&gt; &gt;  \n&gt; &gt;\n&gt; \n&gt; Doing the latter s=\r\nounds more manageable.\n&gt; \n&gt; See http://crawler.archive.org/articles/develop=\r\ner_manual.html#arcreader \n&gt; for a few notes on reading arcs.\n&gt; \n&gt; St.Ack\n&gt; =\r\n\n&gt; P.S. Related, currently there is no means of stopping the crawler from \n=\r\n&gt; the command line.\n&gt; \n&gt; \n&gt; &gt;--- In archive-crawler@yahoogroups.com, &quot;Krist=\r\ninn Sigurdsson&quot; \n&gt; &gt;\n&gt; &gt;&lt;kris@a...&gt; wrote:\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;Michael is correct=\r\n. Jobs are only read from disk during program\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt; &gt;startup. At\n=\r\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;other times in memory chaching is used.\n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt;A=\r\n suitable workaround might by to create a page that accepts (via\n&gt; &gt;&gt;    \n&gt;=\r\n &gt;&gt;\n&gt; &gt;GET) the\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;parameters needed to construct a new job (thi=\r\ns might only be the\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt; &gt;crawl order\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;xml file=B4s=\r\n name) and creates the new job based on it.\n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt;In fact if =\r\nyou go that route you could not write the XML to it&#39;s\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt; &gt;inte=\r\nnded\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;job directory. Instead you would save it somewhere else =\r\nand create\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt; &gt;the new\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;job BASED ON your order. =\r\nThis mirrors how jobs are generally\ncreated in\n&gt; &gt;&gt;Heritrix.\n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;=\r\n&gt;\n&gt; &gt;&gt;I believe that ...webapps/admin/jobs/new.jsp would be a good\n&gt; &gt;&gt;    =\r\n\n&gt; &gt;&gt;\n&gt; &gt;starting point\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;for someone intent on writing this ad=\r\nd on :-)\n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt;- Kris\n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt;-----Ori=\r\nginal Message-----\n&gt; &gt;&gt;From: Michael Stack [mailto:stack@a...] \n&gt; &gt;&gt;Sent: 1=\r\n1. ma=ED 2004 16:57\n&gt; &gt;&gt;To: archive-crawler@yahoogroups.com\n&gt; &gt;&gt;Subject: Re=\r\n: [archive-crawler] Re: submit batch jobs\n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt;penguinoamant=\r\ne2 wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt;First try was not successful.  I create a=\r\n directory called\nbatchjob in\n&gt; &gt;&gt;&gt;the jobs directoy which contains three f=\r\niles: batchjob.job \n&gt; &gt;&gt;&gt;job-batchjob.xml and seeds-batchjob.txt.\n&gt; &gt;&gt;&gt;\n&gt; &gt;=\r\n&gt;&gt;batchjob.job contains\n&gt; &gt;&gt;&gt;20040511155115186\n&gt; &gt;&gt;&gt;batchjob\n&gt; &gt;&gt;&gt;Pending\n&gt;=\r\n &gt;&gt;&gt;false\n&gt; &gt;&gt;&gt;false\n&gt; &gt;&gt;&gt;2\n&gt; &gt;&gt;&gt;/opt/src/heritrix-0.6.0/jobs/batchjob/job-=\r\nbatchjob.xml\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;job-batchjob.xml contains a ton of xml I think I go=\r\nt the important\n&gt; &gt;&gt;&gt;tags such as\n&gt; &gt;&gt;&gt;&lt;name&gt;batchjob&lt;/name&gt;\n&gt; &gt;&gt;&gt;&lt;string n=\r\name=3D&quot;seedsfile&quot;&gt;seeds-batchjob.txt&lt;/string&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;seeds-batchjob has=\r\n the right seeds.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;After doing this I would expect the job to at =\r\nleast show up as\n&gt; &gt;&gt;&gt;      \n&gt; &gt;&gt;&gt;\n&gt; &gt;Pending.  \n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;\n&gt; &gt;=\r\n&gt;&gt;      \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;The code that reads the directory is only run on applica=\r\ntion\nstartup it \n&gt; &gt;&gt;looks like.  Restart.  Does it work?\n&gt; &gt;&gt;\n&gt; &gt;&gt;Here is =\r\nthe pertinent code:\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt;\n&gt;http://crawler.archive.org/x=\r\nref-test/org/archive/crawler/admin/CrawlJobHan=3D\r\nd=3D\n&gt; &gt;l\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;er.html#211\n&gt; &gt;&gt;\n&gt; &gt;&gt;St.Ack\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;    \n&gt; &gt;=\r\n&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;--- In archive-crawler@yahoogroups.com, Mich=\r\nael Stack &lt;stack@a...&gt;\n&gt; &gt;&gt;&gt;wrote:\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;      \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;pen=\r\nguinoamante2 wrote:\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;   \n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;        \n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;Wh=\r\nat are the best practices for submitting a batch of jobs.  I\n&gt; &gt;&gt;&gt;&gt;&gt;     \n&gt;=\r\n &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;          \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;have a\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;      \n&gt; &gt;&gt;&gt;=\r\n\n&gt; &gt;&gt;&gt;&gt;&gt;list of fqdn&#39;s in a database and I want heritrix to consider each\n&gt;=\r\n &gt;&gt;&gt;&gt;&gt;     \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;          \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;one\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;  =\r\n    \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;a seperate job.  The plan so far is to write a script to =\r\npopulate\n&gt; &gt;&gt;&gt;&gt;&gt;     \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;          \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;the\n&gt; &gt;&gt;&gt; \n&gt; &gt;=\r\n&gt;&gt;\n&gt; &gt;&gt;&gt;      \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;jobs directory with the right info.\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;=\r\n&gt;&gt;&gt;&gt;Is this the right way to do it?\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;     \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; =\r\n&gt;&gt;&gt;&gt;&gt;          \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;That sounds right.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;Make sure the=\r\n crawler is the &#39;Crawling state&#39; so that it&#39;ll just\n&gt; &gt;&gt;&gt;&gt;   \n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;=\r\n        \n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;start \n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;      \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;the next j=\r\nob soon as its finished the current job.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;Yours,\n&gt; &gt;&gt;&gt;&gt;St.Ack\n&gt;=\r\n &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;   \n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;        \n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;Thanks,\n&gt; &gt;&gt;&gt;&gt;&gt;Sunny\n&gt; =\r\n&gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;Yahoo! Groups Links\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; =\r\n&gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt;     \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt;&gt; =\r\n         \n&gt; &gt;&gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;Yahoo! Groups Links\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; =\r\n&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;      \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;=\r\n&gt;Yahoo! Groups Sponsor\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;ADVERTISEMENT\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt;    =\r\n\n&gt; &gt;&gt;\n&gt;\n&gt;&lt;http://rd.yahoo.com/SIG=3D129q30o18/M=3D295196.4901138.6071305.30=\r\n01176/D=3Dgrou=3D\r\np=3D\n&gt; &gt;s\n&gt; &gt;  \n&gt; &gt;\n&gt;\n&gt;/S=3D1705004924:HM/EXP=3D1084381409/A=3D2128215/R=\r\n=3D0/SIG=3D10se96mf6/*http:/compani=3D\r\no=3D\n&gt; &gt;n\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;.yahoo.com&gt; click here\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt;  =\r\n  \n&gt; &gt;&gt;\n&gt;\n&gt;&lt;http://us.adserver.yahoo.com/l?M=3D295196.4901138.6071305.30011=\r\n76/D=3Dgroups/=3D\r\nS=3D\n&gt; &gt;=3D\n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;:HM/A=3D2128215/rand=3D708464738&gt; \n&gt; &gt;&gt;\n&gt; &gt;&gt; \n&gt; =\r\n&gt;&gt;\n&gt; &gt;&gt;  _____  \n&gt; &gt;&gt;\n&gt; &gt;&gt;Yahoo! Groups Links\n&gt; &gt;&gt;\n&gt; &gt;&gt;*\tTo visit your grou=\r\np on the web, go to:\n&gt; &gt;&gt;http://groups.yahoo.com/group/archive-crawler/\n&gt; &gt;=\r\n&gt;  \n&gt; &gt;&gt;*\tTo unsubscribe from this group, send an email to:\n&gt; &gt;&gt;archive-cra=\r\nwler-unsubscribe@yahoogroups.com\n&gt; &gt;&gt;\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt;\n&gt;&lt;mailto:archive-craw=\r\nler-unsubscribe@yahoogroups.com?subject=3DUnsubscribe&gt; \n&gt; &gt;  \n&gt; &gt;\n&gt; &gt;&gt;  \n&gt; =\r\n&gt;&gt;*\tYour use of Yahoo! Groups is subject to the Yahoo! Terms of Service\n&gt; &gt;=\r\n&gt;&lt;http://docs.yahoo.com/info/terms/&gt; .\n&gt; &gt;&gt;    \n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; \n&gt;=\r\n &gt;Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; \n&gt; &gt;\n&gt; &gt;  \n&gt; &gt;\n\n\n"}}