{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"ueLbF12pH0UKKgez3N6kxNlrke25PBx0JDNwhOOgtcjR7iZ8l5liyadrf6odfkEiP1_PepY7HpRHuJlrk08qwQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] RFE: New queue assignment policy (repost)","postDate":"1108416495","msgId":1558,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyMTExN0VGLjgwMUBhcmNoaXZlLm9yZz4=","inReplyToHeader":"PDIwMDUwMjE0MTUzMS4xNTc0Mi5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4=","referencesHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBMDZFQzYwQHNrYXJmdXIuYm9rLmxvY2FsPiA8MjAwNTAyMTQxNTMxLjE1NzQyLmNrLWhlcml0cml4QG5ld3NjbHViLmRlPg=="},"prevInTopic":1557,"nextInTopic":1559,"prevInTime":1557,"nextInTime":1559,"topicId":1545,"numMessagesInTopic":13,"msgSnippet":"... Check out the frontier/thread reports for what it says its doing when crawl is running slow.  Are all threads occupied?  Are they all stuck waiting at a","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 4271 invoked from network); 14 Feb 2005 21:36:59 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m19.grp.scd.yahoo.com with QMQP; 14 Feb 2005 21:36:59 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta5.grp.scd.yahoo.com with SMTP; 14 Feb 2005 21:36:59 -0000\r\nReceived: (qmail 16522 invoked by uid 100); 14 Feb 2005 21:20:42 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 14 Feb 2005 21:20:42 -0000\r\nMessage-ID: &lt;421117EF.801@...&gt;\r\nDate: Mon, 14 Feb 2005 13:28:15 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.3) Gecko/20041007 Debian/1.7.3-5\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;0678DB1968EAC7409CC3D0AB7A11B84A06EC60@...&gt; &lt;200502141531.15742.ck-heritrix@...&gt;\r\nIn-Reply-To: &lt;200502141531.15742.ck-heritrix@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: *\r\nX-Spam-Status: No, hits=1.6 required=6.5 tests=AWL,DOMAIN_BODY autolearn=no \n\tversion=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] RFE: New queue assignment policy (repost)\r\nX-Yahoo-Group-Post: member; u=168599281\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nChristian Kohlschuetter wrote:\n\n&gt; One big disadvantage of my current BdbFrontier-approach is of course the\n&gt; slow-down which occurs after about 500.000 pages (goes down from about 80\n&gt; pages/sec. to 20). As far as I can tell, it&#39;s caused by continous \n&gt; page-seen\n&gt; lookups, which is log(n), but I might be wrong. Any ideas?\n\nCheck out the frontier/thread reports for what it says its doing when \ncrawl is running slow.  Are all threads occupied?  Are they all stuck \nwaiting at a particular step processing?  Perhaps you need to up your # \nof queues?\n\nThe already-seen lookup shouldn&#39;t be noticeable for numbers as small as \n500k: ~3ms an insert when the db is that small (See the bdb section on \nthis page -- http://crawler.archive.org/cgi-bin/wiki.pl?AlreadySeen).  \nIt uses bdb -- a b-tree -- so, yeah, log n.\n\nSt.Ack\n\n&gt;\n&gt;\n&gt; Christian\n&gt;\n&gt; On Monday 14 February 2005 15:10, Kristinn Sigurdsson wrote:\n&gt; &gt; Heritrix is still somewhat limited in the size of the crawls. I \n&gt; doubt you\n&gt; &gt; would be able to crawl quite that many pages in one crawl. Also, \n&gt; link depth\n&gt; &gt; of 7 is quite deep on most web sites. Heck, even a link depth of 4 \n&gt; can lead\n&gt; &gt; to crawling the majority of some sites. Of course this will vary \n&gt; from site\n&gt; &gt; to site.\n&gt; &gt;\n&gt; &gt; I&#39;d suggest (if feasible) splitting up the scope into a series of \n&gt; smaller,\n&gt; &gt; independent crawls. This has the additional advantage of allowing you to\n&gt; &gt; monitor each segment much more closely and cutting it off once you are\n&gt; &gt; clearly crawling rubbish.\n&gt; &gt;\n&gt; &gt; The .is domain spans about 11,000 domains and (roughly) 35 million \n&gt; pages.\n&gt; &gt; With the BDB frontier I could run it in one crawl, but I feel I get \n&gt; better\n&gt; &gt; performance running it in 4 seperate batches.\n&gt; &gt;\n&gt; &gt; This is of course only feasible if you can both easily split the \n&gt; scope up\n&gt; &gt; and cross linkage is not a high priority concern.\n&gt; &gt;\n&gt; &gt; - Kris\n&gt; &gt;\n&gt; &gt; &gt; -----Original Message-----\n&gt; &gt; &gt; From: Christian Kohlschuetter [mailto:ck-heritrix@...]\n&gt; &gt; &gt; Sent: 14. febr�ar 2005 13:56\n&gt; &gt; &gt; To: archive-crawler@yahoogroups.com\n&gt; &gt; &gt; Subject: Re: [archive-crawler] RFE: New queue assignment policy\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hey Kris,\n&gt; &gt; &gt;\n&gt; &gt; &gt; indeed, I am currently doing _very_ broad crawls :)\n&gt; &gt; &gt;\n&gt; &gt; &gt; Using Heritrix, I would like to fetch about 60-100 million\n&gt; &gt; &gt; representative\n&gt; &gt; &gt; pages in the DMOZ sphere (currently, the link depth is set to\n&gt; &gt; &gt; 7, which is\n&gt; &gt; &gt; just too much for a host-oriented assignment).\n&gt; &gt; &gt;\n&gt; &gt; &gt; Perhaps you have an idea how to do such a crawl with Heritrix current\n&gt; &gt; &gt; abilities? I would be very interested in a quick solution.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Best regards,\n&gt; &gt; &gt;\n&gt; &gt; &gt; Christian\n&gt; &gt; &gt;\n&gt; &gt; &gt; On Monday 14 February 2005 14:31, Kristinn Sigurdsson wrote:\n&gt; &gt; &gt; &gt; Hey Christian,\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I&#39;ve got a quick question for you: just how many hosts/ips are you\n&gt; &gt; &gt; &gt; crawling??\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I&#39;ve conducted crawls over about 1000-1200 domains (maybe a total of\n&gt; &gt; &gt; &gt; ten-fifty times that many hosts once you count offsite\n&gt; &gt; &gt;\n&gt; &gt; &gt; images etc.) that\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; covered well over a million documents (as many as 5 million\n&gt; &gt; &gt;\n&gt; &gt; &gt; in fact). And\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; this was with the much less efficient HostQueuesFrontier.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I&#39;ve also tested the BDBFrontier (with 1GB of heap) running\n&gt; &gt; &gt;\n&gt; &gt; &gt; on 11 thousand\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; domains (and quite a few more hosts in total) without\n&gt; &gt; &gt;\n&gt; &gt; &gt; running into any\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; problems. That crawl collected over 2 million documents\n&gt; &gt; &gt;\n&gt; &gt; &gt; before I shut it\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; down.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; So, I&#39;m a little surprised by the this. Are you running a\n&gt; &gt; &gt;\n&gt; &gt; &gt; strict broad\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; crawl? The only way I could see the number of hosts become\n&gt; &gt; &gt;\n&gt; &gt; &gt; an issue within\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; the first 1 million documents would be in a very broad\n&gt; &gt; &gt;\n&gt; &gt; &gt; oriented crawl...?\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; - Kris\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; -----Original Message-----\n&gt; &gt; &gt; &gt; From: Christian Kohlschuetter [mailto:ck-heritrix@...]\n&gt; &gt; &gt; &gt; Sent: 14. febr�ar 2005 13:15\n&gt; &gt; &gt; &gt; To: archive-crawler@yahoogroups.com\n&gt; &gt; &gt; &gt; Subject: [archive-crawler] RFE: New queue assignment policy\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Hi,\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; here&#39;s another feature which I would like to contribute.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Currently, I am performing broad crawls using\n&gt; &gt; &gt;\n&gt; &gt; &gt; BroadScope/BdbFrontier.\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; However,\n&gt; &gt; &gt; &gt; due to the number of host- or IP-keyed queues, an\n&gt; &gt; &gt;\n&gt; &gt; &gt; OutOfMemoryError occurs\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; very quickly after starting the crawl. One reason for this\n&gt; &gt; &gt;\n&gt; &gt; &gt; is the RAM-based\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; bookkeeping of subqueues -- the more queues, the more heap.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; I have evaded this by writing a BucketQueueAssignmentPolicy\n&gt; &gt; &gt;\n&gt; &gt; &gt; class, which\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; produces a _fixed_ number of subqueues (&quot;buckets&quot;), not one\n&gt; &gt; &gt;\n&gt; &gt; &gt; per host or per\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; IP. The queue key is computed by hashing the hostname (or the IP, if\n&gt; &gt; &gt; &gt; available) modulo N (a fixed number, such as 1000).\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; This way, I was able to increase the number of fetched\n&gt; &gt; &gt;\n&gt; &gt; &gt; pages from ca.\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; 400,000\n&gt; &gt; &gt; &gt; to 1,000,000. For some other reason, I still get OOMEs, but\n&gt; &gt; &gt;\n&gt; &gt; &gt; I think that is\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; caused by a different problem -- the number of queues did\n&gt; &gt; &gt;\n&gt; &gt; &gt; not grow over the\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; specified limit.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Furthermore, I have modified AbstractFrontier to be able to choose\n&gt; &gt; &gt; &gt; arbitrary\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; queue assignment policies and replaced the current\n&gt; &gt; &gt;\n&gt; &gt; &gt; &quot;ip-politness&quot; option by\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt; a\n&gt; &gt; &gt; &gt; selectbox.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; The patch against CVS HEAD is attached.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Greetings,\n&gt; &gt; &gt;\n&gt; &gt; &gt; --\n&gt; &gt; &gt; Christian Kohlsch�tter\n&gt; &gt; &gt; mailto: ck -at- NewsClub.de\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt; -- \n&gt; Christian Kohlsch�tter\n&gt; mailto: ck -at- NewsClub.de\n&gt;\n&gt; *Yahoo! Groups Sponsor*\n&gt; ADVERTISEMENT\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}