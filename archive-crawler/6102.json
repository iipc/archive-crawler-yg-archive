{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"John Lekashman","from":"John Lekashman &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"4N7db734NPD_Eni9ELHNP-5dhbulDN6sE6E-DrFSf7YcXbpV0VD21wibiVaAkUV0slsetpk_rH-EZHvbtkaXXGMNRkAEG2e7aes","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Web-Scale Frontier","postDate":"1255537091","msgId":6102,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBRDVGOUMzLjMwODA1MDJAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PGhiNHFrOSttanZkQGVHcm91cHMuY29tPg==","referencesHeader":"PGhiNHFrOSttanZkQGVHcm91cHMuY29tPg=="},"prevInTopic":6101,"nextInTopic":6105,"prevInTime":6101,"nextInTime":6103,"topicId":6099,"numMessagesInTopic":6,"msgSnippet":"Hi there, Yeah, I m the guy who did those 8 B crawl he talks about. Like pretty much all java programs, heritrix does better with more memory. I used 16G","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 49213 invoked from network); 14 Oct 2009 16:18:15 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m5.grp.sp2.yahoo.com with QMQP; 14 Oct 2009 16:18:15 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta3.grp.sp2.yahoo.com with SMTP; 14 Oct 2009 16:18:15 -0000\r\nX-Received: from [192.168.1.100] (c-76-126-186-152.hsd1.ca.comcast.net [76.126.186.152])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id n9EGIBB4071756\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 14 Oct 2009 09:18:15 -0700 (PDT)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;4AD5F9C3.3080502@...&gt;\r\nDate: Wed, 14 Oct 2009 09:18:11 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.12) Gecko/20050915\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;hb4qk9+mjvd@...&gt;\r\nIn-Reply-To: &lt;hb4qk9+mjvd@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: John Lekashman &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Re: Web-Scale Frontier\r\nX-Yahoo-Group-Post: member; u=90724651; y=IZ_CaBE_LNbKOvZfW8LXFVsn-TcPPqxB5HOezTrSm-6o\r\nX-Yahoo-Profile: lekash\r\n\r\nHi there,\nYeah, I&#39;m the guy who did those 8 B crawl he talks about.\n\nLike pretty much all java programs, heritrix does better with more\nmemory.\n\nI used 16G memory, and 16 - 32 G swap.  After a few months,\nthe crawls would all pretty much fall down at about 600 - 700M urls\nper system.  Swapping pretty heavily then, unfortunately, as the \nfrontier grew.\n(downloaded.  The discovered frontiers varied widely.)\n\nDon&#39;t know the reason they all tended to have problems at that range,\nGordon and I have talked about it a few times, as to where the\nproblem lies.\n\nJohn\n\nfarbgeist wrote:\n\n&gt;  \n&gt;\n&gt; Thanks for the quick answer Gordon!\n&gt;\n&gt; &gt; Not true; the crawl should continue without problems as long as you&#39;ve\n&gt; &gt; still got free disk space on whatever volume holds your crawl&#39;s &#39;state&#39;\n&gt; &gt; directory. Frontier operations -- testing URIs for prior inclusion, and\n&gt; &gt; enqueuing new URIs -- will become slower as they require seeks/reads\n&gt; &gt; over ever-larger disk structures.\n&gt; &gt;\n&gt;\n&gt; Do you have some examples of typical URIs/s on different \n&gt; configurations (memory/# cpus) for crawls that already use the disk?\n&gt; A first test of a broad crawl (4 hops, 400 toe-threads on a dual core \n&gt; Athlon 3000+ with 2Gb ram and 100Mb bandwith resulted in ~50 URIs/s \n&gt; using Heritrix 2.0.2 after ~ 2 hours which did not change \n&gt; significantly after 10 hours..\n&gt;\n&gt;\n&gt; &gt;\n&gt; &gt; For crawls where a single machine is expected to visit &gt; 100 million\n&gt; &gt; URIs, to avoid the slowdown as the crawl grows, we usually swap the\n&gt; &gt; disk-based already-included class (BdbUriUniqFilter) for an in-memory\n&gt; &gt; implementation (BloomUriUniqFilter) that doesn&#39;t slow over time, but\n&gt; &gt; instead has a small false-positive rate (that then grows if the filter\n&gt; &gt; becomes oversaturated). (The defaults, which are adjustable if you have\n&gt; &gt; more RAM, use 512MB to acheive a 1-in-4-million false-positive rate up\n&gt; &gt; through 125 million discovered URIs.)\n&gt; &gt;\n&gt;\n&gt; Where is the upper limit? Do you need 512MB every 125 million URIs or\n&gt; is the false positive rate increasing drastically?\n&gt;\n&gt; &gt; IA has done crawls up to 2 billion URIs with Heritrix using multiple\n&gt; &gt; machines, and we know of outside teams who have done crawls of over 8\n&gt; &gt; billion URIs using the same general techniques, which can be scaled\n&gt; &gt; further with more machines.\n&gt; &gt;\n&gt; Can you tell numbers of how many machines (RAM, cores, bandwith) you \n&gt; used in which timeframe for the 2 billion URIs? In that case you used \n&gt; the BdbUriUniqFilter, right?\n&gt;\n&gt; &gt; Separate but related: we&#39;ve long been interested in having a\n&gt; &gt; already-included structure matching that described in the Mercator\n&gt; &gt; papers (or as updated in the recent IRLbot paper), which would offer a\n&gt; &gt; disk-based structure that wouldn&#39;t slow as much with growth as our\n&gt; &gt; current implementation.\n&gt; (The manner in which candidate\n&gt; &gt; URIs are passed through a duplicate filter, allowing for batching and\n&gt; &gt; without the assumption of instant enqueuing, was designed to allow \n&gt; these\n&gt; &gt; techniques to be dropped-in when needed.)\n&gt; &gt;\n&gt;\n&gt; Do you believe HBase on a cluster could possibly act as a substitute \n&gt; to DRUM?\n&gt;\n&gt; My test-crawl resulted in less than 1/5 of bandwith usage (about \n&gt; 2000KB/s), while 1,600,000 pages where downloaded and ~ 5,000,000 \n&gt; queued. Why?\n&gt;\n&gt; best regards\n&gt; farbgeist\n&gt;\n&gt; \n\n\n"}}