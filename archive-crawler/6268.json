{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":186477060,"authorName":"lucky goyal","from":"lucky goyal &lt;lg_0811@...&gt;","profile":"lg_0811","replyTo":"LIST","senderId":"ELYQQTPVYKgsy7Vf2umswl1Gegvo9tSGiVQNqLDaq1V_wz6jVmTPeXmxG2WZbNT_gajYHMK4hE-z8Y10kRIdzQPXVAMflg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Need help regarding Heritrix 1.14 on windows platform","postDate":"1262849847","msgId":6268,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYyOTI2Ny4xNjE5MC5xbUB3ZWI1NDUwMi5tYWlsLnJlMi55YWhvby5jb20+","inReplyToHeader":"PDY2NzMxNi45MzcwMS5xbUB3ZWI1NDUwNi5tYWlsLnJlMi55YWhvby5jb20+"},"prevInTopic":6267,"nextInTopic":0,"prevInTime":6267,"nextInTime":6269,"topicId":6267,"numMessagesInTopic":2,"msgSnippet":"Any help on the below issues will be really useful. ... From: lucky goyal  Subject: [archive-crawler] Need help regarding Heritrix 1.14 on","rawEmail":"Return-Path: &lt;lg_0811@...&gt;\r\nX-Sender: lg_0811@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 77315 invoked from network); 7 Jan 2010 07:37:41 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m12.grp.re1.yahoo.com with QMQP; 7 Jan 2010 07:37:41 -0000\r\nX-Received: from unknown (HELO web54502.mail.re2.yahoo.com) (206.190.49.152)\n  by mta2.grp.re1.yahoo.com with SMTP; 7 Jan 2010 07:37:41 -0000\r\nX-Received: (qmail 16194 invoked by uid 60001); 7 Jan 2010 07:37:27 -0000\r\nMessage-ID: &lt;629267.16190.qm@...&gt;\r\nX-YMail-OSG: fgLNapgVM1lIi4u0C7QoAw8cHBNPhAYr4c3c16EWRjSeTX2s1STsH.CVxukTAnuwajbn4FPpCQpMBGO7xg6xsmX2pa.KCLEj6yU8_Exap4_jKA6G18bJ6JK46aeDsdp3VMc9BQw3XMX70PCpe_CyJcL.A5Xo8QYhIXNDxCGuveGvgziZTQxFn4L.BsaOfWhmWm3TGL5G0a4GN5BPUo9l33EDKVnJoO9KAr1_wUE8QEMsmE1jwrHzk90K962lqEn.hJErYs5jVSSCMA9cpcbxsv44BB80Y.hILOB8pPvTYQgEGpkqfFiwsWHj56UqfAK0__tYkNq6apA_7243UZZzQKRTkWjkqUgS8SVY5i8MfaF1tugm0wPE3tl6OGWBjErnU6yLgkmD4FL_nxCeCfQmLcjf5VnZKMdMLZ.z7a_lE3qtvVt9ZknWTqX0YJS1.4OVRCtJVVAKuqFyRYFt.ZrDR5hsf9hSv2ZZZ3Osq1bn0zWKDiv6kokQoCGD5VVkyQZYhj.xioeMscS2iFMJTdQxPWRZ3vV4vGHMXHYVPtBI4raTKm5A0EHxYSdJR_EjSEG.GIdxL53b5rUx.NujyzWO0HhbZDBb3uqnfA.qjNXS5A--\r\nX-Received: from [220.227.32.101] by web54502.mail.re2.yahoo.com via HTTP; Wed, 06 Jan 2010 23:37:27 PST\r\nX-Mailer: YahooMailClassic/9.0.20 YahooMailWebService/0.8.100.260964\r\nDate: Wed, 6 Jan 2010 23:37:27 -0800 (PST)\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;667316.93701.qm@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-553819199-1262849847=:16190&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: lucky goyal &lt;lg_0811@...&gt;\r\nSubject: Re: [archive-crawler] Need help regarding Heritrix 1.14 on windows platform\r\nX-Yahoo-Group-Post: member; u=186477060; y=x3m87AbiuF5IyoFJjcmz1GztuiOfXPOR4l7AL08kDkJjgg\r\nX-Yahoo-Profile: lg_0811\r\n\r\n\r\n--0-553819199-1262849847=:16190\r\nContent-Type: text/plain; charset=utf-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nAny help on the below issues will be really useful.\n\n--- On Thu, 1/7/10, lu=\r\ncky goyal &lt;lg_0811@...&gt; wrote:\n\nFrom: lucky goyal &lt;lg_0811@...&gt;=\r\n\nSubject: [archive-crawler] Need help regarding Heritrix 1.14 on windows pl=\r\natform\nTo: archive-crawler@yahoogroups.com\nDate: Thursday, January 7, 2010,=\r\n 11:00 AM\n\n\n\n\n\n\n\n=C2=A0\n\n\n\n  \n\n\n    \n      \n      \n      Hi All,\n=C2=A0\nI a=\r\nm trying to crawl=C2=A0some=C2=A0websites using heritrix 1.14 on a\nwindows =\r\nmachine(Server 2008, 64-bit machine). I am using JAVA 6\n(32-bit) for runnin=\r\ng the heritrix instance. The heritrix is run from\ncommand line with &quot;-nowui=\r\n&quot; option enabled.=C2=A0I am facing a few issues in\ncrawling. Any help on th=\r\nese=C2=A0shall be=C2=A0greatly appreciated.\n=C2=A0\n1)=C2=A0My goal is to cr=\r\nawl a bunch (20-25) of websites in a single\njob. The pages of my interest i=\r\nn these websites are 2-4 links away from\nthe websites main page(home page).=\r\n Since I am giving the websites URL\nas the seed, the heritrix is fetching a=\r\nll links its discovers from home\npage onwards. I am using deciding scope wi=\r\nth a sequence of decide rules\nas mentioned below.\n=C2=A0\n=C2=A0=C2=A0=C2=A0=\r\n=C2=A0=C2=A0 &lt;newObject name=3D&quot;decide- rules&quot; class=3D&quot;org.archive. crawle=\r\nr.decideru les.DecideRuleSe quence&quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=\r\n=C2=A0 &lt;map name=3D&quot;rules&quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0=C2=A0 &lt;newObject name=3D&quot;acceptAllRule &quot; class=3D&quot;org.archive. crawler.=\r\ndecideru les.AcceptDecide Rule&quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=\r\n=C2=A0=C2=A0 &lt;/newObject&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=\r\n=C2=A0 &lt;newObject name=3D&quot;rejectNotOnDo mainRule&quot; class=3D&quot;org.archive. cra=\r\nwler.decideru les.NotOnDomains DecideRule&quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=\r\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;/stri=\r\nng&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;str=\r\ning name=3D&quot;surts- source-file&quot; /&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;boolean\n name=3D&quot;seeds- as-surt-prefixes &quot;&gt;tru=\r\ne&lt;/boolean&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0 &lt;string name=3D&quot;surts- dump-file&quot; /&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;boolean name=3D&quot;also-check- via&quot;&gt;false&lt;/=\r\nboolean&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=\r\n &lt;boolean name=3D&quot;rebuild- on-reconfig&quot;&gt;true&lt;/boolean&gt;\n=C2=A0=C2=A0=C2=A0=\r\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;/newObject&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=\r\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;newObject name=3D&quot;rejectFilePat ternRule&quot; c=\r\nlass=3D&quot;org.archive. crawler.decideru les.MatchesFileP atternDecideRule &quot;&gt;\n=\r\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;string =\r\nname=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;string\n name=3D&quot;use-preset- pattern&quot;&gt;Custom&lt;/s=\r\ntring&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0\n&lt;=\r\nstring\nname=3D&quot;regexp&quot;&gt;.*(?i)(//.(bmp| gif|jpe?g| png|tiff? |mid|mp2| mp3|m=\r\np4|wav| avi|mov|mpeg| ram|rm|smil| wmv|doc|odt| pdf|ppt|swf| mov|m4v|zip| s=\r\nit|pub|File| exe|eps|ppt| pptx|fla| dmg|csv|png| css|pdf)) $&lt;/string&gt;\n=C2=\r\n=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;/newObject&gt;\n=C2=A0=C2=\r\n=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;newObject name=3D&quot;rejectIfTo=\r\noMa nyHops&quot; class=3D&quot;org.archive. crawler.decideru les.TooManyHopsD ecideRu=\r\nle&quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;in=\r\nteger name=3D&quot;max-hops&quot;&gt;5&lt;/integer&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0=C2=A0=C2=A0 &lt;/newObject&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0=C2=A0 &lt;newObject name=3D&quot;rejectIfPatho logical&quot;\n class=3D&quot;org.archive. =\r\ncrawler.decideru les.Pathological PathDecideRule&quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=\r\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;integer name=3D&quot;max-repetitio n=\r\ns&quot;&gt;3&lt;/integer&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;/new=\r\nObject&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;newObject n=\r\name=3D&quot;rejectIfTooMa nyPathSegs&quot; class=3D&quot;org.archive. crawler.decideru les=\r\n.TooManyPathS egmentsDecideRul e&quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=\r\n=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;integer name=3D&quot;max-path- depth&quot;&gt;10&lt;/integer&gt;\n=\r\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;/newObject&gt;\n=C2=A0=\r\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;newObject name=3D&quot;acceptP=\r\nreReqR ule&quot; class=3D&quot;org.archive. crawler.decideru les.Prerequisite AcceptD=\r\necideRule &quot;&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 &lt;/newOb=\r\nject&gt;\n=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0\n &lt;/map&gt;\n=C2=A0=C2=A0=C2=A0=\r\n=C2=A0=C2=A0 &lt;/newObject&gt;\n=C2=A0\nAlthough I can see that in the mirror fold=\r\ner that no page outside\nthe seed&#39;s domains and subdomains is been crawled, =\r\nBut still the\n&quot;progress-statistic s&quot; file shows a very large number of disc=\r\novered\nURL&#39;s. Running this foccussed=C2=A0crawl for approx. 5 days discover=\r\ns around\n15=C2=A0million=C2=A0URL&#39;s out of which only=C2=A0approx 2 million=\r\n are downloded in 5\ndays. discovery of such a large number=C2=A0of URL&#39;s fr=\r\nom a set of 25 sites\nseems strange. Also I am not able to find a way to see=\r\n the list of\ndiscovered and queued URI&#39;s. \n=C2=A0\nCould someone please shar=\r\ne his/her experience of similar behaviour\nof any pointers to analysing the =\r\nproblem. Specially it would be really\nhelpful, if I could see the frontier =\r\nlog to see the list of\ndiscovered=C2=A0and queued URL&#39;s.\n=C2=A0\n2)=C2=A0Doe=\r\ns=C2=A0the list=C2=A0of discovered URL&#39;s contain out of scope URL&#39;s.\nOr=C2=\r\n=A0is the scope filter already applied before adding the URL to\ndiscovered =\r\nlist.\n=C2=A0\n3)=C2=A0Does heritrix queue duplicate URL&#39;s also.The webpages =\r\nI intend to crawl, will contain a \n=C2=A0=C2=A0=C2=A0 lot of duplicate URL&#39;=\r\ns. Is there any configuration setting to\nprevent duplicate URL&#39;s from being=\r\n queued=C2=A0and crawled in a single crawl\njob run.=C2=A0\n=C2=A0\n4)=C2=A0I =\r\ncan see a lot of &quot;FileNotFoundExcept ion@MirrorWriter &quot;=C2=A0=C2=A0errors\ni=\r\nn the &quot;local-errors&quot; logs. On analysis, it was observed that the error\nis b=\r\necause the URL contains some special characters which are not\nallowed=C2=A0=\r\nin file names in windows, for example : &lt; &gt; &#92; ? | *=C2=A0&quot;\netc. I have trie=\r\nd the character map setting as proposed in heritrix\nwebUI. But it does not =\r\nseem to yield the results. Please let me\nknow=C2=A0some solution to this pr=\r\noblem. It is really important for my\nexercise as a lot of the desired URL&#39;s=\r\n have one or another special\ncharacters in the name.\n=C2=A0\n5) I would also=\r\n need to crawl a few Ajax based websites. As per my\ncurrent understanding, =\r\nthis is not possible with heritrix. Is there any\navalable plugin=C2=A0or an=\r\nother mechanism to crawl such Ajax based websites\nusing heritrix. Any help =\r\non this=C2=A0will be really useful. \n=C2=A0\nI shall be=C2=A0looking forward=\r\n=C2=A0to=C2=A0help on above issues. Kindly also\nlet me know if I am not on =\r\nthe right forum, and advice me if I need to\npost it some place else also.\n=\r\n=C2=A0\nThanks and Regards,\nLucky Goyal \n\n\n\n      \n\n    \n     \n\n    \n    \n\n\n=\r\n \n\n\n\n  \n\n\n\n\n\n\n      \r\n--0-553819199-1262849847=:16190\r\nContent-Type: text/html; charset=utf-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;table cellspacing=3D&quot;0&quot; cellpadding=3D&quot;0&quot; border=3D&quot;0&quot; &gt;&lt;tr&gt;&lt;td valign=3D&quot;=\r\ntop&quot; style=3D&quot;font: inherit;&quot;&gt;Any help on the below issues will be really u=\r\nseful.&lt;br&gt;&lt;br&gt;--- On &lt;b&gt;Thu, 1/7/10, lucky goyal &lt;i&gt;&lt;lg_0811@...&g=\r\nt;&lt;/i&gt;&lt;/b&gt; wrote:&lt;br&gt;&lt;blockquote style=3D&quot;border-left: 2px solid rgb(16, 16=\r\n, 255); margin-left: 5px; padding-left: 5px;&quot;&gt;&lt;br&gt;From: lucky goyal &lt;lg_=\r\n0811@...&gt;&lt;br&gt;Subject: [archive-crawler] Need help regarding Heritr=\r\nix 1.14 on windows platform&lt;br&gt;To: archive-crawler@yahoogroups.com&lt;br&gt;Date:=\r\n Thursday, January 7, 2010, 11:00 AM&lt;br&gt;&lt;br&gt;&lt;div id=3D&quot;yiv1221182166&quot;&gt;\n\n\n\n\n=\r\n\n&lt;span style=3D&quot;display: none;&quot;&gt;&nbsp;&lt;/span&gt;\n\n\n\n    &lt;div id=3D&quot;ygrp-text&quot;&gt;=\r\n\n      \n      \n      &lt;p&gt;&lt;table border=3D&quot;0&quot; cellpadding=3D&quot;0&quot; cellspacing=\r\n=3D&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=3D&quot;font-family: inherit; font-style: inherit; f=\r\nont-variant: inherit; font-weight: inherit; font-size: inherit; line-height=\r\n: inherit; font-size-adjust: inherit; font-stretch: inherit; -x-system-font=\r\n: none;&quot; valign=3D&quot;top&quot;&gt;&lt;div&gt;Hi All,&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;I am tryi=\r\nng to crawl&nbsp;some&nbsp;websites using heritrix 1.14 on a\nwindows machin=\r\ne(Server 2008, 64-bit machine). I am using JAVA 6\n(32-bit) for running the =\r\nheritrix instance. The heritrix is run from\ncommand line with &quot;-nowui&quot; opti=\r\non enabled.&nbsp;I am facing a few issues in\ncrawling. Any help on these&nb=\r\nsp;shall be&nbsp;greatly appreciated.&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;1)&nbsp;=\r\nMy goal is to crawl a bunch (20-25) of websites in a single\njob. The pages =\r\nof my interest in these websites are 2-4 links away from\nthe websites main =\r\npage(home page). Since I am giving the websites URL\nas the seed, the heritr=\r\nix is fetching all links its discovers from home\npage onwards. I am using d=\r\neciding scope with a sequence of decide rules\nas mentioned below.&lt;/div&gt;\n&lt;di=\r\nv&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject name=3D&quot;de=\r\ncide- rules&quot; class=3D&quot;org.archive. crawler.decideru les.DecideRuleSe quence=\r\n&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;map name=3D&quot;rules&quot;&=\r\ngt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject=\r\n name=3D&quot;acceptAllRule &quot; class=3D&quot;org.archive. crawler.decideru les.AcceptD=\r\necide Rule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &=\r\nlt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n &lt;newObject name=3D&quot;rejectNotOnDo mainRule&quot; class=3D&quot;org.archive. crawle=\r\nr.decideru les.NotOnDomains DecideRule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;string name=3D&quot;decision&quot;&gt;REJE=\r\nCT&lt;/string&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n&nbsp;&nbsp; &lt;string name=3D&quot;surts- source-file&quot; /&gt;&lt;br&gt;&nbsp;&nbsp;&n=\r\nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;boolean\n name=3D&quot;s=\r\needs- as-surt-prefixes &quot;&gt;true&lt;/boolean&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;string name=3D&quot;surts- dump-=\r\nfile&quot; /&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n&nbsp; &lt;boolean name=3D&quot;also-check- via&quot;&gt;false&lt;/boolean&gt;&lt;br&gt;&nb=\r\nsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;boolean=\r\n name=3D&quot;rebuild- on-reconfig&quot;&gt;true&lt;/boolean&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nb=\r\nsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject name=3D&quot;rejectFilePat=\r\n ternRule&quot; class=3D&quot;org.archive. crawler.decideru les.MatchesFileP atternDe=\r\ncideRule &quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb=\r\nsp;&nbsp; &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;&lt;br&gt;&nbsp;&n=\r\nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;string\n name=\r\n=3D&quot;use-preset- pattern&quot;&gt;Custom&lt;/string&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n&lt;string\nname=3D&quot;regexp&quot;&gt;=\r\n.*(?i)(//.(bmp| gif|jpe?g| png|tiff? |mid|mp2| mp3|mp4|wav| avi|mov|mpeg| r=\r\nam|rm|smil| wmv|doc|odt| pdf|ppt|swf| mov|m4v|zip| sit|pub|File| exe|eps|pp=\r\nt| pptx|fla| dmg|csv|png| css|pdf)) $&lt;/string&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject name=3D&quot;rejectIfTooMa n=\r\nyHops&quot; class=3D&quot;org.archive. crawler.decideru les.TooManyHopsD ecideRule&quot;&g=\r\nt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &l=\r\nt;integer name=3D&quot;max-hops&quot;&gt;5&lt;/integer&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nb=\r\nsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject name=3D&quot;rejectIfPatho logic=\r\nal&quot;\n class=3D&quot;org.archive. crawler.decideru les.Pathological PathDecideRule=\r\n&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n &lt;integer name=3D&quot;max-repetitio ns&quot;&gt;3&lt;/integer&gt;&lt;br&gt;&nbsp;&nbsp;=\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject name=3D&quot;rejectIf=\r\nTooMa nyPathSegs&quot; class=3D&quot;org.archive. crawler.decideru les.TooManyPathS e=\r\ngmentsDecideRul e&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp;&nbsp; &lt;integer name=3D&quot;max-path- depth&quot;&gt;10&lt;/integer&gt=\r\n;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&=\r\ngt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject=\r\n name=3D&quot;acceptPreReqR ule&quot; class=3D&quot;org.archive. crawler.decideru les.Prer=\r\nequisite AcceptDecideRule &quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n=\r\nbsp;\n &lt;/map&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&gt;&lt;/di=\r\nv&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;Although I can see that in the mirror folder that=\r\n no page outside\nthe seed&#39;s domains and subdomains is been crawled, But sti=\r\nll the\n&quot;progress-statistic s&quot; file shows a very large number of discovered\n=\r\nURL&#39;s. Running this foccussed&nbsp;crawl for approx. 5 days discovers aroun=\r\nd\n15&nbsp;million&nbsp;URL&#39;s out of which only&nbsp;approx 2 million are do=\r\nwnloded in 5\ndays. discovery of such a large number&nbsp;of URL&#39;s from a se=\r\nt of 25 sites\nseems strange. Also I am not able to find a way to see the li=\r\nst of\ndiscovered and queued URI&#39;s. &lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;Could some=\r\none please share his/her experience of similar behaviour\nof any pointers to=\r\n analysing the problem. Specially it would be really\nhelpful, if I could se=\r\ne the frontier log to see the list of\ndiscovered&nbsp;and queued URL&#39;s.&lt;/di=\r\nv&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;2)&nbsp;Does&nbsp;the list&nbsp;of discovered URL=\r\n&#39;s contain out of scope URL&#39;s.\nOr&nbsp;is the scope filter already applied =\r\nbefore adding the URL to\ndiscovered list.&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;3)&n=\r\nbsp;Does heritrix queue duplicate URL&#39;s also.The webpages I intend to crawl=\r\n, will contain a &lt;/div&gt;\n&lt;div&gt;&nbsp;&nbsp;&nbsp; lot of duplicate URL&#39;s. Is =\r\nthere any configuration setting to\nprevent duplicate URL&#39;s from being queue=\r\nd&nbsp;and crawled in a single crawl\njob run.&nbsp;&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;=\r\n\n&lt;div&gt;4)&nbsp;I can see a lot of &quot;FileNotFoundExcept ion@MirrorWriter &quot;&nbs=\r\np;&nbsp;errors\nin the &quot;local-errors&quot; logs. On analysis, it was observed tha=\r\nt the error\nis because the URL contains some special characters which are n=\r\not\nallowed&nbsp;in file names in windows, for example : &lt; &gt; &#92; ? | *&n=\r\nbsp;&quot;\netc. I have tried the character map setting as proposed in heritrix\nw=\r\nebUI. But it does not seem to yield the results. Please let me\nknow&nbsp;so=\r\nme solution to this problem. It is really important for my\nexercise as a lo=\r\nt of the desired URL&#39;s have one or another special\ncharacters in the name.&lt;=\r\n/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;5) I would also need to crawl a few Ajax based=\r\n websites. As per my\ncurrent understanding, this is not possible with herit=\r\nrix. Is there any\navalable plugin&nbsp;or another mechanism to crawl such A=\r\njax based websites\nusing heritrix. Any help on this&nbsp;will be really use=\r\nful. &lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;I shall be&nbsp;looking forward&nbsp;to&=\r\nnbsp;help on above issues. Kindly also\nlet me know if I am not on the right=\r\n forum, and advice me if I need to\npost it some place else also.&lt;/div&gt;\n&lt;div=\r\n&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;Thanks and Regards,&lt;/div&gt;\n&lt;div&gt;Lucky Goyal &lt;br&gt;&lt;/div&gt;&lt;/t=\r\nd&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;\n\n      &lt;/p&gt;\n\n    &lt;/div&gt;\n     \n\n\n\n \n\n\n&lt;/div&gt;&lt;/bl=\r\nockquote&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;br&gt;\n\n      \r\n--0-553819199-1262849847=:16190--\r\n\n"}}