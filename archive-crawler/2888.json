{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"J304MQe1XdMVfoi2b1IoD2q8k170WGcPql4B2OgbeD0iu7FP-g0AvOCP0akfKuQmM4nBhpNZ30pEMxXTPehydboL05QQxVpZ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Can we add content filter ?","postDate":"1149000355","msgId":2888,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0N0M1QUEzLjEwNzA1MDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGU1Z2NocStlcWxrQGVHcm91cHMuY29tPg==","referencesHeader":"PGU1Z2NocStlcWxrQGVHcm91cHMuY29tPg=="},"prevInTopic":2887,"nextInTopic":2889,"prevInTime":2887,"nextInTime":2889,"topicId":2887,"numMessagesInTopic":3,"msgSnippet":"... Not at the moment but it shouldn t be hard to implement.  The fetched pages are readily accessible so you could write your own little processor that does","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 80985 invoked from network); 30 May 2006 15:05:24 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m34.grp.scd.yahoo.com with QMQP; 30 May 2006 15:05:24 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta3.grp.scd.yahoo.com with SMTP; 30 May 2006 15:05:23 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k4UDVDw10482;\n\tTue, 30 May 2006 06:31:13 -0700\r\nMessage-ID: &lt;447C5AA3.1070509@...&gt;\r\nDate: Tue, 30 May 2006 07:45:55 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8.0.1) Gecko/20060127 SeaMonkey/1.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e5gchq+eqlk@...&gt;\r\nIn-Reply-To: &lt;e5gchq+eqlk@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Can we add content filter ?\r\nX-Yahoo-Group-Post: member; u=168599281; y=ZIkR-EGLBELOxAIxGjBckQIwADQgxRt1taEfrYwH-zQVp1xahOlgLYh6\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nanand_akela wrote:\n&gt; Hi,\n&gt;\n&gt; I am learning to use this tool. I understood that you can add filters\n&gt; for URIs, but, I am wondering if there any way to add keywords filter\n&gt; that can look-up content of the webpage and save the information only\n&gt; if there is keyword match.\nNot at the moment but it shouldn&#39;t be hard to implement.  The fetched \npages are readily accessible so you could write your own little \nprocessor that does the search for key words -- perhaps confined to \ntitle, meta, description -- and that set a flag a filter on ARCWriter \nchecked before writing records.\n&gt;\n&gt; Another question... Is there an easy-to-use tool that can split the\n&gt; big ARC file into html files per URL?\nNo.  Such a tool would be welcomed.\n\nThe MirrorWriter processor saves pages to disk as individual HTML pages \n(http://crawler.archive.org/apidocs/org/archive/crawler/writer/MirrorWriterProcessor.html).  \nYou might look at that.  Otherwise, if you wanted to build a tool that \ncut ARCs into individual pages, study the ARCReader: \nhttp://crawler.archive.org/apidocs/org/archive/io/arc/ARCReader.html.\n\nSt.Ack\n\n"}}