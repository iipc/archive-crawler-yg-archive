{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":160886731,"authorName":"Marco Baroni","from":"Marco Baroni &lt;baroni@...&gt;","profile":"kumaraja2000","replyTo":"LIST","senderId":"CNz9kGWkpILIOMY83GXDE2gsQmU9W8UVf3Sef86PIgK6neAH3u2dEaTUjdgBVeio6LRhGcN-5t6Gf6OfePvm7CQ-x0jQrNFQSA-YXg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"slow crawls with 1.8: mail per heritrix list","postDate":"1151487701","msgId":2984,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QTI0RUQ1LjgwMDA1MDFAc3NsbWl0LnVuaWJvLml0Pg=="},"prevInTopic":0,"nextInTopic":2987,"prevInTime":2983,"nextInTime":2985,"topicId":2984,"numMessagesInTopic":5,"msgSnippet":"Dear Heritrixers, After running some very successful surt-scoped crawls with heritrix 1.4 last summer/fall (about 350GB of html-only data per crawl), we have ","rawEmail":"Return-Path: &lt;baroni@...&gt;\r\nX-Sender: baroni@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 72935 invoked from network); 28 Jun 2006 09:41:57 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m31.grp.scd.yahoo.com with QMQP; 28 Jun 2006 09:41:57 -0000\r\nReceived: from unknown (HELO einstein.sslmit.unibo.it) (137.204.200.1)\n  by mta2.grp.scd.yahoo.com with SMTP; 28 Jun 2006 09:41:57 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby einstein.sslmit.unibo.it (Postfix) with ESMTP id 5CC17307F0;\n\tWed, 28 Jun 2006 11:41:44 +0200 (CEST)\r\nReceived: from einstein.sslmit.unibo.it ([127.0.0.1])\n by localhost (einstein.sslmit.unibo.it [127.0.0.1]) (amavisd-new, port 10024)\n with ESMTP id 08296-07; Wed, 28 Jun 2006 11:41:41 +0200 (CEST)\r\nReceived: from [137.204.90.48] (unknown [137.204.90.48])\n\tby einstein.sslmit.unibo.it (Postfix) with ESMTP id B7A5B2E579;\n\tWed, 28 Jun 2006 11:41:41 +0200 (CEST)\r\nMessage-ID: &lt;44A24ED5.8000501@...&gt;\r\nDate: Wed, 28 Jun 2006 11:41:41 +0200\r\nUser-Agent: Thunderbird 1.5.0.2 (Macintosh/20060308)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Eros Zanchetta &lt;eros@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: by amavisd-new at sslmit.unibo.it\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: Marco Baroni &lt;baroni@...&gt;\r\nSubject: slow crawls with 1.8: mail per heritrix list\r\nX-Yahoo-Group-Post: member; u=160886731; y=59Dj1Kg7Dq_KkNpajeWsIY6w7H9lOlSUmRiuENt1Uj_tZo0QjesL\r\nX-Yahoo-Profile: kumaraja2000\r\n\r\nDear Heritrixers,\n\nAfter running some very successful surt-scoped crawls with heritrix 1.4\nlast summer/fall (about 350GB of html-only data per crawl), we have\nrecently come back to using heritrix for more data.\n\nWe are using the same computer (RH Fedora Core 3 with 4 GB RAM, Dual Xeon\n4.3 GHz CPUs and about 2.5 TB hard disk space, kernel version: Linux\n2.6.15-1_SPFsmp #3 SMP, java version: 1.4.2_08, JRE Standard Edition build\n1.4.2_08-b03) we used for the previous crawls, but we updated to Heritrix 1.8.\n\nWe tried several crawls, including one with exactly the same\nparameters of one of our successful earlier crawls. However, we keep\nencountering the same problem, namely: after about 30 minutes of crawling, the\nnumber of active threads starts decreasing, until it gets stuck to 0 or 1\nactive threads. Consequently, the amount of data retrieved stops growing\nsignificantly beyond a few hundreds MB -- we can leave the crawl for days\nin this zombie state, and virtually nothing happens (we might get 1MB more\ndata per hour, or less). We also noticed that the proportion of \ndownloaded-to-queued is very\nhigh (90% and more), whereas in the 1.4 crawls it tended to stabilize\naround 15% or so, until we killed the crawl &#39;cause we had enough data.\nThere are no alerts.\n\nGiven that the manually specified parameters and the machine are the same\nwe used before, we suspect that something changed in the default parameters\nbetween 1.4 and 1.8 that motivates the problem.\n\nThese are the parameters we tune manually in a crawl that is a perfect\nreplica of a successful 1.4 crawl (everything else not listed here is\ninherited from the 1.8 default profile, plus the obvious stuff like the \noperator name, contact email, etc.):\n\n- seeds file of 3,000 seeds (we have also experimented with seed files up\nto 10k seeds, with the same pattern: problems in 1.8 where we did not have\nproblems in 1.4)\n\n- heritrix invoked as follows:\n\n  JAVA_OPTS=&quot;-Xmx1024m&quot; heritrix -a user:pw\n\n- surt scope, based on the following pattern (and no surt extraction from\nseed file):\n\nhttp://(it,\n\n- 150 threads\n\n- Tom Emerson&#39;s &quot;focusing on HTML&quot; filters, i.e.:\n\n&lt;newObject name=&quot;JustHTMLRegExp&quot;\nclass=&quot;org.archive.crawler.filter.URIRegExpFilter&quot;&gt;\n&lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&lt;boolean name=&quot;if-match-return&quot;&gt;true&lt;/boolean&gt;\n&lt;string name=&quot;regexp&quot;&gt;\n.*(?i)&#92;.(a|ai|aif|aifc|aiff|... lots of other extensions\n...|xslt|xwd|xyz|z|zip)$\n&lt;/string&gt;\n&lt;/newObject&gt;\n\n\n&lt;map name=&quot;write-processors&quot;&gt;\n&lt;newObject name=&quot;Archiver&quot;\nclass=&quot;org.archive.crawler.writer.ARCWriterProcessor&quot;&gt;\n&lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&lt;map name=&quot;filters&quot;&gt;\n&lt;newObject name=&quot;JustHTMLWrite&quot;\nclass=&quot;org.archive.crawler.filter.ContentTypeRegExpFilter&quot;&gt;\n&lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&lt;boolean name=&quot;if-match-return&quot;&gt;true&lt;/boolean&gt;\n&lt;string name=&quot;regexp&quot;&gt;(?i)text/html.*&lt;/string&gt;\n&lt;/newObject&gt;\n&lt;/map&gt;\n...\n&lt;/map&gt;\n\n&lt;map name=&quot;fetch-processors&quot;&gt;\n...\n&lt;map name=&quot;midfetch-filters&quot;&gt;\n&lt;newObject name=&quot;JustHTMLMidFetch&quot;\nclass=&quot;org.archive.crawler.filter.ContentTypeRegExpFilter&quot;&gt;\n&lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&lt;boolean name=&quot;if-match-return&quot;&gt;true&lt;/boolean&gt;\n&lt;string name=&quot;regexp&quot;&gt;(?i)text/html.*&lt;/string&gt;\n&lt;/newObject&gt;\n&lt;/map&gt;\n...\n&lt;/map&gt;\n\n\nThat&#39;s it.\n\nAs I said, the same parameters (also, same seed list) used for a\ndefault-profile-based 1.4 crawl worked fine (in the sense that the crawl\nwas healthy and long lived...)\n\nAny advice on what we are doing wrong?\n\nThanks in advance,\n\nMarco\n\n-- \nMarco Baroni\nSSLMIT, University of Bologna\nhttp://sslmit.unibo.it/~baroni\nhttp://wacky.sslmit.unibo.it\n\n\n\n"}}