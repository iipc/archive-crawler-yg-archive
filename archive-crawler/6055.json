{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"C65TXo6H5N2l0OgjelswoL9F4fwdAnqU3iSO2hQLTX4EmfuvA2NeU4voju1lDBkGfd7gTPMHpcfYwfrpg9uN_S1QSeHQAxc","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Re: Basic question: How to limit the crawling scope within a host?","postDate":"1254118213","msgId":6055,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBQzA1MzQ1LjUwMDAzMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGg5bWg3YitzZWlsQGVHcm91cHMuY29tPg==","referencesHeader":"PGg5bWg3YitzZWlsQGVHcm91cHMuY29tPg=="},"prevInTopic":6053,"nextInTopic":6059,"prevInTime":6054,"nextInTime":6056,"topicId":6051,"numMessagesInTopic":6,"msgSnippet":"... It is optional, if you want to specify a lot of SURT prefixes. The one + directive, added to the seeds, is enough for a crawl that starts on","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 58944 invoked from network); 28 Sep 2009 06:10:29 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.re1.yahoo.com with QMQP; 28 Sep 2009 06:10:29 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta2.grp.sp2.yahoo.com with SMTP; 28 Sep 2009 06:10:29 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 6AA7E3575A\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Sun, 27 Sep 2009 23:11:24 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id z2iCBS1TfAUh for &lt;archive-crawler@yahoogroups.com&gt;;\n\tSun, 27 Sep 2009 23:11:22 -0700 (PDT)\r\nX-Received: from [10.0.13.17] (adsl-70-137-138-250.dsl.snfc21.sbcglobal.net [70.137.138.250])\n\tby mail.archive.org (Postfix) with ESMTPSA id 3553E35757\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Sun, 27 Sep 2009 23:11:21 -0700 (PDT)\r\nMessage-ID: &lt;4AC05345.5000305@...&gt;\r\nDate: Sun, 27 Sep 2009 23:10:13 -0700\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;h9mh7b+seil@...&gt;\r\nIn-Reply-To: &lt;h9mh7b+seil@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Basic question: How to limit the crawling\n scope within a host?\r\nX-Yahoo-Group-Post: member; u=137285340; y=l1TonvsRlDNYWxsFM_9OJxIz6Qdbr9qMkOwTJaf8BF2V\r\nX-Yahoo-Profile: gojomo\r\n\r\n\n\nshichuanwuhan@... wrote:\n&gt; Dear Gordon,\n&gt; \n&gt;     Thanks a lot. Your answer gives me a deeper understanding of Heritrix. \n&gt;       \n&gt;     But I still can&#39;t configure it correctly following your instruction so I turn to you for help. Sorry about that :)\n&gt; \n&gt;     1.Is it optional to use &#39;surts-source-file&#39;? I add the URL:&#39;+http://people.cs.cmu.edu/faculty/index.html&#39; in the &#39;seeds&#39; table. Is it enough?\n\nIt is optional, if you want to specify a lot of SURT prefixes.\n\nThe one &#39;+&#39; directive, added to the seeds, is enough for a crawl that \nstarts on &#39;www.cs.cmu.edu&#39; to follow discovered links to \n&#39;people.cs.cmu.edu&#39;.\n\n&gt;     2. When I want to use &#39;surts-source-file&#39;, I don&#39;t know where to put it on my disk. I am using Windows XP. I try to put it in the same directory with &#39;seeds.txt&#39;, but it doesn&#39;t work.\n\nThe path entered here is interpreted relative to the job directory, \nwhere seeds.txt is, so that should work. How do you know it&#39;s not \nfinding the file? Is there an error?\n\n&gt;     3.Would you please give me an example of writing a correct SURT to\n&gt; crawl all html files under &#39;http://people.cs.cmu.edu/faculty/&#39;?\n\nI tried a crawl based on the bundled &#39;deciding-default&#39;, with the seed \n&#39;http://www.cs.cmu.ed&#39; and the added directive \n&#39;+http://people.cs.cmu.edu/faculty/index.html&#39;. It found the \n&#39;http://people.cs.cmu.edu/faculty/index.html&#39; URI from the main site.\n\nThe very first faculty URI listed on that page on &#39;www.cs.cmu.edu&#39; (as \nopposed to some other host like &#39;www-2&#39;) is \n&#39;http://www.cs.cmu.edu/~dga/&#39;. After a short while, my test crawl \nfetched this URI.\n\nSo if this is not working for you, it&#39;s due to some other change you&#39;ve \nmade to the default configuration. Also, if you want to get pages on \nother hostnames, like &#39;www-2.cs.cmu.edu&#39;, you will have to add \nadditional directives.\n\n&gt; Here is my crawl order:\n&gt; \n&gt; 17 Admin 20090927015119 settings logs checkpoints state scratch 0 0 0 100 4096 65536 0 true seeds.txt true ACCEPT true result.txt false true Mozilla/5.0 (compatible; heritrix/@1.14.3@ +http://192.168.0.1) test@... ignore false 5.0 30000 3000 300 30 900 1 0 0 org.archive.crawler.frontier.HostnameQueueAssignmentPolicy false false false true true 3000 100 -1 org.archive.crawler.frontier.UnitCostAssignmentPolicy 300000 50 org.archive.crawler.util.BdbUriUniqFilter false true false false false true 21600 86400 false true false true sha1 true 1200 20000 0 0 false true open ISO-8859-1 true sha1 true true true true false true true true true false true true true true true index.html %2E . true mirror 1023 255 false true LONG true true false true -1 true true false true true\n\nAn actual crawl order would include XML that helps interpret these \nvalues. You must have copied this out of some view that hides that XML. \nIf you need to share an order.xml, you should probably open it directly \nfrom the filesystem in a text editor.\n\n- Gordon @ IA\n\n\n&gt; Thank you in advance.\n&gt; \n&gt; Chuan\n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt; shichuanwuhan@... wrote:\n&gt;&gt;&gt; Hi all,\n&gt;&gt;&gt;\n&gt;&gt;&gt; I am using version 1.14.3. My final goal is to get all the URLs of professors&#39; mainpages on one host i.e www.cs.cmu.edu. So firstly, I plan to fetch all pages that are within the host. However, I fail to do that.\n&gt;&gt;&gt;\n&gt;&gt;&gt; I tried both the traditional &#39;hostscope&#39; and recommended &#39;decidingscope&#39; but I still cannot achieve my goal. As English is not my native language, maybe I misunderstand something in &#39;user manual&#39;. Would someone kindly answer several questions?\n&gt;&gt; The included &#39;deciding-default&#39; profile should work for the purpose of \n&gt;&gt; getting all pages on a single web host.\n&gt;&gt;\n&gt;&gt;&gt; 1.My seed is simply: &#39;http://www.cs.cmu.edu/&#39;. Is it right?\n&gt;&gt; Supplying that as a seed, with the deciding-default settings, should \n&gt;&gt; cause the crawler to:\n&gt;&gt;\n&gt;&gt; (1) start by visiting &quot;http://www.cs.cmu.edu/&quot;, examining the outlinks \n&gt;&gt; of that page for &quot;in-scope&quot; URIs\n&gt;&gt;\n&gt;&gt; (2) evaluate any URIs that begin &quot;http://www.cs.cmu.edu/&quot; as being \n&gt;&gt; &quot;in-scope&quot; (along with some other rules), and thus eligible for \n&gt;&gt; recursive fetching\n&gt;&gt;\n&gt;&gt; The default configuration, with only the single seed, will only wander \n&gt;&gt; off &quot;www.cs.cmu.edu&quot; to fetch URIs that appears necessary to render \n&gt;&gt; another page (like inline references to scripts, images, frames, etc., \n&gt;&gt; or URLs found in Javascript that may auto-load).\n&gt;&gt;\n&gt;&gt; I see that the &#39;faculty&#39; link from &#39;www.cs.cmu.edu&#39; goes to another \n&gt;&gt; host, &#39;people.cs.cmu.edu. Your crawl will not in general visit that \n&gt;&gt; other host without additional scope customization to say those URLs are \n&gt;&gt; of interest. Also, it appears that faculty web pages are on a variety of \n&gt;&gt; hosts (including &#39;www-2&#39; and other departmental servers).\n&gt;&gt;\n&gt;&gt;&gt; 2.Is it possible that I simply use &#39;hostscope&#39; to achieve my goal? \n&gt;&gt; It might be possible, but it is not recommended -- HostScope is \n&gt;&gt; deprecated, less efficient and flexible than the DecidingScope + \n&gt;&gt; SurtPrefixedDecideRule mechanism.\n&gt;&gt;\n&gt;&gt;&gt; 3.If not, how to configure &#39;decidingscope&#39;?\n&gt;&gt; You probably want to tell your crawl that begins at www.cs.cmu.edu that \n&gt;&gt; it may accept URIs on other hosts, like &#39;people.cs.cmu.edu&#39; and others, \n&gt;&gt; as &#39;in-scope&#39;. This involves giving the SurtPrefixedDecideRule more \n&gt;&gt; acceptable &#39;SURT&#39; (URI-like) prefixes.\n&gt;&gt;\n&gt;&gt; This can be done either by specifying a file with such prefixes as the \n&gt;&gt; SurtPrefixedDecideRule&#39;s &#39;surt-source-file&#39;, or by adding lines to your \n&gt;&gt; seeds list that begin &#39;+&#39;.\n&gt;&gt;\n&gt;&gt; You can read more about SURTs as a means of scoping at:\n&gt;&gt;\n&gt;&gt; http://crawler.archive.org/articles/user_manual/config.html#surtprefixscope\n&gt;&gt;\n&gt;&gt; Another strategy might be to start your crawling at the faculty directory:\n&gt;&gt;\n&gt;&gt; http://people.cs.cmu.edu/faculty/index.html\n&gt;&gt;\n&gt;&gt; Supplying that URI as a seed will cause the &quot;implied scope&quot; to be all \n&gt;&gt; URLs beginning &quot;http://people.cs.cmu.edu/faculty/&quot; -- which should get \n&gt;&gt; all the other pages of the directory, as well. *If* you are confident \n&gt;&gt; all homepages always contain the &#39;~&#39; character, you could also add a new \n&gt;&gt; rule to the list of rules, such as a MatchesRegExpDecideRule, that \n&gt;&gt; always ACCEPTs any URI with a &#39;~&#39; character. That would get the \n&gt;&gt; directory, and all the linked pages with &#39;~&#39; anywhere in their URI (and \n&gt;&gt; quite probably other pages, at other hosts and universities, when their \n&gt;&gt; URIs with &#39;~&#39; are discovered).\n&gt;&gt;\n&gt;&gt; Hope this helps,\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}