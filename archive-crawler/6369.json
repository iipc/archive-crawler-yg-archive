{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"PQG4fGCS2DKyuU3N8g7amrqqgFlhfizKmWu1TRDBOI5FVLfKvW_K3qkS8gqrHD52WWignfOaXiSZfLx7CRo_HQ1ST_XSTfM","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] how can I get the queued urls?","postDate":"1265324144","msgId":6369,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRCNkI1MDcwLjgwNzA0MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGhrZTFnYytzbGZtQGVHcm91cHMuY29tPg==","referencesHeader":"PGhrZTFnYytzbGZtQGVHcm91cHMuY29tPg=="},"prevInTopic":6363,"nextInTopic":0,"prevInTime":6368,"nextInTime":6370,"topicId":6363,"numMessagesInTopic":2,"msgSnippet":"... The exact techniques available depend on the version you re using. Some possibilities include: - a frontier dump-pending-at-close /dumpPendingAtClose","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 15746 invoked from network); 4 Feb 2010 22:55:47 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.sp2.yahoo.com with QMQP; 4 Feb 2010 22:55:47 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta3.grp.sp2.yahoo.com with SMTP; 4 Feb 2010 22:55:46 -0000\r\nX-Received: (qmail 56956 invoked from network); 4 Feb 2010 22:55:45 -0000\r\nX-Received: from 71.202.38.39 (HELO ?192.168.23.128?) (71.202.38.39)\n  by relay02.pair.com with SMTP; 4 Feb 2010 22:55:45 -0000\r\nX-pair-Authenticated: 71.202.38.39\r\nMessage-ID: &lt;4B6B5070.8070406@...&gt;\r\nDate: Thu, 04 Feb 2010 14:55:44 -0800\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;hke1gc+slfm@...&gt;\r\nIn-Reply-To: &lt;hke1gc+slfm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] how can I get the queued urls?\r\nX-Yahoo-Group-Post: member; u=137285340; y=8SfaA8lIVJ6Z2hK3FwvS4mzYRgUjegHaiF3WxUvIbvn5\r\nX-Yahoo-Profile: gojomo\r\n\r\nzhongkem@... wrote:\n&gt; after several days running,the size of my heritrix&#39;s queues has become large,more than 4,000,000,I don&#39;t know When I can finish my crawl,maybe several months ..!so I want to reuse this queues,but I don&#39;t know where this queues saved!who can help me?\n\nThe exact techniques available depend on the version you&#39;re using. Some \npossibilities include:\n\n- a frontier &#39;dump-pending-at-close&#39;/dumpPendingAtClose setting, which \ncauses a crawl to unwind all the queues to the crawl.log when \nterminated. (This can take a while!)\n\n- the &#39;view/edit&#39; frontier option in H1&#39;s web UI when the crawl is paused\n\n- analysis of the &#39;recover.gz&#39; log -- which lists each URI that is \nenqueued to the frontier (&#39;F+&#39;) and finished as success or failure \n(&#39;Fs&#39;, &#39;Ff&#39;) -- those left at the end are those added without veing finished\n\n- the H3 scripting console allows arbitrary analysis/extraction of data \nin the running crawl&#39;s data structures via Groovy/Javascript/Beanshell \n(or JRuby or Jython if you choose to install that support)\n\nHope this helps,\n\n- Gordon @ IA\n\n"}}