{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":205318215,"authorName":"Greg Kempe","from":"&quot;Greg Kempe&quot; &lt;thelonghotsummer@...&gt;","profile":"gregkza","replyTo":"LIST","senderId":"N5KICFSfU99PV8nIkGQ8GgsJICiGGEETRSXOqvK41-aqVFc0mK-Mcm7YMRoIrIlK38XS7i6-bV0kewmaIy-h78Komf4gxBZmSrgeFnU7-HM","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Carrying the already-seen list between crawls","postDate":"1148058520","msgId":2870,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGI4MTI1OWEwMDYwNTE5MTAwOHUyMjljZDdmZGtmZjI4MmZmZmZmN2ZlYzE1QG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDQ0NDVEOENBLjMwMjA1MDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PGI4MTI1OWEwMDYwNDE4MTgwN2w3NThiNmJiM3NiZWJhMWNkNWQ2MjhlMDFiQG1haWwuZ21haWwuY29tPgkgPDQ0NDVEOENBLjMwMjA1MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":2792,"nextInTopic":2871,"prevInTime":2867,"nextInTime":2871,"topicId":2791,"numMessagesInTopic":7,"msgSnippet":"Hi Gordon ... Thanks for the tips. I ve got this working well. I found that sorting the list of Fs urls and removing duplicate and dns: entries *substantially*","rawEmail":"Return-Path: &lt;thelonghotsummer@...&gt;\r\nX-Sender: thelonghotsummer@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 23852 invoked from network); 19 May 2006 17:09:15 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m25.grp.scd.yahoo.com with QMQP; 19 May 2006 17:09:15 -0000\r\nReceived: from unknown (HELO py-out-1112.google.com) (64.233.166.180)\n  by mta2.grp.scd.yahoo.com with SMTP; 19 May 2006 17:09:15 -0000\r\nReceived: by py-out-1112.google.com with SMTP id d80so920481pyd\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 19 May 2006 10:08:40 -0700 (PDT)\r\nReceived: by 10.35.22.17 with SMTP id z17mr317519pyi;\n        Fri, 19 May 2006 10:08:40 -0700 (PDT)\r\nReceived: by 10.35.76.16 with HTTP; Fri, 19 May 2006 10:08:40 -0700 (PDT)\r\nMessage-ID: &lt;b81259a00605191008u229cd7fdkff282fffff7fec15@...&gt;\r\nDate: Fri, 19 May 2006 10:08:40 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;4445D8CA.3020507@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nReferences: &lt;b81259a00604181807l758b6bb3sbeba1cd5d628e01b@...&gt;\n\t &lt;4445D8CA.3020507@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Greg Kempe&quot; &lt;thelonghotsummer@...&gt;\r\nSubject: Re: [archive-crawler] Carrying the already-seen list between crawls\r\nX-Yahoo-Group-Post: member; u=205318215; y=JDSs8uwvMLdpnBxewthoVTmQMot-n8uV5HtWs0pjPTIgkQ\r\nX-Yahoo-Profile: gregkza\r\n\r\nHi Gordon\n\n&gt; Another option would be to use the &#39;recover&#39; log; all of the &#39;=\r\nF+&#39;\n&gt; lines are URLs that the crawl considered &#39;seen&#39;.\n&gt;\n&gt; A typical recove=\r\nr-from-log scans the log twice: once to add all\n&gt; the &#39;Fs&#39; (success) URLs t=\r\no the new crawl&#39;s &#39;seen&#39; list, then again\n&gt; to add all the &#39;F+&#39; (scheduled)=\r\n URLs that weren&#39;t already &#39;seen&#39;\n&gt; by the first pass.\n\nThanks for the tips=\r\n. I&#39;ve got this working well.\n\nI found that sorting the list of Fs urls and=\r\n removing duplicate and\ndns: entries *substantially* improves the time take=\r\nn to read them and\nadd them to the already-seen list. It also means the fil=\r\ne gzips to\nabout 1/10th of the original size, which is a bonus.\n\nOn a relat=\r\ned note, how possible is it for a BdbFrontier to recover\nusing only the bdb=\r\n environment? That is, how important is the\norg.archive.crawler.frontier.Bd=\r\nbFrontier.serialized file created\nduring a checkpoint (or any of the other =\r\n.serialized files, for that\nmatter)? If the crawler could resume using only=\r\n the bdb environment,\nexplicit checkpoints could be optional. In such a cas=\r\ne things like\nstatistics may be lost, but at least the already-seen list an=\r\nd\nfrontier would be recoverable. The alternative -- using recover.gz --\nlos=\r\nes the same data but is more cumbersome to resume from.\n\nBtw, thanks for th=\r\ne rapid and useful responses. I&#39;ve found the\nInternet Archive staff to be v=\r\nery attentive and helpful.\n\nThanks\n\nGreg\n\n"}}