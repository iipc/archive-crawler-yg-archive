{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"V9mxHMIu84-6zM0imIsfxhzoxNyKLKbgdfuMUwsHu1mk171fgkTy6EyjHNRvdpZ4NZ7uvULjWQvWw69Vzq8EjgcDN-0hbNU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] why down load rate reduced?","postDate":"1302624413","msgId":7096,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREQTQ3ODlELjUwOTA5QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDgyODM0Ni41MTgyOC5xbUB3ZWIxMjE2MDcubWFpbC5uZTEueWFob28uY29tPg==","referencesHeader":"PDgyODM0Ni41MTgyOC5xbUB3ZWIxMjE2MDcubWFpbC5uZTEueWFob28uY29tPg=="},"prevInTopic":7095,"nextInTopic":0,"prevInTime":7095,"nextInTime":7097,"topicId":7095,"numMessagesInTopic":2,"msgSnippet":"Please check this FAQ area of the project wiki for ideas: https://webarchive.jira.com/wiki/display/Heritrix/crawl+rate+considerations - Gordon @ IA","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 75914 invoked from network); 12 Apr 2011 16:06:55 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m3.grp.sp2.yahoo.com with QMQP; 12 Apr 2011 16:06:55 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.sp2.yahoo.com with SMTP; 12 Apr 2011 16:06:55 -0000\r\nX-Received: (qmail 54311 invoked by uid 0); 12 Apr 2011 16:06:53 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 12 Apr 2011 16:06:53 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4DA4789D.50909@...&gt;\r\nDate: Tue, 12 Apr 2011 09:06:53 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.15) Gecko/20110303 Thunderbird/3.1.9\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: David Stafen &lt;stafend@...&gt;\r\nReferences: &lt;828346.51828.qm@...&gt;\r\nIn-Reply-To: &lt;828346.51828.qm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] why down load rate reduced?\r\nX-Yahoo-Group-Post: member; u=137285340; y=rMrXgPW5oxobVneIH5C4M9folZcCmHwXTTnXOiotPbRW\r\nX-Yahoo-Profile: gojomo\r\n\r\nPlease check this FAQ area of the project wiki for ideas:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/crawl+rate+considerations\n\n- Gordon @ IA\n\nOn 4/11/11 5:59 AM, David Stafen wrote:\n&gt;\n&gt;\n&gt; hello\n&gt; I am using Heritrix 1.14.4 and i want to crawl about 200 million web\n&gt; pages(only text/html).\n&gt; But there is a problem, at the begining of the crawling process the\n&gt; download rate is high(My Bandwidth is 10 Mb /sec) and i can download 20\n&gt; - 30 webpages per second but as times goes up the downrate reduced. My\n&gt; crawler system has 16Gbye RAM and i reserved 10Gb For heap space. i\n&gt; assigned 70 threads to heritrix. but interesting point is that when\n&gt; 5Gbyte of heap space is Used the download rate is low(13 URL per second)\n&gt; and only 23 million webpages is queued and 8 million web pages is\n&gt; downloaded but my doanload rate is low. where is the problem!!!, did i\n&gt; need a system with higher resources. what resources are desire to crawl\n&gt; 200,000 web pages? (after i analysis the out put i found that we crawled\n&gt; 900,000 different domain).\n&gt;\n&gt;\n&gt; \n\n"}}