{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"2RDY-zNQBQ4TGsBuD3kjEpJt8EGThR_J6advq7Vw-2UychMZPfyFgdcxIo-9XBaEddycfiTB2If7mgVVdi8S94d_fktwVUg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] 99 % collected ?","postDate":"1082943181","msgId":344,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwOEM2NkNELjgwNzAxMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGM2Z2VmYytya3YyQGVHcm91cHMuY29tPg==","referencesHeader":"PGM2Z2VmYytya3YyQGVHcm91cHMuY29tPg=="},"prevInTopic":342,"nextInTopic":348,"prevInTime":343,"nextInTime":345,"topicId":342,"numMessagesInTopic":7,"msgSnippet":"... This is currently possible, but you have to pause the crawler first. Once all threads have paused, a new option will appear on the console near the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 54712 invoked from network); 26 Apr 2004 01:33:05 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m12.grp.scd.yahoo.com with QMQP; 26 Apr 2004 01:33:05 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta6.grp.scd.yahoo.com with SMTP; 26 Apr 2004 01:33:05 -0000\r\nReceived: (qmail 12869 invoked by uid 100); 26 Apr 2004 01:27:15 -0000\r\nReceived: from b116-dyn-47.archive.org (HELO archive.org) (gojomo@...@209.237.240.47)\n  by mail-dev.archive.org with SMTP; 26 Apr 2004 01:27:15 -0000\r\nMessage-ID: &lt;408C66CD.8070108@...&gt;\r\nDate: Sun, 25 Apr 2004 18:33:01 -0700\r\nUser-Agent: Mozilla Thunderbird 0.5 (X11/20040208)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c6gefc+rkv2@...&gt;\r\nIn-Reply-To: &lt;c6gefc+rkv2@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-0.0 required=6.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] 99 % collected ?\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nkaisa_kaunonen wrote:\n&gt; Hi all,\n&gt; when crawling a small number of domains it&#39;s easy to get the\n&gt; downloaded/discovered ratio to 95-98% in half a days time. But the end\n&gt; is slow.\n&gt; \n&gt; It often looks like there&#39;s one domain left and one thread fetches a\n&gt; single doc every now and then. When downloaded/discovered stays at 99%\n&gt; for a day I usually terminate the job and think that a couple of docs\n&gt; remained somewhere on the way. \n&gt; \n&gt; But then I wonder how much remains uncollected and what paths might\n&gt; have opened up. For one job the rate has been 99% (16209/16241) over\n&gt; 24 hours. I somehow think this means about 30 docs uncollected. But\n&gt; &#39;Crawl report&#39; page says\n&gt; \n&gt; URIs\n&gt; Discovered: 16241\n&gt; Queued: -178 (queue size &lt;&gt; 30)\n&gt; Finished: Total 16419   Success 16209  Failed 159 Disregarded 51 \n&gt; \n&gt; It would be nice to see all URIs waiting in queue before terminating\n&gt; jobs. Frontier report shows some queues but only their top URIs?\n\nThis is currently possible, but you have to &#39;pause&#39; the crawler first.\nOnce all threads have paused, a new option will appear on the &#39;console&#39;\nnear the other control (stop/terminate/refresh/etc) options: &#39;Inspect\nfrontier URIs&#39;.\n\nThere, you can supply a (java) regular expression to page through\nall pending URIs, and also delete URIs by regular expression if\nnecessary.\n\nWhen done, you can resume the crawl after any changes.\n\nBefore this facility was available, we would rely on watching the\nend of the crawl-log, and the frontier report. When the tail\nactivity was all repetitive, and the frontier only showed pending\nURIs on the problem site, we&#39;d wait a while to see if any novelty\nappeared in the URIs it was crawling but if not, presume all the\npending URIs were like the ones recently completed.\n\n&gt; ****\n&gt; I&#39;ve haven&#39;t been caught in loops lately, maybe because I&#39;ve added\n&gt; some filters to profile. But it&#39;s sometimes hard to guess in advance\n&gt; what might cause a loop. Couldn&#39;t heritrix monitor itself =&gt; when one\n&gt; domain starts to fill queues heritrix could move URIs from other\n&gt; domains upwards and leave the possible loop causing URIs for a later\n&gt; time?\n\nWe have started to add to the frontier/queuing classes two facilities\nfor doing this, one at the URI level and another at the hostname\n(work queue) level. It is not yet functional, and won&#39;t have any\ninterface by the upcoming 1.0 release, but it would work something\nlike:\n\n  - If a URI is classified as so suspicious it should not be\n    fetched without operator approval, it will be added to a\n    side &#39;frozen&#39; queue for its host. In this way, it will neither\n    be tried, nor discarded, but held until the operator makes\n    a decision about it. The UI would highlight the total number\n    of &#39;frozen&#39; URIs and let the operator view/reschedule/delete\n    them as appropriate.\n\n  - If a host becomes problematic or suspicious -- for example\n    if all attempts to connect to it fail, or something about\n    the rate/novelty/pattern of new URIs discovered indicates\n    serious problems, then its entire queue can be &#39;frozen&#39;, and\n    an alert provided to the operator. Then, no further attempts\n    to fetch URIs for that host will occur, but all current and\n    newly-discovered URIs will continue to be collected, so that\n    if the operator &#39;unfreezes&#39; the queue progress will continue.\n\nThe two facilities, while similar, are independent: a particular\nhost&#39;s work queue could be going fine, but then send a few odd\nURIs off to the &#39;frozen&#39; queue, but continue fetching legitimate-\nseeming URIs. Then, due to some new error condition, the whole\nhost queue could be &#39;frozen&#39; in place. If the operator &#39;unfroze&#39;\nthe host queue, the individually frozen URIs would still be on\nhold. Conversely, if the operator approved the individually-frozen\nURIs, but left the host queue frozen, the newly approved individual\nURIs would simply wait for the queue to be reactivated.\n\n- Gordon @ IA\n\n"}}