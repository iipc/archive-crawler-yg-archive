{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"TsYHFDayYbO8GH3LSUW-LKKJc5Da8_Ls796v92IbMLrZmYZY_WjyNaYQSgMf6mypEJSad_SF4K1Ymt9zYvzi5ut6BPJCm3ON","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: small amount of crawled documents and -6 error in logs","postDate":"1154638322","msgId":3156,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0RDI2MUYyLjkwMTA4QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGVhc2M2Nyt1ZzBjQGVHcm91cHMuY29tPg==","referencesHeader":"PGVhc2M2Nyt1ZzBjQGVHcm91cHMuY29tPg=="},"prevInTopic":3153,"nextInTopic":0,"prevInTime":3155,"nextInTime":3158,"topicId":3140,"numMessagesInTopic":5,"msgSnippet":"... Hard to say.  Start with default and work your way up.  See how your throughput changes.  Watch net and disk i/o and your CPU consumption. Try and balance","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 86732 invoked from network); 3 Aug 2006 20:50:54 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m27.grp.scd.yahoo.com with QMQP; 3 Aug 2006 20:50:54 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta1.grp.scd.yahoo.com with SMTP; 3 Aug 2006 20:50:54 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id E0416141569B9;\n\tThu,  3 Aug 2006 13:50:58 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 15921-01-37; Thu, 3 Aug 2006 13:50:55 -0700 (PDT)\r\nReceived: from [192.168.1.204] (c-71-198-60-165.hsd1.ca.comcast.net [71.198.60.165])\n\tby mail.archive.org (Postfix) with ESMTP id C79D11415692C;\n\tThu,  3 Aug 2006 13:50:55 -0700 (PDT)\r\nMessage-ID: &lt;44D261F2.90108@...&gt;\r\nDate: Thu, 03 Aug 2006 13:52:02 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.2) Gecko/20060405 SeaMonkey/1.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;easc67+ug0c@...&gt;\r\nIn-Reply-To: &lt;easc67+ug0c@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: small amount of crawled documents and -6\n error in logs\r\nX-Yahoo-Group-Post: member; u=168599281; y=ubNDh_qsCzawLW7ppB6bu35Vzx2GdPV7w-63DsKKiie_DLZfUIlfG6TW\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\ngoblin_cz wrote:\n&gt;\n&gt; Sorry, I forgotten...\n&gt;\n&gt; How many Toe Threads (max-toe-threads) can be on broad crawl with 4GB\n&gt; RAM and Pentium III 900MHz?\n&gt;\n\n\n\n\n\nHard to say.  Start with default and work your way up.  See how your \nthroughput changes.  Watch net and disk i/o and your CPU consumption.  \nTry and balance the 3.  Try and not have resources dedicated maximally \nto downloading rather than uselessly swapping context.  In the past, \ncrawling against a server giving out infinite URLs (hosts and links) \nrunning all on the one network, there was a sweet spot where throughput \nwas at a maximum.  But crawling the open net is another kettle of fish \nand will be harder to tune.\n\n\n&gt;\n&gt; And what exactly mean seeds.ignored - it appears after start of the\n&gt; crawl and there is only my added surt prefix +http://(cz,\n&gt;\n\n\n\n\n\nAre you talking about emission like that of line #92,  \nhttp://crawler.archive.org/xref/org/archive/crawler/scope/SeedFileIterator.html#92, \nwhere its saying a &#39;line in the seed file&#39; has been ignored?  (Its \nignored because it starts with something like &#39;#&#39;, perhaps a comment \nabout a seed redirect added by a previous Heritrix crawl?).\n\n&gt;\n&gt; Have a nice day.\n&gt;\n\n\n\n\nYou too.\n\nSt.Ack\n\n\n\n&gt;\n&gt;  \n\n\n"}}