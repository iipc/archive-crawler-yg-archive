{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Dc6dbquSjaKfojSok-l5_3AYZijwq7FyWB8sMS5GtCyWHz31AO26ldZGNdpg5lcnTQ92wyrWPkI5HanQa1Dk8H82xBOaMig","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re-discover links","postDate":"1299116115","msgId":7055,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENkVGMDUzLjIwNzA2MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDUyODk2Mi43NDU2MS5xbUB3ZWIxNTkwOC5tYWlsLmNuYi55YWhvby5jb20+","referencesHeader":"PDUyODk2Mi43NDU2MS5xbUB3ZWIxNTkwOC5tYWlsLmNuYi55YWhvby5jb20+"},"prevInTopic":7054,"nextInTopic":7058,"prevInTime":7054,"nextInTime":7056,"topicId":7044,"numMessagesInTopic":11,"msgSnippet":"... Outlinks (specifically outlinks) are not logged to crawl.log. The crawl.log is a log of *completed* URIs. So to appear in crawl.log, a URI has to: (1) be","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 21445 invoked from network); 3 Mar 2011 01:35:18 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m14.grp.re1.yahoo.com with QMQP; 3 Mar 2011 01:35:18 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta3.grp.re1.yahoo.com with SMTP; 3 Mar 2011 01:35:17 -0000\r\nX-Received: (qmail 34887 invoked by uid 0); 3 Mar 2011 01:35:15 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay01.pair.com with SMTP; 3 Mar 2011 01:35:15 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4D6EF053.2070605@...&gt;\r\nDate: Wed, 02 Mar 2011 17:35:15 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.14) Gecko/20110221 Thunderbird/3.1.8\r\nMIME-Version: 1.0\r\nTo: HONGYING YI &lt;y195322@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;528962.74561.qm@...&gt;\r\nIn-Reply-To: &lt;528962.74561.qm@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re-discover links\r\nX-Yahoo-Group-Post: member; u=137285340; y=3uAV_d005w4RBfTjxfpwgCiWmSb-_veXtd9Tu-Meh23I\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 3/2/11 3:47 PM, HONGYING YI wrote:\n&gt; Hi Gordon,\n&gt;\n&gt; Thanks for your reply!\n&gt;\n&gt; I&#39;m still a little confused. Are all the outlinks of a URI logged in the\n&gt; crawl.log whether or not those outlinks have already been discovered?\n\nOutlinks (specifically outlinks) are not logged to crawl.log. The \ncrawl.log is a log of *completed* URIs. So to appear in crawl.log, a URI \nhas to:\n\n(1) be discovered (as a seed or outlink from elsewhere); then...\n(2) pass scoping rules, so that it is enqueued to be fetched; then...\n(3) either be successfully fetched, or suffer definitive failure \n(certain errors from the server or no-success after many retries).\n\nThis crawl.log logging happens arbitrarily long after the URI was first \ndiscovered. (The URI might be waiting in a queue for its chance to be \ncrawled for hours, days, weeks, months.)\n\nIn a typical snapshot crawl, most normal URIs are only fetched once, so \nthey only appear in the crawl.log once. (The &#39;via&#39; URI that is shown \nlater on the crawl.log line is the one other URI from which it was \nspecifically discovered first, even though it may have later been found \nmany times.)\n\n(URIs which must be retried to refresh our knowledge, like DNS and \n/robots.txt, will be fetched and appear in crawl.log multiple times. \nAlso, there are certain non-standard ways you can force a URI to be \nrefetched. But you wouldn&#39;t use those mechanisms to achieve what I \nbelieve is your goal, discovering all the links to a page.)\n\n&gt; I&#39;m not familiar with WARCWriterProcessor, could you explain more on that?\n\nIn Heritrix 1.x releases, the default way to write web content was the \nARCWriterProcessor, which wrote HTTP responses to long transcript files \ncalled ARCs (with extension .arc.gz). Since only the raw content was \nwritten, to get the outlinks again at a later time, you&#39;d need to parse \nthe HTML again.\n\nIn Heritrix 3, a new &#39;WARC&#39; format is preferred, via the \nWARCWriterProcessor. While similar to the ARC format it has more places \nfor extra metadata. By default, it writes not just the HTTP response \n(like in ARC) but also the exact HTTP request and an extra &#39;metadata&#39; \nrecord with a list of the outlinks that were discovered. Outlinks are \nlisted here even if they did not pass scope-testing (so would not be \nenqueued/fetched and thus would never appear in the crawl.log).\n\nYou can use WARCWriterProcessor in H1, also, by replacing the reference \nto ARCWriterProcessor with WARCWriterProcessor.\n\nEven if you don&#39;t want to use WARCWriterProcessor, you could look at its \nsource code to see how it looks at the list of discovered outlinks, and \nlog them to someplace of your choosing, by writing your own custom \nProcessor code to plug into Heritrix.\n\nHope this helps,\n\n- Gordon @ IA\n\n&gt; Thanks!\n&gt;\n&gt; --- *11年3月1日，周二, Gordon Mohr /&lt;gojomo@...&gt;/* 写道：\n&gt;\n&gt;\n&gt;     发件人: Gordon Mohr &lt;gojomo@...&gt;\n&gt;     主题: Re: [archive-crawler] Re-discover links\n&gt;     收件人: &quot;HONGYING YI&quot; &lt;y195322@...&gt;\n&gt;     抄送: archive-crawler@yahoogroups.com\n&gt;     日期: 2011年3月1日,周二,上午7:40\n&gt;\n&gt;     All the outlinks discovered from a URI are inside the CrawlURI during\n&gt;     its processing, until it is finished (and logged in the crawl.log).\n&gt;\n&gt;     If you use the WARCWriterProcessor, they are part of the data\n&gt;     written in\n&gt;     the &#39;metadata&#39; record associated with the URI.\n&gt;\n&gt;     Alternatively, you could insert your own custom processor to log this\n&gt;     data somewhere else (using the WARCWriterProcessor&#39;s writeMetadata\n&gt;     method as a model).\n&gt;\n&gt;     In this way, you can get the link data you want, without revisiting the\n&gt;     same URLs many times (indeed infinitely via link-loops!).\n&gt;\n&gt;     - Gordon @ IA\n&gt;\n&gt;     On 2/28/11 3:16 PM, HONGYING YI wrote:\n&gt;      &gt; Hi Gordon,\n&gt;      &gt; We want to know all the out-going links and in-coming links for each\n&gt;      &gt; page, and in the example, for aol.com, aolhealth.com is an out-going\n&gt;      &gt; link for aol.com, and drugchecker.aolhealth.com is an in-coming\n&gt;     link for\n&gt;      &gt; aol.com. If the link from drugchecker.aolhealth.com to aol.com\n&gt;     does not\n&gt;      &gt; show up, we will not get all the in-coming links for aol.com. Is\n&gt;     there\n&gt;      &gt; any way to retrieve that link? Can any configuration setting in the\n&gt;      &gt; crawl get round that?\n&gt;      &gt; Thanks!\n&gt;      &gt; Yang\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt; --- *11年3月1日，周二, Gordon Mohr /&lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;/*\n&gt;     写道：\n&gt;      &gt;\n&gt;      &gt;\n&gt;      &gt; 发件人: Gordon Mohr &lt;gojomo@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=gojomo@...&gt;&gt;\n&gt;      &gt; 主题: Re: [archive-crawler] Re-discover links\n&gt;      &gt; 收件人: archive-crawler@yahoogroups.com\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=archive-crawler@yahoogroups.com&gt;\n&gt;      &gt; 抄送: &quot;y195322&quot; &lt;y195322@...\n&gt;     &lt;http://cn.mc159.mail.yahoo.com/mc/compose?to=y195322@...&gt;&gt;\n&gt;      &gt; 日期: 2011年3月1日,周二,上午6:33\n&gt;      &gt;\n&gt;      &gt; On 2/28/11 10:20 AM, y195322 wrote:\n&gt;      &gt; &gt; Hi!We are doing web crawling using Heritrix, and found one issue\n&gt;      &gt; in that once a link has been &#39;discovered,&#39; it won&#39;t be\n&gt;      &gt; &#39;rediscovered.&#39; This means that if we have something like:\n&gt;      &gt; &gt; aol.com-&gt;aolhealth.com\n&gt;      &gt; &gt; aolhealth.com-&gt;drugchecker.aolhealth.com\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; and if drugchecker.aolhealth.com links to aol.com, that link will\n&gt;      &gt; not show up in the file.\n&gt;      &gt; &gt;\n&gt;      &gt; &gt; Is there any way to fix this problem? Thanks!\n&gt;      &gt;\n&gt;      &gt; Usually a single snapshot crawl only wants to retrieve each URL\n&gt;      &gt; once, so\n&gt;      &gt; this is by-design, rather than a problem.\n&gt;      &gt;\n&gt;      &gt; Why do you want to fetch aol.com twice in a single crawl? (Or am I\n&gt;      &gt; misunderstanding?)\n&gt;      &gt;\n&gt;      &gt; - Gordon @ IA\n&gt;      &gt;\n&gt;      &gt;\n&gt;\n&gt;\n\n"}}