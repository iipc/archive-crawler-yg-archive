{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"9a3N5KyjeWFuv1KUhs5apLGFmLUhYjZASfWE37cl6ovgoYouMfI1jfasCqPhrSDm36bifs1HCNlqdFqxBN3qSXXoNaHWYIo5","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Advice needed on how to (properly) structure new Heritrix modify and delete functionality","postDate":"1153326074","msgId":3086,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkU1QkZBLjgwNDAzMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIwMDYwNzE5MDgyMDE0LjlCRDQ3MTQxNTZBOTZAbWFpbC5hcmNoaXZlLm9yZz4=","referencesHeader":"PDIwMDYwNzE5MDgyMDE0LjlCRDQ3MTQxNTZBOTZAbWFpbC5hcmNoaXZlLm9yZz4="},"prevInTopic":3085,"nextInTopic":3087,"prevInTime":3085,"nextInTime":3087,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"... To be determined but coarsely, we d start in September.  Beginning of next year we d deliver means of avoiding exact duplicates.  6 months after that,","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60465 invoked from network); 19 Jul 2006 16:34:44 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m41.grp.scd.yahoo.com with QMQP; 19 Jul 2006 16:34:44 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta2.grp.scd.yahoo.com with SMTP; 19 Jul 2006 16:34:43 -0000\r\nReceived: from [192.168.1.106] ([192.168.1.106])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k6JF4Xk15499;\n\tWed, 19 Jul 2006 08:04:33 -0700\r\nMessage-ID: &lt;44BE5BFA.8040306@...&gt;\r\nDate: Wed, 19 Jul 2006 09:21:14 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.4) Gecko/20060516 SeaMonkey/1.0.2\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;20060719082014.9BD4714156A96@...&gt;\r\nIn-Reply-To: &lt;20060719082014.9BD4714156A96@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Advice needed on how to (properly) structure\n new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=168599281; y=eicw5XKpFZNoKGtGDtXwT3_7tLZcrXGVUW2-w7Cp-65DDWKWjrfJz_s0\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nKristinn Sigurï¿½sson wrote:\n&gt;\n&gt;\n&gt; ...\n&gt; &gt; We&#39;ve been talking here at the Archive on how to broach the\n&gt; &gt; topics under discussion here: duplicate/near-duplicate\n&gt; &gt; detection, change-detection, junk-avoidance, etc. Here are a\n&gt; &gt; few notes on what we&#39;ve been thinking.\n&gt; &gt; They pertain to Karl and Kris&#39;s discussion so I&#39;ll add them\n&gt; &gt; in here under the same subject (Implementation is probably\n&gt; &gt; too far off to solve Karl&#39;s needs).\n&gt;\n&gt; Just out of curiosity, what kind of timeframe are you looking at?\n&gt;\n\n\n\n\n\n\n\n\n\n\n\nTo be determined but coarsely, we&#39;d start in September.  Beginning of \nnext year we&#39;d deliver means of avoiding exact duplicates.  6 months \nafter that, off-line processing of previous crawl data will be used to \ninfluence what the crawler fetches next (&quot;Is the current page \n&#39;important&#39;?&quot;).  Then we&#39;d work on detecting change rates to be \ndelivered toward the end of 2007.  Interlaced will be delivery on \nnear-duplicate avoidance, etc.\n\nWe&#39;ll update the list as plans and designs harden.\n\n&gt;\n&gt; &gt; As per Kris&#39;s experience, crawl-time duplicate/change\n&gt; &gt; detection is hard. We&#39;re thinking, like Kris, that at first,\n&gt; &gt; we&#39;ll do this by post-processing crawls. The product of\n&gt; &gt; post-processing will be distilled data-structures that the\n&gt; &gt; crawler can do fast lookups against as its crawling to ask\n&gt; &gt; such questions as: &quot;Is this a duplicate?&quot;; &quot;Is this a near\n&gt; &gt; duplicate?&quot;; &quot;Is this page junk?&quot;; &quot;Is this page\n&gt; &gt; &#39;important&#39;?&quot;; and &quot;Has this page changed?&quot;. Or, more likely,\n&gt; &gt; we&#39;ll aggregate all URL evaluation &#39;plugins&#39; and a crawler\n&gt; &gt; will simply ask, &quot;Should I crawl this?&quot; and get back an\n&gt; &gt; &#39;evaluation&#39; that falls somewhere between 0 and 1 with a\n&gt; &gt; configurable threshold in the crawler saying what level of\n&gt; &gt; evaluations it&#39;ll pursue.\n&gt;\n&gt; Sounds like the DeDuplicator on steroids. Good stuff. Any thoughts on \n&gt; how to\n&gt; rule things as junk? That might be one of the most interesting \n&gt; applications\n&gt; of this.\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMany of the techniques discussed in the literature seem prohibitively \nexpensive resource-wise (e.g. using classification).  TBD.\n\nShould we keep a wiki page to keep a database of junk types observed by \npeople on the list with suggestions for avoidance just as this page, \nhttp://crawler.archive.org/cgi-bin/wiki.pl?TrapDetectionIdeas, lists \ntraps and ways to avoid traps (much of which is yet to be developed)?\n\n....\n&gt;\n&gt;\n&gt; &gt; Blue-skying it, if we could agree on a bulk processing\n&gt; &gt; platform, such as hadoop, and if we could agree on how to\n&gt; &gt; package the fast-lookups -- e.g.\n&gt; &gt; all implement a Lookup Interface that takes a (shrunken?)\n&gt; &gt; CrawlURI returning an object that had a &#39;score&#39; and\n&gt; &gt; &#39;rationale&#39; -- then it seems like we could collaborate/split-the-work.\n&gt;\n&gt; Just how big do you expect the indexes to get? Also, I&#39;ve purposely gone\n&gt; after &#39;high yield&#39; documents (i.e. non text) that serves to limit the \n&gt; number\n&gt; of entries in the index and the number of lookups at crawl time. I take it\n&gt; you are looking for a more complete solution?\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nShould be able to hold the history of multiple country-scale crawls \n(100s of millions).\n\nSt.Ack\n\n\n\n\n\n\n&gt;\n&gt; - Kris\n&gt;\n&gt;  \n\n\n"}}