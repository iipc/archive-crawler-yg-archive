{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"YSaNSZznmGbd-7gCU9BmwT7PWBCqohUXkbKZE5YE79B1XLMrBuTgbuLKEMfq4yn1wkbpB7N8bzqVWmJS-OlcM_rWvBTXCAs","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Crawler running with less than configured threads.","postDate":"1327025096","msgId":7542,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMThDQkM4LjIwNzA1QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDRGMTcyNTMyLjEwNzAxMDlAY3MuY211LmVkdT4=","referencesHeader":"PDRGMDVFOUFELjkwMTA5MDNAY3MuY211LmVkdT4gPDRGMDZCQUZCLjgwMTA0MDJAYXJjaGl2ZS5vcmc+IDw0RjA3MjM5My4yMDYwNzAwQGNzLmNtdS5lZHU+IDw0RjA5RjU0OC40MDQwNzA1QGFyY2hpdmUub3JnPiA8NEYwQjIwQjkuODA1MDQwMUBjcy5jbXUuZWR1PiA8NEYwQjczOUUuMjA5MDcwNUBhcmNoaXZlLm9yZz4gPDRGMTcyNTMyLjEwNzAxMDlAY3MuY211LmVkdT4="},"prevInTopic":7533,"nextInTopic":7628,"prevInTime":7541,"nextInTime":7543,"topicId":7493,"numMessagesInTopic":14,"msgSnippet":"You would need to use all the independent segments to simulate the entire prior crawl completion/discovery/enqueuing history. You should concatenate them all","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 51082 invoked from network); 20 Jan 2012 02:04:59 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m8.grp.sp2.yahoo.com with QMQP; 20 Jan 2012 02:04:59 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta3.grp.sp2.yahoo.com with SMTP; 20 Jan 2012 02:04:59 -0000\r\nX-Received: (qmail 33000 invoked by uid 0); 20 Jan 2012 02:04:57 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay00.pair.com with SMTP; 20 Jan 2012 02:04:57 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4F18CBC8.20705@...&gt;\r\nDate: Thu, 19 Jan 2012 18:04:56 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: David Pane &lt;dpane@...&gt;, archive-crawler@yahoogroups.com\r\nReferences: &lt;4F05E9AD.9010903@...&gt; &lt;4F06BAFB.8010402@...&gt; &lt;4F072393.2060700@...&gt; &lt;4F09F548.4040705@...&gt; &lt;4F0B20B9.8050401@...&gt; &lt;4F0B739E.2090705@...&gt; &lt;4F172532.1070109@...&gt;\r\nIn-Reply-To: &lt;4F172532.1070109@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: Crawler running with less than configured threads.\r\nX-Yahoo-Group-Post: member; u=137285340; y=aLyodgF_OLntUom99tEP45vwXEibGXYfZeTrN2tgwYIl\r\nX-Yahoo-Profile: gojomo\r\n\r\nYou would need to use all the independent segments to simulate the \nentire prior crawl completion/discovery/enqueuing history.\n\nYou should concatenate them all together, or if grooming them beforehand \nto more quickly feed a smaller number of (first) &#39;include&#39; and (second) \n  &#39;schedule&#39; URIs, at least make sure all &#39;include&#39; segments are \nsupplied first, then all &#39;schedule&#39; segments.\n\n- Gordon\n\nOn 1/18/12 12:01 PM, David Pane wrote:\n&gt; Gordon,\n&gt;\n&gt; A couple questions about using the frontier.recover.gz file.\n&gt;\n&gt; 1) In the full recover process, do I need to send all of the\n&gt; frontier.recover.gz.cp00* files or just the last checkpoint? I had\n&gt; stopped the crawl and restarted on Jan 05 so do I need include the\n&gt; frontier.recover.gz.cp00* checkpoints from the previous run as well?\n&gt;\n&gt; 2) If using the split recover, (and if you need to include all of the\n&gt; frontier.recover.gz.cp00* files) can you concatenate all of the files\n&gt; into one?\n&gt;\n&gt; --David\n&gt;\n&gt;\n&gt;\n&gt;&gt;&gt; If the crawler does crash or needs to be stopped and we cannot recover\n&gt;&gt;&gt; from the last checkpoint (or any recent checkpoint) what are our\n&gt;&gt;&gt; options? Do we have to start from an old checkpoint and recrawl all of\n&gt;&gt;&gt; the previously collected pages and data after that old recoverable\n&gt;&gt;&gt; checkpoint?\n&gt;&gt;\n&gt;&gt; Resuming from a known-good checkpoint is best from the standpoint of\n&gt;&gt; perfectly picking up from that self-consistent point.\n&gt;&gt;\n&gt;&gt; The older &#39;recovery log&#39; technique can approximate the crawl state at\n&gt;&gt; other points, at least with respect to URI discovery/completion.\n&gt;&gt; Essentially, from the &#39;frontier-recover&#39; log, this process first treats\n&gt;&gt; all previously-completed URIs as discovered (loads up the &#39;already-seen&#39;\n&gt;&gt; UriUniqFilter). Then, the process reconsiders all URIs discovered in the\n&gt;&gt; earlier run(s). Those that were completed get skipped (because of the\n&gt;&gt; first step), those that weren&#39;t are reenqueued in vaguely the same order\n&gt;&gt; as originally discovered.\n&gt;&gt;\n&gt;&gt; Unfortunately this process can take hours even in a moderately-sized\n&gt;&gt; crawl. For yours it might take days (or weeks), and it&#39;s not itself\n&gt;&gt; checkpointable (or optimized in other ways). It doesn&#39;t restore all\n&gt;&gt; running state for reporting purposes.\n&gt;&gt;\n&gt;&gt; If you groom the logs a bit beforehand (for example not even bothering\n&gt;&gt; to include lines not needed in each step) you can save some of the work,\n&gt;&gt; speeding the process.\n&gt;&gt;\n&gt;&gt; Some notes on this process that could get you started if you need to use\n&gt;&gt; this approach:\n&gt;&gt;\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt;&gt;\n&gt;&gt; - Gordon\n\n"}}