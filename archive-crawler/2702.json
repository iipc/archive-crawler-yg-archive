{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":70288271,"authorName":"seamuslawless","from":"&quot;seamuslawless&quot; &lt;seamuslawless@...&gt;","profile":"seamuslawless","replyTo":"LIST","senderId":"ElULcTIIjdJzDpoROs0UogVH3gfzSxsUw_ZGBTLX6AQOOLNpVF11aFeJNaBFFD9XNA23MBvZgv39iD9GeHGh2uQE3-DYn59PeS-V1hT8co0","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Metadata Harvesting","postDate":"1140716730","msgId":2702,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR0a3NicSs3NXVqQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQzRkRFQkU2LjMwMzA4MDBAYXJjaGl2ZS5vcmc+"},"prevInTopic":2699,"nextInTopic":0,"prevInTime":2701,"nextInTime":2703,"topicId":2698,"numMessagesInTopic":3,"msgSnippet":"Thanks for your help this. You re correct in that I only need the metadata tags from the HTML, I don t require the actual resource itself to be harvested. I","rawEmail":"Return-Path: &lt;seamuslawless@...&gt;\r\nX-Sender: seamuslawless@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 31478 invoked from network); 23 Feb 2006 17:45:39 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m35.grp.scd.yahoo.com with QMQP; 23 Feb 2006 17:45:39 -0000\r\nReceived: from unknown (HELO n3a.bullet.scd.yahoo.com) (66.94.237.37)\n  by mta8.grp.scd.yahoo.com with SMTP; 23 Feb 2006 17:45:39 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n3.bullet.scd.yahoo.com with NNFMP; 23 Feb 2006 17:45:30 -0000\r\nReceived: from [66.218.66.92] by t5.bullet.scd.yahoo.com with NNFMP; 23 Feb 2006 17:45:30 -0000\r\nDate: Thu, 23 Feb 2006 17:45:30 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;dtksbq+75uj@...&gt;\r\nIn-Reply-To: &lt;43FDEBE6.3030800@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;seamuslawless&quot; &lt;seamuslawless@...&gt;\r\nSubject: Re: Metadata Harvesting\r\nX-Yahoo-Group-Post: member; u=70288271; y=AP6pczYUfXt_vloe_87M1w4rrLoaFp9Oo7TwMWUbiLVr-UV9HbQrsg\r\nX-Yahoo-Profile: seamuslawless\r\n\r\n\nThanks for your help this. You&#39;re correct in that I only need the\nmetadata=\r\n tags from the HTML, I don&#39;t require the actual resource\nitself to be harve=\r\nsted.\n\nI will keep searching to see if I can find a crawler that extracts t=\r\nhe\nmetadata tags, I think it would be easier to manipulate that crawler\nrat=\r\nher than coding extractors and writers for heritrix or nutch. If\nnot I migh=\r\nt be asking for more advice on that option ; )\n\nThanks again,\n\nShay\n\n--- In=\r\n archive-crawler@yahoogroups.com, stack &lt;stack@...&gt; wrote:\n&gt;\n&gt; seamuslawles=\r\ns wrote:\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt;\n&gt; &gt; I want to create a cache of metadata descripti=\r\nons of internet pages. I\n&gt; &gt; need a crawler that will allow me to harvest a=\r\nny metadata tags that\n&gt; &gt; exist in a web page describing the content of tha=\r\nt page. I don&#39;t want\n&gt; &gt; to  harvest and archive the actual resource itself=\r\n, merely a\n&gt; &gt; description of the content. I&#39;m not sure if Heritrix is the =\r\nway to go\n&gt; &gt; for this? Has anyone any recommendations on the best open sou=\r\nrce\n&gt; &gt; crawler to use for this situation?\n&gt; I don&#39;t know of a metadata onl=\r\ny harvester though I&#39;m sure one exists \n&gt; somewhere (I once had a link to a=\r\nn RDF crawler but can&#39;t put my\nhands on \n&gt; it currently.  That project soun=\r\nded interesting).\n&gt; \n&gt; If you were to use Heritrix for this task, you would=\r\n need to write your \n&gt; own metadata extractors.  The current extractors in =\r\nHeritrix are \n&gt; aggressively purposed to the extraction of links only.  Sub=\r\nclassing\nthem \n&gt; so they also harvested metadata would probably be an onero=\r\nus task. \n&gt; \n&gt; Your new metadata extractors would likely run after the link=\r\n extractors \n&gt; had done their work and would focus on pulling the META tags=\r\n from html, \n&gt; the headers from http, the meta info from pdfs and ms*, etc.=\r\n  You&#39;d \n&gt; probably just need to read in the first 3 or 4k of the document =\r\nto get \n&gt; what you need.\n&gt; \n&gt; You&#39;d then then need to make your own Writer.=\r\n  Heritrix writes all \n&gt; downloaded to ARC files.  You&#39;d likely want to tur=\r\nn this functionality \n&gt; off -- sounds like you don&#39;t want to keep around th=\r\ne complete download, \n&gt; just metadata only -- in favor of a Writer of our o=\r\nwn composition that \n&gt; instead writes metadata only (You might take some in=\r\nspiration from the \n&gt; current gzipping Writer with its lead-in line of URL,=\r\n harvest data,\netc.).\n&gt; \n&gt; (Long term, we&#39;re working on a new format in whi=\r\nch to store resources \n&gt; called WARC.  It will have means for saving metada=\r\nta, even adding \n&gt; metadata about a resource.  If this format were here now=\r\n you could just \n&gt; use the &#39;WarcWriter&#39; straight).\n&gt; \n&gt; It looks like you&#39;d=\r\n have to do similiar if you used the nutch -- \n&gt; http://lucene.apache.org/n=\r\nutch -- fetcher crawling the web.  You might \n&gt; prefer the batch mode in wh=\r\nich it runs.   After each batch fetching \n&gt; step, you could run a metadata =\r\ndistiller that extracts meta tags from \n&gt; HTML, reads the metadata from PDF=\r\n, etc.  Again, you&#39;d have to write \n&gt; custom metadata parsers plugins.\n&gt; \n&gt;=\r\n Ask more questions.\n&gt; St.Ack\n&gt; \n&gt; &gt;\n&gt; &gt; I&#39;m new to this game so any help y=\r\nou can provide will be greatly\n&gt; &gt; appreciated.\n&gt; &gt;\n&gt; &gt; Shay\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; =\r\n&gt;\n&gt; &gt;\n&gt; &gt; SPONSORED LINKS\n&gt; &gt; Computer security \n&gt; &gt;\n&lt;http://groups.yahoo.c=\r\nom/gads?t=3Dms&k=3DComputer+security&w1=3DComputer+security&w2=3DComputer+t=\r\nraining&c=3D2&s=3D46&.sig=3DBHmcxBg5sKfN9-gcWnJWDg&gt;\n\n&gt; &gt; \tComputer training=\r\n \n&gt; &gt;\n&lt;http://groups.yahoo.com/gads?t=3Dms&k=3DComputer+training&w1=3DCompu=\r\nter+security&w2=3DComputer+training&c=3D2&s=3D46&.sig=3Dv0JjJWA4s7mLnWQWdFx=\r\nuTQ&gt;\n\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n-----------------------------------------------------=\r\n-------------------\n&gt; &gt; YAHOO! GROUPS LINKS\n&gt; &gt;\n&gt; &gt;     *  Visit your group=\r\n &quot;archive-crawler\n&gt; &gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;=\r\n&quot; on the web.\n&gt; &gt;        \n&gt; &gt;     *  To unsubscribe from this group, send a=\r\nn email to:\n&gt; &gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt; &gt;     =\r\n \n&lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=3DUnsubscribe=\r\n&gt;\n&gt; &gt;        \n&gt; &gt;     *  Your use of Yahoo! Groups is subject to the Yahoo!=\r\n Terms of\n&gt; &gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; &gt;\n&gt; &gt;\n&gt; =\r\n&gt;\n------------------------------------------------------------------------\n=\r\n&gt; &gt;\n&gt;\n\n\n\n\n\n"}}