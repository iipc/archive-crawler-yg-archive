{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":299179219,"authorName":"mjjjhjemj","from":"&quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;","profile":"mjjjhjemj","replyTo":"LIST","senderId":"H9BRVOrSqtsmC9fp0gSR_LoxZNXPNANn-Q-ZF8UA7WOuDgGkdknaMfhj9Mv1sslUYK19-22Jki1tgw5FS0V3BBNnKtVC6gFZpJw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Help with crawling sites without robots.txt and return the following responses","postDate":"1232566611","msgId":5641,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdsN3RnaitvOGRqQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGdrcWVtcit1bTBkQGVHcm91cHMuY29tPg=="},"prevInTopic":5636,"nextInTopic":5651,"prevInTime":5640,"nextInTime":5642,"topicId":5636,"numMessagesInTopic":4,"msgSnippet":"... Could someone please help me on this? Thanks, Mike","rawEmail":"Return-Path: &lt;bosoxchamps@...&gt;\r\nX-Sender: bosoxchamps@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 79954 invoked from network); 21 Jan 2009 19:36:52 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m47.grp.scd.yahoo.com with QMQP; 21 Jan 2009 19:36:52 -0000\r\nX-Received: from unknown (HELO n17c.bullet.sp1.yahoo.com) (69.147.64.126)\n  by mta18.grp.scd.yahoo.com with SMTP; 21 Jan 2009 19:36:52 -0000\r\nX-Received: from [69.147.65.174] by n17.bullet.sp1.yahoo.com with NNFMP; 21 Jan 2009 19:36:52 -0000\r\nX-Received: from [66.218.66.87] by t12.bullet.mail.sp1.yahoo.com with NNFMP; 21 Jan 2009 19:36:52 -0000\r\nDate: Wed, 21 Jan 2009 19:36:51 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gl7tgj+o8dj@...&gt;\r\nIn-Reply-To: &lt;gkqemr+um0d@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;\r\nSubject: Re: Help with crawling sites without robots.txt and return the following responses\r\nX-Yahoo-Group-Post: member; u=299179219; y=KpwayGPnvYCv5CA-tSrLIxjtyXBaLQ7nOzKrpcSBB1q8y-2_\r\nX-Yahoo-Profile: mjjjhjemj\r\n\r\n--- In archive-crawler@yahoogroups.com, &quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;\nwrote=\r\n:\n&gt;\n&gt; I would like to configure Heritrix to not even look for a robots.txt.=\r\n\n&gt; I have permission for the sites I am crawling to ignore the robots.txt\n&gt;=\r\n which I have done, but the problem is even when the configuration is\n&gt; set=\r\nup to ignore the robots.txt Heritrix still wants to download first\n&gt; and th=\r\nen apparently ignore if configured to do so. I have also tried\n&gt; the &#39;custo=\r\nm&#39; robots.txt configuration with no success. If the\n&gt; robots.txt is not pre=\r\nsent and the response falls under examples 2 or 3\n&gt; then your are stuck.\n&gt; =\r\n\n&gt; Continues to Crawl site example\n&gt; -------------------------------\n&gt; 1. h=\r\nttp://www.mysite1.com/robots.txt\n&gt; response: 404 Not Found\n&gt; Not Found\n&gt; Th=\r\ne requested URL /robots.txt was not found on this server.\n&gt; * Heritrix cont=\r\ninues to crawl www.mysite1.com\n&gt; \n&gt; Fails to continue to crawl examples\n&gt; -=\r\n----------------------------------\n&gt; 2. http://www.mysite2.com/robots.txt\n&gt;=\r\n response: 301 - moved permanently\n&gt; and then the server issues a 301 and s=\r\nerves back\n&gt; &#39;http://www.mysite2.com/ShortUrlErrorPage.jsp&#39;\n&gt; Invalid Short=\r\n URL pattern Please redefine your Short URL pattern!\n&gt; \n&gt; 3. http://www.mys=\r\nite3/robots.txt\n&gt; response: Connection Interrupted\n&gt; The connection to the =\r\nserver was reset while the page was loading.\n&gt; The network link was interru=\r\npted while negotiating a connection.\n&gt; Please try again.\n&gt;\n\nCould someone p=\r\nlease help me on this?\n\nThanks,\nMike\n\n\n"}}