{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"aZa9UcNIV0-z1uKLNsP9mR0pIbkPPryQJu0Sb5xb31A1usRK588Jhx133rOn3OuEtpreQczgPwwSUblL-5lKyw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Handling identical documents from different sites.","postDate":"1131573932","msgId":2350,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzNzI3MkFDLjcwMDA5MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGRrdDB1dStlNGEyQGVHcm91cHMuY29tPg==","referencesHeader":"PGRrdDB1dStlNGEyQGVHcm91cHMuY29tPg=="},"prevInTopic":2348,"nextInTopic":2353,"prevInTime":2349,"nextInTime":2351,"topicId":2342,"numMessagesInTopic":8,"msgSnippet":"... What would you suggest? One notion we ve been kicking around is having a fast and dumb server sitting in the middle of a coordinated crawling cluster.  A","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 46287 invoked from network); 9 Nov 2005 22:13:01 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m29.grp.scd.yahoo.com with QMQP; 9 Nov 2005 22:13:01 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta3.grp.scd.yahoo.com with SMTP; 9 Nov 2005 22:13:01 -0000\r\nReceived: (qmail 17803 invoked by uid 100); 9 Nov 2005 22:10:17 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.8?) (stack@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 9 Nov 2005 22:10:17 -0000\r\nMessage-ID: &lt;437272AC.7000908@...&gt;\r\nDate: Wed, 09 Nov 2005 14:05:32 -0800\r\nUser-Agent: Debian Thunderbird 1.0.7 (X11/20051017)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;dkt0uu+e4a2@...&gt;\r\nIn-Reply-To: &lt;dkt0uu+e4a2@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-95.0 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Handling identical documents from different\n sites.\r\nX-Yahoo-Group-Post: member; u=168599281; y=YhWv_yDHWzTXIJDFXnIQQnj4R8QaH9SS2lvtYSzr97L5Qb6A1yMXpLb-\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\ncallforshadab wrote:\n\n&gt; How should this be handled in the clustered version (multiple machines\n&gt; running Heritrix)?\n&gt;\nWhat would you suggest?\n\nOne notion we&#39;ve been kicking around is having a fast and dumb server \nsitting in the middle of a coordinated crawling cluster.  A processor \nhosted by alll machines would ask it if the content (hash) had already \nbeen seen. Such a server might also be used to check if an URL is \nalready-seen.  Though the crawl space is split between crawlers, we \nmight run a central server to guard against repeat fetches when queues \nare moved between machines, either because one machine is overloaded so \nwe narrow range its responsible for off-loading to adjacent crawlers or \nbecause a crawler is picking up for a crashed crawler (If I remember \ncorrectly, the ubicrawler folks in their paper allow that a few URLs may \nbe mutiply fetched during the coming and going of crawlers flux).\n\nSt.Ack\n\n"}}