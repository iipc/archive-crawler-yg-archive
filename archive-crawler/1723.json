{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (Internet Archive)","from":"&quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"2o7wQVwWivvntoyfa-NobNzY0Pn6SrDxVJ3PZc8pjEj4okDdyqG9a_MAz73r2gxcH2szumyF0igcqYbZln0Nr6VvgUR_kWEMBPg5A0RWUZ2PkrztucD-1JSb66M","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Problem with some robots.txt files","postDate":"1113419841","msgId":1723,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyNUQ3MDQxLjYwNDA1MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGQzajJ0bSs5N2JyQGVHcm91cHMuY29tPg==","referencesHeader":"PGQzajJ0bSs5N2JyQGVHcm91cHMuY29tPg=="},"prevInTopic":1722,"nextInTopic":0,"prevInTime":1722,"nextInTime":1724,"topicId":1722,"numMessagesInTopic":2,"msgSnippet":"... Looks like an imrpvoement to me. I ll apply it. Thanks, good catch! More comment: The various robots.txt specifications all use the term path to describe","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 99799 invoked from network); 13 Apr 2005 19:17:27 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m25.grp.scd.yahoo.com with QMQP; 13 Apr 2005 19:17:27 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta4.grp.scd.yahoo.com with SMTP; 13 Apr 2005 19:17:27 -0000\r\nReceived: (qmail 31692 invoked by uid 100); 13 Apr 2005 19:17:23 -0000\r\nReceived: from b116-dyn-239.archive.org (HELO ?207.241.238.239?) (gojomo@...@207.241.238.239)\n  by mail-dev.archive.org with SMTP; 13 Apr 2005 19:17:22 -0000\r\nMessage-ID: &lt;425D7041.6040504@...&gt;\r\nDate: Wed, 13 Apr 2005 12:17:21 -0700\r\nUser-Agent: Mozilla Thunderbird 1.0 (X11/20041206)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;d3j2tm+97br@...&gt;\r\nIn-Reply-To: &lt;d3j2tm+97br@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-52.3 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: &quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Problem with some robots.txt files\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nogrenholm wrote:\n&gt; \n&gt; Hi,\n&gt; I&#39;ve found that for some reason or another some webmasters want to\n&gt; allow robots to access scripts, but not with some certain parameters,\n&gt; i.e., in robots.txt:  \n&gt; Disallow: /wiki/index.php?action=edit\n&gt; \n&gt; I&#39;m not sure if this is a allowed thing to specify in robots.txt, but\n&gt; nonetheless people tend to do so sometimes. \n&gt; The problem then is that when Heritrix does a check whether to allow\n&gt; crawling of a URL or not (in  RobotsExclusionPolicy.java) it does only\n&gt; call the URI:s getPath() and therefore doesn&#39;t match it with the above\n&gt; mentioned robots.txt rule, and it will get crawled. Maybe this is what\n&gt; is supposed to happen, but if someone wants to apply the rule above, a\n&gt; call to the URI:s getPathQuery() will fix this.\n&gt; \n&gt; In other words change line 229 in RobotsExclusionPolicy.java\n&gt; from: String p = curi.getUURI().getPath();\n&gt; to: String p = curi.getUURI().getPathQuery();\n&gt; \n&gt; I have no idea what so ever though if this will cause some other\n&gt; horrible problems that I haven&#39;t thought of.\n\nLooks like an imrpvoement to me. I&#39;ll apply it. Thanks, good catch!\n\nMore comment:\n\nThe various robots.txt specifications all use the term &#39;path&#39; to\ndescribe the portion of the URI against which &#39;Disallow&#39; directives\nare matched. If their &#39;path&#39; was intended to mean the same thing as\n&#39;path&#39; in the URI definition specifications (like RFC2396), then it\nwould be compliant to ignore the &#39;query&#39; part. When this code was written,\nwith the robots.txt specs at hand, this was probably the motivation\nfor using getPath().\n\nHowever, there&#39;s no indication the robots.txt specs meant &#39;path&#39; exactly\nthat way -- and none of their examples consider URLs with &#39;query&#39; parts.\n\nIt&#39;s reasonable to believe that the robots.txt design intended the\nstring-matching to apply to the entire path+query portion of the URL.\nInterpreting it that way adds expressiveness without preventing anything\nthat&#39;s possible without such an interpretation. I&#39;ve certainly seen\nrobot.txt files with such directives, and they communicate a legitimate\ndistinction.\n\n- Gordon @ IA\n\n"}}