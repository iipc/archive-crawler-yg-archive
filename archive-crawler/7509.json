{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"t-MCn6yE6mIh5f3av7cnlB5joW11qdd93wBSBkaVvOZLFlPJgXcxnY-r4CUCENAaJfu_YKAQJgjehQdDIf7E_8sZfyE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Question about H3 crawl management","postDate":"1326242757","msgId":7509,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMENEQkM1LjQwNzA3MDBAY3MuY211LmVkdT4=","inReplyToHeader":"PDRGMENEODI5LjkwMzA4MDFAYmF5YXJlYS5uZXQ+","referencesHeader":"PDRGMEM3NjRFLjQwMDA3MDBAY3MuY211LmVkdT4gPDRGMEM4QjQ4LjQwMjAwMDlAYmF5YXJlYS5uZXQ+IDw0RjBDQzlEMy4yMDUwMTA0QGNzLmNtdS5lZHU+IDw0RjBDRDgyOS45MDMwODAxQGJheWFyZWEubmV0Pg=="},"prevInTopic":7508,"nextInTopic":7516,"prevInTime":7508,"nextInTime":7510,"topicId":7505,"numMessagesInTopic":7,"msgSnippet":"John, I am only interested in the successful mime-type: text/html pages.  The mimetype-report.txt report generates this information.  The crawl-report.txt has","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 90715 invoked from network); 11 Jan 2012 00:46:02 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m16.grp.sp2.yahoo.com with QMQP; 11 Jan 2012 00:46:02 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.61)\n  by mta6.grp.sp2.yahoo.com with SMTP; 11 Jan 2012 00:46:02 -0000\r\nX-Received: from [128.2.209.200] (SAVOY.LTI.CS.CMU.EDU [128.2.209.200])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id q0B0jwn1021085\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tTue, 10 Jan 2012 19:45:58 -0500\r\nMessage-ID: &lt;4F0CDBC5.4070700@...&gt;\r\nDate: Tue, 10 Jan 2012 19:45:57 -0500\r\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: John Lekashman &lt;lekash@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;4F0C764E.4000700@...&gt; &lt;4F0C8B48.4020009@...&gt; &lt;4F0CC9D3.2050104@...&gt; &lt;4F0CD829.9030801@...&gt;\r\nIn-Reply-To: &lt;4F0CD829.9030801@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2011.3.18.170322\r\nX-SMTP-Spam-Clean: 8% (\n KNOWN_FREEWEB_URI 0.05, BODY_SIZE_5000_5999 0, BODY_SIZE_7000_LESS 0, ECARD_KNOWN_DOMAINS 0, WEIRD_PORT 0, __BOUNCE_CHALLENGE_SUBJ 0, __BOUNCE_NDR_SUBJ_EXEMPT 0, __C230066_P5 0, __CANPHARM_UNSUB_LINK 0, __CP_URI_IN_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __KNOWN_FREEWEB_URI3 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __SANE_MSGID 0, __STOCK_PHRASE_7 0, __TO_MALFORMED_2 0, __URI_NO_WWW 0, __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.61\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: Re: [archive-crawler] Question about H3 crawl management\r\nX-Yahoo-Group-Post: member; u=500983475; y=-6UGmX9GPjm0vjQ3VJZOQDIjHleaXeUb1e035B5CVrIeQAciCjS5QQ\r\nX-Yahoo-Profile: david_pane1\r\n\r\nJohn,\n\nI am only interested in the successful mime-type: text/html pages.  The \nmimetype-report.txt report generates this information.  The \ncrawl-report.txt has the total crawled bytes.\n\nI am attempting to generate these reports on a regular basis.  The only \nway that I know how is by requesting these urls for each crawler.\n\nhttps://crawler_ipaddress:8443/engine/job/bigcrawl/report/CrawlSummaryReport\nhttps://crawler_ipaddress:8443/engine/job/bigcrawl/report/MimetypesReport\n\nUsing Perl LWP I get these errors:\n\nCan&#39;t connect to crawler_ipaddress:8443 (certificate verify failed)\n\nLWP::Protocol::https::Socket: SSL connect attempt failed with unknown \nerrorerror:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate\n\n--David\n\nOn 1/10/2012 7:30 PM, John Lekashman wrote:\n&gt; Hi David,\n&gt; Why would ssl have anything to do with it?\n&gt; Trying to read the UI from a script isn&#39;t the best way,\n&gt; but good luck if you must.\n&gt;\n&gt; A script:\n&gt; Now, this is pretty rough, and if you are generating various\n&gt; versions of crawl.log, you have to deal with it.\n&gt;\n&gt; # # of pages. Run wc on crawl log and add &#39;em up.\n&gt; #!/bin/bash\n&gt;\n&gt; rm /tmp/$sometmpfilename\n&gt; for i in a1 a2 a3 a4 a5 # or `cat my_list_of_crawlers`\n&gt; do\n&gt; ssh $i wc /home/crawl/logs/crawl.log | awk &#39;{print $1}&#39; &gt;&gt;\n&gt; /tmp/$sometmpfilename\n&gt; echo &quot;+&quot; &gt;&gt; /tmp/$sometmpfilename\n&gt; done\n&gt;\n&gt; echo &quot;+p&quot; &gt;&gt; /tmp/$sometmpfilename\n&gt;\n&gt; cat /tmp/$sometmpfilename | dc\n&gt;\n&gt; And what you do with the output of cat, well, you figure that out.\n&gt;\n&gt; # Total size of crawled data. Pretty much the same thing, although you\n&gt; look at the third field in crawl.log\n&gt; Instead of wc use something like this: the grep -v removes the &#39;-&#39; from\n&gt; 0 length records, which would confuse\n&gt; the output.\n&gt;\n&gt; ssh $i cat crawl.log | awk &#39;{print $3}&#39; | grep -v &#39;-&#39;\n&gt;\n&gt;\n&gt; On 1/10/12 3:29 PM, David Pane wrote:\n&gt;&gt;\n&gt;&gt; Thank you for your responses John.\n&gt;&gt;\n&gt;&gt; Can you be more specific about your thoughts on writing a script to\n&gt;&gt; generate these? I have tried to generate the mime report and crawl\n&gt;&gt; summary reports using perl LWP, but haven&#39;t figured out how to deal with\n&gt;&gt; the SSL certificates.\n&gt;&gt;\n&gt;&gt; --David\n&gt;&gt;\n&gt;&gt; &gt;&gt; 5) Although I can capture the below statistics manually, can you\n&gt;&gt; suggest\n&gt;&gt; &gt;&gt; a way that I can automatically generate/collect the following\n&gt;&gt; statistics\n&gt;&gt; &gt;&gt; from the crawl. I would like to generate this data at least once every\n&gt;&gt; &gt;&gt; 24 hours and possibly as often as every hour.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt; Well, you could write a script to do it.\n&gt;&gt; &gt;\n&gt;&gt; &gt;&gt; a) Total size of crawled data.\n&gt;&gt; &gt;&gt; b) total number of pages crawled (mime-type: text/html).\n&gt;&gt; &gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;\n\n"}}