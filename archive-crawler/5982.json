{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"12zglajbCPH6lh9DsaRWJDu6S3d48d7b42_Z4HQ__KHs72H_8AVYEVW-SOzIRsqkWeKhDW_apOVmiLhGfonLdUXZpCx7ijzuPjiOgAjf","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Can I split seeds for a HashCrawlMapper crawl?","postDate":"1250185356","msgId":5982,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg2MWphYys3MjJlQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDYxMTA1LjM4Ljk5LjQyLjI0NC4xMjUwMTI2NDc3LnNxdWlycmVsQG1haWwuYXJjaGl2ZS5vcmc+"},"prevInTopic":5978,"nextInTopic":5983,"prevInTime":5981,"nextInTime":5983,"topicId":5971,"numMessagesInTopic":8,"msgSnippet":"We used to do it. I also use 2 HashCrawlMapper processors as you described. We were able to run that with about 0.5MM seeds. With the new crawling strategy we","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 49955 invoked from network); 13 Aug 2009 17:43:03 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.re1.yahoo.com with QMQP; 13 Aug 2009 17:43:03 -0000\r\nX-Received: from unknown (HELO n37b.bullet.mail.sp1.yahoo.com) (66.163.168.151)\n  by mta1.grp.sp2.yahoo.com with SMTP; 13 Aug 2009 17:43:03 -0000\r\nX-Received: from [69.147.65.150] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 13 Aug 2009 17:42:39 -0000\r\nX-Received: from [98.137.34.35] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 13 Aug 2009 17:42:39 -0000\r\nDate: Thu, 13 Aug 2009 17:42:36 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h61jac+722e@...&gt;\r\nIn-Reply-To: &lt;61105.38.99.42.244.1250126477.squirrel@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: Can I split seeds for a HashCrawlMapper crawl?\r\nX-Yahoo-Group-Post: member; u=132996324; y=gtyKPu_Kc3gB_zBbNcrq05X9LV_NHPxeVx6BUIiBa7eOxvzDaw\r\nX-Yahoo-Profile: joehung302\r\n\r\nWe used to do it. I also use 2 HashCrawlMapper processors as you described.=\r\n \n\nWe were able to run that with about 0.5MM seeds. With the new crawling s=\r\ntrategy we need to support up to 20MM seeds and I did a proof run with one =\r\ninstance: the crawl rate slows down to 2/3 after 3 days. That would not wor=\r\nk because we usually keep the crawler running for at least 3 weeks.\n\nI was =\r\nguessing that the 20MM seed list caused the problem. Now I want to experime=\r\nnt with split-seeds but are concerned that we might be missing the divert U=\r\nRLs if the seed lists are not the same among crawlers.\n\nWhen a crawler gets=\r\n the divert URLs, will they reject the URL if it comes from a seed that is =\r\nnot in the cralwer&#39;s seed file?\n\nCheers,\n-Joe\n\n\n--- In archive-crawler@yaho=\r\nogroups.com, igor@... wrote:\n&gt;\n&gt; Hi Joe,\n&gt; \n&gt; In the past I used to this wi=\r\nthout splitting the seeds. I used two\n&gt; identical HashCrawlMapper processor=\r\ns: one before the preselector and one\n&gt; after the link scoper.\n&gt; \n&gt; This wa=\r\ny, each crawling node schedules all of the seeds but crawls only\n&gt; ones def=\r\nined by the HashCrawlMapper. What I liked about this is that all\n&gt; of the n=\r\nodes will have the same scope (if based on seeds) which can handy.\n&gt; \n&gt; Hop=\r\ne this helps.\n&gt; i.\n&gt; \n&gt; \n&gt; &gt; I&#39;m using Heritrix 1.14.3.\n&gt; &gt;\n&gt; &gt; Let&#39;s say I=\r\n have\n&gt; &gt; 1. one big seed list consisting of 1MM seeds.\n&gt; &gt; 2. 2 crawler in=\r\nstances to implement HashCrawlMapper.\n&gt; &gt; 3. The crawl scope is domain + 1 =\r\n(implemented through OnDomainDecideRule\n&gt; &gt; with &quot;seeds-as-surt-prefixes&quot;=\r\n=3D=3Dtrue and &quot;also-check-via&quot;=3D=3Dtrue).\n&gt; &gt;\n&gt; &gt; Can I split the seeds u=\r\nsing the same HashCrawlMapper rule so that each\n&gt; &gt; crawler would only get =\r\nseeds that are within its scope? Would there be any\n&gt; &gt; difference if I use=\r\n the same 1MM seeds for both crawlers?\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; The reason why I want to=\r\n do this is, I have 20MM seeds among 12 crawlers.\n&gt; &gt; I&#39;ve tested with one =\r\ninstance handling 20MM seeds and it doesn&#39;t seem to\n&gt; &gt; work. If I can spli=\r\nt the seeds so that each cralwer starts with URLs that\n&gt; &gt; belong to themse=\r\nlves it should make the crawl process easier....\n&gt; &gt;\n&gt; &gt; Thanks,\n&gt; &gt; -Joe\n&gt;=\r\n &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ------------------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! =\r\nGroups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}