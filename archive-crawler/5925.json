{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"ZV93ChgHB5ElTYh0OZxO6tP6c61l6kzq7d9qnZWw5gPmmb14yWyGBXgLGkgfb42k7hkvMLQFYLPl8_0P-hFLSFmDZR-IBes","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Recrawl Issue (Heritrix 2.0.2)","postDate":"1247605868","msgId":5925,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBNUNGNDZDLjIwNzAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDE5N2I1YjIxMDkwNzE0MDkyMGkzYzdjNmUzYXZjZGY2OWU2MDFhM2EzZWIyQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PDE5N2I1YjIxMDkwNzE0MDkyMGkzYzdjNmUzYXZjZGY2OWU2MDFhM2EzZWIyQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":5920,"nextInTopic":5927,"prevInTime":5924,"nextInTime":5926,"topicId":5920,"numMessagesInTopic":3,"msgSnippet":"That s generally the right approach. Can you find more of the exception stack, perhaps in heritrix_out.log? (The IllegalStateException and anything that","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 60907 invoked from network); 14 Jul 2009 21:11:33 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m5.grp.sp2.yahoo.com with QMQP; 14 Jul 2009 21:11:33 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta3.grp.sp2.yahoo.com with SMTP; 14 Jul 2009 21:11:32 -0000\r\nX-Received: (qmail 26084 invoked from network); 14 Jul 2009 21:11:09 -0000\r\nX-Received: from 67.188.14.54 (HELO ?192.168.1.72?) (67.188.14.54)\n  by relay02.pair.com with SMTP; 14 Jul 2009 21:11:09 -0000\r\nX-pair-Authenticated: 67.188.14.54\r\nMessage-ID: &lt;4A5CF46C.2070303@...&gt;\r\nDate: Tue, 14 Jul 2009 14:11:08 -0700\r\nUser-Agent: Thunderbird 2.0.0.22 (Windows/20090605)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;197b5b210907140920i3c7c6e3avcdf69e601a3a3eb2@...&gt;\r\nIn-Reply-To: &lt;197b5b210907140920i3c7c6e3avcdf69e601a3a3eb2@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Recrawl Issue (Heritrix 2.0.2)\r\nX-Yahoo-Group-Post: member; u=137285340; y=mi-9huZyR6OOsIXFpfS8xsNf8LNkWEZ2Or0_M6ww9S9U\r\nX-Yahoo-Profile: gojomo\r\n\r\nThat&#39;s generally the right approach.\n\nCan you find more of the exception stack, perhaps in heritrix_out.log? \n(The &#39;IllegalStateException&#39; and anything that caused/preceded it would \nhelp.)\n\nEspecially if the previous crawl still had full queues at the time it \nended (and perhaps even if not), you might want to take the extra step \nof copying *just* the history info out of the previous crawl&#39;s &#39;state&#39; \ndirectory into the new crawl. (The main() of PersistProcessor provides a \n  standalone facility for doing so between runs.) However, skipping this \nstep shouldn&#39;t cause problems; just a somewhat larger state directory \nand (perhaps) slower startup.\n\nThere are no known problems with this functionality in Heritrix 2.0.x, \nbut I still generally advise using 1.14.3 unless the enhanced \nfrontier-prioritization features of 2.0 (related to custom &#39;precedence&#39; \npolicies) are needed.\n\n- Gordon @ IA\n\nIgnacio Garcia wrote:\n&gt; \n&gt; \n&gt; Hello,\n&gt; \n&gt; We are trying to set up dedupe in our crawls right now but we are having \n&gt; some issues launching the Recrawl jobs.\n&gt; We have no problems adding the processors needed for the initial crawl:\n&gt; \n&gt;    1. FetchHistoryProcessor, after the last fetching processor.\n&gt;    2. PersistStoreProcessor, after all other processors.\n&gt; \n&gt; When that first job is finished, a grep &quot;uri_history&quot; on the state files \n&gt; shows that one of the jdb files matches, so it seems that some \n&gt; information is being written to it.\n&gt; After that, we are creating a new ready job, using the completed initial \n&gt; crawl as the base (so that the state is copied over) and adding the \n&gt; required load processor:\n&gt; \n&gt;    1. PersistLoadProcessor, before any fetch processor\n&gt; \n&gt; We also keep the FetchHistory and PersistStore, to have a cumulative \n&gt; history of the crawls\n&gt; \n&gt; However, when we try to start this Recrawl job, with all three \n&gt; processors, I get the following exception:\n&gt; \n&gt; \n&gt;       javax.management.ReflectionException\n&gt; \n&gt; null\n&gt; \n&gt; javax.management.ReflectionException\n&gt; at \n&gt; org.archive.settings.jmx.LoggingDynamicMBean.dealWithException(LoggingDynamicMBean.java:77)\n&gt; at \n&gt; org.archive.settings.jmx.LoggingDynamicMBean.invoke(LoggingDynamicMBean.java:160)\n&gt; \n&gt; at com.sun.jmx.mbeanserver.DynamicMetaDataImpl.invoke(Unknown Source)\n&gt; at com.sun.jmx.mbeanserver.MetaDataImpl.invoke(Unknown Source)\n&gt; at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown \n&gt; Source)\n&gt; \n&gt; at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)\n&gt; at org.archive.openmbeans.annotations.BeanProxy.invoke(BeanProxy.java:87)\n&gt; at $Proxy9.launchJob(Unknown Source)\n&gt; at org.archive.crawler.webui.CrawlerArea.launch(CrawlerArea.java:155)\n&gt; \n&gt; at \n&gt; jsp.crawler_005farea.do_005flaunch_jsp._jspService(jsp.crawler_005farea.do_005flaunch_jsp:45)\n&gt; at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)\n&gt; at javax.servlet.http.HttpServlet.service(HttpServlet.java:809)\n&gt; \n&gt; at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:459)\n&gt; at \n&gt; org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1054)\n&gt; at org.archive.crawler.webui.AuthFilter.doFilter(AuthFilter.java:79)\n&gt; \n&gt; at \n&gt; org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1045)\n&gt; at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:358)\n&gt; at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:231)\n&gt; \n&gt; at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:629)\n&gt; at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:453)\n&gt; at org.mortbay.jetty.handler.HandlerList.handle(HandlerList.java:49)\n&gt; \n&gt; at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:141)\n&gt; at org.mortbay.jetty.Server.handle(Server.java:303)\n&gt; at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:452)\n&gt; at \n&gt; org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:721)\n&gt; \n&gt; at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:509)\n&gt; at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:209)\n&gt; at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:349)\n&gt; at \n&gt; org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:217)\n&gt; \n&gt; at \n&gt; org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:475)\n&gt; Caused by: java.lang.IllegalStateException\n&gt; ... 30 more\n&gt; \n&gt; I don&#39;t know where the problem is, since I am simply adding the Persist \n&gt; Load processor, and the &quot;sheet&quot; looks good when viewed via the webUI, no \n&gt; errors or missing fields.\n&gt; \n&gt; As detailed in the Features notes for Heritrix 1.12.0, I am adding the \n&gt; processors in their places, but I am still getting this error.\n&gt; Here is a copy of my global.sheet, in case anyone sees a problem with it:\n&gt; \n&gt;     root=map, java.lang.Object\n&gt;     root:metadata=primary,\n&gt;     org.archive.modules.writer.DefaultMetadataProvider\n&gt;     root:metadata:description=string, deep seed crawl\n&gt;     root:metadata:operator-contact-url=string,\n&gt;     http://www.loc.gov/webcapture/\n&gt;     root:metadata:robots-honoring-policy=primary,\n&gt;     org.archive.modules.net.RobotsHonoringPolicy\n&gt;     root:metadata:robots-honoring-policy:type=enum,\n&gt;     org.archive.modules.net.RobotsHonoringPolicy$Type-IGNORE\n&gt;     root:metadata:robots-honoring-policy:user-agents=list, java.lang.String\n&gt;     root:loggerModule=primary,\n&gt;     org.archive.crawler.framework.CrawlerLoggerModule\n&gt;     root:seeds=primary, org.archive.modules.seeds.SeedModuleImpl\n&gt;     root:scope=object, org.archive.modules.deciderules.DecideRuleSequence\n&gt;     root:scope:rules=list, org.archive.modules.deciderules.DecideRule\n&gt;     root:scope:rules:0=object,\n&gt;     org.archive.modules.deciderules.AcceptDecideRule\n&gt;     root:scope:rules:1=object,\n&gt;     org.archive.modules.deciderules.surt.SurtPrefixedDecideRule\n&gt;     root:scope:rules:2=object,\n&gt;     org.archive.modules.deciderules.TransclusionDecideRule\n&gt;     root:scope:rules:2:max-speculative-hops=int, 1\n&gt;     root:scope:rules:2:max-trans-hops=int, 1\n&gt;     root:scope:rules:3=object,\n&gt;     org.archive.modules.deciderules.TooManyPathSegmentsDecideRule\n&gt;     root:scope:rules:3:max-path-depth=int, 1\n&gt;     root:scope:rules:4=object,\n&gt;     org.archive.modules.deciderules.PrerequisiteAcceptDecideRule\n&gt;     root:uriUniqFilter=primary, org.archive.crawler.util.BdbUriUniqFilter\n&gt;     root:queue-assignment-policy=primary,\n&gt;     org.archive.crawler.frontier.SurtAuthorityQueueAssignmentPolicy\n&gt;     root:server-cache=primary, org.archive.modules.net.BdbServerCache\n&gt;     root:credential-store=primary,\n&gt;     org.archive.modules.credential.CredentialStore\n&gt;     root:controller=primary,\n&gt;     org.archive.crawler.framework.CrawlControllerImpl\n&gt;     root:controller:frontier=primary,\n&gt;     org.archive.crawler.frontier.BdbFrontier\n&gt;     root:controller:frontier:rules=list,\n&gt;     org.archive.modules.canonicalize.CanonicalizationRule\n&gt;     root:controller:frontier:rules:0=object,\n&gt;     org.archive.modules.canonicalize.LowercaseRule\n&gt;     root:controller:frontier:rules:1=object,\n&gt;     org.archive.modules.canonicalize.StripUserinfoRule\n&gt;     root:controller:frontier:rules:2=object,\n&gt;     org.archive.modules.canonicalize.StripWWWNRule\n&gt;     root:controller:frontier:rules:3=object,\n&gt;     org.archive.modules.canonicalize.StripSessionIDs\n&gt;     root:controller:frontier:rules:4=object,\n&gt;     org.archive.modules.canonicalize.StripSessionCFIDs\n&gt;     root:controller:frontier:rules:5=object,\n&gt;     org.archive.modules.canonicalize.FixupQueryStr\n&gt;     root:controller:frontier:scope=reference, root:scope\n&gt;     root:controller:processors=map, org.archive.modules.Processor\n&gt;     root:controller:processors:Preselector=object,\n&gt;     org.archive.crawler.prefetch.Preselector\n&gt;     root:controller:processors:Preselector:scope=reference, root:scope\n&gt;     root:controller:processors:Preprocessor=object,\n&gt;     org.archive.crawler.prefetch.PreconditionEnforcer\n&gt;     root:controller:processors:Load=object,\n&gt;     org.archive.modules.recrawl.PersistLoadProcessor\n&gt;     root:controller:processors:DNS=object,\n&gt;     org.archive.modules.fetcher.FetchDNS\n&gt;     root:controller:processors:HTTP=object,\n&gt;     org.archive.modules.fetcher.FetchHTTP\n&gt;     root:controller:processors:History=object,\n&gt;     org.archive.modules.recrawl.FetchHistoryProcessor\n&gt;     root:controller:processors:HTTP:accept-headers=list, java.lang.String\n&gt;     root:controller:processors:HTTP:midfetch-rules=object,\n&gt;     org.archive.modules.deciderules.DecideRuleSequence\n&gt;     root:controller:processors:ExtractorHTTP=object,\n&gt;     org.archive.modules.extractor.ExtractorHTTP\n&gt;     root:controller:processors:ExtractorHTML=object,\n&gt;     org.archive.modules.extractor.ExtractorHTML\n&gt;     root:controller:processors:ExtractorCSS=object,\n&gt;     org.archive.modules.extractor.ExtractorCSS\n&gt;     root:controller:processors:ExtractorJS=object,\n&gt;     org.archive.modules.extractor.ExtractorJS\n&gt;     root:controller:processors:ExtractorSWF=object,\n&gt;     org.archive.modules.extractor.ExtractorSWF\n&gt;     root:controller:processors:Archiver=object,\n&gt;     org.archive.modules.writer.WARCWriterProcessor\n&gt;     root:controller:processors:Archiver:prefix=string, lawsc\n&gt;     root:controller:processors:Updater=object,\n&gt;     org.archive.crawler.postprocessor.CrawlStateUpdater\n&gt;     root:controller:processors:LinksScoper=object,\n&gt;     org.archive.crawler.postprocessor.LinksScoper\n&gt;     root:controller:processors:LinksScoper:logger-module=auto,\n&gt;     root:loggerModule\n&gt;     root:controller:processors:LinksScoper:scope=reference, root:scope\n&gt;     root:controller:processors:Scheduler=object,\n&gt;     org.archive.crawler.postprocessor.FrontierScheduler\n&gt;     root:controller:processors:Store=object,\n&gt;     org.archive.modules.recrawl.PersistStoreProcessor\n&gt;     root:controller:statistics-tracker=object,\n&gt;     org.archive.crawler.framework.StatisticsTrackerImpl\n&gt; \n&gt; \n&gt; We are using Heritrix 2.0.2, in case that makes a difference, or it is \n&gt; known to have problems with the dedupe process.\n&gt; \n&gt; Thank you very much.\n&gt; \n&gt; \n\n"}}