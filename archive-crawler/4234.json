{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"64-mipZMehzpOYW5xw9X7NNlNJ2Ud-kas6Xjk4rtmpQ_yRQWtthf2Cjfyz1-V22rmUrX5BHVIv3zCoJX8MImI_jEeMERFmg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] recover from recovery logs.","postDate":"1178839488","msgId":4234,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2NDNBOUMwLjcwNzA0MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGYxZm9icitjcWZzQGVHcm91cHMuY29tPg==","referencesHeader":"PGYxZm9icitjcWZzQGVHcm91cHMuY29tPg=="},"prevInTopic":4219,"nextInTopic":0,"prevInTime":4233,"nextInTime":4235,"topicId":3774,"numMessagesInTopic":6,"msgSnippet":"... Yes, but unless you ve done a true checkpoint, its contents may be inconsistent. At checkpoints and after a crawl finishes cleanly, the info may be more","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 8160 invoked from network); 10 May 2007 23:21:28 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m43.grp.scd.yahoo.com with QMQP; 10 May 2007 23:21:28 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta10.grp.scd.yahoo.com with SMTP; 10 May 2007 23:21:28 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 2778E141C6F78\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 10 May 2007 16:21:23 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 30536-09-54 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 10 May 2007 16:21:20 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id D6159141BC444\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 10 May 2007 16:21:19 -0700 (PDT)\r\nMessage-ID: &lt;4643A9C0.7070405@...&gt;\r\nDate: Thu, 10 May 2007 16:24:48 -0700\r\nUser-Agent: Thunderbird 1.5.0.10 (X11/20070306)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f1fobr+cqfs@...&gt;\r\nIn-Reply-To: &lt;f1fobr+cqfs@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] recover from recovery logs.\r\nX-Yahoo-Group-Post: member; u=137285340; y=klYbIsW6-IJ_uPy5fJaLZHsWOG5n5xyBsz1jXNhN-yi7\r\nX-Yahoo-Profile: gojomo\r\n\r\nJohn Lekashman wrote:\n&gt; Hi Gordon,\n&gt; I ran into a challenge here as well.\n&gt; \n&gt; I did have a couple of questions on it.\n&gt; \n&gt; Is there anything in the on disk state directory from the \n&gt; old job e.g. all those fine .jdb files, that has relevance to\n&gt; the new crawl?\n\nYes, but unless you&#39;ve done a true checkpoint, its contents may be\ninconsistent. At checkpoints and after a crawl finishes cleanly, the \ninfo may be more useful. (In fact, the Heritrix 1.12 \nduplication-reduction functionality reuses some state from prior crawl&#39;s \nstate/*.jdb data for its purposes.)\n\n&gt; And, related to that, from looking at it, it appears all the\n&gt; priming consists of:\n&gt;  a. write a new recover.gz file, of &#39;successful&#39; downloads.\n&gt;  b. create a frontier in memory.\n&gt; \n&gt; True, or do I miss something?\n\nActually what happens involves two passes over the predecessor crawl&#39;s\ncombined recovery log.\n\nIn pass 1, all URIs that were &#39;finished&#39; in the previous crawl are\nconsidered &#39;already-included&#39; in the second crawl, meaning they won&#39;t be\n  rescheduld. (You have the option of considering only &#39;successes&#39; as\n&#39;finished&#39; or &#39;definitive failures&#39; as &#39;finished&#39; as well. Sometimes, a\nrecovery is a reasonable time to retry all previously retry-exhausted URIs.)\n\nIn pass 2, an attempt to reschedule every URI ever discovered by the\nfirst crawl is made. Of course, those marked &#39;already-included&#39; are not\nrescheduled. So at the end of pass 2, modulo some ordering issues, the\nsecond crawl&#39;s frontier has the same URIs the first crawl did at the\nmoment the recovery log ended (cleanly or not).\n\n(Also, to accelerate the process, the crawler begins active crawling \nbefore pass 2 completes, so some URIs may be rescheduled via rediscovery \nbefore the log-replay reschedules them.)\n\n&gt; I am not concerned about disk space, or time, but I do have\n&gt; some concern about re-crawling places I just was at.  Being\n&gt; more polite is more important every day.  (More on this, later.)\n&gt; \n&gt; Its not out of disk, just doing recovery from logs rather than\n&gt; checkpoint.  I have about 2 crawlers a week just stop working.\n&gt; They do respond to a kill -3, and later -9, but nothing else.\n&gt; \n&gt; Its better to do the recovery from the logs,\n&gt; as its more polite.  (No revisiting\n&gt; the places you just were.)\n\nWe have discussed the possibility of enabling a combined resume -- load\nfrom a checkpoint, but then also replay the recovery log from the\ncheckpoint on. However, as this jumbles the ordering and running stats\nmuch like a complete recovery-from-logs, its advantages seem minor for\nthose whose top priority is minimizing revisited URIs.\n\nWe&#39;d like to know more about any crawlers that stop: final progress-stat\nlog lines before stopping, any suspicious heritrix_out content, etc.\n\n&gt; What one has to do:\n&gt; - gunzip all the recover.gz.000xxx files\n&gt; cp recover.gz.00001 ../../recover/rec1.gz\n&gt; cd ../../recover/\n&gt; gunzip rec1.gz\n&gt; \n&gt; - add a return at the end of them.\n&gt;   cat &gt;&gt; rec1\n&gt; \n&gt; ^D\n&gt; . . . repeat\n&gt; \n&gt; Screw around with the last recover file, to make it usable.\n&gt; cp recover.gz ../../recover/rec.gz\n&gt; cd ../../recover/\n&gt; # Need zcat, cause guzip always tells you: loser, bad file.\n&gt; zcat rec.gz &gt; rec\n&gt; cat &gt;&gt; rec\n&gt; &lt;some random characters you make up to make the last broken\n&gt; url something vaguely valid&gt;\n&gt; ^D\n&gt; \n&gt; Then, \n&gt; cat rec1 rec2 rec3 rec4 rec &gt; recover\n&gt; \n&gt; And of course, make the new job, dealing with the repeated\n&gt; recover.gz.gz problem manually.\n&gt; \n&gt; Still, it appears more reliable, polite and repeatable than\n&gt; checkpointing, albeit with some information loss.\n\nThat&#39;s all reasonable. There&#39;s a lot of potential for improving this \nprocess, but it remains rare enough in our experience that such \nimprovements haven&#39;t been a priority.\n\n- Gordon @ IA\n\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt; Once a crawl hits an out-of-disk condition, it may be in an unresumable \n&gt;&gt; and uncheckpointable state -- cleanly recovering from everywhere this \n&gt;&gt; might happen would be very difficult.\n&gt;&gt;\n&gt;&gt; If you have a known-good checkpoint from earlier, you can recover from \n&gt;&gt; that, in a fresh JVM launch.\n&gt;&gt;\n&gt;&gt; If you want to recover by replaying the recovery logs, you&#39;ll have to \n&gt;&gt; concatenate them together first. This starts a new crawl, but then \n&gt;&gt; primes it to (1) not revisit anything marked as a &#39;success&#39; in the \n&gt;&gt; replayed logs; (2) fill its queues with the same waiting URIs from the \n&gt;&gt; logs. It doesn&#39;t recover all state from the previous crawl, though -- \n&gt;&gt; like running tallies, URI &#39;source&#39; tags (if used), and other data -- so \n&gt;&gt; if those are essential a checkpoint should be used.\n&gt;&gt;\n&gt;&gt; Hope this helps,\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}