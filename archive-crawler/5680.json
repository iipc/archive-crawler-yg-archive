{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"fatbKSYvkhQWJ-dbTBlHAkIJABsVOSf5jqEnFfiz7ZERs9FJoaOli4X6CzeGXwO5Kdjtge4Ldi4Mo7n3dU9PgRsYfSd_UMY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: lock contention in ServerCache.getServerFor()","postDate":"1234823198","msgId":5680,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5OTlFODFFLjkwMDA2MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGduYW9sOStpMnZnQGVHcm91cHMuY29tPg==","referencesHeader":"PGduYW9sOStpMnZnQGVHcm91cHMuY29tPg=="},"prevInTopic":5674,"nextInTopic":5685,"prevInTime":5679,"nextInTime":5681,"topicId":5665,"numMessagesInTopic":8,"msgSnippet":"ServerCache has been noted as a bottleneck before, so this is a very welcome result. Can you post a patch either here or to a JIRA issue for others to review","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 9737 invoked from network); 16 Feb 2009 22:26:37 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m49.grp.scd.yahoo.com with QMQP; 16 Feb 2009 22:26:37 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta16.grp.scd.yahoo.com with SMTP; 16 Feb 2009 22:26:37 -0000\r\nX-Received: (qmail 55846 invoked from network); 16 Feb 2009 22:26:36 -0000\r\nX-Received: from 70.137.166.153 (HELO ?10.0.13.7?) (70.137.166.153)\n  by relay02.pair.com with SMTP; 16 Feb 2009 22:26:36 -0000\r\nX-pair-Authenticated: 70.137.166.153\r\nMessage-ID: &lt;4999E81E.9000605@...&gt;\r\nDate: Mon, 16 Feb 2009 14:26:38 -0800\r\nUser-Agent: Thunderbird 2.0.0.19 (Windows/20081209)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;gnaol9+i2vg@...&gt;\r\nIn-Reply-To: &lt;gnaol9+i2vg@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: lock contention in ServerCache.getServerFor()\r\nX-Yahoo-Group-Post: member; u=137285340; y=wF2YNe0tRWlR38tsUTsuiguS6LiGmNVXgssJ6wJ740g2\r\nX-Yahoo-Profile: gojomo\r\n\r\nServerCache has been noted as a bottleneck before, so this is a very \nwelcome result. Can you post a patch either here or to a JIRA issue for \nothers to review and test?\n\nAs I&#39;d mentioned earlier in our offlist discussion, I didn&#39;t think the \nsimple caching approach would help much, because there are already two \nlevels of caching (CachedBDBMap&#39;s soft-reference object-identity cache, \nand BDB&#39;s byte-array cache) that should minimize the IO/lock-time when \nreading the same key multiple times in sequence.\n\nIt&#39;s good to see that the deeper lock-untangling offers such a big \nspeedup for your crawl.\n\n- Gordon @ IA\n\npbaclace wrote:\n&gt; The &quot;un-knotting&quot; performance change worked.  I see a 2X speedup in\n&gt; heritrix v1.14.2:\n&gt; \n&gt; * 460KB/sec (from 230KB/sec) network usage\n&gt; * 100% cpu with load between 15-19 (as reported by &quot;w&quot; in linux)\n&gt; * disk usage at 600KB/sec (from 300KB/sec) \n&gt; * number of established HTTP sockets:  25 (an average from 13 netstats)\n&gt; \n&gt; Basically, the same run took half the time.  As long as logging is not\n&gt; verbose, the TOE worker threads are blocked on writing the WARC files.\n&gt;  (Actually, this job writes out both WARC and ARC, so it could be\n&gt; improved.) \n&gt; \n&gt; The high load number might seem scary to some people, but it just\n&gt; means the cpu is fully utilized and more cores could help.\n&gt; \n&gt; It requires edits to 3 files plus 3 other files need trivial changes.\n&gt;  The un-knotting has not yet been tested against: multi-core,\n&gt; multi-processor, checkpointing, and recovery.\n&gt; \n&gt; \n&gt; Paul\n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, &quot;pbaclace&quot; &lt;pbaclace@...&gt; wrote:\n&gt;&gt;\n&gt;&gt; A test run of:\n&gt;&gt;   * Heritrix 1.14.2 on an AWS/EC2, small instance, with 100 worker\n&gt;&gt; threads, 1.3M seeds, 900MB heap\n&gt;&gt;\n&gt;&gt; Has the following resource utilization stats:\n&gt;&gt;\n&gt;&gt;   *  230KB/sec of the network\n&gt;&gt;   * 100% cpu with load between 7 and 13\n&gt;&gt;   * disk starts out at 300KB/sec, and 24 hours later is at 1MB/sec\n&gt;&gt;   * number of established HTTP sockets:  ranges from 1 to 7,\n&gt;&gt; occasional spiking to 14\n&gt;&gt;   * Full GC every 10 minutes\n&gt;&gt;\n&gt;&gt; The limiting resource is the cpu.  A one core machine should\n&gt;&gt; theoretically be able to saturate either the network or the disk\n&gt;&gt; bandwidth before the cpu hits the wall, unless it has heavy lock\n&gt;&gt; contention. \n&gt;&gt;\n&gt;&gt; See how many and where threads are waiting in some jstack thread dumps::\n&gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.9.threaddump  |sort |uniq -c\n&gt;&gt;      25         - waiting to lock &lt;0x5c357d90&gt; (a\n&gt;&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt;&gt;      61         - waiting to lock &lt;0x5c382828&gt; (a\n&gt;&gt; org.archive.crawler.datamodel.ServerCache)\n&gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.8.threaddump  |sort |uniq -c\n&gt;&gt;       7         - waiting to lock &lt;0x5c357d90&gt; (a\n&gt;&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt;&gt;      56         - waiting to lock &lt;0x5c382828&gt; (a\n&gt;&gt; org.archive.crawler.datamodel.ServerCache)\n&gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.7.threaddump  |sort |uniq -c\n&gt;&gt;      31         - waiting to lock &lt;0x5c357d90&gt; (a\n&gt;&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt;&gt;      62         - waiting to lock &lt;0x5c382828&gt; (a\n&gt;&gt; org.archive.crawler.datamodel.ServerCache)\n&gt;&gt;\n&gt;&gt; Examination of the FrontierScheduler lock shows that it is held in\n&gt;&gt; threaddumps 7 and 9 by a thread waiting for ServerCache.\n&gt;&gt;\n&gt;&gt; Most threads (about 90) are waiting for a lock on ServerCache in the\n&gt;&gt; method:\n&gt;&gt;\n&gt;&gt;   public synchronized CrawlServer getServerFor(String serverKey)\n&gt;&gt;\n&gt;&gt; Presumably, a simple name to host/server would be fast, but one thread\n&gt;&gt; holds the lock while doing a relatively long BDB read operation. \n&gt;&gt; Obviously, having disk io block all cache lookups is not optimal,\n&gt;&gt; especially when BDB has a lock per file (FileManager).  In my test\n&gt;&gt; case, the bdb data is 3.6GB and there are about 360 *.jdb files in the\n&gt;&gt; job state directory.  If requests to getServerFor(String) were not\n&gt;&gt; synchronized, then BDB should be able to read from multiple *.jdb file\n&gt;&gt; at the same time and threads requesting entries cached in memory by\n&gt;&gt; CachedBDBMap would not need to wait.  \n&gt;&gt;\n&gt;&gt;\n&gt;&gt; I think the following high gain, small code footprint improvements\n&gt;&gt; would help:\n&gt;&gt;\n&gt;&gt; * superficial thread-local caching (1 affected file)\n&gt;&gt; **  the ServerCache lookups are done in many code locations, so it\n&gt;&gt; seems each thread processing a uri might repeatedly do the same lookup\n&gt;&gt; and get stuck waiting\n&gt;&gt; **  a ThreadLocal cache of one key-value pair could be checked before\n&gt;&gt; the Maps in ServerCache.getServerFor(String) before synchronizing on\n&gt;&gt; this instance of ServerCache.\n&gt;&gt; **  this must not interfere with soft reference tracking, of course\n&gt;&gt;\n&gt;&gt; * deep un-knotting by lock-splitting and enabling more concurrency in\n&gt;&gt; ServerCache, CachedBdbMap, and BDB.\n&gt;&gt; ** drop synchronization of ServerCache.getServerFor(String)\n&gt;&gt; ** drop synchronization of  CachedBdbMap.get(Object)\n&gt;&gt; ** use ConcurrentHashMap for CachedBdbMap.memMap\n&gt;&gt; ** drop synchronization of CachedBdbMap.put(K,V) and expose\n&gt;&gt; putIfAbsent(K,V) if needed.\n&gt;&gt; *** ServerCache.createServerFor(String) loses synchronization when\n&gt;&gt; ServerCache.getServerFor(String) drops it.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; My particular crawl job exercises the ServerCache more than most jobs,\n&gt;&gt; but it is analogous to having a very wide, breadth-first crawl. \n&gt;&gt; Characteristics of this performance case are shared by all jobs that\n&gt;&gt; crawl many thousands of hosts.\n&gt;&gt;\n&gt;&gt; Since full GC was occurring about every 10 minutes, the lock\n&gt;&gt; contention was not due to full GC frequency.  A heap histogram showed\n&gt;&gt; about 3700 CrawlServer instances at the end of the run.\n&gt;&gt;\n&gt;&gt; If this un-knotting can work, there should be substantially better\n&gt;&gt; disk and network utilization. \n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Paul\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}