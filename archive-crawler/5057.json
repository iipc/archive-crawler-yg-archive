{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":334689939,"authorName":"Micah Wedemeyer","from":"Micah Wedemeyer &lt;mwedeme@...&gt;","replyTo":"LIST","senderId":"B-1oxnf43B3WLKFDBMMvJHo5I7PtKe_alQy-tC7r6fRRfFciwcyUazWRx_NKEdsfjV-pmOYMANR0QkslFbEjbJVlhzY4Lroqy70","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Multiple threads per site","postDate":"1205438433","msgId":5057,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3RDk4N0UxLjUwNjA0MDZAZW1vcnkuZWR1Pg==","inReplyToHeader":"PDY1NGU2MjE4MDgwMzEyMTM0OXFmYmIzMDc3d2Q4MGQxYWU0YTFjN2E5YjRAbWFpbC5nbWFpbC5jb20+","referencesHeader":"PDY1NGU2MjE4MDgwMzEyMTM0OXFmYmIzMDc3d2Q4MGQxYWU0YTFjN2E5YjRAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":5056,"nextInTopic":0,"prevInTime":5056,"nextInTime":5058,"topicId":5055,"numMessagesInTopic":3,"msgSnippet":"What you re describing could look like a DoS attack from the server side.  Do you have permission from the admins of the site to hit them that hard?  I d","rawEmail":"Return-Path: &lt;mwedeme@...&gt;\r\nX-Sender: mwedeme@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 84620 invoked from network); 13 Mar 2008 20:00:37 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m44.grp.scd.yahoo.com with QMQP; 13 Mar 2008 20:00:37 -0000\r\nX-Received: from unknown (HELO mr4.cc.emory.edu) (170.140.52.93)\n  by mta18.grp.scd.yahoo.com with SMTP; 13 Mar 2008 20:00:37 -0000\r\nX-Received: from [170.140.210.152] (emoryfloatdmz.cc.emory.edu [170.140.52.254])\n\tby mr4.cc.emory.edu (8.13.1/8.13.1) with ESMTP id m2DK0X8b015571\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 13 Mar 2008 16:00:34 -0400\r\nMessage-ID: &lt;47D987E1.5060406@...&gt;\r\nDate: Thu, 13 Mar 2008 16:00:33 -0400\r\nUser-Agent: Mozilla-Thunderbird 2.0.0.9 (X11/20080110)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;654e62180803121349qfbb3077wd80d1ae4a1c7a9b4@...&gt;\r\nIn-Reply-To: &lt;654e62180803121349qfbb3077wd80d1ae4a1c7a9b4@...&gt;\r\nX-Enigmail-Version: 0.95.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nX-emory.edu-MailScanner: Found to be clean\r\nX-emory.edu-MailScanner-From: mwedeme@...\r\nX-Spam-Status: No\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Micah Wedemeyer &lt;mwedeme@...&gt;\r\nSubject: Re: [archive-crawler] Multiple threads per site\r\nX-Yahoo-Group-Post: member; u=334689939\r\n\r\nWhat you&#39;re describing could look like a DoS attack from the server\nside.  Do you have permission from the admins of the site to hit them\nthat hard?  I&#39;d contact them first before trying such an intensive crawl.\n\nMicah\n\n\nJordan Mendler wrote:\n&gt; \n&gt; \n&gt; I have just deployed Heritrix and am in the process of trying to\n&gt; optimize is. We are running it on an 8-core/16GB RAM server and in the\n&gt; passed we have successfully used as many as 3000 threads on this machine\n&gt; when scraping many sites at once.\n&gt; \n&gt; Right now we only want to scrape 1 very large site, and it is going very\n&gt; slow with the 1 thread/site used by Heritrix. Is there any way to\n&gt; configure this, so that we can crawl this site faster by taking\n&gt; advantage of parallel crawls? We are only retrieving 2-5% of the\n&gt; URIs/sec that we were getting when scraping many sites at once.\n&gt; \n&gt; Thanks,\n&gt; Jordan\n&gt; \n\n"}}