{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":280593181,"authorName":"Erik Hetzner","from":"Erik Hetzner &lt;erik.hetzner@...&gt;","profile":"e_hetzner","replyTo":"LIST","senderId":"nCG4K_YjFpR0NY2cbo59b4P1jUstOHQvA8Q2Zo7wHMZEs51jkmEPBhmQ650Pvo-sT9hdN8H7VzEmWVrGkwT3SfDHkK6C1WS3z17o","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] duplicate files retreived","postDate":"1201112749","msgId":4923,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDg3dHpsNGNzYjYud2wlZXJpay5oZXR6bmVyQHVjb3AuZWR1Pg==","inReplyToHeader":"PGZuNHVpZCtvZW1sQGVHcm91cHMuY29tPg==","referencesHeader":"PGZuNHVpZCtvZW1sQGVHcm91cHMuY29tPg=="},"prevInTopic":4917,"nextInTopic":0,"prevInTime":4922,"nextInTime":4924,"topicId":4917,"numMessagesInTopic":2,"msgSnippet":"At Tue, 22 Jan 2008 14:28:29 -0000, ... Recovering crawls using recover.gz is approximate, not exact. See: ","rawEmail":"Return-Path: &lt;Erik.Hetzner@...&gt;\r\nX-Sender: Erik.Hetzner@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 83616 invoked from network); 23 Jan 2008 18:22:08 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m45.grp.scd.yahoo.com with QMQP; 23 Jan 2008 18:22:08 -0000\r\nX-Received: from unknown (HELO MAILGATE.UCOP.EDU) (128.48.123.92)\n  by mta15.grp.scd.yahoo.com with SMTP; 23 Jan 2008 18:22:08 -0000\r\nX-WSS-ID: 0JV40DJ-01-9JP-01\r\nX-Received: from EX.UCOP.EDU (p-irc-exbe01.ucop.edu [128.48.122.86])\n\tby MAILGATE.UCOP.EDU (Tumbleweed MailGate 3.5.0) with ESMTP id 26986B0AD06;\n\tWed, 23 Jan 2008 10:22:30 -0800 (PST)\r\nX-Received: from gales.cdlib.org ([128.48.204.228]) by EX.UCOP.EDU with Microsoft SMTPSVC(6.0.3790.3959);\n\t Wed, 23 Jan 2008 10:22:05 -0800\r\nDate: Wed, 23 Jan 2008 10:25:49 -0800\r\nMessage-ID: &lt;87tzl4csb6.wl%erik.hetzner@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;\r\nIn-Reply-To: &lt;fn4uid+oeml@...&gt;\r\nReferences: &lt;fn4uid+oeml@...&gt;\r\nUser-Agent: Wanderlust/2.15.5 (Almost Unreal) SEMI/1.14.6 (Maruoka)\n FLIM/1.14.8 (=?ISO-8859-4?Q?Shij=F2?=) APEL/10.7 Emacs/22.1\n (i486-pc-linux-gnu) MULE/5.0 (SAKAKI)\r\nMIME-Version: 1.0 (generated by SEMI 1.14.6 - &quot;Maruoka&quot;)\r\nContent-Type: multipart/signed;\n boundary=&quot;pgp-sign-Multipart_Wed_Jan_23_10:25:48_2008-1&quot;; micalg=pgp-sha1;\n protocol=&quot;application/pgp-signature&quot;\r\nContent-Transfer-Encoding: 7bit\r\nReturn-Path: erik.hetzner@...\r\nX-OriginalArrivalTime: 23 Jan 2008 18:22:05.0732 (UTC) FILETIME=[DB8F3640:01C85DEC]\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Erik Hetzner &lt;erik.hetzner@...&gt;\r\nSubject: Re: [archive-crawler] duplicate files retreived\r\nX-Yahoo-Group-Post: member; u=280593181; y=iW5a8ovVPVi-w2b8FuZwaJlIrHayhBn5kALiSdI5yvkVnreq\r\nX-Yahoo-Profile: e_hetzner\r\n\r\n\r\n--pgp-sign-Multipart_Wed_Jan_23_10:25:48_2008-1\r\nContent-Type: text/plain; charset=US-ASCII\r\n\r\nAt Tue, 22 Jan 2008 14:28:29 -0000,\n&quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt; wrote:\n&gt;\n&gt; I performed crawl A and terminated crawl A on my own after several\n&gt; days. I later decided I wanted to continue gathering content where\n&gt; crawl A was terminated. I initated crawl B as a recovery from crawl A\n&gt; using the recover.gz. Upon inspection there a couple hundred files\n&gt; that are duplicate across the two and are non robot.txt files i.e.\n&gt; index.html and other standard .html pages.\n&gt;\n&gt; I thought the crawl would never gather duplicates pages like those\n&gt; mentioned.\n\nRecovering crawls using recover.gz is approximate, not exact. See:\n&lt;http://crawler.archive.org/articles/user_manual/outside.html#recover&gt;,\nesp.:\n\n| If a crawl crashes, the recover.gz journal can be used to recreate\n| APPROXIMATELY the status of the crawler at the time of the crash.\n| [My emphasis.]\n\nI believe that checkpointing offers a more exact recovery. See:\n&lt;http://crawler.archive.org/articles/user_manual/outside.html#checkpoint&gt;\n\nbest,\nErik Hetzner\n;; Erik Hetzner, California Digital Library\n;; gnupg key id: 1024D/01DB07E3\n\r\n--pgp-sign-Multipart_Wed_Jan_23_10:25:48_2008-1\r\nContent-Type: application/pgp-signature\r\n\r\n[ Attachment content not displayed ]\r\n--pgp-sign-Multipart_Wed_Jan_23_10:25:48_2008-1--\r\n\n"}}