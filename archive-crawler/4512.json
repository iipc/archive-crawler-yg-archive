{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"ZqMuxJF3z2WUyddkKzvcm66EIaR04-Lk7Tf20CfidR6h1Lf4Dv2MFunv0H2Uj9DcD_8BtKkMaYqMCLEvtERwVPVWl7cTfYw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] job state directory","postDate":"1187827622","msgId":4512,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2Q0NDRkE2LjMwODAwQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDExODc4MjM1MzAuMjc5Ny4yLmNhbWVsQHB1ZGdlPg==","referencesHeader":"PDExODc4MDE4NzYuNjQwNy4yLmNhbWVsQHB1ZGdlPiAgPDQ2Q0NBMDM3LjUwNjAyMDRAYmF5YXJlYS5uZXQ+IDwxMTg3ODIzNTMwLjI3OTcuMi5jYW1lbEBwdWRnZT4="},"prevInTopic":4511,"nextInTopic":0,"prevInTime":4511,"nextInTime":4513,"topicId":4508,"numMessagesInTopic":5,"msgSnippet":"The state directory is the home of the BerkeleyDB-JE environment used by the crawler. The three main things stored there are: (1) A series of disk-backed maps","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 7727 invoked from network); 23 Aug 2007 00:05:35 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m49.grp.scd.yahoo.com with QMQP; 23 Aug 2007 00:05:35 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta10.grp.scd.yahoo.com with SMTP; 23 Aug 2007 00:05:35 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 33FBE1416BE6B\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 22 Aug 2007 17:05:24 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 20879-01-51 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 22 Aug 2007 17:05:23 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 8CD261416BE35\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 22 Aug 2007 17:05:23 -0700 (PDT)\r\nMessage-ID: &lt;46CCCFA6.30800@...&gt;\r\nDate: Wed, 22 Aug 2007 17:07:02 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (X11/20070604)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;1187801876.6407.2.camel@pudge&gt;  &lt;46CCA037.5060204@...&gt; &lt;1187823530.2797.2.camel@pudge&gt;\r\nIn-Reply-To: &lt;1187823530.2797.2.camel@pudge&gt;\r\nContent-Type: text/plain; charset=US-ASCII; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] job state directory\r\nX-Yahoo-Group-Post: member; u=137285340; y=fZJa2JEC33yC-wR8CoA1ayALONuzFYOzdrzGRtL56K5p\r\nX-Yahoo-Profile: gojomo\r\n\r\nThe state directory is the home of the BerkeleyDB-JE environment used by \nthe crawler.\n\nThe three main things stored there are:\n\n(1) A series of disk-backed maps with one entry per host, server, or \nqueue discovered. These grow with the number of hosts encountered, and \ndo not shrink. It mainly becomes a factor in very broad crawls or those \nhitting expansive collections subdomains (most often when DNS \nwildcarding is encountered).\n\n(2) If using the default BdbUriUniqFilter as the Frontier&#39;s \nalreadyIncluded structure, the list of all previously scheduled URI \nfingerprints. This grows with the total number of URIs scheduled and \ndoes not shrink. (If choosing an alternate UriUniqFilter, like the \nBloom-based ones, this role is handled all in RAM and not the state \ndirectory.)\n\n(3) All queued URIs. This grows with the number of URIs waiting to be \ncrawled, but shrinks as they are completed.\n\nOther factors:\n\n(1) BDB-JE uses an append-only storage format which means obsolete data \nlives on interspersed with live data -- essentially, on-disk garbage. \nIts background cleaning/compacting parameters can be adjusted to \ntolerate less dead data -- achieving a smaller disk footprint at the \ncost of more processing/IO.\n\n(2) The maps and URIs above are essentially serialized Java objects, \nalbeit with BDB&#39;s more compact serialization.\n\nWe usually crawl on machines with at least 1.5TB and as much as 3TB of \ndisk space, so don&#39;t usually have problems with very big state \ndirectories. There&#39;s certainly room for optimizations there; some \nprofiles where disks are constrained but CPU plentiful might even \nconsider gzipping the serialized objects for some easy savings.\n\nSo: there&#39;s no steady-state as long as the crawl is still discovering \nnew hosts and URIs. Once it stops discovering new hosts and URIs, the \nstate size should decrease as URIs are completed (removed from queues), \nbut even there&#39;s still a floor determined by the map objects, \nalreadyIncluded structure, and BDB-JE utilization parameters.\n\n- Gordon @ IA\n\nTed Dziuba wrote:\n&gt; So is the size of the state directory a function of number of URLs\n&gt; visited, and not amount of data downloaded?\n&gt; \n&gt; Tangentially, I think that EC2 will do for our purposes, we&#39;re not\n&gt; looking to get an enormous crawl.  I had just blown it on the initial\n&gt; configuration of Heritrix.\n&gt; \n&gt; Thanks,\n&gt; \n&gt; Ted\n&gt; \n&gt; On Wed, 2007-08-22 at 13:44 -0700, lekash wrote:\n&gt;&gt; Nope. No steady state.\n&gt;&gt; 8G is chump change for a long running crawler.\n&gt;&gt; Its a state db of where have you been. So, of course it\n&gt;&gt; grows, as you run longer.\n&gt;&gt;\n&gt;&gt; I believe the largest I&#39;ve seen is about 2T, before I ended\n&gt;&gt; a crawl.\n&gt;&gt;\n&gt;&gt; I thought about crawling with it on EC2, and decided it would need\n&gt;&gt; a different architecture to be viable there.\n&gt;&gt;\n&gt;&gt; John\n&gt;&gt;\n&gt;&gt; Ted Dziuba wrote:\n&gt;&gt;\n&gt;&gt;&gt; I&#39;ve got Heritrix running in Amazon EC2, and I&#39;m still mucking\n&gt;&gt; around\n&gt;&gt;&gt; with the configuration.\n&gt;&gt;&gt;\n&gt;&gt;&gt; I let a crawl run for a few days, and the job&#39;s state/ directory got\n&gt;&gt;&gt; huge, like 8 GB, to the point that it filled up EC2&#39;s / mount point.\n&gt;&gt; I\n&gt;&gt;&gt; moved everything to /mnt, where you&#39;re supposed to throw data on\n&gt;&gt; EC2,\n&gt;&gt;&gt; but my question is, why does this directory grow so large? Does it\n&gt;&gt; have\n&gt;&gt;&gt; a steady-state size?\n&gt;&gt;&gt;\n&gt;&gt;&gt; Ted\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}