{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"atcdvvZmMo4YZxKNt_UtiF2soUn--x7-S78Ynu6iX05U7HZmxFao6LsegdgdX-PsrCA1-TurF6oSrisP5giSRF31VzpFU10","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Disk Space Problem","postDate":"1302906349","msgId":7111,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREQThDNUVELjcwNDAyMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDREQTU4QzVDLjMwMjAyMDVAYmliYWxleC5vcmc+","referencesHeader":"PDREQTFBM0Q4LjMwOTA3MDhAYmliYWxleC5vcmc+IDw0REEyQjRDRi42MDIwNDA5QGFyY2hpdmUub3JnPiA8NERBNThDNUMuMzAyMDIwNUBiaWJhbGV4Lm9yZz4="},"prevInTopic":7101,"nextInTopic":0,"prevInTime":7110,"nextInTime":7112,"topicId":7092,"numMessagesInTopic":4,"msgSnippet":"... I think the most common technique (in both H1 and H3) is to adjust the scope rules in the running crawl, and make sure the Prescoper processor is present","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 29999 invoked from network); 15 Apr 2011 22:25:51 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.sp2.yahoo.com with QMQP; 15 Apr 2011 22:25:51 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta2.grp.sp2.yahoo.com with SMTP; 15 Apr 2011 22:25:51 -0000\r\nX-Received: (qmail 20997 invoked by uid 0); 15 Apr 2011 22:25:50 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay01.pair.com with SMTP; 15 Apr 2011 22:25:50 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4DA8C5ED.7040204@...&gt;\r\nDate: Fri, 15 Apr 2011 15:25:49 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.15) Gecko/20110303 Thunderbird/3.1.9\r\nMIME-Version: 1.0\r\nTo: Mahmoud Mubarak &lt;mahmoud.mubarak@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;4DA1A3D8.3090708@...&gt; &lt;4DA2B4CF.6020409@...&gt; &lt;4DA58C5C.3020205@...&gt;\r\nIn-Reply-To: &lt;4DA58C5C.3020205@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Disk Space Problem\r\nX-Yahoo-Group-Post: member; u=137285340; y=vZXiBV57fijA20TICRbWqBczMC2c35fKm-3VJAftumNa\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 4/13/11 4:43 AM, Mahmoud Mubarak wrote:\n&gt;\n&gt; Thanks, Gordon for your reply.\n&gt;&gt; A crawler collecting many valueless URIs deep on &#39;trap&#39; paths will have\n&gt;&gt; a larger state directory, so keeping an eye on the largest queues, and\n&gt;&gt; adding mid-crawl adjustments to either � (1) block more unwanted URIs\n&gt;&gt; from being enqueued; or (2) quickly discard unwanted URIs from where\n&gt;&gt; they&#39;ve collected � may help shrink your state directory significantly.\n&gt;&gt;\n&gt;\n&gt; I can get the largest queues from frontier summary report interface.\n&gt; But, how can I block URIs from being queued or even discard unwanted URIs?\n\nI think the most common technique (in both H1 and H3) is to adjust the \nscope rules in the running crawl, and make sure the &#39;Prescoper&#39; \nprocessor is present and that &#39;recheckScope&#39; is true. (Rechecking the \nscope is wasteful if you never change the scope mid-crawl.) Then, URIs \nthat fail the new scope are quickly rejected when they come off their \nqueues. (These late-rejected URIs *are* logged to crawl.log, because \nthey were at one point enqueued. URIs that fail initial scope testing \nnever appear in crawl.log.)\n\nOften, our crawl operators will include an extra Regex- and/or \nSURT-based scoping rule for REJECTing URIs at crawl start, initially \nempty or disabled. (Our default configuration in H3 includes such a \nrule.) Then, as the crawl progresses, new regexes/SURT-prefixes are added.\n\nModifying the existing rules mid-crawl is best done while the crawl is \npaused. In H1 it&#39;s done via the same web interface as was initially \nconfigured. In H3, you need to use the &#39;Bean browse&#39; or &#39;Scripting \nconsole&#39; to mutate the live beans with the new info. OR, you could \ncheckpoint the crawl, terminate it, edit the CXML, and relaunch as a \nrecovery from the checkpoint.\n\nIn H1 a paused crawl also offers a &#39;view/edit&#39; web page for editing \nexisting queues by regular-expression � but it&#39;s pretty slow and the \ncrawl must remain paused as it works. In H3 a careful expert user could \nprobably achieve the same effect with a script in the &#39;Scripting console&#39;.\n\nOften new rules can be narrowly applied to H1 host-override or H3 \n&#39;Sheet&#39;-overlaid domain- or SURT-prefix-specific subset of URIs. This is \nespecially important to avoid applying a long list of global block \nregexes everywhere that are only relevant for a few domains/URIs. \nMid-crawl sheet-overlay adjustments in H3 require using the &#39;Scripting \nconsole&#39;.\n\nOne other old trick is to make use of the Prescoper&#39;s &#39;blockAll&#39; \nsetting. If &#39;true&#39;, it treats every URI it sees as scope-blocked. Of \ncourse, setting this globally would essentially end the crawl. If \ninstead it&#39;s left &#39;false&#39; globally, but then \nH1-overriden/H3-sheet-overlaid to be &#39;true&#39; for some subset of URIs, \njust those URIs are quick-rejected when they come off their queues. So \nthis can work like other scope rule updates to quickly unload/reject all \nURIs in a domain queue that&#39;s no longer of interest.\n\nI&#39;d love to be able to promote a few of these common techiques into the \nH3 user-interface as one-click or simple-form operations, to clear or \nfilter an existing queue/site-rules... but that may have to wait for a \nfuture release that focuses on web usability, or an outside \ncontribution. (If anyone wants to work on this, let me know and we can \nbrainstorm features/steps and discuss the best way to integrate with \nother features.)\n\n- Gordon @ IA\n\n"}}