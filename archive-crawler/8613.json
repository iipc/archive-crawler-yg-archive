{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"ShtCqUO4ApSlJzVbwR8yclXjHYOQPfb6bJp9q06hH9-6TJ1w56rBUE3CGlM7lI9oGb08RYyViuy9RKmfKrfQ0M3sSWl3Ohw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Rejecting too many hops","postDate":"1411169039","msgId":8613,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU0MUNCQjBGLjQwODAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGx2Z2g4ZCtqNzBkcjZAWWFob29Hcm91cHMuY29tPg==","referencesHeader":"PGx2Z2g4ZCtqNzBkcjZAWWFob29Hcm91cHMuY29tPg=="},"prevInTopic":8611,"nextInTopic":8614,"prevInTime":8611,"nextInTime":8614,"topicId":8611,"numMessagesInTopic":6,"msgSnippet":"... Can you be more specific about what you mean by depth ? After giving the crawler a starting URL (seed), do you want it to follow all links to all","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 32346 invoked by uid 102); 19 Sep 2014 23:24:04 -0000\r\nX-Received: from unknown (HELO mtaq5.grp.bf1.yahoo.com) (10.193.84.36)\n  by m7.grp.bf1.yahoo.com with SMTP; 19 Sep 2014 23:24:04 -0000\r\nX-Received: (qmail 17773 invoked from network); 19 Sep 2014 23:24:04 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (98.139.245.164)\n  by mtaq5.grp.bf1.yahoo.com with SMTP; 19 Sep 2014 23:24:04 -0000\r\nX-Received: (qmail 6836 invoked by uid 0); 19 Sep 2014 23:24:03 -0000\r\nX-Received: from 67.160.203.122 (HELO probook.local) (67.160.203.122)\n  by relay02.pair.com with SMTP; 19 Sep 2014 23:24:03 -0000\r\nX-pair-Authenticated: 67.160.203.122\r\nMessage-ID: &lt;541CBB0F.4080303@...&gt;\r\nDate: Fri, 19 Sep 2014 16:23:59 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;lvgh8d+j70dr6@...&gt;\r\nIn-Reply-To: &lt;lvgh8d+j70dr6@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nSubject: Re: [archive-crawler] Rejecting too many hops\r\nX-Yahoo-Group-Post: member; u=137285340; y=6hqK5ifDk6Pp2qK1UlU1QKHljkeKSzN71223hl9lT7fa\r\nX-Yahoo-Profile: gojomo\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\n\r\nOn 9/18/14, 11:08 PM, kapiljadhav4@... [archive-crawler] wrote:\n&gt; HI\n&gt; I want to crawl urls to a particular depth.so how to achieve this? I am\n&gt; using heritrix 1.14.3\n&gt;\n&gt; I tried max-hops and max-path-depth\n\nCan you be more specific about what you mean by &#39;depth&#39;?\n\nAfter giving the crawler a starting URL (seed), do you want it to follow \nall links to all hostnames? Or just some?\n\nWhat setting values did you try, and did those result in *too little* or \n*too much* collected?\n\n- Gordon\n\n"}}