{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":254719850,"authorName":"steve@archive.org","from":"&quot;steve@...&quot; &lt;steve@...&gt;","profile":"stearcorg","replyTo":"LIST","senderId":"Fj2WSEdMbGJjMJLpaZwNHJDz246Fkh2gjKHEjqlqBxfhAA2TF3cRkXs8Is7zk2qU6u9vfHyfjkZWV8751yyYAsIxOA5YRv5AlEgjJLNo","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] twitter crawling","postDate":"1245349726","msgId":5895,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBM0E4NzVFLjgwMzA5MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRBM0EzOUVGMDIwMDAwMTIwMDNCOEJDNEBudGd3Z2F0ZS5sb2MuZ292Pg==","referencesHeader":"PDRBM0EzOUVGMDIwMDAwMTIwMDNCOEJDNEBudGd3Z2F0ZS5sb2MuZ292Pg=="},"prevInTopic":5893,"nextInTopic":5896,"prevInTime":5894,"nextInTime":5896,"topicId":5893,"numMessagesInTopic":4,"msgSnippet":"... i believe the search API limit is 1500 status up to 1.5 weeks back, and the max rpp is 100. so you can get 15 pages of 100 statuses, or 100 pages of 15","rawEmail":"Return-Path: &lt;steve@...&gt;\r\nX-Sender: steve@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 39124 invoked from network); 18 Jun 2009 18:28:54 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m7.grp.re1.yahoo.com with QMQP; 18 Jun 2009 18:28:54 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta2.grp.sp2.yahoo.com with SMTP; 18 Jun 2009 18:28:53 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id DBA74269559\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 18 Jun 2009 11:28:48 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id TkZkgCog-cMg for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 18 Jun 2009 11:28:47 -0700 (PDT)\r\nX-Received: from takomaki.local (c-67-170-223-242.hsd1.ca.comcast.net [67.170.223.242])\n\tby mail.archive.org (Postfix) with ESMTPSA id AEB00269539\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 18 Jun 2009 11:28:47 -0700 (PDT)\r\nMessage-ID: &lt;4A3A875E.8030906@...&gt;\r\nDate: Thu, 18 Jun 2009 11:28:46 -0700\r\nUser-Agent: Thunderbird 2.0.0.21 (Macintosh/20090302)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;4A3A39EF02000012003B8BC4@...&gt;\r\nIn-Reply-To: &lt;4A3A39EF02000012003B8BC4@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;steve@...&quot; &lt;steve@...&gt;\r\nSubject: Re: [archive-crawler] twitter crawling\r\nX-Yahoo-Group-Post: member; u=254719850; y=1MVj7o8SectsTMMP9NnjDXUzE4DZtk0fv0wmHtRVNy0-\r\nX-Yahoo-Profile: stearcorg\r\n\r\nOn 6/18/09 9:58 AM, Gina Jones wrote:\n&gt; Twitter provides up to 100 pages of what has been tweeted about the \n&gt; query term.\n\ni believe the search API limit is 1500 status up to 1.5 weeks\nback, and the max rpp is 100. so you can get 15 pages of 100\nstatuses, or 100 pages of 15 statuses[1].\n\n&gt; Unfortunately, the max-id for the additional 99 pages always \n&gt; start with the number of the latest tweet on the fly  so I can&#39;t really\n&gt; specify that because I have to be concerned with users accessing it via \n&gt; wayback as part of the collection.\n\nyou can use the &quot;since&quot; and &quot;until&quot; operators[2] to exclude\nstatuses outside of the day of interest, and the max_id is\nnot needed to capture all available statuses. e.g. statuses\non 2009-06-15\n\nhttp://search.twitter.com/search?q=sotomayor+since:2009-06-14+until:2009-06-15\n\nreturns 9 pages of results with rpp=100. a 404 is encountered\nfor pages outside of the range.\n\n &gt; I am trying to maximize the efficiency of this crawl, limiting the\n &gt; extraneous stuff.\n\ni believe you could remove the TransclusionDecideRule to keep\nfrom crawling content outside of the twitter search result[3].\n\n\n/steve@...\n\n\n[1] pagination limits\nhttp://apiwiki.twitter.com/Things-Every-Developer-Should-Know#6Therearepaginationlimits\n\n[2] Twitter Search API Method: search\nhttp://apiwiki.twitter.com/Twitter-Search-API-Method%3A+search\n\n[3] A.1. Avoiding Too Much Dynamic Content\nhttp://crawler.archive.org/articles/user_manual/usecases.html\n\n"}}