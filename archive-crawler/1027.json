{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"LJks7WEO2O_MZr2pC7_j7iXl7HT9JJtmLdYgJsDglSyqEtQS5qhnVMlRMS2oKx86vPjsqQJm2q4o0FsnVI4BFKCylkXu4y5ilsK5Ie4tCQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] Large experimental crawl","postDate":"1095933005","msgId":1027,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBEMTZAbG9raS5ib2suaGkuaXM+","inReplyToHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDI5QzRFNkNAbG9raS5ib2suaGkuaXM+"},"prevInTopic":1026,"nextInTopic":1038,"prevInTime":1026,"nextInTime":1028,"topicId":1005,"numMessagesInTopic":6,"msgSnippet":"Hi Sebastian, The main problem with running broad crawls (over a larger number of hosts) lies in the memory overhead associated with each active host. That is,","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 54365 invoked from network); 23 Sep 2004 09:49:58 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m16.grp.scd.yahoo.com with QMQP; 23 Sep 2004 09:49:58 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta3.grp.scd.yahoo.com with SMTP; 23 Sep 2004 09:49:58 -0000\r\nReceived: (qmail 1466 invoked by uid 100); 23 Sep 2004 09:39:22 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (kris@...@130.208.152.83)\n  by mail-dev.archive.org with RC4-MD5 encrypted SMTP; 23 Sep 2004 09:39:22 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Thu, 23 Sep 2004 09:50:05 -0000\r\nMessage-ID: &lt;D958110B273CD511ACC100B0D079AA4E01960D16@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0039_01C4A152.B69D5150&quot;\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nImportance: Normal\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1441\r\nIn-Reply-To: &lt;D958110B273CD511ACC100B0D079AA4E029C4E6C@...&gt;\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.6 required=7.0 tests=AWL,CLICK_BELOW,\n\tHTML_FONTCOLOR_BLUE,HTML_FONTCOLOR_UNKNOWN,HTML_MESSAGE autolearn=ham \n\tversion=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] Large experimental crawl\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n------=_NextPart_000_0039_01C4A152.B69D5150\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi Sebastian,\n\n \n\nThe main problem with running broad crawls (over a larger=\r\n number of hosts)\nlies in the memory overhead associated with each active h=\r\nost.\n\n \n\nThat is, each host that is being crawled requires memory, both for=\r\n host\nspecific information and the =91top=92 of the per host queue of waiti=\r\nng URIs.\n\n \n\nOne of the best way to combat this is to enable site first, li=\r\nmiting the\nnumber of active hosts to the bare minimum needed to fully occup=\r\ny the\ncrawler. Judging from your post I=92m unsure if that is acceptiable t=\r\no you\nsince this will favor certain hosts until they are exhausted before m=\r\noving\non.\n\n \n\nAnother thing that can be done to mitigate the memory use is =\r\nto limit the\nnumber of URIs kept in memory for each host. This is done by s=\r\netting the\n=93host-queues-memory-capacity=94 setting to a smaller value. Th=\r\ne default is\n200, with a true broad crawl this is too much. I=92d suggest s=\r\nomething in the\nrange of 20-50. A low value will incur more disk access but=\r\n save memory.\n\n \n\nOther then the site-first and host-queues-memory-capacity=\r\n there isn=92t all\nthat much you can to limit memory use by configuring Her=\r\nitrix. I know the\nguys at the Archive are looking at some ideas for limitin=\r\ng memory use\nfurther but that is still a long way away.\n\n \n\nAs for hardware=\r\n and heap size. Well the latter is easy. Assign as large a\nheap as the hard=\r\nware allows. I=92ve been running on a macine with 1.5GB RAM\nand I usually a=\r\nssign 1.25GB to the java heap. \n\nAs for hardware you=92ll want plenty of me=\r\nmory (duh) and a fast processor also\nhelps if you are concerned about the s=\r\npeed the crawl runs at. The crawl\ntends to be limited by the processing pow=\r\ner availible. If you choose to\nlimit memory use by lowering the host-queues=\r\n-memory-capacity significantly a\nfast HD will be very useful. I=92d suggest=\r\n having different HD for\nstate/scratch/log files and ARC files.\n\n \n\n- Kris\n=\r\n\n \n\n-----Original Message-----\nFrom: Sebastian de Castelberg [mailto:sdecas=\r\nte@...] \nSent: 23. september 2004 09:32\nTo: archive-crawler@yahoogroups.=\r\ncom\nSubject: [archive-crawler] Large experimental crawl\n\n \n\nHi,\n\nfor a rese=\r\narch-project we have to implement two different random-walk \nalgorithms for=\r\n uniform page sampling. This needs us to gather about \n2-4Mio. URL&#39;s.\nSo we=\r\n have to chose an adapted Broad Crawl, which chooses the URL&#39;s, \nwhich are =\r\nfed back into the frontier, randomly. So the fetch queue\nwouldn&#39;t grow expo=\r\nnential. We also do not need to write the whole \ncontent to disk.\nHeritrix =\r\nseemed to work quite well as crawler for antother project \n(thanks for the =\r\ngood development work at this place). But we got often \nproblems, based on =\r\nmemory limitations.\n\nOn the known limitations page, there&#39;s written that it=\r\n is possible to \ncrawl about 6Mio URL&#39;s and about 10000 hosts with default =\r\nsettings.\nMy question is: What&#39;s the best setup to reach this number of URL=\r\n&#39;s\n(HW/Java heap size/Heritrix config)?\nDo we need special hardware, or can=\r\n it be done with a common pc (p4 \n2.6GHz 512-1024 MB ram)?\n\nWe planned to u=\r\nse Debian GNU/Linux as os. Maybe there&#39;s someone who has \nalready experienc=\r\nes with large-scaled crawls and can give me some hints.\n\nthanks\nsebastian d=\r\ne castelberg\n\n\n\n\n\nYahoo! Groups Sponsor\n\n\n\nADVERTISEMENT\n \n&lt;http://us.ard.y=\r\nahoo.com/SIG=3D129bk1u5n/M=3D295196.4901138.6071305.3001176/D=3Dgr\noups/S=\r\n=3D1705004924:HM/EXP=3D1096018592/A=3D2128215/R=3D0/SIG=3D10se96mf6/*http:/=\r\ncompa\nnion.yahoo.com&gt; click here\n\n\n \n&lt;http://us.adserver.yahoo.com/l?M=3D29=\r\n5196.4901138.6071305.3001176/D=3Dgroups/S=3D\n:HM/A=3D2128215/rand=3D3358524=\r\n30&gt; \n\n \n\n  _____  \n\nYahoo! Groups Links\n\n*\tTo visit your group on the web, =\r\ngo to:\nhttp://groups.yahoo.com/group/archive-crawler/\n  \n*\tTo unsubscribe f=\r\nrom this group, send an email to:\narchive-crawler-unsubscribe@yahoogroups.c=\r\nom\n&lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=3DUnsubscrib=\r\ne&gt; \n  \n*\tYour use of Yahoo! Groups is subject to the Yahoo! Terms of Servic=\r\ne\n&lt;http://docs.yahoo.com/info/terms/&gt; . \n\n\r\n------=_NextPart_000_0039_01C4A152.B69D5150\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;html&gt;\n\n&lt;head&gt;\n&lt;META HTTP-EQUIV=3D&quot;Content-Type&quot; CONTENT=3D&quot;text/html; char=\r\nset=3Diso-8859-1&quot;&gt;\n\n\n&lt;meta name=3DGenerator content=3D&quot;Microsoft Word 10 (f=\r\niltered)&quot;&gt;\n\n&lt;style&gt;\n&lt;!--\n /* Font Definitions */\n @font-face\n\t{font-family:=\r\nWingdings;\n\tpanose-1:5 0 0 0 0 0 0 0 0 0;}\n@font-face\n\t{font-family:Tahoma;=\r\n\n\tpanose-1:2 11 6 4 3 5 4 4 2 4;}\n /* Style Definitions */\n p.MsoNormal, li=\r\n.MsoNormal, div.MsoNormal\n\t{margin:0cm;\n\tmargin-bottom:.0001pt;\n\tfont-size:=\r\n12.0pt;\n\tfont-family:&quot;Times New Roman&quot;;}\na:link, span.MsoHyperlink\n\t{color:=\r\nblue;\n\ttext-decoration:underline;}\na:visited, span.MsoHyperlinkFollowed\n\t{c=\r\nolor:blue;\n\ttext-decoration:underline;}\ntt\n\t{font-family:&quot;Courier New&quot;;}\nsp=\r\nan.EmailStyle18\n\t{font-family:Arial;\n\tcolor:navy;}\n@page Section1\n\t{size:59=\r\n5.3pt 841.9pt;\n\tmargin:72.0pt 90.0pt 72.0pt 90.0pt;}\ndiv.Section1\n\t{page:Se=\r\nction1;}\n /* List Definitions */\n ol\n\t{margin-bottom:0cm;}\nul\n\t{margin-bott=\r\nom:0cm;}\n--&gt;\n&lt;/style&gt;\n\n&lt;/head&gt;\n\n&lt;body lang=3DIS link=3Dblue vlink=3Dblue&gt;\n\n=\r\n&lt;div class=3DSection1&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy fa=\r\nce=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;H=\r\ni Sebastian,&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=\r\n=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;col=\r\nor:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 colo=\r\nr=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;co=\r\nlor:navy&#39;&gt;The main problem with running broad crawls\n(over a larger number =\r\nof hosts) lies in the memory overhead associated with\neach active host.&lt;/sp=\r\nan&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DAri=\r\nal&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;&lt;/s=\r\npan&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DAr=\r\nial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;That is,=\r\n each host that is being crawled\nrequires memory, both for host specific in=\r\nformation and the &#8216;top&#8217;\nof the per host queue of waiting URIs.&lt;=\r\n/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3D=\r\nArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;=\r\n&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=\r\n=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;One=\r\n of the best way to combat this is to\nenable site first, limiting the numbe=\r\nr of active hosts to the bare minimum\nneeded to fully occupy the crawler. J=\r\nudging from your post I&#8217;m unsure if\nthat is acceptiable to you since =\r\nthis will favor certain hosts until they are\nexhausted before moving on.&lt;/s=\r\npan&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DAr=\r\nial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;&lt;/=\r\nspan&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DA=\r\nrial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;Another=\r\n thing that can be done to mitigate\nthe memory use is to limit the number o=\r\nf URIs kept in memory for each host.\nThis is done by setting the &#8220;hos=\r\nt-queues-memory-capacity&#8221; setting\nto a smaller value. The default is =\r\n200, with a true broad crawl this is too\nmuch. I&#8217;d suggest something =\r\nin the range of 20-50. A low value will incur\nmore disk access but save mem=\r\nory.&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy fa=\r\nce=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&=\r\nnbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy f=\r\nace=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;=\r\nOther then the site-first and host-queues-memory-capacity\nthere isn&#8217;t=\r\n all that much you can to limit memory use by configuring\nHeritrix. I know =\r\nthe guys at the Archive are looking at some ideas for limiting\nmemory use f=\r\nurther but that is still a long way away.&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMso=\r\nNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n1=\r\n0.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMs=\r\noNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n=\r\n10.0pt;font-family:Arial;color:navy&#39;&gt;As for hardware and heap size. Well th=\r\ne\nlatter is easy. Assign as large a heap as the hardware allows. I&#8217;ve=\r\n been\nrunning on a macine with 1.5GB RAM and I usually assign 1.25GB to the=\r\n java\nheap. &lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 color=\r\n=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-size:\n10.0pt;font-family:Arial;col=\r\nor:navy&#39;&gt;As for hardware you&#8217;ll want plenty\nof memory (duh) and a fas=\r\nt processor also helps if you are concerned about the\nspeed the crawl runs =\r\nat. The crawl tends to be limited by the processing power\navailible. If you=\r\n choose to limit memory use by lowering the host-queues-memory-capacity\nsig=\r\nnificantly a fast HD will be very useful. I&#8217;d suggest having differen=\r\nt\nHD for state/scratch/log files and ARC files.&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=\r\n=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-s=\r\nize:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p clas=\r\ns=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font-=\r\nsize:\n10.0pt;font-family:Arial;color:navy&#39;&gt;- Kris&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p cla=\r\nss=3DMsoNormal&gt;&lt;font size=3D2 color=3Dnavy face=3DArial&gt;&lt;span style=3D&#39;font=\r\n-size:\n10.0pt;font-family:Arial;color:navy&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;div =\r\nstyle=3D&#39;border:none;border-left:solid blue 1.5pt;padding:0cm 0cm 0cm 4.0pt=\r\n&#39;&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D2 face=3DTahoma&gt;&lt;span lang=3DEN-US st=\r\nyle=3D&#39;font-size:\n10.0pt;font-family:Tahoma&#39;&gt;-----Original Message-----&lt;br&gt;=\r\n\n&lt;b&gt;&lt;span style=3D&#39;font-weight:bold&#39;&gt;From:&lt;/span&gt;&lt;/b&gt; Sebastian de Castelbe=\r\nrg\n[mailto:sdecaste@...] &lt;br&gt;\n&lt;b&gt;&lt;span style=3D&#39;font-weight:bold&#39;&gt;Sent:&lt;=\r\n/span&gt;&lt;/b&gt; 23. september 2004 09:32&lt;br&gt;\n&lt;b&gt;&lt;span style=3D&#39;font-weight:bold&#39;=\r\n&gt;To:&lt;/span&gt;&lt;/b&gt;\narchive-crawler@yahoogroups.com&lt;br&gt;\n&lt;b&gt;&lt;span style=3D&#39;font-=\r\nweight:bold&#39;&gt;Subject:&lt;/span&gt;&lt;/b&gt; [archive-crawler] Large\nexperimental crawl=\r\n&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=3DMsoNormal&gt;&lt;font size=3D3 face=3D&quot;Times New R=\r\noman&quot;&gt;&lt;span style=3D&#39;font-size:\n12.0pt&#39;&gt;&nbsp;&lt;/span&gt;&lt;/font&gt;&lt;/p&gt;\n\n&lt;p class=\r\n=3DMsoNormal&gt;&lt;tt&gt;&lt;font size=3D2 face=3D&quot;Courier New&quot;&gt;&lt;span style=3D&#39;font-si=\r\nze:\n10.0pt&#39;&gt;Hi,&lt;/span&gt;&lt;/font&gt;&lt;/tt&gt;&lt;font size=3D2 face=3D&quot;Courier New&quot;&gt;&lt;span=\r\n\nstyle=3D&#39;font-size:10.0pt;font-family:&quot;Courier New&quot;&#39;&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font f=\r\nace=3D&quot;Courier New&quot;&gt;for a research-project we have to implement two\ndiffere=\r\nnt random-walk &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;algorithms f=\r\nor uniform page sampling. This needs\nus to gather about &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;t=\r\nt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;2-4Mio. URL&#39;s.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=\r\n=3D&quot;Courier New&quot;&gt;So we have to chose an adapted Broad Crawl, which\nchooses =\r\nthe URL&#39;s, &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;which are fed ba=\r\nck into the frontier, randomly. So\nthe fetch queue&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;fon=\r\nt face=3D&quot;Courier New&quot;&gt;wouldn&#39;t grow exponential. We also do not need to\nwr=\r\nite the whole &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;content to di=\r\nsk.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;Heritrix seemed to work =\r\nquite well as crawler for\nantother project &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=\r\n=3D&quot;Courier New&quot;&gt;(thanks for the good development work at this\nplace). But =\r\nwe got often &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;problems, base=\r\nd on memory limitations.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New=\r\n&quot;&gt;On the known limitations page, there&#39;s written\nthat it is possible to &lt;/f=\r\nont&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;crawl about 6Mio URL&#39;s and abo=\r\nut 10000 hosts with\ndefault settings.&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Cou=\r\nrier New&quot;&gt;My question is: What&#39;s the best setup to reach\nthis number of URL=\r\n&#39;s&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;(HW/Java heap size/Heritr=\r\nix config)?&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;Do we need speci=\r\nal hardware, or can it be done\nwith a common pc (p4 &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;f=\r\nont face=3D&quot;Courier New&quot;&gt;2.6GHz 512-1024 MB ram)?&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;=\r\n&lt;font face=3D&quot;Courier New&quot;&gt;We planned to use Debian GNU/Linux as os. Maybe\n=\r\nthere&#39;s someone who has &lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;alr=\r\neady experiences with large-scaled crawls and\ncan give me some hints.&lt;/font=\r\n&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;tt&gt;&lt;font face=3D&quot;Courier New&quot;&gt;thanks&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;tt&gt;&lt;=\r\nfont face=3D&quot;Courier New&quot;&gt;sebastian de castelberg&lt;/font&gt;&lt;/tt&gt;&lt;br&gt;\n&lt;/span&gt;&lt;/=\r\nfont&gt;&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n\r\n------=_NextPart_000_0039_01C4A152.B69D5150--\r\n\n"}}