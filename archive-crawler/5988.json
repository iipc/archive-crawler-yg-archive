{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"P8EDVc4JZj6Ek5Dfswhp21Q711r5YLgeIodqlZKE2dyk86ROE5fV_RMqK6zYcddf0D0puzQmy84mwWT8DjQObISLgITIY7Y_AT29DXw6","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Can I split seeds for a HashCrawlMapper crawl?","postDate":"1250539359","msgId":5988,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg2Y2QwditrODVoQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDUwMzU2Ljk4LjIzNC44Ni4xNjcuMTI1MDMxMzk0Ni5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg=="},"prevInTopic":5987,"nextInTopic":0,"prevInTime":5987,"nextInTime":5989,"topicId":5971,"numMessagesInTopic":8,"msgSnippet":"I think the 20MM or so seeds got reduced to about 12MM domains. I m now trying with a smaller partition (.com partition) and the seeds are reduced to about 4","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 35550 invoked from network); 17 Aug 2009 20:02:53 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m5.grp.re1.yahoo.com with QMQP; 17 Aug 2009 20:02:53 -0000\r\nX-Received: from unknown (HELO n37b.bullet.mail.sp1.yahoo.com) (66.163.168.151)\n  by mta1.grp.sp2.yahoo.com with SMTP; 17 Aug 2009 20:02:53 -0000\r\nX-Received: from [69.147.65.173] by n37.bullet.mail.sp1.yahoo.com with NNFMP; 17 Aug 2009 20:02:40 -0000\r\nX-Received: from [98.137.35.13] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 17 Aug 2009 20:02:40 -0000\r\nDate: Mon, 17 Aug 2009 20:02:39 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h6cd0v+k85h@...&gt;\r\nIn-Reply-To: &lt;50356.98.234.86.167.1250313946.squirrel@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: Can I split seeds for a HashCrawlMapper crawl?\r\nX-Yahoo-Group-Post: member; u=132996324; y=z4K1jfBZ0MSOBpeDZbb93aIRYDCm15SQp-OkAy7eHHjIQKd-MA\r\nX-Yahoo-Profile: joehung302\r\n\r\nI think the 20MM or so seeds got reduced to about 12MM domains.\n\nI&#39;m now tr=\r\nying with a smaller partition (.com partition) and the seeds are reduced to=\r\n about 4 to 5MM range. The crawl has been running fine for 3 days now @ 230=\r\n0KB/s.\n\nNote that in previous crawls, we were using &quot;broad&quot; scope so I imag=\r\nine the deciding criteria is much simplier. With domain-based scope I think=\r\n each URL needs to compare with MMs of domains so the computation I assume =\r\nis much bigger. That&#39;s why I am doing all the proof crawls to make sure the=\r\n crawl works up to MMs of seeds/domains.\n\nCheers,\n-Joe\n--- In archive-crawl=\r\ner@yahoogroups.com, igor@... wrote:\n&gt;\n&gt; The seed lists that I used were ~25=\r\nMM and had no problems. However, the\n&gt; seeds were coming from only a few mi=\r\nllion domains.\n&gt; \n&gt; Did your seed list go from 0.5MM domains to 20MM domain=\r\ns?\n&gt; \n&gt; i.\n&gt; \n&gt; \n&gt; \n&gt; &gt; We used to do it. I also use 2 HashCrawlMapper proc=\r\nessors as you\n&gt; &gt; described.\n&gt; &gt;\n&gt; &gt; We were able to run that with about 0.=\r\n5MM seeds. With the new crawling\n&gt; &gt; strategy we need to support up to 20MM=\r\n seeds and I did a proof run with\n&gt; &gt; one instance: the crawl rate slows do=\r\nwn to 2/3 after 3 days. That would\n&gt; &gt; not work because we usually keep the=\r\n crawler running for at least 3 weeks.\n&gt; &gt;\n&gt; &gt; I was guessing that the 20MM=\r\n seed list caused the problem. Now I want to\n&gt; &gt; experiment with split-seed=\r\ns but are concerned that we might be missing the\n&gt; &gt; divert URLs if the see=\r\nd lists are not the same among crawlers.\n&gt; &gt;\n&gt; &gt; When a crawler gets the di=\r\nvert URLs, will they reject the URL if it comes\n&gt; &gt; from a seed that is not=\r\n in the cralwer&#39;s seed file?\n&gt; &gt;\n&gt; &gt; Cheers,\n&gt; &gt; -Joe\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --- In ar=\r\nchive-crawler@yahoogroups.com, igor@ wrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt; Hi Joe,\n&gt; &gt;&gt;\n&gt; &gt;&gt; In =\r\nthe past I used to this without splitting the seeds. I used two\n&gt; &gt;&gt; identi=\r\ncal HashCrawlMapper processors: one before the preselector and one\n&gt; &gt;&gt; aft=\r\ner the link scoper.\n&gt; &gt;&gt;\n&gt; &gt;&gt; This way, each crawling node schedules all of=\r\n the seeds but crawls only\n&gt; &gt;&gt; ones defined by the HashCrawlMapper. What I=\r\n liked about this is that all\n&gt; &gt;&gt; of the nodes will have the same scope (i=\r\nf based on seeds) which can\n&gt; &gt;&gt; handy.\n&gt; &gt;&gt;\n&gt; &gt;&gt; Hope this helps.\n&gt; &gt;&gt; i.\n=\r\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; &gt; I&#39;m using Heritrix 1.14.3.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt; Let&#39;s say I have\n=\r\n&gt; &gt;&gt; &gt; 1. one big seed list consisting of 1MM seeds.\n&gt; &gt;&gt; &gt; 2. 2 crawler in=\r\nstances to implement HashCrawlMapper.\n&gt; &gt;&gt; &gt; 3. The crawl scope is domain +=\r\n 1 (implemented through\n&gt; &gt;&gt; OnDomainDecideRule\n&gt; &gt;&gt; &gt; with &quot;seeds-as-surt-=\r\nprefixes&quot;=3D=3Dtrue and &quot;also-check-via&quot;=3D=3Dtrue).\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt; Can I sp=\r\nlit the seeds using the same HashCrawlMapper rule so that each\n&gt; &gt;&gt; &gt; crawl=\r\ner would only get seeds that are within its scope? Would there be\n&gt; &gt;&gt; any\n=\r\n&gt; &gt;&gt; &gt; difference if I use the same 1MM seeds for both crawlers?\n&gt; &gt;&gt; &gt;\n&gt; &gt;=\r\n&gt; &gt;\n&gt; &gt;&gt; &gt; The reason why I want to do this is, I have 20MM seeds among 12\n=\r\n&gt; &gt;&gt; crawlers.\n&gt; &gt;&gt; &gt; I&#39;ve tested with one instance handling 20MM seeds and=\r\n it doesn&#39;t seem\n&gt; &gt;&gt; to\n&gt; &gt;&gt; &gt; work. If I can split the seeds so that each=\r\n cralwer starts with URLs\n&gt; &gt;&gt; that\n&gt; &gt;&gt; &gt; belong to themselves it should m=\r\nake the crawl process easier....\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt; Thanks,\n&gt; &gt;&gt; &gt; -Joe\n&gt; &gt;&gt; &gt;\n&gt;=\r\n &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt; ------------------------------------\n&gt; &gt;&gt;=\r\n &gt;\n&gt; &gt;&gt; &gt; Yahoo! Groups Links\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n=\r\n&gt; &gt;\n&gt; &gt; ------------------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; =\r\n&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}