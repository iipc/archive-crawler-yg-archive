{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":202546906,"authorName":"jkilleen74","from":"&quot;jkilleen74&quot; &lt;james@...&gt;","profile":"jkilleen74","replyTo":"LIST","senderId":"Xp7VJpoAge6PcOteH7K4kYO1IWhgnAxXQXYQ31y0NBaACJsWAo-ty0_z5LOz0wMBNUtyo2RLDRzZz6kpKfKMIgB7DwCkzy97NO0jAplaGS9BJ1hg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Redirections OK but not extracting actual pages","postDate":"1098283061","msgId":1097,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNsNXQ3bCtmcmhiQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":1098,"prevInTime":1096,"nextInTime":1098,"topicId":1097,"numMessagesInTopic":11,"msgSnippet":"I m running Heritrix 1.0.4 on a PC (Windows XP); I m having a problem where any crawl jobs seem to start fine - retrieve the DNS and robots.txt as appropriate,","rawEmail":"Return-Path: &lt;james@...&gt;\r\nX-Sender: james@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 26209 invoked from network); 20 Oct 2004 14:38:02 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m22.grp.scd.yahoo.com with QMQP; 20 Oct 2004 14:38:02 -0000\r\nReceived: from unknown (HELO n22a.bulk.scd.yahoo.com) (66.94.237.51)\n  by mta2.grp.scd.yahoo.com with SMTP; 20 Oct 2004 14:38:01 -0000\r\nReceived: from [66.218.66.58] by n22.bulk.scd.yahoo.com with NNFMP; 20 Oct 2004 14:37:42 -0000\r\nReceived: from [66.218.67.153] by mailer7.bulk.scd.yahoo.com with NNFMP; 20 Oct 2004 14:37:42 -0000\r\nDate: Wed, 20 Oct 2004 14:37:41 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;cl5t7l+frhb@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 1641\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.51\r\nFrom: &quot;jkilleen74&quot; &lt;james@...&gt;\r\nSubject: Redirections OK but not extracting actual pages\r\nX-Yahoo-Group-Post: member; u=202546906\r\nX-Yahoo-Profile: jkilleen74\r\n\r\n\nI&#39;m running Heritrix 1.0.4 on a PC (Windows XP); I&#39;m having a problem \nwhere any crawl jobs seem to start fine - retrieve the DNS and \nrobots.txt as appropriate, redirect as appropriate etc. But once they \nhit an actual page e.g. somewebsite/index.html, the crawl comes to an \nend. It&#39;s as if the page contained no data or links, yet I know this \nis not the case (I can browse to it through IE perfectly normally.)\nIn my crawl.log, I see something like the following e.g.:\n\n20041020142929438     1         62 dns:www.st-andrews.ac.uk P \nhttp://www.st-andrews.ac.uk/study.shtml text/dns #000 0 - -\n\n20041020142930190   404          0 http://www.st-\nandrews.ac.uk/robots.txt P http://www.st-andrews.ac.uk/study.shtml \ntext/html #000 251 - -\n\n20041020142931601   200          0 http://www.st-\nandrews.ac.uk/study.shtml - - text/html #000 142 - 3t\n\nNote the figures in the second column, for amount of data downloaded. \nFor the front page (study.shtml), it seems to have downloaded zero \ndata. Yet the crawl completes without any obvious errors or alerts - \nit seems to have found this front page, then decided that not only \nare there no links to follow, but not even any data to retrieve. The \nsame pattern is seen whether I respect or ignore robots.txt, for any \nnumber of sites I&#39;ve tried to examine so far. I&#39;m not applying any \nunusual filters, just a standard crawl.\n\nI&#39;m running this behind a firewall, so am using the Proxy server \npatch proved a while back, which seems to be working (without it, any \ncrawl job just stalled on the very first step.)\n\nI&#39;m probably missing something obvious, but any suggestions \nappreciated!\n\nJim K\n\n\n\n\n\n\n\n"}}