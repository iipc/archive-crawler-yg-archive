{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":299179219,"authorName":"mjjjhjemj","from":"&quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;","profile":"mjjjhjemj","replyTo":"LIST","senderId":"GPxKty3KUHcwAafEHsSgM74cBVVXvS1aGokBuHkxF38P9A5MYVkP4GYixCUkuqZSAYA4yq020EgsmGA8PTFT7Sq6BBAJMoohC14","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Looking for way to break large crawl into two or more simultaneous crawls","postDate":"1194544482","msgId":4671,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZndmloMityNzZxQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":4674,"prevInTime":4670,"nextInTime":4672,"topicId":4671,"numMessagesInTopic":2,"msgSnippet":"I am performing a very large domain crawl that is going extremely slow as many of the pages are dynamically created pages from database driven sites. As a","rawEmail":"Return-Path: &lt;bosoxchamps@...&gt;\r\nX-Sender: bosoxchamps@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 16915 invoked from network); 8 Nov 2007 17:54:42 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m55.grp.scd.yahoo.com with QMQP; 8 Nov 2007 17:54:42 -0000\r\nX-Received: from unknown (HELO n36a.bullet.mail.sp1.yahoo.com) (66.163.168.130)\n  by mta16.grp.scd.yahoo.com with SMTP; 8 Nov 2007 17:54:42 -0000\r\nX-Received: from [216.252.122.217] by n36.bullet.mail.sp1.yahoo.com with NNFMP; 08 Nov 2007 17:54:42 -0000\r\nX-Received: from [66.218.69.5] by t2.bullet.sp1.yahoo.com with NNFMP; 08 Nov 2007 17:54:42 -0000\r\nX-Received: from [66.218.66.86] by t5.bullet.scd.yahoo.com with NNFMP; 08 Nov 2007 17:54:42 -0000\r\nDate: Thu, 08 Nov 2007 17:54:42 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;fgvih2+r76q@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;\r\nSubject: Looking for way to break large crawl into two or more simultaneous crawls\r\nX-Yahoo-Group-Post: member; u=299179219; y=s5OTU6iT2FHIzxSvaHvJWTdigbRSgozAubSjU092vRkaM6YU\r\nX-Yahoo-Profile: mjjjhjemj\r\n\r\nI am performing a very large domain crawl that is going extremely slow\nas m=\r\nany of the pages are dynamically created pages from database\ndriven sites. =\r\nAs a result the crawl is preceding along well below the\npoliteness settings=\r\n configured. The crawl is presently gathering data\nat 6KB/sec. Typically th=\r\ne rates are anywhere from 5KB/sec to 10KB/sec\non the database driven sites =\r\nwithin the domain. Very slow.\n\nMy question is: Is there a way to split the =\r\ncrawl in two or more\nsimultaneous crawls that could reference the same hist=\r\nory so that\npages would not be duplicated by each subcrawl (two or more).\n\n=\r\nI could configure the politeness settings so the summed crawls would\nnot ex=\r\nceed the overall demand that I have set for the single crawl\nconfiguration.=\r\n\n\nThanks,\nMike\n\n\n\n\n"}}