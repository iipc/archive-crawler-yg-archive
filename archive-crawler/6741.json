{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"JBPNJB7cgG5cdPB6BXfCXTWuM_zCSVprrVbe2M5kmWx4vSkB2S9yvTABZljM-7YgyDe_jbq_eRPIxQLfc53UmAe5SjgRq6w","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Simple, single-site crawl &quot;hangs&quot; on last few URIs?","postDate":"1285183295","msgId":6741,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOUE1NzNGLjgwNTA2MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEFBTkxrVGk9eWh1TVpHRk8xNWs0T2FIN1RSUHllVF9ncj1CaUR6ejlRV0ZOd0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGk9eWh1TVpHRk8xNWs0T2FIN1RSUHllVF9ncj1CaUR6ejlRV0ZOd0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6740,"nextInTopic":6742,"prevInTime":6740,"nextInTime":6742,"topicId":6739,"numMessagesInTopic":4,"msgSnippet":"This is typical -- the last few URIs/queues often represent URIs that are unresponsive, and goe into a slow-retry cycle that (with the defaults of 15 minutes","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 34317 invoked from network); 22 Sep 2010 19:21:38 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m1.grp.sp2.yahoo.com with QMQP; 22 Sep 2010 19:21:38 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta1.grp.sp2.yahoo.com with SMTP; 22 Sep 2010 19:21:38 -0000\r\nX-Received: (qmail 64962 invoked from network); 22 Sep 2010 19:21:36 -0000\r\nX-Received: from 188.22.26.105 (HELO silverbook.local) (188.22.26.105)\n  by relay02.pair.com with SMTP; 22 Sep 2010 19:21:36 -0000\r\nX-pair-Authenticated: 188.22.26.105\r\nMessage-ID: &lt;4C9A573F.8050605@...&gt;\r\nDate: Wed, 22 Sep 2010 12:21:35 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.9) Gecko/20100825 Thunderbird/3.1.3\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Zach Bailey &lt;zach.bailey@...&gt;\r\nReferences: &lt;AANLkTi=yhuMZGFO15k4OaH7TRPyeT_gr=BiDzz9QWFNw@...&gt;\r\nIn-Reply-To: &lt;AANLkTi=yhuMZGFO15k4OaH7TRPyeT_gr=BiDzz9QWFNw@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Simple, single-site crawl &quot;hangs&quot; on last few\n URIs?\r\nX-Yahoo-Group-Post: member; u=137285340; y=EwjSjZmmc4YoAFZGhU3pBUxzSktnyOP8FeEGEW3pNW-M\r\nX-Yahoo-Profile: gojomo\r\n\r\nThis is typical -- the last few URIs/queues often represent URIs that \nare unresponsive, and goe into a slow-retry cycle that (with the \ndefaults of 15 minutes between retries, and up to 30 retries) takes many \nhours to finally give up for each URI. (And, if there are many URIs on \nthe queue, then arbitrarily long for each to fail all its tries.)\n\nWhen the crawl is small enough you can investigate these, it&#39;s worth \nlooking to see if in fact the URIs/queues in question represent sites \nyou can&#39;t visit in a web browser.\n\nIn this case, it appears the junk URIs are likely:\n\n  dns:www\n  http://www\n  dns:ssl\n  http://ssl\n\nPerhaps, one of your legitimate pages had these fragments as bad URIs. \nOr possibly, our more &#39;speculative&#39; link-extraction (for content like \nJavascript) found some potential-URI strings that reduced to these \n(perhaps after visiting some other page the site doesn&#39;t usually generate).\n\nIt can be instructive to investigate these to total understanding, on a \nsmall crawl, to become familiar with the range of corner cases. And, you \ncould start building rules to automatically discard known problem \npatterns in future crawls so that future crawls terminate cleanly.\n\nBut, it&#39;s normal to have some lingering uncrawlable URIs, especially as \nthe crawls get larger, and unless they seem important at a quick glance \nit&#39;s common to just consider the crawl done and manually terminate the job.\n\n- Gordon @ IA\n\n\nOn 9/22/10 12:05 PM, Zach Bailey wrote:\n&gt;\n&gt;\n&gt; I&#39;ve been proving out our use of heritrix over the past week or so and\n&gt; I&#39;ve noticed the following issue when doing a simple, single-domain crawl.\n&gt;\n&gt; Here is how I have the crawl configured - it is a mostly vanilla crawl\n&gt; configuration save for the following modifications:\n&gt;\n&gt; * metadata.operatorContactUrl specified (obviously)\n&gt; * seeds.textSource.value set to a single domain\n&gt; * Scope chain modifications: TooManyHopsDecideRule.maxHops = 3 and added\n&gt; MatchesFilePatternDecideRule with decision=REJECT and usePreset=ALL\n&gt; right before the PrerequisiteAcceptDecideRule\n&gt; * fetch processor modifications: removed extractorJS, extractorCSS, and\n&gt; extractorSWF\n&gt;\n&gt; The crawl runs fine and when it gets down to the last couple of URLs it\n&gt; &quot;hangs&quot; and does not complete. Looking at the job status page I see:\n&gt;\n&gt; *Totals*\n&gt;     215 downloaded + 4 queued = 219 total\n&gt;     2.5 MiB crawled (2.5 MiB novel, 0 B dup-by-hash, 0 B not-modified)\n&gt;\n&gt; *Frontier*\n&gt;     17 URI queues: 2 active (0 in-process; 0 ready; 2 snoozed); 0\n&gt; inactive; 0 ineligible; 0 retired; 15 exhausted [RUN: 0 in, 0 out]\n&gt;\n&gt; Examining the frontier report I see the following:\n&gt;\n&gt;   -----===== SNOOZED QUEUES =====-----\n&gt; SNOOZED#0:\n&gt; Queue ssl, (p1)\n&gt;    2 items\n&gt;     wakes in: 7m25s888ms\n&gt;      last enqueued: dns:ssl\n&gt;        last peeked: dns:ssl\n&gt;     total expended: 4 (total budget: -1)\n&gt;     active balance: 2996\n&gt;     last(avg) cost: 1(1)\n&gt;     totalScheduled fetchSuccesses fetchFailures fetchDisregards fetchResponses robotsDenials successBytes totalBytes fetchNonResponses\n&gt;     2 0 0 0 0 0 0 0 5\n&gt;     SimplePrecedenceProvider\n&gt;     1\n&gt;\n&gt; SNOOZED#1:\n&gt; Queue www, (p1)\n&gt;    2 items\n&gt;     wakes in: 7m25s889ms\n&gt;      last enqueued: dns:www\n&gt;        last peeked: dns:www\n&gt;     total expended: 4 (total budget: -1)\n&gt;     active balance: 2996\n&gt;     last(avg) cost: 1(1)\n&gt;     totalScheduled fetchSuccesses fetchFailures fetchDisregards fetchResponses robotsDenials successBytes totalBytes fetchNonResponses\n&gt;     2 0 0 0 0 0 0 0 5\n&gt;     SimplePrecedenceProvider\n&gt;     1\n&gt;\n&gt; So, it looks like there are some weird items being put into these queues\n&gt; that don&#39;t belong there and it&#39;s hanging the crawl job? Is there a\n&gt; configuration option I can tweak to retire these queues or clear them\n&gt; out after a specified idle period? Or, is this the result of a\n&gt; misconfiguration somewhere?\n&gt;\n&gt; Thanks,\n&gt; -Zach\n&gt;\n&gt;\n&gt; \n\n"}}