{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"6bNmVLHVXQxzUKY7fQs4n0tghsFJwLN0GWu4dC8wi58IegQ3p0ZDQNCEMXr9Y20WMi9ytKNvB9NJYfdyhphCoMEGoIcx9m4","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling","postDate":"1183059393","msgId":4392,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODQwREMxLjQwNzA2MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ2ODMwMjEyLjMwNjAzMDlAZ3JhZW1lcy5jb20+","referencesHeader":"PDkyOTI4NS41MDQ4Ny5xbUB3ZWI1MDMwNi5tYWlsLnJlMi55YWhvby5jb20+IDw0NjgyQTQ3Qy44MDUwODA1QGFyY2hpdmUub3JnPiA8NDY4MzAyMTIuMzA2MDMwOUBncmFlbWVzLmNvbT4="},"prevInTopic":4385,"nextInTopic":4394,"prevInTime":4391,"nextInTime":4393,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"... The proper link is: http://crawler.archive.org/hcc (The crawler.archive.org homepage sidebar link was wrong but should be fixed now.) Sorry, there isn t","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 3654 invoked from network); 28 Jun 2007 19:36:53 -0000\r\nReceived: from unknown (66.218.66.72)\n  by m53.grp.scd.yahoo.com with QMQP; 28 Jun 2007 19:36:53 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta14.grp.scd.yahoo.com with SMTP; 28 Jun 2007 19:36:53 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id E6D531416011F\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 28 Jun 2007 12:36:19 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 15685-04-26 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 28 Jun 2007 12:36:19 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 4AE48141600D9\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 28 Jun 2007 12:36:19 -0700 (PDT)\r\nMessage-ID: &lt;46840DC1.4070607@...&gt;\r\nDate: Thu, 28 Jun 2007 12:36:33 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (X11/20070604)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;929285.50487.qm@...&gt; &lt;4682A47C.8050805@...&gt; &lt;46830212.3060309@...&gt;\r\nIn-Reply-To: &lt;46830212.3060309@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=137285340; y=qtbguYUYGFmE8oUCDUiQdCnioeLqmcF2hRRbVDF-smcc\r\nX-Yahoo-Profile: gojomo\r\n\r\nlists@... wrote:\n&gt; HCC seems to offer a number of answers to this kind of configuration but \n&gt; getting hold of documentation regarding it (try \n&gt; http://crawler.sourceforge.net/hcc ) is a slightly challenging.  Is HCC \n&gt; still in active use? If so, can someone point me to a tad more doco.?\n\nThe proper link is:\n\n   http://crawler.archive.org/hcc\n\n(The crawler.archive.org homepage sidebar link was wrong but should be \nfixed now.)\n\nSorry, there isn&#39;t much more in the way of documentation.\n\nHCC is used at the Internet Archive to help manage the multiple crawlers \ndoing episodic, independent jobs for the Archive-It service, but not to \nmanage sets of cooperating crawlers on a large crawl.\n\n&gt; As an aside - How many toes should a Heritrix on a quad core/cpu system \n&gt; be able to mange if it is assumed that memory/io/bandwidth are removed \n&gt; from the equation?\n\nWe know no hard-and-fast rules; our technique has been &#39;add more as long \nas they seem to increase throughput&#39;. We usually use 150-250 on \ndual-core machines.\n\n- Gordon @ IA\n\n&gt; Regards,\n&gt; Graeme\n&gt; \n&gt; Gordon Mohr wrote:\n&gt;&gt;\n&gt;&gt; Jigar Patel wrote:\n&gt;&gt; &gt; Thanks a lot Gordon,\n&gt;&gt; &gt;\n&gt;&gt; &gt; You solved my problem.\n&gt;&gt; &gt;\n&gt;&gt; &gt; One more thing I want to know that,\n&gt;&gt; &gt; Can I use same settings for two different independent machines for \n&gt;&gt; distributed crawling ?\n&gt;&gt;\n&gt;&gt; Yes; that is the usual case for distributed crawling: each of the\n&gt;&gt; crawlers has the exact same initial configuration, *except* for the\n&gt;&gt; HashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to\n&gt;&gt; decline to process URIs mapped to the others.\n&gt;&gt;\n&gt;&gt; There is no automatic facility for sharing the settings; you must\n&gt;&gt; manually copy the order.xml (and possibly other files) between crawlers.\n&gt;&gt; Also, there is no automatic facility for cross-feeding the URIs each\n&gt;&gt; crawler rejects. You must watch the crawl.log (or diversions logs from\n&gt;&gt; the mapper) and decide if the URIs should be fed to the sibling crawlers.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; &gt; Please tell me how does it work and coordinate with different machine ?\n&gt;&gt; &gt;\n&gt;&gt; &gt; Thanks\n&gt;&gt; &gt;\n&gt;&gt; &gt; Jigar\n&gt;&gt; &gt;\n&gt;&gt; &gt; Gordon Mohr &lt;gojomo@... &lt;mailto:gojomo%40archive.org&gt;&gt; wrote:\n&gt;&gt; &gt; Jigar Patel wrote:\n&gt;&gt; &gt;&gt; Presently I am running two heritrix instances on the same machine on\n&gt;&gt; &gt;&gt; different port...\n&gt;&gt; &gt;\n&gt;&gt; &gt; In general, you only want to use distributed crawling, with URIs\n&gt;&gt; &gt; partitioned across separate cooperating crawlers, to spread a crawl \n&gt;&gt; over\n&gt;&gt; &gt; multiple independent machines. If using a single machine, a single\n&gt;&gt; &gt; crawler instance will be more efficient.\n&gt;&gt; &gt;\n&gt;&gt; &gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule\n&gt;&gt; &gt;&gt; I added HashCrawlMapper at two places as you suggested\n&gt;&gt; &gt;&gt; I made same configuration setting and seed file at each place.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; But as I run my job it gives me following error in seed file and\n&gt;&gt; &gt;&gt; nothing was crawled.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; Heritrix(-5002)-Blocked by custom prefetch processor\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; Please let me know why I am getting such error...\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; Is anything missing ?\n&gt;&gt; &gt;\n&gt;&gt; &gt; This is the expected crawl.log result for URIs that are considered by a\n&gt;&gt; &gt; crawler, but mapped to be handled by one of the others in the group of\n&gt;&gt; &gt; crawlers. With a proper configuration, some but not all lines in your\n&gt;&gt; &gt; crawl.log will have this code.\n&gt;&gt; &gt;\n&gt;&gt; &gt; For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and\n&gt;&gt; &gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; \n&gt;&gt; of &#39;2&#39;.\n&gt;&gt; &gt;\n&gt;&gt; &gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, \n&gt;&gt; but\n&gt;&gt; &gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the\n&gt;&gt; &gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the\n&gt;&gt; &gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.\n&gt;&gt; &gt;\n&gt;&gt; &gt; - Gordon @ IA\n&gt;&gt; &gt;\n&gt;&gt; &gt;&gt; Regards,\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; Jigar Patel\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; --- In archive-crawler@yahoogroups.com \n&gt;&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Gordon Mohr &lt;gojomo@...&gt;\n&gt;&gt; &gt;&gt; wrote:\n&gt;&gt; &gt;&gt;&gt; nt_bdr wrote:\n&gt;&gt; &gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n&gt;&gt; &gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we\n&gt;&gt; &gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600\n&gt;&gt; &gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8\n&gt;&gt; &gt;&gt; machines\n&gt;&gt; &gt;&gt;&gt; (&gt;1 billion URLs fetched).\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; For background see some previous threads including:\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2909 \n&gt;&gt; &lt;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&gt;\n&gt;&gt; &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/3060 \n&gt;&gt; &lt;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&gt;\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; Roughly how we do it:\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; - Use BloomFilterUriUniqFilter with its defaults -- which devotes\n&gt;&gt; &gt;&gt;&gt; about 500MB to this structure and keeps the false-positive\n&gt;&gt; &gt;&gt; (mistakenly\n&gt;&gt; &gt;&gt;&gt; believed to have been previously-scheduled) rate under 1-in-4-\n&gt;&gt; &gt;&gt; million up\n&gt;&gt; &gt;&gt;&gt; through 125 million URIs discovered.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+\n&gt;&gt; &gt;&gt; heap\n&gt;&gt; &gt;&gt;&gt; - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in\n&gt;&gt; &gt;&gt; queues\n&gt;&gt; &gt;&gt;&gt; named by the reversed-host (com,example,) rather than usual host\n&gt;&gt; &gt;&gt;&gt; (example.com)\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor\n&gt;&gt; &gt;&gt; chain:\n&gt;&gt; &gt;&gt;&gt; * Once, immediately before the PreconditionEnforcer. This one\n&gt;&gt; &gt;&gt; has\n&gt;&gt; &gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any\n&gt;&gt; &gt;&gt; scheduled\n&gt;&gt; &gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)\n&gt;&gt; &gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has\n&gt;&gt; &gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any\n&gt;&gt; &gt;&gt; discovered\n&gt;&gt; &gt;&gt;&gt; outlinks before they are scheduled.)\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a\n&gt;&gt; &gt;&gt; number 0\n&gt;&gt; &gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and\n&gt;&gt; &gt;&gt; all\n&gt;&gt; &gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers,\n&gt;&gt; &gt;&gt; n).\n&gt;&gt; &gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the\n&gt;&gt; &gt;&gt; SURT\n&gt;&gt; &gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI\n&gt;&gt; &gt;&gt; is\n&gt;&gt; &gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to\n&gt;&gt; &gt;&gt; a\n&gt;&gt; &gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally.\n&gt;&gt; &gt;&gt;&gt; Depending on the character of your crawl, you may want to feed\n&gt;&gt; &gt;&gt; these\n&gt;&gt; &gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore\n&gt;&gt; &gt;&gt; them.\n&gt;&gt; &gt;&gt;&gt; The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key\n&gt;&gt; &gt;&gt; before\n&gt;&gt; &gt;&gt;&gt; mapping -- used to ensure that all subdomains of example.com are\n&gt;&gt; &gt;&gt; treated\n&gt;&gt; &gt;&gt;&gt; the same as example.com for mapping purposes. The first match of\n&gt;&gt; &gt;&gt; this\n&gt;&gt; &gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small\n&gt;&gt; &gt;&gt;&gt; example would be:\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything\n&gt;&gt; &gt;&gt; through\n&gt;&gt; &gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it\n&gt;&gt; &gt;&gt; uses\n&gt;&gt; &gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns\n&gt;&gt; &gt;&gt; that\n&gt;&gt; &gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs,\n&gt;&gt; &gt;&gt; like\n&gt;&gt; &gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level\n&gt;&gt; &gt;&gt;&gt; domains.)\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; - All crawlers are launched with the same configuration,\n&gt;&gt; &gt;&gt; including\n&gt;&gt; &gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate.\n&gt;&gt; &gt;&gt; Seeds\n&gt;&gt; &gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early\n&gt;&gt; &gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed\n&gt;&gt; &gt;&gt; are\n&gt;&gt; &gt;&gt;&gt; done so by an external process/scripts.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; - Gordon @ IA\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; Yahoo! Groups Links\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; ---------------------------------\n&gt;&gt; &gt; Food fight? Enjoy some healthy debate\n&gt;&gt; &gt; in the Yahoo! Answers Food & Drink Q&A.\n&gt;&gt;\n&gt;&gt;  \n&gt; \n\n\n"}}