{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"e7mPzPZSGUiGZgyCl2Mg7DZQrTKgpZkW3Z72zwcHx6WY0LW4of0y_88vfvYSl8Ij7ALedg4PkEocpHmLXrtRvnfBUgHOPHA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Too many open files Exception","postDate":"1293144003","msgId":6941,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREMTNDRkMzLjcwMDAyMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDREMTM5ODFBLjIwMTA3MDdAZ21haWwuY29tPg==","referencesHeader":"PEFBTkxrVGk9ZjYwK3QwNDZuelZna3gzcGhodThHcUo1ZjZ3MTlfRDVBa3FSQUBtYWlsLmdtYWlsLmNvbT4gPDREMTI4NzdELjUwMzA4MDhAZ21haWwuY29tPiA8NEQxMkU0RTguNTAxMDkwOEBhcmNoaXZlLm9yZz4gPDREMTM5ODFBLjIwMTA3MDdAZ21haWwuY29tPg=="},"prevInTopic":6940,"nextInTopic":0,"prevInTime":6940,"nextInTime":6942,"topicId":6936,"numMessagesInTopic":5,"msgSnippet":"The info on observed file-descriptor usage is real scenarios is very useful! I was just saying any such FD usage that s a direct function of ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 61502 invoked from network); 23 Dec 2010 22:40:06 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m14.grp.re1.yahoo.com with QMQP; 23 Dec 2010 22:40:06 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta3.grp.re1.yahoo.com with SMTP; 23 Dec 2010 22:40:05 -0000\r\nX-Received: (qmail 78116 invoked by uid 0); 23 Dec 2010 22:40:04 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay02.pair.com with SMTP; 23 Dec 2010 22:40:04 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4D13CFC3.7000206@...&gt;\r\nDate: Thu, 23 Dec 2010 14:40:03 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nReferences: &lt;AANLkTi=f60+t046nzVgkx3phhu8GqJ5f6w19_D5AkqRA@...&gt; &lt;4D12877D.5030808@...&gt; &lt;4D12E4E8.5010908@...&gt; &lt;4D13981A.2010707@...&gt;\r\nIn-Reply-To: &lt;4D13981A.2010707@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Too many open files Exception\r\nX-Yahoo-Group-Post: member; u=137285340; y=j6NHEkhxDpQXsW-l8t2MwROzoz3XS5vaNSvWIm04EuYb\r\nX-Yahoo-Profile: gojomo\r\n\r\nThe info on observed file-descriptor usage is real scenarios is very useful!\n\nI was just saying any such FD usage that&#39;s a direct function of \nnumber-of-ToeThreads is *probably* not Adam&#39;s specific issue -- given \nwhat he&#39;s already reported about raising the limit significantly. \nFurther,  occasional excursions in a busy crawl with hundreds of threads \nabove 1024 open files should be expected, and alone isn&#39;t an indication \nof anything wrong.\n\nSince any thread is only doing either HTTP or DNS in one processing-run, \nslow HTTP *or* DNS can nudge the total open descriptor load towards a \ntemporary peak. For example, all threads waiting for DNS responses, or \nall threads waiting for slow HTTP servers, should equally lead to a peak \nnumber of socket descriptors in use.\n\n*If* &#39;lsof&#39; or other monitors show more HTTP+DNS socket descriptors than \nthreads, it could be an indication of sockets not being properly closed \nin certain error conditions. (Often, GC/finalization will still clean \nthings up, but in large heaps perhaps not in time to prevent reaching \nthe open-files limit, and we never want to rely on finalization to \ncorrect any missing-close.)\n\nEarlier and development versions have occasionally had variants of this \nproblem, but not (as far as I know) 1.14.4 or 3.0.0. If Adam&#39;s problem \nis another variant of this, it should be obvious in &#39;lsof&#39; output either \njust after, or not long before, the error is encountered, and there \nmight be other clues in the logs. (Also, in order to use up tens or \nhundred of thousands of descriptors, the opens might have to outpace \nGC/finalization -- some sort of fast failure that hardly uses any object \nallocations.)\n\nIf any monitoring shows a surge in HTTP/DNS descriptor count above \nthread count, please report as a potential problem right away.\n\n- Gordon @ IA\n\nOn 12/23/10 10:42 AM, Kenji Nagahashi wrote:\n&gt; Hi Gordon,\n&gt; Ah, sorry for jumping in being ignorant of past conversation!\n&gt;\n&gt; But let me say, using low-default open-files limit is not the point of\n&gt; my post. Our crawlers have been doing just fine with that low-limit with\n&gt; 200-300 threads, running for 20+ days, until we encounter too many open\n&gt; files error last week. We were not sure if it actually hit the limit for\n&gt; good reason, or some other factors were at play. Before raising the\n&gt; limit, I wanted to know what&#39;s really happening inside. What I found was\n&gt; surge in number of DNS UDP sockets, and ~400 files which I don&#39;t know if\n&gt; I have control on. I&#39;m sorry if such information is not useful.\n&gt;\n&gt; While it is easy to raise limit to big number, also it is good to know\n&gt; things are in control. For us, slowing down crawl a bit is much better\n&gt; than getting BDB destroyed!\n&gt;\n&gt; Thanks,\n&gt; Kenji\n&gt;\n&gt; (12/22/10 9:58 PM), Gordon Mohr wrote:\n&gt;&gt; Based on prior exchanges with Adam, I think his problem is other than\n&gt;&gt; hitting the low-default 1024 open-files limit, which I believe he&#39;s\n&gt;&gt; already raised. (Notably, my understanding is that his crawl ran for\n&gt;&gt; weeks with 300 threads before encountering problems -- though perhaps,\n&gt;&gt; something has reverted the limit.) I&#39;ve asked him separately for more\n&gt;&gt; information that might narrow down the cause.\n&gt;&gt;\n&gt;&gt; Anyone doing non-trivial crawling should increase their open-files limit\n&gt;&gt; before launching Heritrix, as noted in the older H1-based FAQ...\n&gt;&gt;\n&gt;&gt; http://crawler.archive.org/faq.html#toomanyopenfiles\n&gt;&gt;\n&gt;&gt; You&#39;ll find similar advice for many other IO-intensive packages -- for\n&gt;&gt; example the Lucene, HBase, Hypertable, and other FAQs have similar entries.\n&gt;&gt;\n&gt;&gt; We should probably add a more prominent warning in the various H1/H3\n&gt;&gt; getting started docs, and (if practical) even do a sanity-check on\n&gt;&gt; launch that warns people who haven&#39;t already raised this limit.\n&gt;&gt;\n&gt;&gt; Historically, most crawling machines at IA have had this limit raised to\n&gt;&gt; 32768, though it can harmlessly be much larger as well. Kenji, I&#39;m not\n&gt;&gt; sure why your machines would have still had the impractically-small\n&gt;&gt; default limit.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; On 12/22/10 3:19 PM, Kenji Nagahashi wrote:\n&gt;&gt;&gt; Hi Adam,\n&gt;&gt;&gt;\n&gt;&gt;&gt; Coincidentally, I&#39;ve been investigating the same &quot;too many open files&quot;\n&gt;&gt;&gt; error in our crawl projects. Based on what I&#39;ve found so far, it appears\n&gt;&gt;&gt; Heritrix hit per-process maximum of open file descriptors (1024 in our\n&gt;&gt;&gt; case). Following is what I learned from monitoring 5 hour test crawl\n&gt;&gt;&gt; (Heritrix 3) - probably the something similar must be happening in your\n&gt;&gt;&gt; crawl project, too.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Heritrix 3 has ~80 always-open file descriptors for JARs, log files,\n&gt;&gt;&gt; ARC/WARCs, BDB and listening sockets.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Each active crawling thread can open up to 3 file descriptors at one\n&gt;&gt;&gt; time for fetching and recording HTTP request/response. Our crawler is\n&gt;&gt;&gt; configured with max 200 threads, so there can be up to 600 open file\n&gt;&gt;&gt; descriptors at one time, but usually much less than that (max 318 in our\n&gt;&gt;&gt; case).\n&gt;&gt;&gt;\n&gt;&gt;&gt; In addition to total 400 open fds above, there are other 4 kinds of file\n&gt;&gt;&gt; descriptors, which reached 400 in total during the test. Now it is\n&gt;&gt;&gt; pretty close to per-process limit of 1024.\n&gt;&gt;&gt;\n&gt;&gt;&gt; One of &quot;additional&quot; open descriptors is UDP socket for DNS query. I saw\n&gt;&gt;&gt; max 75 sockets open at one time. If DNS server gets overloaded and slows\n&gt;&gt;&gt; down, there could be more.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Also as crawl job proceed, BDB gets bigger and BDB seems to keep more\n&gt;&gt;&gt; database files open.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Summing up, with following factors combined, Heritrix could run out of\n&gt;&gt;&gt; file descriptors and start failing severely:\n&gt;&gt;&gt;\n&gt;&gt;&gt; - many threads\n&gt;&gt;&gt; - shallow crawl on many web sites (such crawl can go very fast, too)\n&gt;&gt;&gt; - long-running crawl\n&gt;&gt;&gt; - slow DNS queries\n&gt;&gt;&gt;\n&gt;&gt;&gt; Probably crawling with less threads is the easiest (but not so reliable)\n&gt;&gt;&gt; way to prevent this problem, but Heritrix could do a kind of throttling\n&gt;&gt;&gt; by limiting the number of concurrent DNS queries.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Kenji @ Internet Archive\n&gt;&gt;&gt;\n&gt;&gt;&gt; (12/22/10 4:17 AM), Adam Brokeš wrote:\n&gt;&gt;&gt;&gt; [Attachment(s)&lt;#TopText&gt;  from =?UTF-8?B?QWRhbSBCcm9rZcWh?= included\n&gt;&gt;&gt;&gt; below]\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Hallo folks,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; I am working on domain crawl and I have encounter few strange\n&gt;&gt;&gt;&gt; problems. First of all, the crawl has some rapid slowdown (ten times\n&gt;&gt;&gt;&gt; slower). This was caused by managing (or comparing) cookies. Well, we\n&gt;&gt;&gt;&gt; have decided that disabling cookies for bulk crawl is not big issue.\n&gt;&gt;&gt;&gt; After few days, when 4 TB was harvested, heritrix threw too many open\n&gt;&gt;&gt;&gt; files exception. I tried to recover, but all my attempts failed. I\n&gt;&gt;&gt;&gt; restarted heritrix, build new job based on the old one. Lower the\n&gt;&gt;&gt;&gt; toethreads to 250 and started. This order xml is quite the same as I\n&gt;&gt;&gt;&gt; used last year for crawling 9.5 TB. After two and half days with 1TB\n&gt;&gt;&gt;&gt; crawled heritrix suddenly stop with exception threw on all threads. I\n&gt;&gt;&gt;&gt; have attached the exception in the bottom. The limit on the open files\n&gt;&gt;&gt;&gt; in the system is set to more than one million. Frankly, I am not sure\n&gt;&gt;&gt;&gt; how to track down this problem. Especially when this setting was\n&gt;&gt;&gt;&gt; working last year (probably on 1.14.3).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Thank you very much.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Best regards.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Brokes\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; HW&  SW config\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; 3485312 KB max heap\n&gt;&gt;&gt;&gt; 32bit java6 update22\n&gt;&gt;&gt;&gt; heritrix 1.14.4\n&gt;&gt;&gt;&gt; 4 core Intel(R) Xeon(R) CPU E5420 @ 2.50GHz\n&gt;&gt;&gt;&gt; 8 GiB RAM\n&gt;&gt;&gt;&gt; 64bit Debian 5.0.6\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Exception\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Fatal exception in ToeThread #181: (in thread &#39;ToeThread #181: &#39;)\n&gt;&gt;&gt;&gt; Exception:\n&gt;&gt;&gt;&gt; com.sleepycat.util.RuntimeExceptionWrapper: (JE 3.3.82) fetchTarget of\n&gt;&gt;&gt;&gt; 0x32/0x1ed6a7 parent IN=1266390 IN class=com.sleepycat.je.tree.BIN\n&gt;&gt;&gt;&gt; lastFullVersion=0x8b2/0x56a066 parent.getDirty()=false state=0\n&gt;&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt;&gt; Cause: com.sleepycat.je.DatabaseException: (JE 3.3.82) fetchTarget of\n&gt;&gt;&gt;&gt; 0x32/0x1ed6a7 parent IN=1266390 IN class=com.sleepycat.je.tree.BIN\n&gt;&gt;&gt;&gt; lastFullVersion=0x8b2/0x56a066 parent.getDirty()=false state=0\n&gt;&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.tree.IN.fetchTarget(IN.java:1230)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.je.dbi.CursorImpl.searchAndPosition(CursorImpl.java:2103)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchInternal(Cursor.java:1778)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchAllowPhantoms(Cursor.java:1748)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.search(Cursor.java:1615)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.getSearchKey(Cursor.java:1067)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.doGetSearchKey(RangeCursor.java:965)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.getSearchKey(RangeCursor.java:592)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.DataCursor.doGetSearchKey(DataCursor.java:577)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.DataCursor.getSearchKey(DataCursor.java:559)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.getValue(StoredContainer.java:299)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at com.sleepycat.collections.StoredMap.get(StoredMap.java:227)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.getOrUse(ObjectIdentityBdbCache.java:264)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:217)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:75)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.activateInactiveQueue(WorkQueueFrontier.java:732)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.next(WorkQueueFrontier.java:634)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at org.archive.crawler.framework.ToeThread.run(ToeThread.java:147)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Stacktrace: com.sleepycat.util.RuntimeExceptionWrapper: (JE 3.3.82)\n&gt;&gt;&gt;&gt; fetchTarget of 0x32/0x1ed6a7 parent IN=1266390 IN\n&gt;&gt;&gt;&gt; class=com.sleepycat.je.tree.BIN lastFullVersion=0x8b2/0x56a066\n&gt;&gt;&gt;&gt; parent.getDirty()=false state=0\n&gt;&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.convertException(StoredContainer.java:466)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.getValue(StoredContainer.java:306)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at com.sleepycat.collections.StoredMap.get(StoredMap.java:227)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.getOrUse(ObjectIdentityBdbCache.java:264)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:217)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:75)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.activateInactiveQueue(WorkQueueFrontier.java:732)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.next(WorkQueueFrontier.java:634)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at org.archive.crawler.framework.ToeThread.run(ToeThread.java:147)\n&gt;&gt;&gt;&gt; Caused by: com.sleepycat.je.DatabaseException: (JE 3.3.82) fetchTarget\n&gt;&gt;&gt;&gt; of 0x32/0x1ed6a7 parent IN=1266390 IN class=com.sleepycat.je.tree.BIN\n&gt;&gt;&gt;&gt; lastFullVersion=0x8b2/0x56a066 parent.getDirty()=false state=0\n&gt;&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.tree.IN.fetchTarget(IN.java:1230)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.je.dbi.CursorImpl.searchAndPosition(CursorImpl.java:2103)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchInternal(Cursor.java:1778)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchAllowPhantoms(Cursor.java:1748)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.search(Cursor.java:1615)\n&gt;&gt;&gt;&gt; at com.sleepycat.je.Cursor.getSearchKey(Cursor.java:1067)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.doGetSearchKey(RangeCursor.java:965)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.getSearchKey(RangeCursor.java:592)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.DataCursor.doGetSearchKey(DataCursor.java:577)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.DataCursor.getSearchKey(DataCursor.java:559)\n&gt;&gt;&gt;&gt; at\n&gt;&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.getValue(StoredContainer.java:299)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; ... 7 more\n&gt;&gt;&gt;&gt; --\n&gt;&gt;&gt;&gt; Adam Brokeš\n&gt;&gt;&gt;&gt; http://www.brokes.net\n&gt;&gt;&gt;&gt; adam.brokes@...&lt;mailto:adam.brokes%40gmail.com&gt;\n&gt;&gt;&gt;&gt; 42799740\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}