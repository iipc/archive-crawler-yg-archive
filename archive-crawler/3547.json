{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"UIfiwioEP8mY2t_sfDlzFw6g3sYe_9SNmays0K0j38HYsBVuLBGB8UWEWKQ3RTZozBlnpGVcPcVgNC58Bsg3pQpJKenOhZDIMzsU7BDU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Feature suggestion for the frontier","postDate":"1164067514","msgId":3547,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVqdGZycSs4cnRwQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":3550,"prevInTime":3546,"nextInTime":3548,"topicId":3547,"numMessagesInTopic":6,"msgSnippet":"Hi, We ve been using heritrix with crawl map for crawling  1B urls. We current have 8 crawl machines at one time. Under this configuration, the crawl can only","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 19202 invoked from network); 21 Nov 2006 00:05:23 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m21.grp.scd.yahoo.com with QMQP; 21 Nov 2006 00:05:23 -0000\r\nReceived: from unknown (HELO n16.bullet.sp1.yahoo.com) (69.147.64.213)\n  by mta5.grp.scd.yahoo.com with SMTP; 21 Nov 2006 00:05:16 -0000\r\nReceived: from [216.252.122.216] by n16.bullet.sp1.yahoo.com with NNFMP; 21 Nov 2006 00:05:15 -0000\r\nReceived: from [66.218.69.2] by t1.bullet.sp1.yahoo.com with NNFMP; 21 Nov 2006 00:05:15 -0000\r\nReceived: from [66.218.66.81] by t2.bullet.scd.yahoo.com with NNFMP; 21 Nov 2006 00:05:15 -0000\r\nDate: Tue, 21 Nov 2006 00:05:14 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;ejtfrq+8rtp@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Feature suggestion for the frontier\r\nX-Yahoo-Group-Post: member; u=132996324; y=Y6WqB7PJyzCNwUTpLVJOCW8BnvEYxaCejNLrzTBrJODQXrwTOQ\r\nX-Yahoo-Profile: joehung302\r\n\r\nHi,\n\nWe&#39;ve been using heritrix with crawl map for crawling &gt; 1B urls. We \nc=\r\nurrent have 8 crawl machines at one time. \n\nUnder this configuration, the c=\r\nrawl can only be successful if the \ncrawlers don&#39;t fail. Hence we&#39;re really=\r\n focused on how to keep the \ncrawler running as long as possible.\n\nWe thoug=\r\nht it might be a good idea to *limit* bdb growth after \ncertain size. And w=\r\ne also know that bdb size has a lot to do with \nthe to-be-crawled URL list.=\r\n Hence the question: Does it make sense \nto set a upper limit on the Fronti=\r\ner so, when the to-be-cralwed URL \nreached a certain number (say, 120M urls=\r\n), stop collecting to-be-\ncrawled and throw any new links away? The idea is=\r\n, in order to \ndownload 120M URLs, we probably would have a to-be-crawled l=\r\nist of \n400M, and bdb probably won&#39;t survive under that size.\n\nDoes it make=\r\n sense?\n\ncheers,\n-Joe\n\n\n\n"}}