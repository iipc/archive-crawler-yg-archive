{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"adjCgQGezoPmefLFhYyOKzG-s2EG7qtQja3tZZhrIwNP2x3uBdaVlji7kgRdbIdhhjWdHgiziy2E65YpaGIewFoHygf80aOv","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Help! What always NOTCRAWLED","postDate":"1114747726","msgId":1770,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyNzFCMzRFLjIwODAwMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGQ0czlrZys3dGh0QGVHcm91cHMuY29tPg==","referencesHeader":"PGQ0czlrZys3dGh0QGVHcm91cHMuY29tPg=="},"prevInTopic":1768,"nextInTopic":1771,"prevInTime":1769,"nextInTime":1771,"topicId":1760,"numMessagesInTopic":10,"msgSnippet":"Try using ip address instead of the hostname of the proxy. I had this problem just recently. I don t see this issue in the Bug list so I am not sure if the it","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 17452 invoked from network); 29 Apr 2005 04:09:03 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m13.grp.scd.yahoo.com with QMQP; 29 Apr 2005 04:09:03 -0000\r\nReceived: from unknown (HELO smtp812.mail.sc5.yahoo.com) (66.163.170.82)\n  by mta5.grp.scd.yahoo.com with SMTP; 29 Apr 2005 04:09:03 -0000\r\nReceived: from unknown (HELO ?127.0.0.1?) (ichoi@...@64.167.99.228 with plain)\n  by smtp812.mail.sc5.yahoo.com with SMTP; 29 Apr 2005 04:09:02 -0000\r\nMessage-ID: &lt;4271B34E.2080004@...&gt;\r\nDate: Thu, 28 Apr 2005 21:08:46 -0700\r\nUser-Agent: Mozilla Thunderbird 0.8 (Windows/20040913)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;d4s9kg+7tht@...&gt;\r\nIn-Reply-To: &lt;d4s9kg+7tht@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Re: Help! What always NOTCRAWLED\r\nX-Yahoo-Group-Post: member; u=137477665\r\nX-Yahoo-Profile: iranitovic\r\n\r\nTry using ip address instead of the hostname of the proxy. I had this \nproblem just recently. I don&#39;t see this issue in the Bug list so I am \nnot sure if the it has been fixed or not.....\nTake care,\ni.\n\n&gt; Dear Gordon,\n&gt;  \n&gt; \n&gt;&gt;The server(s) may be completely unreachable -- so Heritrix isn&#39;t\n&gt;&gt;even getting a 404 for /robots.txt. From the same machine on which\n&gt;&gt;Heritrix is running, can you access the sites with a web browser?\n&gt;&gt;(Do you need to use a web proxy to do so?)\n&gt; \n&gt; \n&gt; The same machine running heritrix can access those sites with web\n&gt; browser. However it is behind a squid proxy server. Proxy setting was\n&gt; defined in global profile.\n&gt; \n&gt; Is it the proxy that causing this problem? If so any work around?\n&gt; \n&gt; \n&gt;&gt;Alternative, something about the crawl configuration may be preventing\n&gt;&gt;/robots.txt from being fetched. If you have changed the scope, added\n&gt;&gt;filters, or reduced the allowable &#39;max-trans-hops&#39; for your crawl,\n&gt;&gt;this might be the case.\n&gt; \n&gt; \n&gt; I didnt change much on default setting, only reduce those delay time\n&gt; to half the default setting. Instead, for the last try i increase\n&gt; max-trans-hop and link to 5000. So craw job will not stary if no\n&gt; robots.txt been fetch at the first hand? If so how about those site\n&gt; that didn&#39;t contain a robots.txt file?\n&gt; \n&gt; \n&gt;&gt;Does the crawl show as &#39;finished&#39; very quickly? If so, your crawl\n&gt;&gt;configuration is probably to blame -- the /robots.txt URLs were never\n&gt;&gt;tried.\n&gt;&gt;\n&gt;&gt;If the crawl shows as running for a while, but makes no progress, it&#39;s\n&gt;&gt;retrying the /robots.txt multiple times with pauses between, but\n&gt; \n&gt; failing --\n&gt; \n&gt;&gt;in which case it&#39;s probably your network. (After it is done with\n&gt; \n&gt; retries,\n&gt; \n&gt;&gt;the /robots.txt URLs will show in the crawl.log with whatever error\n&gt;&gt;caused their last failure.)\n&gt;&gt;\n&gt; \n&gt; \n&gt; The craw takes some times to finish, about 7-10 minutes. It show\n&gt; finished on the status.\n&gt; \n&gt; Again, thanks for help.\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n\n"}}