{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225011788,"authorName":"Karl Wright","from":"Karl Wright &lt;kwright@...&gt;","profile":"daddywri","replyTo":"LIST","senderId":"d4wF73X6RjYXsXi89S8u8e1LaF6nxoivTNqvxKSQOzAIQ443wXbihYNEOPdJBDYrn-va6NlRvdbVCJ4hJoJE_3xK_jOW7ryeVtA","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Robots.txt parsing problem?","postDate":"1141907036","msgId":2743,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0MTAxRTVDLjQwOTA3MDhAbWV0YWNhcnRhLmNvbT4=","inReplyToHeader":"PDQ0MEZGRDNFLjQwNzA2MDhAYXJjaGl2ZS5vcmc+","referencesHeader":"PDQ0MEYxQkE3LjIwNTAyMDNAbWV0YWNhcnRhLmNvbT4gPDQ0MEYxRTQ2LjgwMzAwMDJAYXJjaGl2ZS5vcmc+IDw0NDBGMjJDQS41MDIwMzAwQG1ldGFjYXJ0YS5jb20+IDw0NDBGRkQzRS40MDcwNjA4QGFyY2hpdmUub3JnPg=="},"prevInTopic":2742,"nextInTopic":2771,"prevInTime":2742,"nextInTime":2744,"topicId":2736,"numMessagesInTopic":7,"msgSnippet":"... we re not; we re discarding them.  So we are out of luck. ... I was initially suspicious because the readLine() javadoc said it only recognized line","rawEmail":"Return-Path: &lt;kwright@...&gt;\r\nX-Sender: kwright@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 74732 invoked from network); 9 Mar 2006 12:25:49 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m29.grp.scd.yahoo.com with QMQP; 9 Mar 2006 12:25:49 -0000\r\nReceived: from unknown (HELO metacarta.com) (65.77.47.18)\n  by mta9.grp.scd.yahoo.com with SMTP; 9 Mar 2006 12:25:48 -0000\r\nReceived: from localhost (silene.metacarta.com [65.77.47.24])\n\tby metacarta.com (Postfix) with ESMTP id A488C5180EF\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  9 Mar 2006 07:22:35 -0500 (EST)\r\nReceived: from metacarta.com ([65.77.47.18])\n\tby localhost (silene.metacarta.com [65.77.47.24]) (amavisd-new, port 10024)\n\twith ESMTP id 25566-06 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 9 Mar 2006 07:22:34 -0500 (EST)\r\nReceived: from [192.168.1.102] (146-115-66-62.c3-0.lex-ubr1.sbo-lex.ma.cable.rcn.com [146.115.66.62])\n\tby metacarta.com (Postfix) with ESMTP id EE696518056\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  9 Mar 2006 07:22:32 -0500 (EST)\r\nMessage-ID: &lt;44101E5C.4090708@...&gt;\r\nDate: Thu, 09 Mar 2006 07:23:56 -0500\r\nUser-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;440F1BA7.2050203@...&gt; &lt;440F1E46.8030002@...&gt; &lt;440F22CA.5020300@...&gt; &lt;440FFD3E.4070608@...&gt;\r\nIn-Reply-To: &lt;440FFD3E.4070608@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: by amavisd-new-20030616-p10 (Debian) at metacarta.com\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Karl Wright &lt;kwright@...&gt;\r\nSubject: Re: [archive-crawler] Robots.txt parsing problem?\r\nX-Yahoo-Group-Post: member; u=225011788; y=Q-BiOocQZ_r4E0Gq23cWJLix-gNnsh3s5Z1wrubCyLpq_gg\r\nX-Yahoo-Profile: daddywri\r\n\r\nGordon Mohr (archive.org) wrote:\n&gt; Karl Wright wrote:\n&gt; \n&gt;&gt;I don&#39;t know what the robots.txt looked like prior to that, of course, \n&gt;&gt;since this is not my website.\n&gt; \n&gt; \n&gt; If you are saving crawl results to ARCs, the robots.txt that was \n&gt; consulted will be in the crawl ARCs -- as will be each daily refetch.\n&gt; \n\nwe&#39;re not; we&#39;re discarding them.  So we are out of luck.\n\n&gt; \n&gt;&gt;What do you think about the fact that the final line is EOF-terminated, \n&gt;&gt;not newline terminated?  Would that upset Heritrix&#39; parser?\n&gt; \n&gt; \n&gt; Such bugs are not out of the question, but I&#39;ve just tried a test \n&gt; crawl of the URI from your log -- and Heritrix with a default \n&gt; config refused to fetch it because of the robots rules (which \n&gt; still lack a final newline).\n&gt; \n&gt; Also, I&#39;ve never noticed (nor heard reports) that the \n&gt; BufferedReader.readLine() method, used in the robots.txt parsing \n&gt; (see org.archive.crawler.datamodel.Robotstxt), mishandles a final \n&gt; EOF-terminated line.\n&gt; \n\nI was initially suspicious because the readLine() javadoc said it only \nrecognized line terminators CR, LF, and CR/LF.  But some tests confirmed \nthat it actually works correctly.\n\n&gt; So I strongly suspect Igor&#39;s theory is correct: the robots.txt \n&gt; became more restrictive during the crawl, and the crawler was \n&gt; still consulting the older version at the time the page was fetched.\n&gt; \n\nIt&#39;s funny they would change their robots.txt and then turn around a few \nhours later and ding us for not looking at the new one.  We&#39;re trying to \nget a copy of the old one to confirm that all is well.\n\n&gt; The robots expiration can be made very small, but there will \n&gt; always be some window where the consulted robots rules could be \n&gt; out-of-date by the time a followup fetch is executed.\n&gt; \n\nYes, but it seems that google has established a 1-day cache time, so \npeople are familiar with that as the standard.\n\nThank you again,\nKarl\n\n&gt; - Gordon @ IA\n&gt; \n&gt; \n&gt;&gt;Karl\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;&gt;Hi,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;We got dinged again by using Heritrix in that a crawlee complained that \n&gt;&gt;&gt;&gt;we were ignoring their robots.txt file.  On the face of it, they look \n&gt;&gt;&gt;&gt;like they are correct in complaining:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;http://uaelp.pennnet.com/robots.txt\n&gt;&gt;&gt;&gt;===================================\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;# pennwell robots.txt\n&gt;&gt;&gt;&gt;# updated 3/6/06 by bwn\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;User-agent: Googlebot\n&gt;&gt;&gt;&gt;Disallow: /Search/\n&gt;&gt;&gt;&gt;Disallow: /search/\n&gt;&gt;&gt;&gt;Disallow: /Userreg/\n&gt;&gt;&gt;&gt;Disallow: /userreg/\n&gt;&gt;&gt;&gt;Disallow: /Nav/\n&gt;&gt;&gt;&gt;Disallow: /nav/\n&gt;&gt;&gt;&gt;Disallow: /js\n&gt;&gt;&gt;&gt;Disallow: /JS\n&gt;&gt;&gt;&gt;Disallow: /whitepapers/wp_redirect.cfm\n&gt;&gt;&gt;&gt;Disallow: /*.js$\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;User-agent: *\n&gt;&gt;&gt;&gt;Disallow: /Search/\n&gt;&gt;&gt;&gt;Disallow: /search/\n&gt;&gt;&gt;&gt;Disallow: /Userreg/\n&gt;&gt;&gt;&gt;Disallow: /userreg/\n&gt;&gt;&gt;&gt;Disallow: /Nav/\n&gt;&gt;&gt;&gt;Disallow: /nav/\n&gt;&gt;&gt;&gt;Disallow: /js\n&gt;&gt;&gt;&gt;Disallow: /JS\n&gt;&gt;&gt;&gt;Disallow: /whitepapers/wp_redirect.cfm\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;heritrix crawl log portion\n&gt;&gt;&gt;&gt;==========================\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;2006-03-08T16:11:41.295Z   200      17266 \n&gt;&gt;&gt;&gt;http://uaelp.pennnet.com/whitepapers/wp_redirect.cfm?id=305 LLL \n&gt;&gt;&gt;&gt;http://uaelp.pennnet.com/whitepapers/wp.cfm?id=305 text/html #075 \n&gt;&gt;&gt;&gt;20060308161137362+1062 R5C73R3EPNNO4UYMWRPBTUJD3TWW32X7 2t\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;According to our reading of the robots.txt spec, this URL should not \n&gt;&gt;&gt;&gt;have been crawled.  The only reason I can find for the failure may be \n&gt;&gt;&gt;&gt;that the last line is not terminated with a newline, but rather just an EOF.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Any thoughts?\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Karl\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; \n&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n&gt; \n\n\n"}}