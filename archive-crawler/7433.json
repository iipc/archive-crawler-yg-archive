{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":325624130,"authorName":"Noah Levitt","from":"Noah Levitt &lt;nlevitt@...&gt;","profile":"nlevitt","replyTo":"LIST","senderId":"yxU6HeBXC3C9tmYjyFgzvHvqrLuhcztktRBvorhp7qNSrwglUlSzkWbUrYPugtLIB81VFl1cO9JkwthCMFyaAIzNIK6kAaT6","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] How to not ConsiderRobotsPrecondition ?","postDate":"1323741557","msgId":7433,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFRTZCMTc1LjYwOTAyMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGpia3BmcStjaWdjQGVHcm91cHMuY29tPg==","referencesHeader":"PGpia3BmcStjaWdjQGVHcm91cHMuY29tPg=="},"prevInTopic":7421,"nextInTopic":0,"prevInTime":7432,"nextInTime":7434,"topicId":7421,"numMessagesInTopic":2,"msgSnippet":"Hello, Sites without robots.txt are considered to be completely open to crawling, same as if they had an all-permissive robots.txt. Heritrix still fetches","rawEmail":"Return-Path: &lt;nlevitt@...&gt;\r\nX-Sender: nlevitt@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 90559 invoked from network); 13 Dec 2011 01:59:19 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m8.grp.sp2.yahoo.com with QMQP; 13 Dec 2011 01:59:19 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.224.6)\n  by mta2.grp.sp2.yahoo.com with SMTP; 13 Dec 2011 01:59:19 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id CB8786840151;\n\tMon, 12 Dec 2011 17:59:18 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id TNurEl8BD8mQ; Mon, 12 Dec 2011 17:59:17 -0800 (PST)\r\nX-Received: from [208.70.27.155] (desktop-nlevitt.sf.archive.org [208.70.27.155])\n\tby mail.archive.org (Postfix) with ESMTPSA id B39F56840140;\n\tMon, 12 Dec 2011 17:59:17 -0800 (PST)\r\nMessage-ID: &lt;4EE6B175.6090202@...&gt;\r\nDate: Mon, 12 Dec 2011 17:59:17 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.2.23) Gecko/20110922 Thunderbird/3.1.15\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: tereo27 &lt;tereo27@...&gt;\r\nReferences: &lt;jbkpfq+cigc@...&gt;\r\nIn-Reply-To: &lt;jbkpfq+cigc@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Noah Levitt &lt;nlevitt@...&gt;\r\nSubject: Re: [archive-crawler] How to not ConsiderRobotsPrecondition ?\r\nX-Yahoo-Group-Post: member; u=325624130; y=pQ48QA490neL1f7Xrqf2FSwo690AocSAniUdaxZ_19i7HA\r\nX-Yahoo-Profile: nlevitt\r\n\r\nHello,\n\nSites without robots.txt are considered to be completely open to \ncrawling, same as if they had an all-permissive robots.txt.\n\nHeritrix still fetches robots.txt even if the policy is IGNORE, it just \nignores the rules.\n\nSo perhaps something else is going wrong with your crawl. If you&#39;re \nstill having trouble, feel free to respond with more details about your \nconfiguration and the results you&#39;re seeing.\n\nNoah\n\nOn 12/06/2011 02:06 AM, tereo27 wrote:\n&gt; Hi,\n&gt;\n&gt; I&#39;m working with Heritrix Engine 3.1.0.\n&gt; Some of the websites i want to crawl don&#39;t have a robots.txt file.\n&gt; Considering PreconditionEnforcer rules, those websites are ignored. How can i force heritrix to crawl them ?\n&gt;\n&gt; In other hand, for websites having a robots.txt file, i also try to ignore robots rules by adding in the crawler-beans.cxml :\n&gt; metadata.robotsPolicyName=IGNORE\n&gt; However, the crawl process continues to considering robots.txt rules.\n&gt;\n&gt; Any help will be appreciated.\n&gt;\n&gt; Thanks\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n\n"}}