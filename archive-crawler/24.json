{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"&quot;Gordon Mohr&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"6_3vRjcQvf4euIDS2WnZD4M5VeFH1Q74DoHupwNce7Moz_zFhDTZKJMqXTSUPA7F8YDGNgIcTZoOs2ZHpyK_u1qZSSZ5rd-ARw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"DNS and HTTP staging (was Re: [archive-crawler] Re: Web crawler work ??","postDate":"1047079841","msgId":24,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAxYTQwMWMyZTUwMSQ5MjI3NDNkMCQ0ZGViZWRkMUBXT1JLU1RBVElPTjIxPg==","referencesHeader":"PDM3ZWQwMWMyZDczNCQ0ZjQ0NjliMCRkNTAwYThjMEBSZWRkeUdCPiA8MDM4MDAxYzJkN2I2JDJjMTIxZjAwJDNhZWJlZGQxQGdvam92YWlvPiA8M2ZlMDAxYzJkOWM3JGU1OGE0YTgwJGQ1MDBhOGMwQFJlZGR5R0I+IDwwMGRjMDFjMmRhMDckNjZlZTEzNjAkM2FlYmVkZDFAZ29qb3ZhaW8+IDwwNzY5MDFjMmRlODEkMTNmY2ZlOTAkZDUwMGE4YzBAdGlkZWxwYXJrLmlzb2Z0dGVjaGluZGlhLmNvbT4gPDAwOGUwMWMyZGY2ZSQzZDQ4NGJlMCQ0ZGViZWRkMUBXT1JLU1RBVElPTjIxPiA8MGVjNjAxYzJlNDA5JDBiYzYwMmMwJGQ1MDBhOGMwQHRpZGVscGFyay5pc29mdHRlY2hpbmRpYS5jb20+IDwxMTY0MDFjMmU0YzYkZmNhMjU1ZjAkZDUwMGE4YzBAdGlkZWxwYXJrLmlzb2Z0dGVjaGluZGlhLmNvbT4="},"prevInTopic":0,"nextInTopic":25,"prevInTime":23,"nextInTime":25,"topicId":24,"numMessagesInTopic":5,"msgSnippet":"These are good decompositions of the steps involved, and the LGPL dnsjava library looks very useful for our needs. My tendency would be to think fewer stages","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (EGP: mail-8_2_6_1); 7 Mar 2003 23:30:55 -0000\r\nReceived: (qmail 41220 invoked from network); 7 Mar 2003 23:30:54 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m10.grp.scd.yahoo.com with QMQP; 7 Mar 2003 23:30:54 -0000\r\nReceived: from unknown (HELO mail.archive.org) (209.237.232.3)\n  by mta3.grp.scd.yahoo.com with SMTP; 7 Mar 2003 23:30:54 -0000\r\nReceived: from WORKSTATION21 (dynamic-77.archive.org [209.237.235.77])\n\tby mail.archive.org (8.12.8/8.10.2) with SMTP id h27MpuCY018141;\n\tFri, 7 Mar 2003 14:51:56 -0800\r\nMessage-ID: &lt;01a401c2e501$922743d0$4debedd1@WORKSTATION21&gt;\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nCc: &lt;wcr-team@...&gt;\r\nReferences: &lt;37ed01c2d734$4f4469b0$d500a8c0@ReddyGB&gt; &lt;038001c2d7b6$2c121f00$3aebedd1@gojovaio&gt; &lt;3fe001c2d9c7$e58a4a80$d500a8c0@ReddyGB&gt; &lt;00dc01c2da07$66ee1360$3aebedd1@gojovaio&gt; &lt;076901c2de81$13fcfe90$d500a8c0@...&gt; &lt;008e01c2df6e$3d484be0$4debedd1@WORKSTATION21&gt; &lt;0ec601c2e409$0bc602c0$d500a8c0@...&gt; &lt;116401c2e4c6$fca255f0$d500a8c0@...&gt;\r\nSubject: DNS and HTTP staging (was Re: [archive-crawler] Re: Web crawler work ??\r\nDate: Fri, 7 Mar 2003 15:30:41 -0800\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2800.1106\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1106\r\nFrom: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nThese are good decompositions of the steps involved, and the LGPL dnsjava\nlibrary looks very useful for our needs.\n\nMy tendency would be to think fewer stages are better -- and when\ncommunicating between our stages, rather than using a shared RequestMap,\ninclude on the requesting event whatever context will be needed to deal\nwith the response when it arrives.\n\nYou can see this technique used in the ostore.network.ADns class I referenced\nyesterday -- the user_data object lets any client of the ADns stage recover\nthe state it needs to deal with ADns&#39;s eventual response. (We may still need\nthe equivalent of a RequestMap to deal with mapping UDP response packets to\nthe lookups that triggered them -- but that&#39;d then be a map that can be private\nto a single stage.)\n\nNothing in the DNS procedure needs to block -- once you assume asynchronous\nUDP replies and an in-RAM response cache -- and thus one thread should be\nas good as (in fact better than) N threads.\n\nSimilarly, I don&#39;t think more than 1 UDP socket will be necessary for\nall DNS lookups, because as a practical matter, 1 open UDP socket will\nnever be &quot;busy&quot; in a way that additional UDP sockets would help.\n\n--\nRegarding 404 and other explicit HTTP app errors: these should be recorded\ninto the CrawlURI, and forwarded to the next processing stage, just as\nwith successes.\n\nRedirects are a special case we haven&#39;t discussed much yet; they would seem\ncandidates for some sort of expedited fetching. I think, though, such\nexpedited activity must pass through the full cycle of stages for\nproper logging and policy application. (In contrast, retries in the face\nof transient errors, which may, at least in some cases, be fed immediately\nback to the Fetching or Preprocessing stages.)\n\n--\nThe issue of timeouts (esp. in HTTP) is thorny. From what I hear about\ncrawler traps, we need to not only to be able to deal with hangs, but\nalso various infinite-length trickles.\n\nIf any of the HTTP-transactions-in-progress is not making sufficient\nprogress, according to some set of progress-per-time-unit thresholds,\nwe have to be ready to give up on it. We should mark it as an error,\nbut probably retain any partial info we did get for later analysis.\n\nSeparating this out into its own stage increases synchronization issues.\nIf timeout analysis can instead be part of the same single-threaded stage\nwhere HTTP some-progress-made events (from sockets) are handled, I think\nit can be handled very efficiently:\n\n   - keep all outstanding transactions on a linked-list\n   - whenever progress is reported (from the socket),\n     and that progress keeps the transaction above\n     acceptable thresholds, move that transaction to\n     the head of the list\n   - check the tail of the list occasionally to see\n     if the worst transactions need to be cut\n\nThat &quot;occasional&quot; check could mostly occur as part of normal\nprogress-packet processing; an extremely infrequent timer could\ntrigger reevaluation in the rare case where all progress on all\nsockets has stopped...\n\n- Gordon\n\n----- Original Message -----\nFrom: G.B.Reddy\nTo: archive-crawler@yahoogroups.com\nCc: wcr-team@...\nSent: Friday, March 07, 2003 8:31 AM\nSubject: Re: [archive-crawler] Re: Web crawler work ??\n\n\n\nMore insight on the DNS stages.\n\nAs stated in the design earlier, &quot;DNS Querying Stage&quot;, &quot;DNS Response Processing Stage&quot; and &quot;Timeout and Retry Handling Stage&quot;\naccess/update the shared RequestMap. So, they need to be synchronized. We were just thinking whether these need to be three separate\nstages or could it be one stage multiplexing each of those incoming event types. The pros and cons of them are as below.\n\nSingle stage benefit / Multi Stage Issue :\n- If it is single stage, then synchronization is not done across stages. Even though, synchronization anyway would be needed\ninternally in case of single stage, conceptually it looks neater not to synchronize across stages.\n\nSingle stage issue / Multi Stage benefit :\n- In case of single stage, the event queue would be one and this fact pulls our legs when we want to handle overload conditions.\nSince there is no clear count of the distinct elements in the queue, it becomes difficult to analyse and condition the system\naccordingly. Whereas, in multi stage, each one having its own queue, conditioning/prioritizating becomes easy.\n\nI think we would end up going in for multi stage, unless SEDA could take care of it by itself.\n\nAnd in case of parallel stages, like the ones in post processing, I think most often, synchronization across them may be\nunavoidable.\n\nThanks,\nReddy\n\n\n----- Original Message -----\nFrom: G.B.Reddy\nTo: archive-crawler@yahoogroups.com\nCc: wcr-team@...\nSent: Thursday, March 06, 2003 11:21 PM\nSubject: Re: [archive-crawler] Re: Web crawler work ??\n\n\nGordon and Raymie,\n\nBelow are the various stages and their design with the issues involved in the DNS Resolver and HTTP Client implementation.\n\nDNS History/Cache Handling Stage :\n\nOverview:\n- Maintains successful lookups in cache.\n- Does negative caching.\n- Times itself to clean the expired entries based on TTL. (Would use the ssTimer SEDA APIs to schedule itself periodically)\n- This stage would be dummy or could be skipped as of now since we want to do caching later.\n\nEvents:\n- Two types of events : DNSCacheLookupEvent, DNSCacheUpdateEvent.\n- DNSCacheLookupEvent : If entry is found in cache, the ipaddress is set in the CrawlURI object and is enqueued into the &quot;Page\nRequesting Stage&quot;. Else, it is enqueued into the &quot;DNS Querying Stage&quot;.\n- DNSCacheUpdateEvent : This event is published by the &quot;DNS Response Processing Stage&quot; every after successful/failed lookup inorder\nto update the cache.\n\nOther notes:\n- This stage could be single threaded else lot of synchronization might be needed.\n- Resubmitting events on queue full exceptions while enqueuing into this stage&#39;s queue should be handled by the caller by scheduling\nit in future.\n\n\nDNS Querying Stage :\n\nOverview:\n- Sends the actual DNS ARecord query packets to the DNS Server. (The response packets are processed in a later stage)\n- Maintains a pool of DatagramSocket objects.\n\nEvents:\n- SendDNSQueryEvent : This is published by the &quot;DNS History/Cache Handling Stage&quot; when cache miss happens. The DNS query is formed\nand sent out. The response handling sink is set as the &quot;DNS Response Processing Stage&quot;.\n\nImplementation:\n- A pool of DatagramSockets of a configurable maximum size is maintained. It will be filled incrementally. All these datagram\nsockets will be registered to the selector maintained by the SEDA internals. It would be ideal if this pool gets shrunk or expanded\nbased on the requirement. If it is not shrunk back, then it is an unnecessary overhead on the selector. The reason behind having a\npool is to restrict the number of ports the selector has to listen upon and also not to create individual DatagramSocket objects for\nevery query. Can this logic of bounded pool, be implemented as a Controller in the SEDA framework (just like the\nThreadPoolController) is an open question.\n- When an event comes in, a free datagram socket in the pool will be utilized for sending the message. If all sockets are engaged,\nthe incoming event should be postponed to reenter again after a period of time.\n- This stage additionally has also to maintain the list of messages sent out, their IDs and the request timestamps. Let us call this\n&quot;RequestMap&quot; for future reference. The ID is the integer, described in the RFC DNS message format, used to map the\nrequest-responses. The request timestamp will be made use of in query timeout handling (discussed later).\n\nParameters to this stage :\n- The DNS server hostname/ipaddress. If this is not given, then the /etc/resolve.conf will be parsed to get the name server (only\nthe primary would be taken as of now.). As a next step we will have to build a round-robin way of querying the various name servers\nin resolve.conf, inorder to be polite with them.\n- If resolve.conf is not present, the local host will be assumed as the name server.\n\n\nDNS Response Processing Stage :\n\nOverview:\n- Processes DNS responses.\n\nImplementation:\n- When the DNS datagram packets are received, the ID field in the header should be used to match the corresponding request packet.\n- Check for timeouts, and discard it if it had timed out; else, set the ipaddress/canonical name in the CrawlURI object and enqueue\nit to the &quot;Page Requesting Stage&quot;. In addition, enqueue an event into the &quot;DNS Cache Handling Stage&quot; for it to update its cache. Do\nthe same, even on DNS Errors like &quot;Name not found authoritative error&quot;.\n- The request entry in the RequestMap (maintained for timeout handling) should be removed. This map, being shared across stages,\nshould be synchronized.\n\n\nHTTP Page Requesting Stage :\n\nOverview:\n- Connects to host and sends GET requests for pages.\n\nEvents:\n- Handles two types of events - StartFetchEvent and ConnectionCompleteEvent.\n- The StartFetchEvent will make a TCP connect request to the host. While doing so, we will register the current stage itself to\nreceive back the ConnectionComplete events. Once we receive this ConnectionCompleteEvent, we should send a HTTP GET request to the\npage. The response handling sink is set as the &quot;HTTP Response Processing Stage&quot;. Write failures should be handled.\n\n\nHTTP Response Processing Stage :\n\nOverview:\n- Processes downloaded pages.\n\nImplementation:\n- Check for timeouts, and discard it if it had timed out; else, read the packets.\n- Once the response is completely read, the request entry in the RequestMap (maintained for timeout handling) should be removed.\n- One issue here is when we are reading lengthy HTML pages, we might receive half of the page and it might stop after that. So,\nessentially the timeout should be applied between chunks of reception.\n- Where should the errors like &quot;404 Not Found&quot;, etc be propogated ???\n\n\nTimeout and Retry Handling Stage :\n\nOverview:\nThis is a single threaded stage which enumerates through the RequestMap and checks for timeouts. The timed out CrawlURIs will be\nretried until retry count exhausts. This stage will be self-timed periodically using the SEDA ssTimer APIs.\n\nOther Notes:\nThis timeout handling is a common stuff between the DNS requests and the HTTP requests.\n\nParameters to this stage:\n- DNS timeout value.\n- HTTP timeout value.\n- DNS retry count.\n- HTTP retry count. ( This would be 1 ).\n\n\nOne other thing that could be done is that, the events by themselves will contain information as to which next stage the output has\nto traverse. This will be flexible and no hardcoding is needed. Especially, in making this non-blocking DNS library an open-source,\nit would come handy. Moreover, many users might not want it to be over SEDA. So, we will have to give other interfaces as well.\n\nI am presently using the library classes given by dnsjava-1.3.2. ( http://sourceforge.net/projects/dnsjava/ ). This is an LGPL java\nbased synchronous implementation of DNS Resolver. I only make use of the classes which encapsulate the formation of request packets,\nparses response packets and the various ResourceRecord classes. This library is being used by Java Apache Mail Enterprise Server (\nhttp://james.apache.org/ ). So, it should be pretty reliable and tested. Moreover it has support for IPv6, compression and security\nwhich we can make use of later.\n\nThanks,\nReddy\n\n\n\n----- Original Message -----\nFrom: Gordon Mohr\nTo: archive-crawler@yahoogroups.com\nCc: Raymie Stata ; wcr-team@...\nSent: Saturday, March 01, 2003 2:43 AM\nSubject: Re: [archive-crawler] Re: Web crawler work ??\n\n\nSounds like a reasonable plan.\n\nBy &quot;local name server&quot; do you mean something *very* local -- for example,\na standard nameserver we run on the same machine?\n\nThat would seem to offer other benefits -- such as minimizing the modes\nof DNS lookup we have to do and offloading caching to another piece of\nsoftware (at least at first).\n\n- Gordon\n\n----- Original Message -----\nFrom: G.B.Reddy\nTo: archive-crawler@yahoogroups.com\nCc: Raymie Stata ; wcr-team@...\nSent: Thursday, February 27, 2003 8:55 AM\nSubject: Re: [archive-crawler] Re: Web crawler work ??\n\n\nGordon and Raymie,\n\nHere goes the proposal for the asynchronous DNS lookup API implementation.\n\nWe shall implement a minimal resolver which is capable of sending DNS request packets and processing response packets in an\nasynchrounous nio fashion. This resolver class will contact a local name server and rely on it to do the actual lookup. The local\nname server will be configured to support recursion and better would be to use a name server which does lookup asynchronously. (\nSQUID has asynchronous DNS lookup facilities ).  Even if the local name server is not asynchrounous, our java resolver being\nasynchronous will be good enough since our primary goal is that we do not want any blocking code in our crawler implementation. This\nidea even sounds good considering the fact we would only reinvent the same wheel if we opt to implement a complete full-fledged\nresolver implementation which complies with the RFC 1035 and 1034. We can definitely implement this full-fledged resolver but the\nreal concern is that this would require a lot of testing and the efforts to make it rock solid in terms of robustness would be huge.\n\nSo, the various jobs that we would have to do to build our Simple Asynchronous DNS lookup API would be\n    -- Request packet formation and reply packet parsing in the exact RFC format.\n    -- Use non-blocking IO APIs and do UDP. (We might not need TCP since the name server is only in the local network.)\n    -- Do canonical name queries and A record queries.\n    -- Implement timeouts.\n    -- Implement caching based on TTL. ( This may have to be deferred as pointed by Raymie earlier. )\n    -- Integrate with SEDA.\n\nThanks,\nReddy\n\n----- Original Message -----\nFrom: Gordon Mohr\nTo: G.B.Reddy\nCc: Raymie Stata ; wcr-team@... ; archive-crawler@yahoogroups.com\nSent: Saturday, February 22, 2003 5:37 AM\nSubject: [archive-crawler] Re: Web crawler work ??\n\n\n[CC&#39;ing to archive-crawler@yahoogroups.com]\n\nReddy writes:\n\n&gt; On the first cut do we need to look at implementing an asynchronous DNS\n&gt; lookup mechanism. If we are not, then it is going to be two stages, viz.\n&gt; DNSCacheHandlingStage and ResolvingStage, that can be employed using the\n&gt; blocking DNS lookup calls in Java. The first stage, DNSCacheHandlingStage,\n&gt; would check if the entry is available in the cache. If available, he would\n&gt; set the resolved address in the CrawlURI object and enqueue it to the\n&gt; appropriate next stage. If the cache doesn&#39;t contain the entry, then he\n&gt; would pass the request to the Resolving stage which would call the\n&gt; InetAddress.getByName blocking method to resolve it. The getByName result\n&gt; would be set in the CrawlURI object as before and enqueued into the\n&gt; appropriate next stage. In addition to this, the Resolving stage will\n&gt; enqueue another event into the DNSCacheHandlingStage to enable him update\n&gt; his cache. So, the DNSCacheHandlingStage would be handling two types of\n&gt; events, one is the lookup events and the other is the update cache events.\n&gt;\n&gt; One problem here is that the InetAddress class does not expose its cache\n&gt; variables to its users. Even we cannot check if the cache has an entry\n&gt; before calling the getByName method. So, we should be disabling the java\n&gt; cache ( using the policy file ) and implementing our own caching mechanism.\n&gt; ( The DNSCacheHandlingStage would have to additionally do the job of\n&gt; throwing away the expired entries in the cache also.)\n&gt;\n&gt; Let me know your comments on this.\n\nThis looks like a good first cut. I&#39;m still working to improve my\nunderstanding of the best way to use the staged style, mostly by\nlooking at their HTTP and HTTP Server (Haboob) code.\n\nIt seems that they&#39;ve tended to use a single Stage object to do\nmany different steps/aspects of one process, by switching on the\ntype of QueueElement received.\n\nSo for example their seda.sandStorm.seda.apps.Haboob.http.HttpRecv\naccepts events of types....\n\n  - httpConnection\n  - httpRequest\n  - SinkClosedEvent\n  - timerEvent\n\nAnd their seda.sandStorm.lib.http.httpServer accepts events of\ntypes...\n\n  - ATcpInPacket\n  - ATcpConnection\n  - aSocketErrorEvent\n  - SinkDrainedEvent\n  - SinkCloggedEvent\n  - SinkClosedEvent\n  - ATcpListenSuccessEvent\n\nThey also use Sinks that are not associated with stages; rather,\nthey interface to unstaged components which nonetheless result in\nan eventual event to some supplied answer Sink. See for example\nseda.sandStorm.lib.http.httpConnection.\n\nSo perhaps as a matter of grouping related tasks, the same Stage object\nshould be re-entered over the course of a lookup, with different triggering\nevents. For example, you might want to reenter a single DNSResolvingStage\nover the course of cache lookup, lookup-initiation, result-receiving (or\ntimeout), etc. I&#39;m not sure; use your judgement as to how many stages are\nreally needed.\n\n&gt; P.S : We found some openly available async dns client APIs in C language.\n\nThat could be useful as a model. (I doubt we&#39;d want to call out to C\nfor this simple step, though -- and if we nailed down a truly async Java\nDNS facility, a lot of open source projects would probably be quite happy.)\n\nAlso: I heard back from Patrick Eaton about SEDA-style async HTTP client\ncode... he has a rough implementation for simple usage, and he knows of\nanother one at Berkeley which goes deeper into HTTP/1.1 conformance and\noptimal performance. I&#39;ve asked him to forward whatever additional code\nor details he can.\n\n- Gordon\n\n\n\n\nTo unsubscribe from this group, send an email to:\narchive-crawler-unsubscribe@yahoogroups.com\n\n\n\nYour use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n\n\nYahoo! Groups Sponsor\nADVERTISEMENT\n\n\n\n\nTo unsubscribe from this group, send an email to:\narchive-crawler-unsubscribe@yahoogroups.com\n\n\n\nYour use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n\n\n\nTo unsubscribe from this group, send an email to:\narchive-crawler-unsubscribe@yahoogroups.com\n\n\n\nYour use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n\n\nYahoo! Groups Sponsor\nADVERTISEMENT\n\n\n\n\nTo unsubscribe from this group, send an email to:\narchive-crawler-unsubscribe@yahoogroups.com\n\n\n\nYour use of Yahoo! Groups is subject to the Yahoo! Terms of Service.\n\n\n"}}