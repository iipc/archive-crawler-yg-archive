{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":191507321,"authorName":"robeger","from":"&quot;robeger&quot; &lt;reger@...&gt;","profile":"robeger","replyTo":"LIST","senderId":"9udbqWV8O1dDDtfTyUGce7VKA06PDY725XJN7PD2F_GUysQlN6Y1-a3yvpg81kOT09FNtbPZTO3EXbXVb43nK-vh","spamInfo":{"isSpam":false,"reason":"0"},"subject":"New issue with DomainSensitiveFrontier","postDate":"1102703641","msgId":1269,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNwY3E2cCtjZjliQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGNvdXJwaSt2NTYxQGVHcm91cHMuY29tPg=="},"prevInTopic":1268,"nextInTopic":1270,"prevInTime":1268,"nextInTime":1270,"topicId":1235,"numMessagesInTopic":20,"msgSnippet":"Found another problem with the DomainSensitiveFrontier.  The max docs per site setting seems to actually act as a max docs for the crawl setting.  If I crawl a","rawEmail":"Return-Path: &lt;reger@...&gt;\r\nX-Sender: reger@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 12193 invoked from network); 10 Dec 2004 18:34:57 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m23.grp.scd.yahoo.com with QMQP; 10 Dec 2004 18:34:57 -0000\r\nReceived: from unknown (HELO n8a.bulk.scd.yahoo.com) (66.94.237.42)\n  by mta1.grp.scd.yahoo.com with SMTP; 10 Dec 2004 18:34:57 -0000\r\nReceived: from [66.218.66.58] by n8.bulk.scd.yahoo.com with NNFMP; 10 Dec 2004 18:34:02 -0000\r\nReceived: from [66.218.67.175] by mailer7.bulk.scd.yahoo.com with NNFMP; 10 Dec 2004 18:34:02 -0000\r\nDate: Fri, 10 Dec 2004 18:34:01 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;cpcq6p+cf9b@...&gt;\r\nIn-Reply-To: &lt;courpi+v561@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 3137\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.42\r\nFrom: &quot;robeger&quot; &lt;reger@...&gt;\r\nSubject: New issue with DomainSensitiveFrontier\r\nX-Yahoo-Group-Post: member; u=191507321\r\nX-Yahoo-Profile: robeger\r\n\r\n\nFound another problem with the DomainSensitiveFrontier.  The max docs\nper site setting seems to actually act as a max docs for the crawl\nsetting.  If I crawl a single site with max docs per site set to 10, I\nget 10 pages crawled, but if I do the same with 2 sites, I end up\ngetting 5 per site (10 total, not sure if the ratio would work out\nevenly all the time).\n\nRob.\n\n\n--- In archive-crawler@yahoogroups.com, &quot;ogrenholm&quot;\n&lt;oskar.grenholm@k...&gt; wrote:\n&gt; \n&gt; Hey, I did reply to this just shortly before I left for the weekend, \n&gt; but it seems that the reply didn&#39;t make it (hope this one do).\n&gt; \n&gt; And yeah, it doesn&#39;t work now, but it sure did when I tested it back \n&gt; on 1.0. But since I set up the job I wanted it to do (downloading \n&gt; the frontpage of all Swedish online newspapers) I haven&#39;t looked att \n&gt; in anymore.\n&gt; \n&gt; But now I had a look at this at Friday and I think the problem is \n&gt; that the method deleteMatchedItems() in TieredQueue.java doesn&#39;t \n&gt; update the length of the queue and therefore when asked if it&#39;s \n&gt; empty answers no. One solution might be to add a call to \n&gt; recalculateLength() in deleteMatchedItems(), just before it returns, \n&gt; i.e. \n&gt; public long deleteMatchedItems(Predicate matcher) {\n&gt;         long count = 0;\n&gt;         for (int i = 0; i &lt;= lastQueue; i++) {\n&gt;             count += innerQueues[i].deleteMatchedItems(matcher);\n&gt;         }\n&gt;         recalculateLength();  //ADDED\n&gt;         return count;\n&gt;     }\n&gt;  \n&gt; I did this and it seems to work (only basic testing, then weekend). \n&gt; If someone think this is a bad idea (yes, I mean you Stack) or has a \n&gt; better solution, please let me know.\n&gt; \n&gt; /Oskar\n&gt; \n&gt; Ps. I have a better look at this on Monday. Ds.\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; &gt; robeger wrote:\n&gt; &gt; \n&gt; &gt; &gt;\n&gt; &gt; &gt; Just tried this out.  It seems that the crawl never &quot;finishes&quot;.  \n&gt; Tried\n&gt; &gt; &gt; it on a single URL, limiting it to 10 docs for that site.  Once \n&gt; it\n&gt; &gt; &gt; reaches 10, it starts giving -6000 as the status code for \n&gt; subsequent\n&gt; &gt; &gt; pages, but the crawl never finishes, console just sits at &quot;13 of \n&gt; 39\n&gt; &gt; &gt; DOWNLOADED/QUEUED RATIO&quot;.  It also seems to count docs from\n&gt; &gt; &gt; macromedia.com as part of the 10.  If I switch back to the \n&gt; default\n&gt; &gt; &gt; frontier, it crawls the site and finishes.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; &lt;&gt;The way that the DomainSensitiveFrontier works is that when it \n&gt; hits the\n&gt; &gt; max-docs-for-this-domain threshold, it marks all remaining queued \n&gt; URLs as\n&gt; &gt; &#39;deleted&#39;. But Frontier continues to run. As it comes across these\n&gt; &gt; deleted URLs that are still in the queue, it discards them logging \n&gt; as\n&gt; &gt; crawl.log -- the -6000s (&#39;Deleted from Frontier by user&#39;).\n&gt; &gt; \n&gt; &gt; That the crawl never finishes is a bug (I just tried it and had \n&gt; same issue.\n&gt; &gt; Is your heritrix_out.log filled with exceptions?). I made an issue \n&gt; to cover\n&gt; &gt; this: &#39;[ 1078581 ] DomainSensitiveFrontier never finishes&#39;.  Let \n&gt; me see if\n&gt; &gt; we can get the fella that donated this Frontier to fix it.\n&gt; &gt; \n&gt; &gt; (The macromedias being counted has to do with your scope settings.\n&gt; &gt; Try changing max-embed-hops)\n&gt; &gt; \n&gt; &gt; St.Ack\n&gt; &gt; \n&gt; &gt; \n\n\n\n\n"}}