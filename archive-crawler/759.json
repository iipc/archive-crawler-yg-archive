{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":172190008,"authorName":"Andy Boyko","from":"Andy Boyko &lt;aboy@...&gt;","profile":"andyboyko","replyTo":"LIST","senderId":"6JeFbja2N_w_2DtvxjK3zXcfaKlivKMUORY0_W9QoR5eOStkWve2ax3M5Wq-2QrP2_haOu4UKwR3Or23KGjglg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Congrats, ARC tools, link extractors, PageRank, ...","postDate":"1091828231","msgId":759,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxMTNGQTA3LjMwMDAyMDZAbG9jLmdvdj4="},"prevInTopic":0,"nextInTopic":760,"prevInTime":758,"nextInTime":760,"topicId":759,"numMessagesInTopic":3,"msgSnippet":"Congratulations, Heritrixians, on 1.0 -- you ve done a tremendous amount in a remarkably short time.  As the St.Ack says: good stuff! So with the code freeze","rawEmail":"Return-Path: &lt;aboy@...&gt;\r\nX-Sender: aboy@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 83659 invoked from network); 6 Aug 2004 21:37:23 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m14.grp.scd.yahoo.com with QMQP; 6 Aug 2004 21:37:23 -0000\r\nReceived: from unknown (HELO sun8.loc.gov) (140.147.249.48)\n  by mta6.grp.scd.yahoo.com with SMTP; 6 Aug 2004 21:37:23 -0000\r\nReceived: from [140.147.131.81] (amerprt1.loc.gov [140.147.131.81])\n\tby sun8.loc.gov  with ESMTP id i76LbJFa015124\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 6 Aug 2004 17:37:22 -0400 (EDT)\r\nMessage-ID: &lt;4113FA07.3000206@...&gt;\r\nDate: Fri, 06 Aug 2004 17:37:11 -0400\r\nUser-Agent: Mozilla Thunderbird 0.7.1 (Windows/20040626)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 140.147.249.48\r\nFrom: Andy Boyko &lt;aboy@...&gt;\r\nSubject: Congrats, ARC tools, link extractors, PageRank, ...\r\nX-Yahoo-Group-Post: member; u=172190008\r\nX-Yahoo-Profile: andyboyko\r\n\r\nCongratulations, Heritrixians, on 1.0 -- you&#39;ve done a tremendous amount \nin a remarkably short time.  As the St.Ack says: &quot;good stuff!&quot;\n\nSo with the code freeze lifted, maybe now&#39;s a good time to combine a \ncouple of the discussions going on on this list.  Ansi&#39;s question about \nPageRank leads to the topic of DAT files, which historically have been \nthe best (only) source for out-linking information for a crawl, but \nthere isn&#39;t yet an open-source tool that can create DATs from ARCs. \nI&#39;ve appended some notes about DAT extraction I made.  If nobody else is \nactively on the problem, I&#39;d like to take a shot at the second approach, \nthe command-line tool.  We&#39;ve got plenty of ARC readers, after all :)\n\nThings get better for this problem with the next version of the ARC \nformat, discussed here a while back (or on the wiki?), in which ARC \nrecords include item metadata.  With that, we can just put the extracted \nURLs in the item&#39;s header when it&#39;s written.  Is there still work to be \ndone on the new ARC spec?\n\nRegards,\nAndy Boyko  aboy@...\nLibrary of Congress\n\n\n\nDAT notes:\n\nThe DAT file is a summary of the ARC, containing headers for each piece \nof content, but omitting the actual content.  It also adds some \ninformation created from analyzing the content, such as a list of links \nembedded in an HTML file.  The DAT file has historically been the input \nto creating the CDX index, but it is straightforward to create a CDX \ndirectly from an ARC as well, and the DAT is not otherwise used during \npresentation with Wayback Machine or similar tools.  What DAT files do \nuniquely provide, though, is a list of extracted outbound links from a \ngiven URL, which has utility in a number of interesting problems in \nanalyzing crawl output.\n\nAt the moment, the only tool to create a DAT file is the Alexa tool \nav_procarc, implemented in C++.  The Alexa code is not open source, and \nnot available outside IA, so another approach is needed.  Even if the \ntool were available, it would be preferable to extract links using the \nsame algorithm used by the crawler, or at least with a standard tool.\n\nSo in its simplest form, the problem before us is to obtain the list of \nURLs referenced by a given document, without using av_procarc to get \nthem.  Consider the following approaches:\n\n1. Alter Heritrix to write the extracted links directly into the ARC \nrecord, as metadata\n* Pro: no other tools needed; ARC is self-contained\n* Pro: implementation is easy; right data is available at ARC writing time\n* Con: doesn&#39;t help with previously collected content\n* Con: requires update to ARC format\n\n2. Write a replacement for av_procarc that reads an ARC and generates a \nDAT, using the Heritrix extractors to find the links again.\n* Pro: reuses crawler code, ensuring we find the same URLs crawler did\n* Pro: works on any previously collected ARCs\n* Con: the extractors are currently very tied to data structures only \navailable during the crawl, and would need to be refactored to support \nthis use\n\n3. Read the Heritrix recovery log recovery.gz, which includes links as \nthey are extracted, along with the URL of the item containing them.\n* Pro: Available immediately after crawl with minimal processing\n* Con: requires complex correlation with ARC files if full DAT files are \ndesired (versus just the list of links), since the ARC and the recovery \nlog will not necessarily be in the same order (use intermediate DB?)\n* Con: requires access to the recovery file, only available in the right \nformat with Heritrix 1.0 crawls\n\n\n"}}