{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (archive.org)","from":"&quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Wd1_CtmqL4SPpO_mxXRZboVAP89uCYYJosr8r5X-IHw6xm7gwNCfHk2GYpo9u-k6dB8uqc4W8LPG0QbcPsaY20g3kV1XRTaukVkjjm3glzJZ7I8j2WmY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Large crawl experience (like, 500M links)","postDate":"1135044125","msgId":2450,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzQTc2NjFELjQwNjA1MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGRvN2ZnbStxbWRAZUdyb3Vwcy5jb20+","referencesHeader":"PGRvN2ZnbStxbWRAZUdyb3Vwcy5jb20+"},"prevInTopic":2447,"nextInTopic":2454,"prevInTime":2449,"nextInTime":2451,"topicId":2391,"numMessagesInTopic":12,"msgSnippet":"... We ve run into problems under 64bit JVMs, and they seem mostly attributable to the fact that the JVM s object pointers are larger and thus the same object","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 28810 invoked from network); 20 Dec 2005 02:02:14 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m25.grp.scd.yahoo.com with QMQP; 20 Dec 2005 02:02:14 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta5.grp.scd.yahoo.com with SMTP; 20 Dec 2005 02:02:14 -0000\r\nReceived: (qmail 23572 invoked by uid 100); 20 Dec 2005 01:57:44 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.10?) (gojomo@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 20 Dec 2005 01:57:44 -0000\r\nMessage-ID: &lt;43A7661D.4060507@...&gt;\r\nDate: Mon, 19 Dec 2005 18:02:05 -0800\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc3 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;do7fgm+qmd@...&gt;\r\nIn-Reply-To: &lt;do7fgm+qmd@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-77.3 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 2:12:4:0\r\nFrom: &quot;Gordon Mohr (archive.org)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Large crawl experience (like, 500M links)\r\nX-Yahoo-Group-Post: member; u=137285340; y=Jzfy6_tdjmtSMdhTAOajZUJOvFWkYZ0TCrBf41Uj6dgX\r\nX-Yahoo-Profile: gojomo\r\n\r\njoehung302 wrote:\n&gt; I did a proof crawling using BroadScope and 22K seeds. I got OOME \n&gt; within a day. I then checkpoint it, restart the crawler, start \n&gt; another crawl from the checkpoint, OOME within a day.\n&gt; \n&gt; I then changed to use 5K seeds and BroadScope, OOME within a day. \n&gt; Restart with the checkpoint and still OOME within a day.\n&gt; \n&gt; I then run 5K seeds with DomainScope (kind of given up on \n&gt; broadscope). OOME within a day.\n&gt; \n&gt; I have my JVM set to -Xmx1500m. BTW, I&#39;m using 64 bit JDK1.5.\n&gt; \n&gt; One thing that I observed is, broad scope runs much faster than \n&gt; domain scope under roughly the same condition. In both broadscope \n&gt; runs I was able to top 1000KB/s bandwidth limit with around 50% cpu \n&gt; usage. In the domain scope run I can only get to 500KB/s throughput \n&gt; with 100% cpu busy. \n&gt; \n&gt; I used to be able to run 1.0.4 for a week with &lt;1K seeds and get \n&gt; around 1M links per day. I thought the bdb improvement should be \n&gt; able to take more seeds and run longer. I really want the crawler to \n&gt; run with a big seed list because we&#39;re going to seed my big crawl \n&gt; with links from ODP. \n&gt; \n&gt; Any suggestions that I can try?\n\nWe&#39;ve run into problems under 64bit JVMs, and they seem mostly\nattributable to the fact that the JVM&#39;s object pointers are larger\nand thus the same object structures will take up more RAM.\n\nThis post from a Sun engineer suggests a rule of thumb of a 40%\nlarger heap to be comparable to a 32bit JVM heap:\n\nhttp://forum.java.sun.com/thread.jspa?threadID=671184\n(see reply #8)\n\nSo your 1500m heap in a 64bit JVM may be roughly comparable to a\n1071m heap in a 32bit JVM.\n\nFurther, as noted in the 1.6 release notes, BerkeleyDB-JE 2.0.90&#39;s\ninternal mechanisms for staying within the budgetted cache size\nare inaccurate under 64bit JVMs, so rather than the default 60%\ncache size, 40% or even 30% would be safer.\n\nEven with these adjustments, there are still a few structures in\nthe frontier that slowly grow without bound in a broad crawl. We\naim to constrain the last of these by the 1.8 release, leading to\ncrawls that wobble (slow down) rather than ever falling down (OOME),\nas long as there&#39;s still disk space.\n\nBdbUriUniqFilter helps defer an OOME until those other structures\nbecome a problem, by not letting the URL already-seen structures\ngrow without bound. However, it&#39;s pretty inefficient for this kind\nof set-membership testing, especially once the crawl is big/disperse\nsuch that the cache isn&#39;t helping much. (It gets very slow.)\n\nBloomUriUniqFilter offers another option: its speed doesn&#39;t degrade\nwith the number of URIs crawled. However, this comes at the cost of\na higher false-positive rate (misrecognizing a URI as already-seen\nwhen it hasn&#39;t been) -- and once the crawl gets larger than the size\nthe Bloom filter was designed for, the false-positive rate grows to\napproach 100%. The default parameters use ~500MB to achieve a 1-in-\n4 million false-positive rate through 125 million URLs; these can\nbe tuned via System properties. (See the BloomUriUniqFilter source\nand http://crawler.archive.org/cgi-bin/wiki.pl?BloomUriUniqFilter\nfor details.)\n\nWe&#39;ve started work on another UriUniqFilter that uses a batch\nmerging technique described in the 2001 &quot;High-Performance Web\nCrawling&quot; paper by Mark Najork and Allan Heydon, in section 3.2,\n&quot;Efficient Duplicate URL Eliminators&quot;. A rough version is in CVS\nnow but it will need more tuning to match or surpass the existing\noptions. The hope is that it will offer adequate performance into\nthe hundreds of millions of URIs without hitting the walls of the\ncurrent options.\n\nRegarding the difference between DomainScope and BroadScope\nperformance:\n\nAll the &#39;classic&#39; limited scopes -- DomainScope, HostScope,\nPathScope -- use an inefficient linear probe against all\nacceptable patterns (usually, all seeds) to test if a URI is\nin scope. So, with a large number of seeds, they&#39;re slow\nCPU hogs.\n\nSurtPrefixScope can do anything they can, and much more\nefficiently, so it&#39;s worth it to recast anything you were\nusing DomainScope for to use SurtPrefixScope instead.\n\n--\n\nOne other thing which should help a little in the BdbUriUniqFilter\nperformance bottleneck is to use the &#39;queue budgetting&#39; features\nso that the crawler concentrates on a specific queue (host) for a\nwhile, then rotates it out of activity to give other queues a chance.\nIn the BdbFrontier expert settings, this means making sure the\n&#39;cost-policy&#39; is something other than ZeroCostAssignmentPolicy,\nand tending to make the &#39;balance-replenish-amount&#39; larger rather\nthan smaller. The current defaults for these are OK, but if you&#39;ve\nchanged them you may have decreased the potential for the BDB cache\nto benefit from site-locality patterns in discovered links.\n\nHope this helps,\n\n- Gordon @ IA\n\n&gt; --- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; \n&gt;&gt;joehung302 wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;&gt;Use the bloom filter option for the already-seen in \n&gt; \n&gt; BdbFrontier. \n&gt; \n&gt;&gt;&gt;Seems\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;to work better when a machine goes above 30-50million.  Bloom\n&gt;&gt;&gt;\n&gt;&gt;&gt;becomes\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;saturated at 125million so thats about the upperbound per \n&gt; \n&gt; machine at\n&gt; \n&gt;&gt;&gt;the\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;moment unless you up the bloom filter size  (but its already \n&gt; \n&gt; big and\n&gt; \n&gt;&gt;&gt;&gt;you&#39;ll start eating into heap the crawler is using going about \n&gt; \n&gt; its\n&gt; \n&gt;&gt;&gt;other\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;business).  Thereafter the rate of false positives -- reports \n&gt; \n&gt; that\n&gt; \n&gt;&gt;&gt;we&#39;ve\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;seen an URL when in fact we haven&#39;t -- starts to increase \n&gt; \n&gt; (Read the\n&gt; \n&gt;&gt;&gt;&gt;BloomFilter javadoc for more on its workings).\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;How confident do you guys feel that if I use broad-scope I can go\n&gt;&gt;&gt;above 50M links (or even 100M links) without OOME on a single \n&gt; \n&gt; machine?\n&gt; \n&gt;&gt;\n&gt;&gt;I&#39;d suggest you startup a proofing test crawl with BroadScope and \n&gt; \n&gt; see it \n&gt; \n&gt;&gt;does.\n&gt;&gt;\n&gt;&gt;On machines with specs like those listed below we&#39;ve pulled down \n&gt;&gt; &gt;50Million documents per instance with &gt;125million discovered.  \n&gt; \n&gt; Scope \n&gt; \n&gt;&gt;was not BroadScope.  Once or twice we OOME&#39;d but thought is that \n&gt;&gt;probable cause has been addressed in 1.6 release (If there is an \n&gt; \n&gt; OOME, \n&gt; \n&gt;&gt;you can checkpoint, restart and recover the crawl.  Often it will \n&gt;&gt;continue the crawl as it avoids an exact replay of the \n&gt; \n&gt; circumstances \n&gt; \n&gt;&gt;that brought on the OOME).\n&gt;&gt;\n&gt;&gt;One thing I forgot to add to yesterday&#39;s list is regular \n&gt; \n&gt; checkpointing \n&gt; \n&gt;&gt;-- every 4 hours or so.\n&gt;&gt;\n&gt;&gt;St.Ack\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;-bash-3.00$ uname -a\n&gt;&gt;Linux crawling015.archive.org 2.6.11-1.27_FC3smp #1 SMP Tue May 17 \n&gt;&gt;20:43:11 EDT 2005 i686 athlon i386 GNU/Linux\n&gt;&gt;\n&gt;&gt;-bash-3.00$ more /etc/issue\n&gt;&gt;Fedora Core release 3 (Heidelberg)\n&gt;&gt;Kernel &#92;r on an &#92;m\n&gt;&gt;\n&gt;&gt;Dual AMD Opteron(tm) Processor 246  w/ cpu MHz         : 2009.374 \n&gt; \n&gt; and \n&gt; \n&gt;&gt;cache size      : 1024 KB\n&gt;&gt;\n&gt;&gt;[crawling013 5] ~ &gt; /lib/libc.so.6\n&gt;&gt;GNU C Library stable release version 2.3.4 (20050218), by Roland \n&gt; \n&gt; McGrath \n&gt; \n&gt;&gt;et al.\n&gt;&gt;Copyright (C) 2005 Free Software Foundation, Inc.\n&gt;&gt;This is free software; see the source for copying conditions.\n&gt;&gt;There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A\n&gt;&gt;PARTICULAR PURPOSE.\n&gt;&gt;Configured for i586-suse-linux.\n&gt;&gt;Compiled by GNU CC version 3.3.5 20050117 (prerelease) (SUSE \n&gt; \n&gt; Linux).\n&gt; \n&gt;&gt;Compiled on a Linux 2.6.9 system on 2005-06-10.\n&gt;&gt;Available extensions:\n&gt;&gt;      GNU libio by Per Bothner\n&gt;&gt;      crypt add-on version 2.1 by Michael Glad and others\n&gt;&gt;      linuxthreads-0.10 by Xavier Leroy\n&gt;&gt;      GNU Libidn by Simon Josefsson\n&gt;&gt;      NoVersion patch for broken glibc 2.0 binaries\n&gt;&gt;      BIND-8.2.3-T5B\n&gt;&gt;      libthread_db work sponsored by Alpha Processor Inc\n&gt;&gt;      NIS(YP)/NIS+ NSS modules 0.19 by Thorsten Kukuk\n&gt;&gt;Thread-local storage support included.\n&gt;&gt;For bug reporting instructions, please see:\n&gt;&gt;&lt;http://www.gnu.org/software/libc/bugs.html&gt;.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;We used sun 1.5.0:\n&gt;&gt;\n&gt;&gt;-bash-3.00$ /usr/local/jdk1.5.0_03/bin/java -version\n&gt;&gt;java version &quot;1.5.0_03&quot;\n&gt;&gt;Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_03-\n&gt; \n&gt; b07)\n&gt; \n&gt;&gt;Java HotSpot(TM) Server VM (build 1.5.0_03-b07, mixed mode)\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;That to me that seems to be the deciding factor on whether we \n&gt; \n&gt; should\n&gt; \n&gt;&gt;&gt;start with 5 beefy machines and hope each one can go up to 100M \n&gt; \n&gt; links,\n&gt; \n&gt;&gt;&gt;or with 10 less beefy machines and each one can go up to 50M \n&gt; \n&gt; links\n&gt; \n&gt;&gt;&gt;without OOME.\n&gt;&gt;&gt;\n&gt;&gt;&gt;I know I&#39;m shooting darts in the dark now...I have to start the\n&gt;&gt;&gt;project planning soon so I&#39;d like to take my best guess with all \n&gt; \n&gt; the\n&gt; \n&gt;&gt;&gt;advices I can get.\n&gt;&gt;&gt;\n&gt;&gt;&gt;cheers,\n&gt;&gt;&gt;-joe\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;-----------------------------------------------------------------\n&gt; \n&gt; -------\n&gt; \n&gt;&gt;&gt;YAHOO! GROUPS LINKS\n&gt;&gt;&gt;\n&gt;&gt;&gt;    *  Visit your group &quot;archive-crawler\n&gt;&gt;&gt;      &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the \n&gt; \n&gt; web.\n&gt; \n&gt;&gt;&gt;       \n&gt;&gt;&gt;    *  To unsubscribe from this group, send an email to:\n&gt;&gt;&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;&gt;      &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?\n&gt; \n&gt; subject=Unsubscribe&gt;\n&gt; \n&gt;&gt;&gt;       \n&gt;&gt;&gt;    *  Your use of Yahoo! Groups is subject to the Yahoo! Terms \n&gt; \n&gt; of\n&gt; \n&gt;&gt;&gt;      Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;-----------------------------------------------------------------\n&gt; \n&gt; -------\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n\n\n"}}