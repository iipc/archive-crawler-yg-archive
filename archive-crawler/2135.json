{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":182123250,"authorName":"Kaisa Kaunonen","from":"&quot;Kaisa Kaunonen&quot; &lt;kaisa.kaunonen@...&gt;","profile":"kaisa_kaunonen","replyTo":"LIST","senderId":"3bzI4WvO2UENNLcCAmSgxE7YcA44P_gvcrhBaS15-PuGX2f9G4xk6Odl9WLibuCBGAsB3WkcfnFxmaxZRbqmJlr2i1ZJ0TTaHrM1S7uLmbUgFhoZ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: multimachine crawling","postDate":"1124713072","msgId":2135,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRlY2ZwZytidmlrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQzMDM4RjRELjcwODA3MDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":2118,"nextInTopic":2136,"prevInTime":2134,"nextInTime":2136,"topicId":2118,"numMessagesInTopic":12,"msgSnippet":"Hi, the question of distributed crawls looks very interesting. To participate in discussion, I have some general type comments on it. Do all machines in a","rawEmail":"Return-Path: &lt;kaisa.kaunonen@...&gt;\r\nX-Sender: kaisa.kaunonen@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 15873 invoked from network); 22 Aug 2005 12:18:52 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m15.grp.scd.yahoo.com with QMQP; 22 Aug 2005 12:18:52 -0000\r\nReceived: from unknown (HELO n6.bulk.dcn.yahoo.com) (216.155.201.59)\n  by mta1.grp.scd.yahoo.com with SMTP; 22 Aug 2005 12:18:51 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [216.155.201.64] by n6.bulk.dcn.yahoo.com with NNFMP; 22 Aug 2005 12:17:56 -0000\r\nReceived: from [66.218.66.59] by mailer1.bulk.dcn.yahoo.com with NNFMP; 22 Aug 2005 12:17:55 -0000\r\nReceived: from [66.218.66.84] by mailer8.bulk.scd.yahoo.com with NNFMP; 22 Aug 2005 12:17:55 -0000\r\nDate: Mon, 22 Aug 2005 12:17:52 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;decfpg+bvik@...&gt;\r\nIn-Reply-To: &lt;43038F4D.7080703@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 2132\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Kaisa Kaunonen&quot; &lt;kaisa.kaunonen@...&gt;\r\nSubject: Re: multimachine crawling\r\nX-Yahoo-Group-Post: member; u=182123250; y=ouZoZmnvy4_BLb2SIfentngAXBA2cOh-7VtjsIaZuainaGFjodkFAZ4\r\nX-Yahoo-Profile: kaisa_kaunonen\r\n\r\nHi,\nthe question of distributed crawls looks very interesting. To\nparticipate in discussion, I have some general type comments on it.\n\nDo all machines in a cluster use the same profile for a crawl? Each\nmachine may have different disk space/bandwidth/CPU so an identical\nprofile for everyone may not be practical. What about one\n`super&#39; profile with some common options set so that each\ncluster member can adjust the performance related options. How many\nprofiles should user write for a single distributed crawl?\n\nIt&#39;s not quite clear to me how one divides a large unknown net\nspace into non-overlapping partitions before crawling.\n\nIf you want to crawl whole *.com, you have 1) a large list of seeds\ndivided between machines and 2) each machine has instructions to\ncollect everything it meets in *.com starting from own seeds and\nexcluding those URLs which are in subdomains defined by seeds in other\nmachines (what else could you say?)\n\nThis definition still leaves many URLs which can be collected from\nseveral cluster members. Maybe one should think carefully about the\ndepth of each sub crawl. Less depth means less chance that some URLs\nare reached from more than one machine. \n\nWhat&#39;s the best way to divide seeds between machines: let Heritrix\ncompute and decide or let the user assign them to cluster members. How\nwell can you decide seed distribution before knowing anything about\nsites being crawled. Sometimes ten bad seeds cause more work and\ntrouble than thousand better behaving sites and addresses.\n\nWhat about reusing old profiles or jobs? Would it be possible to reuse\nthe information how the seeds were divided between machines on a\nprevious round? Or maybe it doesn&#39;t take much time to partition\nthe net/seeds again for a new crawl.\n\nBest, ..\n\n\n\n--- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; I&#39;ve added some notes on multimachine crawling to the wiki, here: \n&gt;\nhttp://crawler.archive.org/cgi-bin/wiki.pl?Multima\nchineCrawl#practical.  \n&gt; It&#39;d be great to hear people&#39;s feedback on how they think a &#39;hive\nof \n&gt; heritrices&#39; might coordinate to achieve a large-scale crawl.\n&gt; Yours,\n&gt; St.Ack\n\n\n\n\n"}}