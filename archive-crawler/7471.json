{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":468364830,"authorName":"Mahmoud A. Mubarak","from":"&quot;Mahmoud A. Mubarak&quot; &lt;mahmoud.mubarak@...&gt;","profile":"mahmoudmubarak@ymail.com","replyTo":"LIST","senderId":"cHu24945TNyF8xh8GHjvi2K0au1V5MIDA67IgzbKYEU0XlNmEX5l3-Wt41FXh1z7NMi2_l3ALckjpAytgBCT4FD46ntUgvEUcNTmoR6aE3-N7GhlZTsEJkY","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Problem with robots.txt IGNORE policy","postDate":"1324552833","msgId":7471,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpjdjNxMSs4OThmQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRDNjQ0NUQ5LjYwNzAxQGFyY2hpdmUub3JnPg=="},"prevInTopic":6677,"nextInTopic":7472,"prevInTime":7470,"nextInTime":7472,"topicId":6671,"numMessagesInTopic":9,"msgSnippet":"... I have set  the calculateRobotsOnly flag on PreconditionEnforcer and It worked. But, how can I ignore robots.txt for one or more seeds, not all of them? ","rawEmail":"Return-Path: &lt;mahmoud.mubarak@...&gt;\r\nX-Sender: mahmoud.mubarak@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 42730 invoked from network); 22 Dec 2011 11:20:49 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m1.grp.sp2.yahoo.com with QMQP; 22 Dec 2011 11:20:49 -0000\r\nX-Received: from unknown (HELO ng3-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.31)\n  by mta5.grp.sp2.yahoo.com with SMTP; 22 Dec 2011 11:20:49 -0000\r\nX-Received: from [98.137.0.82] by ng3.bullet.mail.gq1.yahoo.com with NNFMP; 22 Dec 2011 11:20:35 -0000\r\nX-Received: from [69.147.65.149] by tg2.bullet.mail.gq1.yahoo.com with NNFMP; 22 Dec 2011 11:20:35 -0000\r\nX-Received: from [98.137.34.72] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 22 Dec 2011 11:20:34 -0000\r\nDate: Thu, 22 Dec 2011 11:20:33 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jcv3q1+898f@...&gt;\r\nIn-Reply-To: &lt;4C6445D9.60701@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Mahmoud A. Mubarak&quot; &lt;mahmoud.mubarak@...&gt;\r\nSubject: Re: Problem with robots.txt IGNORE policy\r\nX-Yahoo-Group-Post: member; u=468364830; y=tsK2dLgxwR-Iz9vqggE2XDqCyVu3-BaIWKM4YwlGYWPL1YyzqQQh0sZ3oUzaV_s912TB\r\nX-Yahoo-Profile: mahmoudmubarak@...\r\n\r\n\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n\n=\r\n&gt; An option for simulating what you want without changing the current \n&gt; IG=\r\nNORE or adding a new policy would be to run your crawl with a CLASSIC \n&gt; ro=\r\nbots-respecting policy, but set the &#39;calculateRobotsOnly&#39; flag on \n&gt; Precon=\r\nditionEnforcer. Rather than canceling the fetching of URIs that \n&gt; are robo=\r\nts-precluded, this setting merely marks them up with an annotation.\n&gt; \n&gt; - =\r\nGordon @ IA\n&gt;\n\nI have set  the &#39;calculateRobotsOnly&#39; flag on PreconditionEn=\r\nforcer and It worked. But, how can I ignore robots.txt for one or more seed=\r\ns, not all of them? \n\nThanks in advance.\n\nMahmoud A. Mubarak\nBibliotheca Al=\r\nexandrina\n\n\n"}}