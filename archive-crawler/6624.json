{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"1VKUQqkQTDhrjF3a_-pdueR2AtJc4PkZWdnsrk73zkyioJBn2dgffKBn_I1O5oGX3916qTl1WIQpa6VIuCeorclq-OJWSBU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Crawling web search engine seed-list, url id","postDate":"1280356961","msgId":6624,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDNTBCMjYxLjcwMDA5MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGkycGYwNys0MHVrQGVHcm91cHMuY29tPg==","referencesHeader":"PGkycGYwNys0MHVrQGVHcm91cHMuY29tPg=="},"prevInTopic":6623,"nextInTopic":0,"prevInTime":6623,"nextInTime":6625,"topicId":6623,"numMessagesInTopic":2,"msgSnippet":"... I think you re saying: you want to issue a bunch of queries against a search engine, and use the results as seeds or to otherwise feed your crawl. Usually,","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 68575 invoked from network); 28 Jul 2010 22:42:46 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m14.grp.re1.yahoo.com with QMQP; 28 Jul 2010 22:42:46 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.re1.yahoo.com with SMTP; 28 Jul 2010 22:42:46 -0000\r\nX-Received: (qmail 94248 invoked from network); 28 Jul 2010 22:42:40 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 28 Jul 2010 22:42:40 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4C50B261.7000908@...&gt;\r\nDate: Wed, 28 Jul 2010 15:42:41 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.10) Gecko/20100512 Thunderbird/3.0.5\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;compa.marco&quot; &lt;compa.marco@...&gt;\r\nReferences: &lt;i2pf07+40uk@...&gt;\r\nIn-Reply-To: &lt;i2pf07+40uk@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Crawling web search engine seed-list, url id\r\nX-Yahoo-Group-Post: member; u=137285340; y=QvtiP91fdkwUbf0Kk48yzNjJdMqX1g12PqQQNe_skWxS\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 7/28/10 7:31 AM, compa.marco wrote:\n&gt; Hi, i am near to complete my seed list to crawl the entire italian web.\n&gt; I already inserted in the list some site directory like DMOZ.\n&gt; Can i insert in the seed list some web search engine using the query string that rappresent site:.it or there are some issues doing this??\n&gt; (i suppose i need the classic robots policy in the settings)\n&gt;\n&gt; I want to identify the URI crawled there is yet a way to doing this or i have to code this solution in a customized Processor??\n\nI think you&#39;re saying: you want to issue a bunch of queries against a \nsearch engine, and use the results as seeds or to otherwise feed your crawl.\n\nUsually, a search engine&#39;s terms of use or /robots.txt prohibit \nautomated scraping of results pages, except via some approved, limited \nmechanisms (like an official API).\n\nStill, it&#39;s a common practice, and perhaps you have the legal authority \nor permission of the search engine to do so. Still, you may want to \nseparate that step into a different crawl, possibly even with different \ntools, that&#39;s done first to expand your set of starting URLs. (Mixing \nthe special practices needed for this scraping with the rest of your \ncrawl may unnecessarily complicate things.)\n\n&gt; If i want to crawl all the image, pdf, doc, css and other resource on the web but only if they are in the scope of my search (.it) which filter should i set??\n\nThat&#39;s an easy task for the &#39;SURT&#39;-based scoping suppored by Heritrix \n1.X and Heritrix 3.X. If you add to your SURT-based scoping rules a \nprefix &quot;http://(it,&quot; then all .IT TLD URLs will be ruled-in.\n\nSee these Heritrix documentation or FAQ pages for more info:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/Crawl+Scope\nhttps://webarchive.jira.com/wiki/display/Heritrix/national+or+regional+domain+scope\n\n- Gordon @ IA\n\n"}}