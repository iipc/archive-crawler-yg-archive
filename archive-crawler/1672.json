{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (Internet Archive)","from":"&quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"uUwj5It7T9byvUH3vHOx-WWTe8l3PwN09EDB6vjatRzsDWMaoD9EAFWVC83zmo37uiQnSFBAOE6cfgd39JIXLGUz6iQB7xTy5iLephVSJ7d7V5_X8D2KSevP1Bc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Optimize URL scheduling?","postDate":"1111178948","msgId":1672,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyM0IzRUM0LjYwMjA1MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIwMDUwMzE3MTQzNC41MTgzMC5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4=","referencesHeader":"PDIwMDUwMzE3MTQzNC41MTgzMC5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4="},"prevInTopic":1671,"nextInTopic":0,"prevInTime":1671,"nextInTime":1673,"topicId":1666,"numMessagesInTopic":4,"msgSnippet":"... It s very odd that all ToeThreads would be in the same Postselector stage for so long. If you can catch this state again, it would be interesting to ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 78130 invoked from network); 18 Mar 2005 20:49:16 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m22.grp.scd.yahoo.com with QMQP; 18 Mar 2005 20:49:16 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta6.grp.scd.yahoo.com with SMTP; 18 Mar 2005 20:49:15 -0000\r\nReceived: (qmail 11707 invoked by uid 100); 18 Mar 2005 20:31:36 -0000\r\nReceived: from b116-dyn-239.archive.org (HELO ?207.241.238.239?) (gojomo@...@207.241.238.239)\n  by mail-dev.archive.org with SMTP; 18 Mar 2005 20:31:36 -0000\r\nMessage-ID: &lt;423B3EC4.6020507@...&gt;\r\nDate: Fri, 18 Mar 2005 12:49:08 -0800\r\nUser-Agent: Mozilla Thunderbird 1.0 (X11/20041206)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;200503171434.51830.ck-heritrix@...&gt;\r\nIn-Reply-To: &lt;200503171434.51830.ck-heritrix@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.2 required=6.0 tests=AWL autolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: &quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Optimize URL scheduling?\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nChristian Kohlschuetter wrote:\n&gt; Dear all,\n&gt; \n&gt; during my ODP-centric broad crawls (using Heritrix CVS HEAD with my \n&gt; BucketQueueAssignmentPolicy), I just noticed that - very often - all 50 \n&gt; ToeThreads seem to stay in the Postselector processor for a long time (40s in \n&gt; some cases), resulting in a very slow throughput of avg. 14 docs/sec \n&gt; (currently 1214032 of 3378892 discovered pages were crawled).\n\nIt&#39;s very odd that all ToeThreads would be in the same Postselector stage\nfor so long. If you can catch this state again, it would be interesting to\ncollect:\n\n  - a threads report dump, to know what URIs they&#39;re working on\n  - a few Java thread dumps, a few seconds apart, to see which if any\n    ToeThreads are making any progress\n  - &#39;top&#39; info to see CPU utilization/states\n\nIn particular, I wonder if when you see this, you&#39;re hitting any\npages with a gigantic nubmer (hundreds of thousands? millions?) of\noutlinks.\n\n&gt; I guess that it is simply caused by the already-seen test and the URI enqueue \n&gt; operation, which are both called in the Postselector class (requiring \n&gt; synchronized access to the underlying Bdb database).\n\nThe BDB databases are supposed to only require synchronization over\nlocal key regions, and different threads should (generally) be\nintensive on disjoint regions of the database -- but it could be the case\nthat our usage patterns involve more contention than is necessary, or\nalways have to synchronize through the btree root node. A closer look\nwould probably highlight areas for improvement.\n\n&gt; As far as I understand, the crawling cycle simply processes all chains \n&gt; (pre-processors, fetchers, extractors etc.) in line. This means that any \n&gt; congestion in the post-processing stage will unavoidably delay the processing \n&gt; of succeeding entries in the queue of scheduled URLs (&quot;ready-queue&quot;). \n&gt; Increasing the number of ToeThreads would make things even worse because many \n&gt; more already-seen/enqueue requests would then have to be synchronized with \n&gt; the underlying bdb database.\n&gt; \n&gt; I think, decoupling the crawling and link-enqueueing stages (= making them run \n&gt; in parallel), could reduce the overall delay and improve crawling \n&gt; performance: Couldn&#39;t the Postselector simply enqueue all extracted Links \n&gt; (including duplicates -- ie. no deduping overhead) to a Thread-Local, \n&gt; disk-backed queue (&quot;postprocess-queue&quot;)? Another Thread/pool of Threads would \n&gt; then dequeue items from that queue and try to schedule them (doing the \n&gt; already-seen test and enqueue them into the ready-queue).\n&gt; \n&gt; The Postselector would almost instantly finish and let the &quot;crawling&quot; \n&gt; ToeThread pass on to the next scheduled URL. On the other side, the queue of \n&gt; extracted links will grow very fast, but that should not render a problem (it \n&gt; is disk-based). And at some point, if that queue becomes very long, we can \n&gt; still pause crawling new pages until the postprocess-queue&#39;s size reaches a \n&gt; low-water threshold again.\n&gt; \n&gt; Does that make sense?\n\nTo some extent, yes.\n\nThe old HostQueuesFrontier used a similar, but not identical strategy, to\nminimize the entries into frontier-synchronized code. (See batchSchedule().)\nHowever, upon &#39;finish&#39; of a URI, all pending discovered URIs were integrated\nto the frontier, in the same ToeThread (See batchFlush().)\n\nIt has been important to do it this way because the discovered URIs often need\nto be considered for the crawl to continue properly. For example, a required\nDNS lookup or robots.txt fetch is treated the same as a &#39;discovered&#39; URI, and\nmust be scheduled before all other URIs on the same host -- or else the other\ntop  URIs in that queue will be tried prematurely, possibly reaching their\nretry limit and/or triggering redundant DNS/robots schedulings.\n\nSo the ideal may be queue-centric synchronization, with inserts and and\ndequeues on any queue not blocking others, plus the added possibility of\ndeferring/batching queue-synchronized actions.\n\nThe BdbFrontier is close to queue-centric synchronization, though\nwithout any real batching to minimize synchronize operations. A closer\nlook could probably reduce contention further, without yet resorting\nto additional thread-local or per-host holding queues, or extra threads.\n\n- Gordon @ IA\n\n\n\n\n\n\n"}}