{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","replyTo":"LIST","senderId":"GsIODYtH6DHuXoCadE1cr7pl5HR_Ub0MjY4nLjASTrXtACQ6x2WVmSWt1xtlTn8HbS6cHSLjUmjgXiSBdkNWLg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Out-of-the-box defaults?","postDate":"1088642672","msgId":561,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwRTM1RTcwLjgwNDA2MDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQwRDg5OTBGLjgwOTAyMDRAbG9jLmdvdj4=","referencesHeader":"PDQwQzhEN0ZBLjUwNTAyQGxvYy5nb3Y+IDw0MEM4RjZFMS45MDlAYXJjaGl2ZS5vcmc+IDw0MEQ4OTkwRi44MDkwMjA0QGxvYy5nb3Y+"},"prevInTopic":550,"nextInTopic":0,"prevInTime":560,"nextInTime":562,"topicId":528,"numMessagesInTopic":4,"msgSnippet":"Good man Andy.  Igor and I reviewed your patch and applied it along with your suggestion that by default, the max heap size should be set to 256megs (See ","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 84665 invoked from network); 1 Jul 2004 00:51:21 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m7.grp.scd.yahoo.com with QMQP; 1 Jul 2004 00:51:21 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 1 Jul 2004 00:51:21 -0000\r\nReceived: (qmail 25020 invoked by uid 100); 1 Jul 2004 00:42:32 -0000\r\nReceived: from unknown (HELO ?209.237.240.13?) (stack@...@209.237.240.13)\n  by mail-dev.archive.org with SMTP; 1 Jul 2004 00:42:32 -0000\r\nMessage-ID: &lt;40E35E70.8040600@...&gt;\r\nDate: Wed, 30 Jun 2004 17:44:32 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7) Gecko/20040624 Debian/1.7-2\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;40C8D7FA.50502@...&gt; &lt;40C8F6E1.909@...&gt; &lt;40D8990F.8090204@...&gt;\r\nIn-Reply-To: &lt;40D8990F.8090204@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.4 required=6.0 tests=AWL,HTML_MESSAGE autolearn=ham \n\tversion=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Out-of-the-box defaults?\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\nGood man Andy.  Igor and I reviewed your patch and applied it along with \nyour suggestion that by default, the max heap size should be set to \n256megs (See \nhttps://sourceforge.net/tracker/index.php?func=detail&aid=983109&group_id=73833&atid=539102).\nThanks,\nSt.Ack\n\nAndy Boyko wrote:\n\n&gt;I&#39;m belatedly responding to Igor&#39;s thoughtful response about what the \n&gt;out-of-the-box configuration should be like.  Based on his comments, \n&gt;I&#39;ve got a patch to the initial profile&#39;s order.xml (as of Heritrix \n&gt;0.10.0) which makes the following changes:\n&gt;\n&gt;  - enables the CSS and SWF extractors\n&gt;  - adds the PathDepth filter and PathologicalPath filters to the\n&gt;    exclude filter of the scope, with the default depth of 20 and\n&gt;    default path repetitions of 3.\n&gt;  - enables recheck-scope in the Preselector, which was necessary\n&gt;    to support the above filters correctly\n&gt;\n&gt;Should this patch be applied to the initial profile?  I&#39;d like the \n&gt;extractors turned on, and am agnostic about the filters (but note that \n&gt;it&#39;s tricky to get those filters configured so they actually work, so if \n&gt;it&#39;s not on in the default, it needs documenting).\n&gt;\n&gt;Note that there are a few other changes in there, as the profile was \n&gt;edited by the Web UI, and so it reflects the values written by the code. \n&gt;  Nothing significant except for the addition of what I guess are a few \n&gt;new configurations.\n&gt;\n&gt;Regards,\n&gt;Andy Boyko  aboy@...\n&gt;Library of Congress\n&gt;\n&gt;\n&gt;Igor Ranitovic wrote:\n&gt;  \n&gt;\n&gt;&gt;&gt;- Invocation: it seems pretty clear that crawls are safest with some \n&gt;&gt;&gt;memory headroom.  Can the bin/heritrix script default JAVA_OPTS to \n&gt;&gt;&gt;&quot;-Xmx256m&quot; if it&#39;s not otherwise set?\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;Sure. Maybe we should adopt IBM&#39;s JVM approach where default size of the heap is a half of a \n&gt;&gt;system&#39;s physical memory.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;- Extractors: how experimental are the non-enabled variants?  I recall \n&gt;&gt;&gt;that in February, we were encouraged to use HTML2 and CSS, but that some \n&gt;&gt;&gt;leaked memory (PDF?  DOC?  SWF?).  Is it safe to change the defaults to, \n&gt;&gt;&gt;at the least, include CSS?  Of the potentially leaky ones, SWF seems the \n&gt;&gt;&gt;most compelling for enabling by default.\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;We should have HTTP, HTML, CSS, JS and SWF be defualt extractors.\n&gt;&gt;We changed SWF extractor to use memory more efficiently and I have not have memory problems with it \n&gt;&gt;since.\n&gt;&gt;PDF and DOC extractors are still problematic. I have been working on a new, more memory efficient \n&gt;&gt;PDF extractor but is not ready yet. DOC parsing is a problem since DOCs cannot be parsed by treating \n&gt;&gt;them as randomly accesable streams. Beacause of this it is necessary to load entire DOCs into memory \n&gt;&gt;in order to parse them.\n&gt;&gt;HTML2 extractor(horrible name btw) is making two passes on javascript code. One pass examains all \n&gt;&gt;strings in javascript code and second pass parses javascript code as html. HTML extractor is making \n&gt;&gt;only the fisrt pass. I believe that HTML extracotr got better and that there is no need of HTML2 \n&gt;&gt;anymore.\n&gt;&gt;I will have to do some comparison to confirm this.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;- Filters: If I understand the &quot;recheck-scope&quot; setting on the \n&gt;&gt;&gt;preselector, it needs to be set for scope changes during the crawl to be \n&gt;&gt;&gt;detected and honored.  Assuming it doesn&#39;t affect performance too much, \n&gt;&gt;&gt;can it default to on?\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;I have no preferences on this one. Though, it seems right as is.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;Is it worthwhile to enable by default either PathDepth or \n&gt;&gt;&gt;PathologicalPath filters, presumably with generous but non-infinite values?\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;I agree. I usually set PathDepth to 20 and PathologicalPath to max of 3 repetitions of a pattern.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;- Politeness: I know per-host bandwidth usage got moved to the &quot;expert&quot; \n&gt;&gt;&gt;section, but it might be good to default the per-host cap to something \n&gt;&gt;&gt;maybe T1-like (thus, perhaps 150KBps or so) to avoid pummelling sites \n&gt;&gt;&gt;with large files, when crawling from a large pipe.  (Is the per-host cap \n&gt;&gt;&gt;like the total-bandwidth cap, in that it doesn&#39;t actually constrain \n&gt;&gt;&gt;instantaneous traffic, only the average?)\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;I am not sure if need to do this. It seems that default values of dynamic politeness will \n&gt;&gt;significantly delay request to sites with large files when crawling from a large pipe.\n&gt;&gt;During fetching we just might be OK by relying on TCP&#39;s congestion control and not worry about \n&gt;&gt;solely saturating the sites&#39; bandwidth.\n&gt;&gt;\n&gt;&gt;Take care.\n&gt;&gt;i.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;  \n&gt;\n&gt;------------------------------------------------------------------------\n&gt;\n&gt;--- Simple/order.xml\t2004-06-04 18:45:39.000000000 -0400\n&gt;+++ Simple-filtered/order.xml\t2004-06-22 16:03:35.213161040 -0400\n&gt;@@ -4,17 +4,17 @@\n&gt;     &lt;name&gt;Simple&lt;/name&gt;\n&gt;     &lt;description&gt;Profile: Simple crawl&lt;/description&gt;\n&gt;     &lt;operator&gt;Admin&lt;/operator&gt;\n&gt;-    &lt;organization /&gt;\n&gt;-    &lt;audience /&gt;\n&gt;-    &lt;date&gt;20040409202922&lt;/date&gt;\n&gt;+    &lt;organization/&gt;\n&gt;+    &lt;audience/&gt;\n&gt;+    &lt;date&gt;20040622200205&lt;/date&gt;\n&gt;   &lt;/meta&gt;\n&gt;   &lt;controller&gt;\n&gt;     &lt;string name=&quot;settings-directory&quot;&gt;settings&lt;/string&gt;\n&gt;-    &lt;string name=&quot;disk-path&quot;&gt;&lt;/string&gt;\n&gt;-    &lt;string name=&quot;scratch-path&quot;&gt;scratch&lt;/string&gt;\n&gt;-    &lt;string name=&quot;state-path&quot;&gt;state&lt;/string&gt;\n&gt;+    &lt;string name=&quot;disk-path&quot;/&gt;\n&gt;     &lt;string name=&quot;logs-path&quot;&gt;logs&lt;/string&gt;\n&gt;     &lt;string name=&quot;checkpoints-path&quot;&gt;checkpoints&lt;/string&gt;\n&gt;+    &lt;string name=&quot;state-path&quot;&gt;state&lt;/string&gt;\n&gt;+    &lt;string name=&quot;scratch-path&quot;&gt;scratch&lt;/string&gt;\n&gt;     &lt;long name=&quot;max-bytes-download&quot;&gt;0&lt;/long&gt;\n&gt;     &lt;long name=&quot;max-document-download&quot;&gt;0&lt;/long&gt;\n&gt;     &lt;long name=&quot;max-time-sec&quot;&gt;0&lt;/long&gt;\n&gt;@@ -28,6 +28,15 @@\n&gt;         &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;         &lt;boolean name=&quot;if-matches-return&quot;&gt;true&lt;/boolean&gt;\n&gt;         &lt;map name=&quot;filters&quot;&gt;\n&gt;+          &lt;newObject name=&quot;pathdepth&quot; class=&quot;org.archive.crawler.filter.PathDepthFilter&quot;&gt;\n&gt;+            &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;+            &lt;integer name=&quot;max-path-depth&quot;&gt;20&lt;/integer&gt;\n&gt;+            &lt;boolean name=&quot;path-less-or-equal-return&quot;&gt;false&lt;/boolean&gt;\n&gt;+          &lt;/newObject&gt;\n&gt;+          &lt;newObject name=&quot;pathologicalpath&quot; class=&quot;org.archive.crawler.filter.PathologicalPathFilter&quot;&gt;\n&gt;+            &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;+            &lt;integer name=&quot;repetitions&quot;&gt;3&lt;/integer&gt;\n&gt;+          &lt;/newObject&gt;\n&gt;         &lt;/map&gt;\n&gt;       &lt;/newObject&gt;\n&gt;       &lt;newObject name=&quot;additionalScopeFocus&quot; class=&quot;org.archive.crawler.filter.FilePatternFilter&quot;&gt;\n&gt;@@ -61,15 +70,18 @@\n&gt;       &lt;integer name=&quot;min-interval-ms&quot;&gt;1000&lt;/integer&gt;\n&gt;       &lt;integer name=&quot;max-retries&quot;&gt;30&lt;/integer&gt;\n&gt;       &lt;long name=&quot;retry-delay-seconds&quot;&gt;900&lt;/long&gt;\n&gt;+      &lt;boolean name=&quot;hold-queues&quot;&gt;false&lt;/boolean&gt;\n&gt;+      &lt;integer name=&quot;host-valence&quot;&gt;1&lt;/integer&gt;\n&gt;       &lt;integer name=&quot;total-bandwidth-usage-KB-sec&quot;&gt;0&lt;/integer&gt;\n&gt;       &lt;integer name=&quot;max-per-host-bandwidth-usage-KB-sec&quot;&gt;0&lt;/integer&gt;\n&gt;+      &lt;integer name=&quot;host-queues-memory-capacity&quot;&gt;200&lt;/integer&gt;\n&gt;     &lt;/newObject&gt;\n&gt;     &lt;map name=&quot;pre-fetch-processors&quot;&gt;\n&gt;       &lt;newObject name=&quot;Preselector&quot; class=&quot;org.archive.crawler.prefetch.Preselector&quot;&gt;\n&gt;         &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;         &lt;map name=&quot;filters&quot;&gt;\n&gt;         &lt;/map&gt;\n&gt;-        &lt;boolean name=&quot;recheck-scope&quot;&gt;false&lt;/boolean&gt;\n&gt;+        &lt;boolean name=&quot;recheck-scope&quot;&gt;true&lt;/boolean&gt;\n&gt;         &lt;boolean name=&quot;block-all&quot;&gt;false&lt;/boolean&gt;\n&gt;         &lt;string name=&quot;block-by-regexp&quot;/&gt;\n&gt;       &lt;/newObject&gt;\n&gt;@@ -112,11 +124,21 @@\n&gt;         &lt;map name=&quot;filters&quot;&gt;\n&gt;         &lt;/map&gt;\n&gt;       &lt;/newObject&gt;\n&gt;+      &lt;newObject name=&quot;ExtractorCSS&quot; class=&quot;org.archive.crawler.extractor.ExtractorCSS&quot;&gt;\n&gt;+        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;+        &lt;map name=&quot;filters&quot;&gt;\n&gt;+        &lt;/map&gt;\n&gt;+      &lt;/newObject&gt;\n&gt;       &lt;newObject name=&quot;ExtractorJS&quot; class=&quot;org.archive.crawler.extractor.ExtractorJS&quot;&gt;\n&gt;         &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;         &lt;map name=&quot;filters&quot;&gt;\n&gt;         &lt;/map&gt;\n&gt;       &lt;/newObject&gt;\n&gt;+      &lt;newObject name=&quot;ExtractorSWF&quot; class=&quot;org.archive.crawler.extractor.ExtractorSWF&quot;&gt;\n&gt;+        &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt;+        &lt;map name=&quot;filters&quot;&gt;\n&gt;+        &lt;/map&gt;\n&gt;+      &lt;/newObject&gt;\n&gt;     &lt;/map&gt;\n&gt;     &lt;map name=&quot;write-processors&quot;&gt;\n&gt;       &lt;newObject name=&quot;Archiver&quot; class=&quot;org.archive.crawler.writer.ARCWriterProcessor&quot;&gt;\n&gt;  \n&gt;\n\n\n"}}