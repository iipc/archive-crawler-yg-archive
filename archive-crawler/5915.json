{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":140025715,"authorName":"progre55","from":"&quot;progre55&quot; &lt;ikromchik@...&gt;","profile":"progre55","replyTo":"LIST","senderId":"pWSwIuryYBTmiXhORWW6CyZ5Xfq677iOxGWetfJ3mpIbtTK0pwQymPnr5i9_gNT5mXTXsBZHkh03VYaEPptrzC3AEm0Vzz0","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Crawling RSS links","postDate":"1246866877","msgId":5915,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGgyc2FqdCtwaWxmQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":5914,"nextInTime":5916,"topicId":5915,"numMessagesInTopic":1,"msgSnippet":"Hi people! I need to write a crawler that could find all the RSS links on a domain and store them in a database. However, I wouldnt like to crawl whole domains","rawEmail":"Return-Path: &lt;ikromchik@...&gt;\r\nX-Sender: ikromchik@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 52905 invoked from network); 6 Jul 2009 07:54:57 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m4.grp.re1.yahoo.com with QMQP; 6 Jul 2009 07:54:57 -0000\r\nX-Received: from unknown (HELO n42b.bullet.mail.sp1.yahoo.com) (66.163.168.156)\n  by mta3.grp.sp2.yahoo.com with SMTP; 6 Jul 2009 07:54:57 -0000\r\nX-Received: from [69.147.65.173] by n42.bullet.mail.sp1.yahoo.com with NNFMP; 06 Jul 2009 07:54:38 -0000\r\nX-Received: from [98.137.34.72] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 06 Jul 2009 07:54:38 -0000\r\nDate: Mon, 06 Jul 2009 07:54:37 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h2sajt+pilf@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;progre55&quot; &lt;ikromchik@...&gt;\r\nSubject: Crawling RSS links\r\nX-Yahoo-Group-Post: member; u=140025715; y=Y0Nn-d75npMLWTLbZVMSzfThZaQu0tmD--tzC-9MrfcC-4YQ\r\nX-Yahoo-Profile: progre55\r\n\r\nHi people!\n\nI need to write a crawler that could find all the RSS links on =\r\na domain and store them in a database. However, I wouldnt like to crawl who=\r\nle domains by downloading the whole site content and then trashing the rest=\r\n, just to collect several links.\n\nAs far as I get it, I would use several l=\r\nink extractors, and on the writer part, would have a custom writer that wou=\r\nld write the link to a database depending on its mimeType and ending (xml o=\r\nr rss)?\n\nAny other suggestions on how I could use the crawler effectively t=\r\no achieve this, please?\n\nAppreciate!\n\nIkrom\n\n\n"}}