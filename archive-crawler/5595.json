{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"KaX8NU9-PlwJbE2fGnwp-JUlo-M3TxfJbC6szK9ORQo9d1u5wJKGr8poKAg-T4yG_mRAlWi6viVtcQXUK61RoG8aRaJEnUM","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] 74 millions docs - Out Of Memory","postDate":"1228437009","msgId":5595,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5Mzg3NjExLjcwNzA1QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGdoNzJjcis4M3U3QGVHcm91cHMuY29tPg==","referencesHeader":"PGdoNzJjcis4M3U3QGVHcm91cHMuY29tPg=="},"prevInTopic":5594,"nextInTopic":0,"prevInTime":5594,"nextInTime":5596,"topicId":5593,"numMessagesInTopic":3,"msgSnippet":"Re: java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE As Noah notes, this is a known issue which should be fixed in a future 2.x release. In","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 7952 invoked from network); 5 Dec 2008 00:30:10 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m53.grp.scd.yahoo.com with QMQP; 5 Dec 2008 00:30:10 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta17.grp.scd.yahoo.com with SMTP; 5 Dec 2008 00:30:10 -0000\r\nX-Received: (qmail 29508 invoked from network); 5 Dec 2008 00:30:09 -0000\r\nX-Received: from 67.170.220.186 (HELO ?192.168.1.11?) (67.170.220.186)\n  by relay02.pair.com with SMTP; 5 Dec 2008 00:30:09 -0000\r\nX-pair-Authenticated: 67.170.220.186\r\nMessage-ID: &lt;49387611.70705@...&gt;\r\nDate: Thu, 04 Dec 2008 16:30:09 -0800\r\nUser-Agent: Thunderbird 2.0.0.18 (Windows/20081105)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;gh72cr+83u7@...&gt;\r\nIn-Reply-To: &lt;gh72cr+83u7@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] 74 millions docs - Out Of Memory\r\nX-Yahoo-Group-Post: member; u=137285340; y=EO2A0x0THlcu8ewBiF74wYg8wXj6U6MjMgeMf1E6nIAC\r\nX-Yahoo-Profile: gojomo\r\n\r\nRe: java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n\nAs Noah notes, this is a known issue which should be fixed in a future \n2.x release. In the meantime, its effects should be limited to \npreventing &gt;2GB HTML documents from having their links discovered.\n\nFor comparison, some other broad crawlers have a policy of not even \nlooking for links past the first ~100KB of a resource. If upon \nexamination the affected pages have crucial outlinks you need to \ndiscover, we could try backporting the 2.x fix or some other workaround \nto a future 1.14.x patch release.\n\nI am a little confused by the combination of the stack trace you \nforwarded (which was in ExtractorHTML) and the example URL you gave \n(http://dermoporadkyne.cz/poradenstvi.pdf). The ExtractorPDF uses a \ndifferent link-extraction method that should be unaffected by the 2GB \nmapping limit -- so I would *not* expect a similar exception for a PDF \nfile.... unless perhaps it misidentifies itself as HTML?\n\nIn our crawls we usually set some max-size limit for individual \nresources in FetchHTTP (such as 100MB, 700MB, or 1GB) -- so content will \nhave already been truncated before link-extraction occurs.\n\nRe: OutOfMemoryErrors\n\nThese are a concern because I don&#39;t know any outstanding bugs which \ncould account for them, given your apparent configuration.\n\nWhen an OOME happens, we pause the crawl in the hope some interaction \nand info-extraction will be possible (via the web UI or other tools). \nHowever, since the OOME may have occurred anywhere, crucial in-memory \nstructures may be corrupt, and even the &#39;pause&#39; may not have completely \ncleanly. Thus the current crawl-launch may be unstable/unresumable. (I \ndiscuss some other options for continuing the crawl below.)\n\nTo get a better idea of why the OOME happened, it could help to see more \nof the &quot;Serious error&quot; stacks. Feel free to send them to me off-list, if \nyou&#39;d like.\n\nAlso, the &#39;jmap&#39; tool, especially as &#39;jmap -histo JVM_PID&#39;, shows the \ndistribution of live objects, and so might suggest what&#39;s gone wrong by \nwhat objects are overrepresented (compared to other runs where the crawl \nprogresses indefinitely). So if the JVM is still alive, please forward \nthe first ~30 lines of &#39;jmap -histo&#39; output.\n\nTo try continuing the crawl, the two main options are: (1) resuming from \na checkpoint; or (2) launching a new crawl which is initialized with the \nprevious crawl&#39;s &#39;recovery log&#39;. These are discussed a bit in the \nHeritrix User Manual:\n\nhttp://crawler.archive.org/articles/user_manual/outside.html#recover\n\nYou *might* be able to make a valid checkpoint after the OOMEs, if the \ncrawler has paused cleanly -- but as noted above, after any OOME all \nbets are off as the the stability of current in-memory structures.\n\nYou may have already been making regular checkpoints; if so you can \nresume from one of the older ones, and the crawl should essentially \nproceed from the moment of the checkpoint. That might mean some number \nof URIs are repeat-crawled.\n\nUsing the recovery log is really launching an all-new crawl, but \npreinitializing the crawler&#39;s &#39;already-seen&#39; set and pending queues \nbased on two passes over the previous crawl&#39;s recovery log. As a result \nyou have a reasonable simulation of the first crawl&#39;s queues/seen-set at \nthe time the recovery log ended. (Ordering may be somewhat different; \nstate other than the queues/seen-set is not restored; richer URI state \nlike &#39;source-tagging&#39; is lost.)\n\nHope this helps,\n\n- Gordon @ IA\n\ngoblin_cz wrote:\n&gt; Hi,\n&gt; \n&gt; the Czech crawl again :) . I started with default profile and set some\n&gt; specific rules (100MB limit etc.) and run the crawl again. You can\n&gt; find the order.xml here:\n&gt; \n&gt; http://raptor.webarchiv.cz/heritrix/order.xml\n&gt; \n&gt; Tech spec:\n&gt; Heritrix 1.14.2\n&gt; 8 core Xeon\n&gt; 8GB RAM\n&gt; 64bit sun java\n&gt; 3GB java heap\n&gt; Debian 4.1\n&gt; \n&gt; Everything goes without any serious trouble. The only exception that\n&gt; was thrown was:\n&gt; \n&gt; java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n&gt; Stacktrace: java.lang.IllegalArgumentException: Size exceeds\n&gt; Integer.MAX_VALUE\n&gt; \tat sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:707)\n&gt; \tat\n&gt; org.archive.io.GenericReplayCharSequence.getReadOnlyMemoryMappedBuffer(GenericReplayCharSequence.java:277)\n&gt; \tat\n&gt; org.archive.io.GenericReplayCharSequence.decodeToFile(GenericReplayCharSequence.java:219)\n&gt; \tat\n&gt; org.archive.io.GenericReplayCharSequence.(GenericReplayCharSequence.java:164)\n&gt; \tat\n&gt; org.archive.io.RecordingOutputStream.getReplayCharSequence(RecordingOutputStream.java:559)\n&gt; \tat\n&gt; org.archive.io.RecordingOutputStream.getReplayCharSequence(RecordingOutputStream.java:515)\n&gt; \tat\n&gt; org.archive.io.RecordingInputStream.getReplayCharSequence(RecordingInputStream.java:314)\n&gt; \tat\n&gt; org.archive.util.HttpRecorder.getReplayCharSequence(HttpRecorder.java:295)\n&gt; \tat\n&gt; org.archive.crawler.extractor.ExtractorHTML.extract(ExtractorHTML.java:540)\n&gt; \tat\n&gt; org.archive.crawler.extractor.Extractor.innerProcess(Extractor.java:67)\n&gt; \tat org.archive.crawler.framework.Processor.process(Processor.java:112)\n&gt; \tat\n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:302)\n&gt; \tat org.archive.crawler.framework.ToeThread.run(ToeThread.java:151)\n&gt; \n&gt; This happened on few pages ï¿½ for example\n&gt; http://dermoporadkyne.cz/poradenstvi.pdf and I guess it is caused by\n&gt; never ending scripts.\n&gt; \n&gt; After six days of crawling (I had 74 millions of documents and 4 TB of\n&gt; data) happened something much more serious. Heritrix threw about 150\n&gt; exceptions and paused itself.\n&gt; All exceptions look like this:\n&gt; Serious error occured trying to process &#39;CrawlURI\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543\n&gt; LLL\n&gt; http://www.sperky.vltava.cz/DANFIL/vyrobce=1001130574/?category-id=16706\n&gt; in Scheduler&#39;\n&gt; [ToeThread #132:\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543\n&gt;  CrawlURI\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543\n&gt; LLL\n&gt; http://www.sperky.vltava.cz/DANFIL/vyrobce=1001130574/?category-id=16706\n&gt;    0 attempts\n&gt;     in processor: Scheduler\n&gt;     ACTIVE for 1h44m26s863ms\n&gt;     step: ABOUT_TO_BEGIN_PROCESSOR for 1h44m5s663ms\n&gt;     java.lang.Thread.getStackTrace(Thread.java:1436)\n&gt;     org.archive.crawler.framework.ToeThread.reportTo(ToeThread.java:514)\n&gt;     org.archive.crawler.framework.ToeThread.reportTo(ToeThread.java:592)\n&gt;     org.archive.util.DevUtils.extraInfo(DevUtils.java:65)\n&gt;    \n&gt; org.archive.crawler.framework.ToeThread.seriousError(ToeThread.java:230)\n&gt;    \n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:325)\n&gt;     org.archive.crawler.framework.ToeThread.run(ToeThread.java:151)\n&gt; ]\n&gt;            timestamp  discovered      queued   downloaded      \n&gt; doc/s(avg)  KB/s(avg)   dl-failures   busy-thread   mem-use-KB \n&gt; heap-size-KB   congestion   max-depth   avg-depth\n&gt; 2008-11-30T19:49:37Z   226231290   147999609     74002484        \n&gt; 0(121.9)    0(6828)        293362           197      2339025      \n&gt; 2831488     7,213.49      320897         104\n&gt;  (in thread &#39;ToeThread #132:\n&gt; http://www.sperky.vltava.cz/produkt-sperky.esp?product-id=860134354&seo-hint:product-name=Zlat%C4%82%CB%9D%20prsten%20Danfil%20DF1565&category-id=17543&#39;;\n&gt; in processor &#39;Scheduler&#39;)\n&gt; \n&gt; java.lang.OutOfMemoryError: Java heap space\n&gt; Stacktrace: java.lang.OutOfMemoryError: Java heap space\n&gt; \tat java.lang.Class.getDeclaredFields0(Native Method)\n&gt; \tat java.lang.Class.privateGetDeclaredFields(Class.java:2291)\n&gt; \tat java.lang.Class.getDeclaredField(Class.java:1880)\n&gt; \tat java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1610)\n&gt; \tat java.io.ObjectStreamClass.access$700(ObjectStreamClass.java:52)\n&gt; \tat java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:425)\n&gt; \tat java.security.AccessController.doPrivileged(Native Method)\n&gt; \tat java.io.ObjectStreamClass.(ObjectStreamClass.java:413)\n&gt; \tat java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:310)\n&gt; \tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1106)\n&gt; \tat\n&gt; java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1509)\n&gt; \tat\n&gt; java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1474)\n&gt; \tat\n&gt; java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1392)\n&gt; \tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1150)\n&gt; \tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326)\n&gt; \tat\n&gt; com.sleepycat.bind.serial.SerialBinding.objectToEntry(SerialBinding.java:171)\n&gt; \tat com.sleepycat.collections.DataView.useValue(DataView.java:548)\n&gt; \tat com.sleepycat.collections.DataCursor.initForPut(DataCursor.java:824)\n&gt; \tat com.sleepycat.collections.DataCursor.put(DataCursor.java:758)\n&gt; \tat\n&gt; com.sleepycat.collections.StoredContainer.putKeyValue(StoredContainer.java:319)\n&gt; \tat com.sleepycat.collections.StoredMap.put(StoredMap.java:257)\n&gt; \tat org.archive.util.CachedBdbMap.expungeStaleEntry(CachedBdbMap.java:562)\n&gt; \tat\n&gt; org.archive.util.CachedBdbMap.expungeStaleEntries(CachedBdbMap.java:533)\n&gt; \tat org.archive.util.CachedBdbMap.get(CachedBdbMap.java:358)\n&gt; \tat\n&gt; org.archive.crawler.datamodel.ServerCache.getHostFor(ServerCache.java:146)\n&gt; \tat\n&gt; org.archive.crawler.datamodel.ServerCache.getHostFor(ServerCache.java:175)\n&gt; \tat\n&gt; org.archive.crawler.framework.WriterPoolProcessor.getHostAddress(WriterPoolProcessor.java:344)\n&gt; \tat\n&gt; org.archive.crawler.writer.ARCWriterProcessor.innerProcess(ARCWriterProcessor.java:132)\n&gt; \tat org.archive.crawler.framework.Processor.process(Processor.java:112)\n&gt; \tat\n&gt; org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:302)\n&gt; \tat org.archive.crawler.framework.ToeThread.run(ToeThread.java:151)\n&gt; \n&gt; When I resume the crawl, only new exceptions are thrown and the crawl\n&gt; is getting back to the paused state.\n&gt; I&#39;ve searched and browsed this mailing list and there are a lot of\n&gt; topics about OOM, but I am not 100% sure, which one can be exactly the\n&gt; same situation and actual.\n&gt; \n&gt; Can I modify the crawl to be able resume it? \n&gt; If not, how to crawl it again without storing already downloaded data?\n&gt; (checkpoint and recovery?)\n&gt; What can I do to avoid this situation?\n&gt; \n&gt; Thank you,\n&gt; \n&gt; Best regards\n&gt; \n&gt; Adam\n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}