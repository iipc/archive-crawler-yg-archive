{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","replyTo":"LIST","senderId":"JDGtqapmqy7IdgVJxHyxrtXTytV-Y1MS7hC3s2IQp8UB1DJwSZ01-Ikj3upKmF7anxHexi7UWzvEIX1btZ4_FQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] what&#39;s wrong with reading robots.txt","postDate":"1095990049","msgId":1032,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxNTM3QjIxLjIwMDAwMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGNpbWpwZCtoZ2FpQGVHcm91cHMuY29tPg==","referencesHeader":"PGNpbWpwZCtoZ2FpQGVHcm91cHMuY29tPg=="},"prevInTopic":1016,"nextInTopic":1035,"prevInTime":1031,"nextInTime":1033,"topicId":1004,"numMessagesInTopic":12,"msgSnippet":"... Its taken me a while to get back to you.  Pardon me. I tried your order file and seeds with both HEAD and 1.0.4 via a proxy server and both crawled","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 3401 invoked from network); 24 Sep 2004 01:47:32 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m22.grp.scd.yahoo.com with QMQP; 24 Sep 2004 01:47:32 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta1.grp.scd.yahoo.com with SMTP; 24 Sep 2004 01:47:32 -0000\r\nReceived: (qmail 18784 invoked by uid 100); 24 Sep 2004 01:37:07 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 24 Sep 2004 01:37:07 -0000\r\nMessage-ID: &lt;41537B21.2000003@...&gt;\r\nDate: Thu, 23 Sep 2004 18:40:49 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.2) Gecko/20040820 Debian/1.7.2-4\r\nX-Accept-Language: en-us\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;cimjpd+hgai@...&gt;\r\nIn-Reply-To: &lt;cimjpd+hgai@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.1 required=7.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] what&#39;s wrong with reading robots.txt\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\nbjhong02 wrote:\n\n&gt;I downloaded the binary of Heritrix 1.0.0, and tried to test it with\n&gt;the  seed: www.uiuc.edu. It succeed in dns lookup, but failed to read\n&gt;robots.txt (really exist).  The error log is as follows.\n&gt;\n&gt;\n&gt;  \n&gt;\nIts taken me a while to get back to you.  Pardon me.\n\nI tried your order file and seeds with both HEAD and 1.0.4 via a proxy \nserver and both crawled UIUC.edu fine.\n\nI&#39;m a little stumped.  Is there something particular about the proxy on \nyour end?  Can you wget/curl/etc. via this proxy without problem (You \nsay you can wget the robots.txt buy you don&#39;t say if this is via the \nproxy)?  Do you have to use the proxy?  Have you tried crawling from \nelsewhere where you don&#39;t need a proxy?  Does crawl still fail? (You \nseemed to be able to crawl www.yahoo.com.cn according to a subsequent \nemail; was that via the proxy?).\n\nYours,\nSt.Ack\n\n\n&gt;20040920124037089    -3          - http://www.uiuc.edu/robots.txt P\n&gt;http://www.uiuc.edu/ no-type #001 - - -\n&gt; java.io.IOException: Socket timedout: Read timed out\n&gt;\tat\n&gt;org.archive.io.RecordingInputStream.readFullyOrUntil(RecordingInputStream.java:205)\n&gt;\tat org.archive.crawler.fetcher.FetchHTTP.innerProcess(FetchHTTP.java:282)\n&gt;\tat org.archive.crawler.framework.Processor.process(Processor.java:106)\n&gt;\tat\n&gt;org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:267)\n&gt;\tat org.archive.crawler.framework.ToeThread.run(ToeThread.java:146)\n&gt;\n&gt;\n&gt;My another questioin is, how about if there&#39;s no robots.txt? it seems,\n&gt;in the normal setting, if there&#39;s no robots.txt for a server, the html\n&gt;pages on it will not be crawled. \n&gt;\n&gt;thanks\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;\n&gt;  \n&gt;\n\n\n"}}