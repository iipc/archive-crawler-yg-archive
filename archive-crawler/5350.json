{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"YQHWC_p1ii3fgNVZARxu9VZNfLl714O3uGmGIEx0VVk5v05P6AMlVRVpOiIacnelbDttC22g9TAL3EFLG39sGhjbkH2ITLK8YsacIA0nYg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: &quot;Balanced&quot; crawl frontier","postDate":"1215438930","msgId":5350,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGc0dDc4aStyazZqQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGc0b2xoMys2M28yQGVHcm91cHMuY29tPg=="},"prevInTopic":5347,"nextInTopic":5352,"prevInTime":5349,"nextInTime":5351,"topicId":5342,"numMessagesInTopic":5,"msgSnippet":"Yes, each sub-domain gets its own queue. If you are only crawling 20 sites, a quick fix (at least if your using Heritrix 1.x) is to create an override for each","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 81901 invoked from network); 7 Jul 2008 13:55:31 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m47.grp.scd.yahoo.com with QMQP; 7 Jul 2008 13:55:31 -0000\r\nX-Received: from unknown (HELO n50d.bullet.mail.sp1.yahoo.com) (66.163.169.48)\n  by mta16.grp.scd.yahoo.com with SMTP; 7 Jul 2008 13:55:30 -0000\r\nX-Received: from [216.252.122.217] by n50.bullet.mail.sp1.yahoo.com with NNFMP; 07 Jul 2008 13:55:30 -0000\r\nX-Received: from [209.73.164.86] by t2.bullet.sp1.yahoo.com with NNFMP; 07 Jul 2008 13:55:30 -0000\r\nX-Received: from [66.218.66.91] by t8.bullet.scd.yahoo.com with NNFMP; 07 Jul 2008 13:55:30 -0000\r\nDate: Mon, 07 Jul 2008 13:55:30 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g4t78i+rk6j@...&gt;\r\nIn-Reply-To: &lt;g4olh3+63o2@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: Re: &quot;Balanced&quot; crawl frontier\r\nX-Yahoo-Group-Post: member; u=163406187; y=UOoDwVZtlZAA7AYIqiQMwqD99XClpba9Hoar2sRs36LT9kvx\r\nX-Yahoo-Profile: kristsi25\r\n\r\nYes, each sub-domain gets its own queue. If you are only crawling 20\nsites,=\r\n a quick fix (at least if your using Heritrix 1.x) is to create\nan override=\r\n for each domain and set the &quot;force-queue-assignment&quot; on\nthe frontier to do=\r\nmain.name for each domain.\n\nThe force-queue-assignment will override the us=\r\nual queue assignment\nand each URI will be assigned to a queue with the supp=\r\nlied name. By\nsetting this for a specific domain, you effectively lump it a=\r\nnd any\nsub-domains into one queue.\n\n- Kris\n\n--- In archive-crawler@yahoogro=\r\nups.com, &quot;nickbirren&quot; &lt;nickbirren@...&gt;\nwrote:\n&gt;\n&gt; My problem is that when I=\r\n use 20 seeds (each for a different site),\n&gt; the ones that have many sub-do=\r\nmains seem to get more crawler\n&gt; &quot;attention&quot;. If I understand correctly, ea=\r\nch sub-domain will have its\n&gt; own queue?\n&gt; \n&gt; --- In archive-crawler@yahoog=\r\nroups.com, Noah Levitt &lt;nlevitt@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Heritrix does a pretty goo=\r\nd job of running in a &quot;balanced&quot; manner by \n&gt; &gt; default. The list of urls y=\r\net to be crawled (the frontier) is divided \n&gt; &gt; into queues by host, and th=\r\nere is a delay between fetches of each\n&gt; url in \n&gt; &gt; a single queue, so hos=\r\nts/queues generally aren&#39;t starved out. Have\nyou \n&gt; &gt; encountered this prob=\r\nlem in a real crawl?\n&gt; &gt; \n&gt; &gt; Noah\n&gt; &gt; \n&gt; &gt; nickbirren wrote:\n&gt; &gt; &gt; Is ther=\r\ne a way with Heritrix to define a job with many starting\n&gt; &gt; &gt; points, one =\r\nper site, then have the job run in a &quot;balanced&quot;\nmanner so\n&gt; &gt; &gt; that each s=\r\nite gets its share of bandwidth / processing. I&#39;m\ntrying to\n&gt; &gt; &gt; &quot;avoid st=\r\narvation&quot; of sites that do not have many links in them on\n&gt; &gt; &gt; each page b=\r\ny other sites who do, and if I&#39;m not mistaken, will &quot;take\n&gt; &gt; &gt; over&quot; the c=\r\nrawl frontier after a few iterations.\n&gt; &gt; &gt;\n&gt; &gt; &gt; My other option is to run=\r\n many crawl engines, one per site, but this\n&gt; &gt; &gt; seems cumbersome and is p=\r\nrobably less scaleable.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Thanks,\n&gt; &gt; &gt; -Nick\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; -=\r\n-----------------------------------\n&gt; &gt; &gt;\n&gt; &gt; &gt; Yahoo! Groups Links\n&gt; &gt; &gt;\n&gt;=\r\n &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}