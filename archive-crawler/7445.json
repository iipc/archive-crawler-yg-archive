{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"1MCSHpISaK3SJKEP--A0YLz1ZD7sxYdABnolIGKlSL9y9ZLNa47zBwbnTOEXQ6G7E0WYfvpeWkJGpm5UWkamSSJeVFY","spamInfo":{"isSpam":false,"reason":"2"},"subject":"Re: [archive-crawler] blacklist surt","postDate":"1323886993","msgId":7445,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFRThFOTkxLjQwMjAxMDBAY3MuY211LmVkdT4=","inReplyToHeader":"PDRFRTgwOUQ3LjcwMTAxMDBAYXJjaGl2ZS5vcmc+","referencesHeader":"PGpjNThodCt1Z3VwQGVHcm91cHMuY29tPiA8NEVFODA5RDcuNzAxMDEwMEBhcmNoaXZlLm9yZz4="},"prevInTopic":7442,"nextInTopic":7477,"prevInTime":7444,"nextInTime":7446,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"Noah, Thank you for your response. --David","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 50138 invoked from network); 14 Dec 2011 18:23:18 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m2.grp.sp2.yahoo.com with QMQP; 14 Dec 2011 18:23:18 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.95)\n  by mta4.grp.sp2.yahoo.com with SMTP; 14 Dec 2011 18:23:17 -0000\r\nX-Received: from [128.2.209.200] (SAVOY.LTI.CS.CMU.EDU [128.2.209.200])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id pBEINEUi031868\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tWed, 14 Dec 2011 13:23:14 -0500\r\nMessage-ID: &lt;4EE8E991.4020100@...&gt;\r\nDate: Wed, 14 Dec 2011 13:23:13 -0500\r\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: Noah Levitt &lt;nlevitt@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;jc58ht+ugup@...&gt; &lt;4EE809D7.7010100@...&gt;\r\nIn-Reply-To: &lt;4EE809D7.7010100@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2011.5.19.222118\r\nX-SMTP-Spam-Clean: 8% (\n KNOWN_FREEWEB_URI 0.05, BODY_SIZE_4000_4999 0, BODY_SIZE_5000_LESS 0, BODY_SIZE_7000_LESS 0, DATE_TZ_NEG_0500 0, ECARD_KNOWN_DOMAINS 0, __ANY_URI 0, __BOUNCE_CHALLENGE_SUBJ 0, __BOUNCE_NDR_SUBJ_EXEMPT 0, __CANPHARM_UNSUB_LINK 0, __CP_URI_IN_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __INT_PROD_ONLINE 0, __KNOWN_FREEWEB_URI3 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __SANE_MSGID 0, __TO_MALFORMED_2 0, __URI_NO_WWW 0, __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.95\r\nX-eGroups-Msg-Info: 2:2:2:0:2\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: Re: [archive-crawler] blacklist surt\r\nX-Yahoo-Group-Post: member; u=500983475; y=180t3l5UUym7hiQ_S8vqjG0RKf0Hlb17goZ1snVzpv7ORU5CvZRxKw\r\nX-Yahoo-Profile: david_pane1\r\n\r\nNoah,\n\nThank you for your response.\n\n--David\n\nOn 12/13/2011 9:28 PM, Noah Levitt wrote:\n&gt; Hello David,\n&gt;\n&gt;  &gt; 1) Can the crawler be configured to use the blaklist surt list to\n&gt; filter the seed list as well as discovered URIs? If so, how can I do this.\n&gt;\n&gt; Yes, in order to do this you need to enable recheckScope on the\n&gt; preselector bean. Without that setting, urls are checked against the\n&gt; scope after they&#39;ve been discovered, which is a step that seeds do not\n&gt; go through. With recheckScope urls are checked against the scope at the\n&gt; beginning of their run through the processing chain. So most of the time\n&gt; this is a redundant check, but not in the case of seeds. (Another case\n&gt; where recheckScope is useful is when the crawl operator changes the\n&gt; scope rules during the course of the crawl.)\n&gt;\n&gt;  &gt; 2) Although I had an IP address in my SURT file, the crawler still\n&gt; had some URIs crawled a domain at that address. Is there something\n&gt; incorrect about my configuration or SURT file format?\n&gt;\n&gt; Surts are only checked against the url, so a surt with an ip address\n&gt; will not match a url with a domain name, regardless of what the domain\n&gt; resolves to. There is no existing DecideRule in heritrix to block by ip\n&gt; address. One could be written though. (If you go down that path you\n&gt; should look at e.g. FetchHTTP to see how to obtain the resolved ip\n&gt; address. The recheckScope setting might be useful here too, because the\n&gt; domain name may not have been looked up for a given url at the time it&#39;s\n&gt; discovered.)\n&gt;\n&gt; Noah\n&gt;\n&gt; On 12/12/2011 08:02 AM, david_pane1 wrote:\n&gt;&gt;\n&gt;&gt; I am trying to create and use a large blacklist (1 million ).\n&gt;&gt;\n&gt;&gt; I created a surt file with lines in the format similar to both of these:\n&gt;&gt;\n&gt;&gt; +http://(128.2.42.10\n&gt;&gt; +http://(com,domain,\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; &lt;!-- ...but REJECT those from a configurable (initially empty) set of\n&gt;&gt; REJECT SURTs... --&gt;\n&gt;&gt; &lt;bean\n&gt;&gt; class=&quot;org.archive.modules.deciderules.surt.SurtPrefixedDecideRule&quot;&gt;\n&gt;&gt; &lt;property name=&quot;decision&quot; value=&quot;REJECT&quot;/&gt;\n&gt;&gt; &lt;property name=&quot;seedsAsSurtPrefixes&quot; value=&quot;false&quot;/&gt;\n&gt;&gt; &lt;property name=&quot;surtsDumpFile&quot; value=&quot;${launchId}/negative-surts.dump&quot; /&gt;\n&gt;&gt; &lt;property name=&quot;surtsSource&quot;&gt;\n&gt;&gt; &lt;bean class=&quot;org.archive.spring.ConfigFile&quot;&gt;\n&gt;&gt; &lt;property name=&quot;path&quot; value=&quot;myblacklist.surt&quot; /&gt;\n&gt;&gt; &lt;/bean&gt;\n&gt;&gt; &lt;/property&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; 1) Can the crawler be configured to use the blaklist surt list to\n&gt;&gt; filter the seed list as well as discovered URIs? If so, how can I do\n&gt;&gt; this.\n&gt;&gt;\n&gt;&gt; 2) Although I had an IP address in my SURT file, the crawler still had\n&gt;&gt; some URIs crawled a domain at that address. Is there something\n&gt;&gt; incorrect about my configuration or SURT file format?\n&gt;&gt;\n&gt;&gt; --David\n&gt;&gt;\n&gt;&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@...&gt; wrote:\n&gt;&gt;&gt; On 11/7/11 12:46 PM, David Pane wrote:\n&gt;&gt;&gt;&gt; Thank you Gordon.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; We would also like to have a blacklist of unwanted sites. This\n&gt;&gt;&gt;&gt; blacklist\n&gt;&gt;&gt;&gt; would possibly contain millions hosts. Do you have any experience in\n&gt;&gt;&gt;&gt; the\n&gt;&gt;&gt;&gt; resources needed for this? Would this slow down the speed of the crawl?\n&gt;&gt;&gt; I&#39;ve not done anything with a blacklist that big. You certainly wouldn&#39;t\n&gt;&gt;&gt; want to use a list of regexes!\n&gt;&gt;&gt;\n&gt;&gt;&gt; The SURT-based DecideRules use a sorted list of prefixes, all in memory,\n&gt;&gt;&gt; and have roughly O(log n) lookup. Perhaps that would work for your\n&gt;&gt;&gt; purposes, if you have a big RAM machine; you should do some tests and\n&gt;&gt;&gt; back-of-the-envelope calculations to check for sure. If the blacklist\n&gt;&gt;&gt; entries are always hosts, some other hash-based structure might work\n&gt;&gt;&gt; with even less RAM overhead and O(1) lookup.\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; In our small crawls that we have been running, we found that\n&gt;&gt;&gt;&gt; although we\n&gt;&gt;&gt;&gt; have setup for a breadth first crawl, days into the crawl and 10&#39;s of\n&gt;&gt;&gt;&gt; million of pages crawled, the crawler has still only crawled a small\n&gt;&gt;&gt;&gt; percentage of seeds (under 10% of a 2 million host seed list - 1200\n&gt;&gt;&gt;&gt; threads). It appears that most of the seeds are in a separate queue so\n&gt;&gt;&gt;&gt; would cycling through the queues (balance-replenish-amount to a lower\n&gt;&gt;&gt;&gt; amount maybe 100) help cover all of the queues?\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n\n"}}