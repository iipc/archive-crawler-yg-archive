{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":182123250,"authorName":"Kaisa Kaunonen","from":"&quot;Kaisa Kaunonen&quot; &lt;kaisa.kaunonen@...&gt;","profile":"kaisa_kaunonen","replyTo":"LIST","senderId":"jOwcfkArvQp0UaprTpvLrR-7xDT64vgvTtixQWQ5x-egla9Y2F-0qZH3snN6D13DRCV__COS1U-LCVh72CJXQKQqmrOE7UnBhll2Ah662i2FSRZ8","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Large scale crawling with Heritrix","postDate":"1085220312","msgId":439,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM4bjhrbys0NzVqQGVHcm91cHMuY29tPg==","inReplyToHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBDNzFAbG9raS5ib2suaGkuaXM+"},"prevInTopic":434,"nextInTopic":440,"prevInTime":438,"nextInTime":440,"topicId":432,"numMessagesInTopic":9,"msgSnippet":"Hi How do you handle dynamic urls (or better urls pointing to dynamic files). Even large search engines are very careful with them. *) Reject all urls which","rawEmail":"Return-Path: &lt;kaisa.kaunonen@...&gt;\r\nX-Sender: kaisa.kaunonen@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 7956 invoked from network); 22 May 2004 10:05:25 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m24.grp.scd.yahoo.com with QMQP; 22 May 2004 10:05:25 -0000\r\nReceived: from unknown (HELO n22.grp.scd.yahoo.com) (66.218.66.78)\n  by mta6.grp.scd.yahoo.com with SMTP; 22 May 2004 10:05:25 -0000\r\nReceived: from [66.218.66.120] by n22.grp.scd.yahoo.com with NNFMP; 22 May 2004 10:05:13 -0000\r\nDate: Sat, 22 May 2004 10:05:12 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;c8n8ko+475j@...&gt;\r\nIn-Reply-To: &lt;D958110B273CD511ACC100B0D079AA4E01960C71@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Length: 6549\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.78\r\nFrom: &quot;Kaisa Kaunonen&quot; &lt;kaisa.kaunonen@...&gt;\r\nSubject: Re: Large scale crawling with Heritrix\r\nX-Yahoo-Group-Post: member; u=182123250\r\nX-Yahoo-Profile: kaisa_kaunonen\r\n\r\nHi \n\nHow do you handle dynamic urls (or better urls pointing to dynamic \nfi=\r\nles). Even large search engines are very careful with them.\n\n*) Reject all =\r\nurls which contain parameters longer than 10 \ncharacters, because those par=\r\nameters are probably session ids. Length \n10 is arbitrary, could be 7, 8, 1=\r\n2 or something else? Filters are \neasy to construct if one only knew what &#39;=\r\nnumber&#39; works in most cases.\n\nhttp://www=85..net/?id=3D12hswjdufh27e6djjsu3=\r\n778s\n\n*)  Reject urls with too many parameters. Set maximum number of \npara=\r\nmeters to some value: are 4 parameters acceptable, but 5 too much \n(or some=\r\n other limit)?\n\nhttp:/www=85...net/?\nproject_id=3D2&ykieli=3Dfi&startfrom=\r\n=3D0&stopto=3D6&commands=3D329&pic=3D\n\n*) Long file paths: how many slashes=\r\n can you tolerate before they \nlook suspicious? Does the pathological path =\r\nfilter only understand \nsimple paths like .../img/img/img where one pattern=\r\n repeats itself. \nWhat about longer cycles in paths .../css/fi/forms/css/fi=\r\n/forms/... \n(Well, if you have cycles then the path is too long anyway.)\n \n=\r\nhttp://www=85...net/fi/organisaatiot/2380/fi/css\n/fi/forms/fi/css/fi/css/\nf=\r\ni/css/fi/css/fi/forms/fi/forms/form.phtml?\nform_id=3DId585bf028a26d65a9cd67=\r\n0ef16b8921=A7ion=3D314\n\n\nkaisa\n\n\n--- In archive-crawler@yahoogroups.com, &quot;K=\r\nristinn Sigurdsson&quot; \n&lt;kris@a...&gt; wrote:\n&gt; Hi all,\n&gt; \n&gt; This week I&#39;ve been =\r\nexperimenting with running a crawl over the \nentire .is\n&gt; TLD. As expected =\r\nI&#39;ve encountered several problems and limitations \nthat I\n&gt; thought might o=\r\nf interest to this community as this is (as far as I \nknow)\n&gt; the first att=\r\nempt to run Heritrix on such a scale. Crawling has \nbeen done\n&gt; with very r=\r\necent development builds.\n&gt; \n&gt; Many of the limits I&#39;ve encountered are alre=\r\nady known but I&#39;m \nreiterating\n&gt; them here for the sake of completeness.\n&gt; =\r\n\n&gt; The .is TLD has a little over 10000 registered second level \ndomains. A =\r\nlist\n&gt; of them with a www prefix was used as a seed list. Instead of one \no=\r\nf the\n&gt; scopes shipped with Heritrix I wrote a custom scope that limited \nt=\r\nhe crawl\n&gt; to hosts with the .is suffix (plus the usual transitive includes=\r\n).\n&gt; \n&gt; The first problem I encountered was Heritrix&#39;s excessive use of fil=\r\ne\n&gt; handlers. In addition to numerous other files used by Heritrix each \non=\r\ne of\n&gt; the 10000+ domains immediately needed its own host queue with two \nf=\r\nile\n&gt; handlers each. This led me to (with the help of Gordon and Michael) \n=\r\nto\n&gt; redesign the so called DiskBackedQueues so that they only have open \nf=\r\niles\n&gt; when they are large enough to warrant it. I.e. when items in them \na=\r\nre too\n&gt; many to fit into the memory &#39;head&#39; that they have (200 files for \n=\r\nthe host\n&gt; queues).\n&gt; \n&gt; This fix drastically reduced the number of open fi=\r\nles and for the \nmost part\n&gt; eliminated this problem. With it out of the wa=\r\ny it was possible to \nlaunch a\n&gt; crawl with this many seeds. \n&gt; \n&gt; Initial =\r\nprogress was quite impressive, running at over 50 documents \nper\n&gt; second i=\r\nnitially. Eventually it started to gradually drop and now \nseveral\n&gt; days l=\r\nater it stands at around 17 documents per second. I&#39;m unsure \nof the\n&gt; reas=\r\non for this gradual decline. It may be related to increasing \nsizes of\n&gt; va=\r\nrious data structures. \n&gt; \n&gt; Over the course of the past 3 days the crawl h=\r\nas downloaded nearly \n4 million\n&gt; documents totaling over 150 GB. In additi=\r\non some 11 million plus \ndocuments\n&gt; have been discovered and are waiting p=\r\nrocessing. While impressive \nthis is\n&gt; still only scratching the top of the=\r\n .is domain. I estimate that \ncurrently\n&gt; documents 3 link hops from the se=\r\neds are being processed.\n&gt; \n&gt; Memory use was initially my main concern. The=\r\n crawl is running on a \nmachine\n&gt; with 1.5 GB RAM and the JVM max heap size=\r\n was set to 1.25 GB. To \ndate the\n&gt; JVM has only allocated itself 1 GB and =\r\ngarbage collection still \ndrops the\n&gt; memory being used to almost half that=\r\n.\n&gt; \n&gt; Disk use by the disk bound queues however has been much greater \nthe=\r\nn I\n&gt; anticipated. With said 11 million URLs waiting in the queues they \nno=\r\nw take\n&gt; up about 16 GB. This comes out at about 1.6 KB per URI. This will =\r\n\nturn out\n&gt; to be the limiting factor for my current crawl since the disk i=\r\nn \nquestion\n&gt; only has another 3 GB free so it will be exhausted soon.\n&gt; \n&gt;=\r\n Even with much larger disks, say 200 GB, crawls will be limited to \nhaving=\r\n\n&gt; 120-130 million URIs waiting. This seems like a huge number until \nyou\n&gt;=\r\n consider that the .is domain would seem to have at least that many \ndocume=\r\nnts\n&gt; based on this crawl. Doing a crawl like this on an even larger \nscale=\r\n would\n&gt; seem to merit trying to reduce the size (on disk) of these URIs.\n&gt;=\r\n \n&gt; Of course a crawl of that scope is not possible until the list of \nalre=\r\nady\n&gt; seen URIs can be disk backed.  With the current method of using 4 \nby=\r\nte\n&gt; fingerprints for each encountered URI 1 GB of memory can hold \naround =\r\n28\n&gt; million URIs. Even with a machine with 4 GB RAM would not be able \nto =\r\nscale\n&gt; up to even a full .is TLD crawl.\n&gt; \n&gt; \n&gt; \n&gt; Some of my thoughts on =\r\ndealing with the limits follow:\n&gt; \n&gt; Moving the list of encountered URIs to=\r\n disk seems to be imperative. \nBut that\n&gt; will pose even greater demands on=\r\n disk space so I would suggest \nthat we\n&gt; remain aware of that issue and tr=\r\ny (whenever possible) to limit the \nsize of\n&gt; the data being written to dis=\r\nk to that which is actually needed. \nThat may\n&gt; have the additional benefit=\r\n of speeding I/O operations (even if only\n&gt; marginally). Perhaps it should =\r\nalso be possible to split the data \namong many\n&gt; disk (of course this can b=\r\ne done at the OS level to some extent).\n&gt; \n&gt; Multi machine setup might alle=\r\nviate some of this but for any sort \nof large\n&gt; scale crawl (targeting 100 =\r\nmillion +) each machine would probably be\n&gt; handling tens of millions URIs =\r\nat least. For truly large scale \ncrawls (like\n&gt; the ones I know IA wants to=\r\n do) the demands grow even more.\n&gt; \n&gt; Another approach which might limit th=\r\ne problem is the \nimplementation of the\n&gt; site first crawling strategy. In =\r\nmy crawl all .is domains are \ntackled in\n&gt; parallel. This makes the crawl a=\r\n true breadth first crawl. A site \nfirst\n&gt; approach would let Heritrix focu=\r\ns it&#39;s efforts on a limited number \nof sites\n&gt; at a time. Since the usual p=\r\nattern for crawling a site is an initial\n&gt; explosion of available URIs foll=\r\nowed by a steady decline and \neventual\n&gt; exhaustion we would likely not win=\r\nd up having as huge a number of \nURIs\n&gt; waiting (at least not as quickly). =\r\nThis does nothing for the limits \nimposed\n&gt; by the demands on RAM memory th=\r\nough.\n&gt; \n&gt; \n&gt; Kristinn Sigur=F0sson\n&gt; Software engineer\n&gt; National and Univ=\r\nersity Library of Iceland\n\n\n"}}