{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"kucqLDGJGIAbhaZzJQAZI6JEd95rx1hv4kUUbXyPWdkjtgPzPnwrI1hSA2wVloRcaUWqxuVyCx5spkSRA1nV3UwLECNgPAM","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: questions before we restart the crawl","postDate":"1327123356","msgId":7560,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMUE0QjlDLjUwOTAwMDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRGMUEwNEJDLjYwMTA0MDJAY3MuY211LmVkdT4=","referencesHeader":"PDRGMTU5NEQwLjIwOTA4MDhAY3MuY211LmVkdT4gPDRGMTVCQjNBLjUwMzA2QGFyY2hpdmUub3JnPiA8NEYxOUIzOTEuMTA3MDQwMUBjcy5jbXUuZWR1PiA8NEYxQTA0QkMuNjAxMDQwMkBjcy5jbXUuZWR1Pg=="},"prevInTopic":7556,"nextInTopic":7562,"prevInTime":7559,"nextInTime":7561,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"You ve provided an overwhelming amount of information and we may be dealing with multiple issues, some of which have roots going back earlier than the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 99694 invoked from network); 21 Jan 2012 05:22:38 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m10.grp.sp2.yahoo.com with QMQP; 21 Jan 2012 05:22:38 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta3.grp.sp2.yahoo.com with SMTP; 21 Jan 2012 05:22:37 -0000\r\nX-Received: (qmail 60224 invoked by uid 0); 21 Jan 2012 05:22:35 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay00.pair.com with SMTP; 21 Jan 2012 05:22:35 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4F1A4B9C.5090007@...&gt;\r\nDate: Fri, 20 Jan 2012 21:22:36 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: David Pane &lt;dpane@...&gt;, archive-crawler@yahoogroups.com\r\nCc: Noah Levitt &lt;nlevitt@...&gt;\r\nReferences: &lt;4F1594D0.2090808@...&gt; &lt;4F15BB3A.50306@...&gt; &lt;4F19B391.1070401@...&gt; &lt;4F1A04BC.6010402@...&gt;\r\nIn-Reply-To: &lt;4F1A04BC.6010402@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=137285340; y=P7unoGQvSH_jRpncx9YXY_QdICvJc9FrKncpC66jg1UV\r\nX-Yahoo-Profile: gojomo\r\n\r\nYou&#39;ve provided an overwhelming amount of information and we may be \ndealing with multiple issues, some of which have roots going back \nearlier than the diagnostic data we now have available.\n\nA few key points of emphasis:\n\n  - we&#39;ve not run crawls with 1200 threads before, or on hardware \nsimilar to yours, so our experience is only vaguely suggestive\n\n  - it&#39;s not the lower thread counts that are the real source of \nconcern; you can even adjust the number of threads mid-crawl. It&#39;s that \nthe error that killed the threads almost certainly left a queue in a \n&#39;phantom&#39; state where no progress would be made crawling its URIs, each \ntime it happened, on each resume leading to the current state.\n\n  - without having understood and fixed whatever software or system \nproblems caused the earliest/most-foundational errors in your crawl, \nit&#39;s impossible to say how likely they are to recur.\n\nWith that in mind, I&#39;ll try to provide quick answers to your other \nquestions...\n\nOn 1/20/12 4:20 PM, David Pane wrote:\n&gt;\n&gt; We have collected about 550 million pages along with the images and\n&gt; supporting documents on our 5 instance crawl that was started Dec. 23rd.\n&gt; Although we are please with the amount of data we captured to date, we\n&gt; are very concerned about the state of the Heritrix instances. If fact,\n&gt; we aren&#39;t very confident that the instances will last until the end of\n&gt; February. We are now running on a total of over 500 less threads than\n&gt; the configured 1200 threads/instance.\n&gt;\n&gt; 0 - not running right now.\n&gt; 1 - running on 1198 ( 2 less)\n&gt; 2 - running on 931 (269 less)\n&gt; 3 - running on 987 (213 less)\n&gt; 4 - running on 1170 (30 less)\n&gt;\n&gt; Since we are seriously considering throwing away this past month&#39;s work\n&gt; and starting over, we would like to pick your brain on some strategies\n&gt; that will help us avoid getting into this situation again. We were\n&gt; hoping to be done crawling by the end of February so this restart will\n&gt; put us behind schedule.\n&gt;\n&gt; 1) Can we continue from here but with &quot;clean&quot; Heritrix instances?\n&gt;\n&gt; Is there a way that we can continue from the this point forward, but\n&gt; start with Heritrix instances that will not be corrupt due to sever\n&gt; error? (e.g. using the\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery ) If\n&gt; so, would you recommend doing this? You mentioned that this could be\n&gt; time consuming. Each of our instances has downloaded around 170M URIs,\n&gt; they have over 700M queued URIs, what is your time estimate for\n&gt; something this large?\n&gt;\n&gt; We are willing to sacrifice a few days to get our crawler to a clean\n&gt; state again so we can crawl for another 30 days at the pace we have been\n&gt; crawling.\n\nYou can do a big &#39;frontier-recover&#39; log replay to avoid recrawling the \nsame URIs, and approximate the earlier queue state. Splitting/filters \nthe logs manually beforehand as alluded to in the wiki page can speed \nthis process somewhat... but given the size of all your log-segments \nthat log grooming beforehand is itself likely to be a lengthy process.\n\nI don&#39;t think we&#39;ve ever done it with logs of 170M crawled / 870M \ndiscovered before, nor on any hardware comparable to yours. So it&#39;s \nimpossible to project its duration in your environment. It&#39;s taken 2-3 \ndays for us on smaller crawls, slower hardware.\n\nAn added complication is that this older frontier-recover-log replay \ntechnique happens in its own thread separate from the checkpointing \nprocess, so it is not, itself, accurately checkpointed during the long \nreload process.\n\nAt nearly 1B discovered URIs per node, even if you are using the \nalternate BloomUriUniqFilter, if you are using it at its default size \n(~500MB) it will now be heavily saturated and thus returning many \nfalse-positives causing truly unique URIs to be rejected as duplicates. \n(If you&#39;re using a significantly larger filter, you may not yet be at a \nhigh false-positive rate: you&#39;d have to do the bloom filter math. If \nyou&#39;re still using BdbUriUniqFilter, you&#39;re way way past the point where \nits disk seeks have usually made it too slow for our purposes.)\n\n&gt; 2) What can be done to avoid corrupting the Heritrix instances?\n&gt;\n&gt; - What kind of strategies might we take to keep the crawl error free?\n&gt;\n&gt; - Do you think the SEVER errors that we have seen are deterministic or\n&gt; random (e.g., triggered by occasional flaky network conditions, disks,\n&gt; race conditions, or whatever)?\n\nHard to say. The main thing I could suggest is watch very closely and \nwhen a SEVERE error occurs, prioritize diagnosing and resolving the \ncause while the info is fresh.\n\n&gt; - Do you believe that we can reliably backup to the previous checkpoint\n&gt; if we watch the logs and stop as soon as we see the first SEVER error?\n&gt; If we do this, do you speculate that the same SEVER will occur again?\n\nResuming from the latest checkpoint before an error believed to corrupt \nthe on-disk state will be the best strategy.\n\nIf we never figure out the real cause, but run the same software on the \nsame machine, yes, I expect the same problem will recur!\n\n&gt; - Is there any reason why a Heritrix instance that is run while binded\n&gt; to one ip address can&#39;t be resumed binded to a different ip address?\n\nOnly the web UI to my knowledge binds to a chosen address, and it is \ncommon to have it bind to all. I don&#39;t expect the outbound requests \nwould be hurt by a machine changing its IP address while the crawl was \nrunning, but I would run a test to be sure if that was an important, \nexpected transition.\n\n&gt; 3) Should we configure the crawler with more instances and switch\n&gt; between them?\n&gt;\n&gt; We have seen that we can run a single instance to 100M pages +\n&gt; supporting images and documents. Perhaps this means that we need 10 or\n&gt; more instances instead of 5. That raises the possibility of running 2\n&gt; instances per machine. If we could run 2, or even 4, instances on a\n&gt; single machine, they would each run half as long.\n\nI don&#39;t think the problems as reported are specifically due to one \nnode&#39;s progress growing beyond a certain size, but it might be the case \nthat giant instances are more likely to suffer from, and harder to \nrecover from, single glitches (eg a single disk error). On the other \nhand, many instances introduce more redundant overhead costs (certain \ndata structures, cross-feeding URIs if you&#39;re doing that, etc.).\n\n&gt; - Can you suggest a way to start/stop instances from a script so we can\n&gt; change between instances automatically?\n\nNot a mode I&#39;ve thought much about.\n\n&gt; - Have you seen frequent starting / stopping of instances introduce\n&gt; instability?\n\nNo... but it might make you notice latent issues sooner.\n\n&gt; 4) Crawl slows but restarting seems to improve the speed again.\n&gt;\n&gt; We noticed that the all of our instances would initially run at a fast\n&gt; pace. We would collect an average of 25M + pages/day for 2-3 days and\n&gt; then the crawl would slow down to 10M pages/day over the next few days.\n&gt; (these numbers are totals of all 5 instances combined). When we\n&gt; restarted the instances, the average pages would improve back to 25M +\n&gt; pages/day. The total crawled numbers (TiB) also reflected the slow down.\n&gt;\n&gt; - Is this something that others have experienced as well?\n\nI don&#39;t recall hearing other reports of speed boosts after \ncheckpoint-resumes but others may have more experience.\n\n&gt; 5) We are capturing tweets from twitter, harvesting the urls and want to\n&gt; crawl those urls within 1 day of receiving the tweet. Can you recommend\n&gt; a strategy for doing this with the 5 instances we are running?\n&gt;\n&gt; - Do we need to run a separate crawler dedicated to this? If so, can you\n&gt; suggest a way to crawl out from the tweeted urls but when we get\n&gt; additional urls from the tweets, quickly change focus to these urls\n&gt; instead of the ones branching out. When adding urls as seeds, can you\n&gt; set a high priority to crawl those before the discovered urls? Do you\n&gt; recommend maybe setting up a specific crawl for these urls and then only\n&gt; crawl a few hopes from the seeds - injecting the urls from the tweets as\n&gt; seeds?\n\nDedicating a special script or crawler to URIs that come from such a \nconstrained source (Twitter feeds), or that need to be crawled in a \nspecial timeframe, or according to other special limits (fewer hops), \ncould make sense.\n\nIt would take some customization of the queueing-policy or &#39;precedence&#39; \nfeatures of Heritrix to allow URIs added mid-crawl to be prioritized \nabove those already discovered and queued. The most simple possible \ncustomization might be a UriPrecedencePolicy that takes all zero-hop \nURIs (which all seeds and most direct-fed URIs would be) and gives them \na higher precedence (lower precedence number) than all other URIs.\n\n&gt; 6) I think the answer is no for this question, but I will ask it anyway.\n&gt; If you have a Heritrix instance that is configured for 1200 threads on\n&gt; one machine, can you recover from a checkpoint from that 1200 thread\n&gt; configuration on a different machine with an Heritrix instance that is\n&gt; configured for less threads (e.g. the default 25 threads)?\n\nYes - there&#39;s no need to keep the thread count the same after a resume. \nNone of the checkpoint structures (or usual disk structures) are based \non the number of worker threads (&#39;ToeThreads&#39;)... as mentioned above you \ncan even vary the number of threads in a running crawl.\n\n- Gordon\n\n"}}