{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"YlJOEb5YdgKRy2ZOA4qsp0T0LxkE_-6-9UAocy4p2_tUi2fWSRjKnAG-D_XGEULiuvMSWuUz1lGTkNyoD2hg2ixXiEk0V0A","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Carrying the already-seen list between crawls","postDate":"1145428170","msgId":2792,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0NDVEOENBLjMwMjA1MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGI4MTI1OWEwMDYwNDE4MTgwN2w3NThiNmJiM3NiZWJhMWNkNWQ2MjhlMDFiQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGI4MTI1OWEwMDYwNDE4MTgwN2w3NThiNmJiM3NiZWJhMWNkNWQ2MjhlMDFiQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2791,"nextInTopic":2870,"prevInTime":2791,"nextInTime":2793,"topicId":2791,"numMessagesInTopic":7,"msgSnippet":"... Another option would be to use the recover log; all of the F+ lines are URLs that the crawl considered seen . A typical recover-from-log scans the log","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 52211 invoked from network); 19 Apr 2006 06:29:38 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m34.grp.scd.yahoo.com with QMQP; 19 Apr 2006 06:29:38 -0000\r\nReceived: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta3.grp.scd.yahoo.com with SMTP; 19 Apr 2006 06:29:38 -0000\r\nReceived: (qmail 64994 invoked from network); 19 Apr 2006 06:29:31 -0000\r\nReceived: from unknown (HELO ?10.0.10.13?) (unknown)\n  by unknown with SMTP; 19 Apr 2006 06:29:31 -0000\r\nX-pair-Authenticated: 71.141.108.212\r\nMessage-ID: &lt;4445D8CA.3020507@...&gt;\r\nDate: Tue, 18 Apr 2006 23:29:30 -0700\r\nUser-Agent: Mozilla Thunderbird 1.0.7-1.1.fc4 (X11/20050929)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;b81259a00604181807l758b6bb3sbeba1cd5d628e01b@...&gt;\r\nIn-Reply-To: &lt;b81259a00604181807l758b6bb3sbeba1cd5d628e01b@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Carrying the already-seen list between crawls\r\nX-Yahoo-Group-Post: member; u=137285340; y=Y5L_aIfjFQWRooR3NjWKEF_JC7zjQhXnWklMlzJjwCPL\r\nX-Yahoo-Profile: gojomo\r\n\r\nGreg Kempe wrote:\n&gt; Hi\n&gt; \n&gt; Occasionally we want to be able to restart the crawler and start a new\n&gt; job and carry across the already-seen list, but nothing else (such as\n&gt; the frontier, up-time and url counters etc.)\n&gt; \n&gt; My original idea for not carrying the frontier across was to recover\n&gt; the crawl from a checkpoint, pause it, flush the frontier and inject a\n&gt; new set of seeds. There is no quick-and-easy way to flush the\n&gt; frontier, though. Dropping entries from the frontier based on a regex\n&gt; like &quot;.*&quot; is very time consuming (we have a frontier of 4.5m urls).\n&gt; \n&gt; Is there a simpler method for carrying the already-seen list between\n&gt; jobs without any other crawl data? Another option would be to extract\n&gt; the appropriate bdb from the checkpoint and allow it to be\n&gt; (de)serialised independently.\n&gt; \n&gt; Our crawls are usually around 5 million urls, with around 6 GB of bdb\n&gt; files. We want to preserve the already-seen list for as long as\n&gt; possible.\n\nAnother option would be to use the &#39;recover&#39; log; all of the &#39;F+&#39; \nlines are URLs that the crawl considered &#39;seen&#39;.\n\nA typical recover-from-log scans the log twice: once to add all \nthe &#39;Fs&#39; (success) URLs to the new crawl&#39;s &#39;seen&#39; list, then again \nto add all the &#39;F+&#39; (scheduled) URLs that weren&#39;t already &#39;seen&#39; \nby the first pass.\n\nYou could thus take old crawl recover logs, perhaps filter them as \nappropriate for your needs, then feed them as the &#39;recover&#39; source \nfor the new crawl.\n\nThe recovery-scanning can take a while with giant logs but is \nlikely to be faster than the frontier-flushing idea -- at least \nwith current code. We&#39;ve recovered crawls with tens of millions of \nURLs finished and pending.\n\nA quicker way to drop all (or large related groups) of URIs from \nthe frontier sounds like a useful feature; I&#39;ve made an RFE:\n\n   https://sourceforge.net/support/tracker.php?aid=1472775\n\nThe ultimate vision would be for crawls to be &#39;eternal&#39; if \ndesired: opened, closed, moved, and editted almost like documents, \nwith controls over how much of the &#39;seen&#39; and queued URLs carry \nover through time and relaunches. (This could be many releases \naway, though.)\n\n- Gordon @ IA\n\n\n"}}