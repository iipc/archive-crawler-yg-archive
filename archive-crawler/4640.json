{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":194872127,"authorName":"hinoglu","from":"&quot;hinoglu&quot; &lt;hinoglu@...&gt;","profile":"hinoglu","replyTo":"LIST","senderId":"wFx6dnZJslpeYtHLAIcn5ZRYECYOM_Kl9AAmFCwHa0oLlOEAPw2mSGfNM3D5Gog7f7LwNR-1vRdhk5aIlKZNa95sk5E","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: java.lang.OutOfMemoryError: GC overhead limit exceeded","postDate":"1193760791","msgId":4640,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZnN2w2bisxMGoyZ0BlR3JvdXBzLmNvbT4=","inReplyToHeader":"PGZnNWlhdSs5ODI3QGVHcm91cHMuY29tPg=="},"prevInTopic":4638,"nextInTopic":4642,"prevInTime":4639,"nextInTime":4641,"topicId":4629,"numMessagesInTopic":9,"msgSnippet":"hi, ... i ve changed the value of bdb-cache-percent from 0 to 20, and also to 30 and tried restarting the crawling jobs, server went down faster than before.","rawEmail":"Return-Path: &lt;hinoglu@...&gt;\r\nX-Sender: hinoglu@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 20537 invoked from network); 30 Oct 2007 16:13:11 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m36.grp.scd.yahoo.com with QMQP; 30 Oct 2007 16:13:11 -0000\r\nX-Received: from unknown (HELO n16.bullet.sp1.yahoo.com) (69.147.64.213)\n  by mta16.grp.scd.yahoo.com with SMTP; 30 Oct 2007 16:13:11 -0000\r\nX-Received: from [216.252.122.216] by n16.bullet.sp1.yahoo.com with NNFMP; 30 Oct 2007 16:13:11 -0000\r\nX-Received: from [66.218.69.1] by t1.bullet.sp1.yahoo.com with NNFMP; 30 Oct 2007 16:13:11 -0000\r\nX-Received: from [66.218.66.80] by t1.bullet.scd.yahoo.com with NNFMP; 30 Oct 2007 16:13:11 -0000\r\nDate: Tue, 30 Oct 2007 16:13:11 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;fg7l6n+10j2g@...&gt;\r\nIn-Reply-To: &lt;fg5iau+9827@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;hinoglu&quot; &lt;hinoglu@...&gt;\r\nSubject: Re: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\nX-Yahoo-Group-Post: member; u=194872127; y=xMD2ZH87O7kS4II5g36KLHpGDzL36IZjRbArJC5g9CRFdA\r\nX-Yahoo-Profile: hinoglu\r\n\r\nhi,\n \n&gt; &gt; Also remember that every crawl creates its own BerkeleyDB-JE \n&gt; &gt;=\r\n environment, and each such environment tries to limit its cache by \n&gt; &gt; de=\r\nfault to using 60% of the total heap. So, if you leave this\n&gt; default in \n&gt;=\r\n &gt; place, and launch 2 simultaneous crawls, their environments will\n&gt; seek =\r\nto \n&gt; &gt; limit themselves to 120% of the heap -- almost guaranteed to cause =\r\n\n&gt; &gt; problems unless the crawls wrap up very quickly.\n&gt; \n&gt; hm i&#39;ve got most=\r\n of the settings untouched, should&#39;ve read the\n&gt; instructions better :( i&#39;l=\r\nl try again by setting the bdb heap around \n&gt; 20-25%, but again it seems th=\r\nat: if i have 5-6 instances at the same\n&gt; time(since i have around 20 sites=\r\n to crawl), then i&#39;ll probably have\n&gt; some troubles. so, what would be the =\r\noptimum setting for this value?\n&gt;\n\ni&#39;ve changed the value of bdb-cache-perc=\r\nent from 0 to 20, and also to 30 \nand tried restarting the crawling jobs, s=\r\nerver went down faster than\nbefore. what should be the optimum value?  \n\n\n"}}