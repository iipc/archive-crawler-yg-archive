{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":336053106,"authorName":"Sergey Khenkin","from":"Sergey Khenkin &lt;skhenkin@...&gt;","profile":"skhenkin@ymail.com","replyTo":"LIST","senderId":"eGI44DCNmKgNyf4j3TN1xhk6yvLa1tnYpv41kkbqWhHbX6Fbj2813Od_4F9ba54y_TfO8FXXdN9GW2Fd6Lx4EeGJGPJIXNU0PWM","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: How to download HTML files from a relative path","postDate":"1240782195","msgId":5802,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5RjRENTczLjQwMTA2MDVAZ21haWwuY29tPg==","inReplyToHeader":"PGd0MjZsNytpcXFkQGVHcm91cHMuY29tPg==","referencesHeader":"PGd0MjZsNytpcXFkQGVHcm91cHMuY29tPg=="},"prevInTopic":5797,"nextInTopic":5803,"prevInTime":5801,"nextInTime":5803,"topicId":5795,"numMessagesInTopic":11,"msgSnippet":"Hm, rather an interesting task. I tried the DecidingScope described below (an excerpt from the job s oreder.xml): ","rawEmail":"Return-Path: &lt;skhenkin@...&gt;\r\nX-Sender: skhenkin@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 97814 invoked from network); 26 Apr 2009 21:43:59 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m1.grp.re1.yahoo.com with QMQP; 26 Apr 2009 21:43:59 -0000\r\nX-Received: from unknown (HELO mail-bw0-f170.google.com) (209.85.218.170)\n  by mta3.grp.sp2.yahoo.com with SMTP; 26 Apr 2009 21:43:58 -0000\r\nX-Received: by bwz18 with SMTP id 18so2031472bwz.28\n        for &lt;archive-crawler@yahoogroups.com&gt;; Sun, 26 Apr 2009 14:43:11 -0700 (PDT)\r\nX-Received: by 10.204.57.67 with SMTP id b3mr4592754bkh.128.1240782191093;\n        Sun, 26 Apr 2009 14:43:11 -0700 (PDT)\r\nReturn-Path: &lt;skhenkin@...&gt;\r\nX-Received: from ?192.168.1.3? ([213.227.229.75])\n        by mx.google.com with ESMTPS id 31sm7042845fkt.21.2009.04.26.14.43.10\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Sun, 26 Apr 2009 14:43:10 -0700 (PDT)\r\nMessage-ID: &lt;49F4D573.4010605@...&gt;\r\nDate: Mon, 27 Apr 2009 00:43:15 +0300\r\nUser-Agent: Thunderbird 2.0.0.21 (Windows/20090302)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;gt26l7+iqqd@...&gt;\r\nIn-Reply-To: &lt;gt26l7+iqqd@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Sergey Khenkin &lt;skhenkin@...&gt;\r\nSubject: Re: [archive-crawler] Re: How to download HTML files from a relative\n path\r\nX-Yahoo-Group-Post: member; u=336053106; y=hjB3nJk0oa62UyyoF-W0R7GTdRnP6TqXmhEQLYrqo-VMpc6L00sFKhvHGzzr\r\nX-Yahoo-Profile: skhenkin@...\r\n\r\nHm, rather an interesting task.\n\nI tried the DecidingScope described below (an excerpt from the job&#39;s \noreder.xml):\n\n     &lt;newObject name=&quot;scope&quot; \nclass=&quot;org.archive.crawler.deciderules.DecidingScope&quot;&gt;\n       &lt;boolean name=&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n       &lt;string name=&quot;seedsfile&quot;&gt;seeds.txt&lt;/string&gt;\n       &lt;boolean name=&quot;reread-seeds-on-config&quot;&gt;true&lt;/boolean&gt;\n       &lt;newObject name=&quot;decide-rules&quot; \nclass=&quot;org.archive.crawler.deciderules.DecideRuleSequence&quot;&gt;\n         &lt;map name=&quot;rules&quot;&gt;\n           &lt;newObject name=&quot;rejectByDefault&quot; \nclass=&quot;org.archive.crawler.deciderules.RejectDecideRule&quot;&gt;\n           &lt;/newObject&gt;\n           &lt;newObject name=&quot;acceptIfPrereq&quot; \nclass=&quot;org.archive.crawler.deciderules.PrerequisiteAcceptDecideRule&quot;&gt;\n           &lt;/newObject&gt;\n           &lt;newObject name=&quot;acceptHome&quot; \nclass=&quot;org.archive.crawler.deciderules.MatchesRegExpDecideRule&quot;&gt;\n             &lt;string name=&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n             &lt;string name=&quot;regexp&quot;&gt;.*/home&#92;?page=.*&lt;/string&gt;\n           &lt;/newObject&gt;\n           &lt;newObject name=&quot;acceptArticles&quot; \nclass=&quot;org.archive.crawler.deciderules.MatchesRegExpDecideRule&quot;&gt;\n             &lt;string name=&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n             &lt;string name=&quot;regexp&quot;&gt;.*/article/[0-9]*$&lt;/string&gt;\n           &lt;/newObject&gt;\n         &lt;/map&gt;\n       &lt;/newObject&gt;\n     &lt;/newObject&gt;\n\nAnd started with the http://www.citeulike.org/home seed.\n\nIt basically tries to go through pages like\nhttp://www.citeulike.org/home?page=1, http://www.citeulike.org/home?page=2,\n...\n\nand like\nhttp://www.citeulike.org/user/shung/article/4408244\nhttp://www.citeulike.org/user/VGreiff/article/4406467\n...\n\nThe latter ones are what you are looking for.\nThe former ones are also necessary as they contain links to the articles.\n\nIt&#39;s not a bad start as overhead (home pages) is rather small, you \nmostly get the articles. But a lot of improvements should be made:\n1. Reject content under pdf_options directory.\n2. Add tags to the crawling process in order to harvest more articles.\n3. But limit to the global tags (not user level tags like \nhttp://www.citeulike.org/user/VGreiff/tag/immunology).\n4. Try to avoid duplicate articles in group/user.\nAnd perhaps a lot more.\n\nRegards,\nSergey.\n\n&gt; First , thanks a lot for the reply !\n&gt; \n&gt; I am using 1.1.4.3 ( is there any advantage of moving to the new 2.0 ?)\n&gt; \n&gt; I want to crawl this site :www.citeulike.org , but the default will \n&gt; download all html , css, images etc\n&gt; \n&gt; I just need the HTML pages that are generated from this path for example\n&gt; \n&gt; http://www.citeulike.org/user/reyez/article/4406466 \n&gt; &lt;http://www.citeulike.org/user/reyez/article/4406466&gt;\n&gt; \n&gt; Adding HTML or HTM would generate the page also ?!\n&gt; http://www.citeulike.org/user/reyez/article/4406466.html \n&gt; &lt;http://www.citeulike.org/user/reyez/article/4406466.html&gt;\n&gt; http://www.citeulike.org/user/reyez/article/4406466.htm \n&gt; &lt;http://www.citeulike.org/user/reyez/article/4406466.htm&gt;\n&gt; \n&gt; So I used/tried\n&gt; \n&gt; http://www.citeulike.org/user/.*/article/.* \n&gt; &lt;http://www.citeulike.org/user/.*/article/.*&gt;\n&gt; \n&gt; and\n&gt; \n&gt; http://www.citeulike.org/user/.*/article/.* \n&gt; &lt;http://www.citeulike.org/user/.*/article/.*&gt;&#92;.html$\n&gt; \n&gt; with DecidingScope and MatchesRegExpDecideRule , and many other options \n&gt; but I didn&#39;t succeed. I got some images and some hosts that are other \n&gt; than citeulike.org, and some HTML pages that I don&#39;t need.\n&gt; \n&gt; So I add this to get only the HTML\n&gt; \n&gt; 1. Add the ContentTypeRegExFilter as a midfetch-filter and \n&gt; write-processor Archiver filter.\n&gt; \n&gt; 2. In the settings for both filters set the if-match-return to true, and \n&gt; use the regexp &#39;(?i)text/html.*&#39;\n&gt; \n&gt; 3) MatchesRegExpDecideRule\n&gt; REJECT\n&gt; \n&gt; .*(?i)&#92;.(a|ai|aif|aifc|aiff|asc|avi|bcpio|bin|bmp|bz2|c|cdf|cgi|cg&#92;\n&gt; m|class|cpio|cpp?|cpt|csh|css|cxx|dcr|dif|dir|djv|djvu|dll|dmg|dms|doc|dtd|dv|dv&#92;\n&gt; i|dxr|eps|etx|exe|ez|gif|gram|grxml|gtar|h|hdf|hqx|ice|ico|ics|ief|ifb|iges|igs|&#92;\n&gt; iso|jnlp|jp2|jpe|jpeg|jpg|js|kar|latex|lha|lzh|m3u|mac|man|mathml|me|mesh|mid|mi&#92;\n&gt; di|mif|mov|movie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|nc|o|oda|ogg|pbm|pct|p&#92;\n&gt; db|pdf|pgm|pgn|pic|pict|pl|png|pnm|pnt|pntg|ppm|ppt|ps|py|qt|qti|qtif|ra|ram|ras&#92;\n&gt; |rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|sgml|sh|shar|silo|sit|skd|skm|skp|skt|smi|smi&#92;\n&gt; l|snd|so|spl|src|srpm|sv4cpio|sv4crc|svg|swf|t|tar|tcl|tex|texi|texinfo|tgz|tif|&#92;\n&gt; tiff|tr|tsv|ustar|vcd|vrml|vxml|wav|wbmp|wbxml|wml|wmlc|wmls|wmlsc|wrl|xbm|xht|x&#92;\n&gt; html|xls|xml|xpm|xsl|xslt|xwd|xyz|z|zip)$\n&gt; \n&gt; This delayed the crawling process. (any other helpful idea ?)\n&gt; \n&gt; What I need now is how to download only that HTM/HTML pages from this path\n&gt; http://www.citeulike.org/user/.*/article/.* \n&gt; &lt;http://www.citeulike.org/user/.*/article/.*&gt;&#92;.html$\n&gt; \n&gt; ?\n&gt; \n&gt; Because I am getting some HTML pages from other links , example\n&gt; http://www.citeulike.org/user/birolilu/author/Siu:H \n&gt; &lt;http://www.citeulike.org/user/birolilu/author/Siu:H&gt;\n&gt; \n&gt; I added a list of rejected pages , in the scope\n&gt; \n&gt; http://www.citeulike.org/user/.*/author/.* \n&gt; &lt;http://www.citeulike.org/user/.*/author/.*&gt;\n&gt; http://www.citeulike.org/news &lt;http://www.citeulike.org/news&gt;\n&gt; http://www.citeulike.org/faq &lt;http://www.citeulike.org/faq&gt;\n&gt; http://www.citeulike.org/tag/internet \n&gt; &lt;http://www.citeulike.org/tag/internet&gt;\n&gt; http://www.citeulike.org/pdf_options &lt;http://www.citeulike.org/pdf_options&gt;\n&gt; http://www.citeulike.org/groups &lt;http://www.citeulike.org/groups&gt;\n&gt; http://www.citeulike.org/user/.*/tag/.* \n&gt; &lt;http://www.citeulike.org/user/.*/tag/.*&gt;\n&gt; http://www.citeulike.org/howto &lt;http://www.citeulike.org/howto&gt;\n&gt; http://www.citeulike.org/journals &lt;http://www.citeulike.org/journals&gt;\n&gt; \n&gt; But that stop the crawling !\n&gt; \n&gt; I added the NotMatchesRegExpDecideRule in the ARCWriterProcessor , but \n&gt; that didn&#39;t help aslo .\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com \n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Sergey Khenkin \n&gt; &lt;skhenkin@...&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; Hi,\n&gt;  &gt;\n&gt;  &gt; I&#39;m not sure which heritrix version you are using but in 1.14.3 you can\n&gt;  &gt; do this using DecidingScope and MatchesRegExpDecideRule with regexp like\n&gt;  &gt;\n&gt;  &gt; http://www.SITENAME/.*/TAG/.* &lt;http://www.SITENAME/.*/TAG/.*&gt;\n&gt;  &gt;\n&gt;  &gt; or maybe\n&gt;  &gt;\n&gt;  &gt; http://www.SITENAME/.*/TAG/.* &lt;http://www.SITENAME/.*/TAG/.*&gt;&#92;.html?$\n&gt;  &gt;\n&gt;  &gt; depending on what you want to achieve.\n&gt;  &gt;\n&gt;  &gt; But as far as I understand it is also important how links to these HTML\n&gt;  &gt; are gathered during the crawl. Most probably they are found inside other\n&gt;  &gt; HTML documents which won&#39;t be matched by the regexps above. This\n&gt;  &gt; should be addressed somehow.\n&gt;  &gt;\n&gt;  &gt; Regards,\n&gt;  &gt; Sergey\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; &gt; How can I download HTML files only from this path for example\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; http://www.SITENAME/MEMBERNAME &lt;http://www.SITENAME/MEMBERNAME&gt; \n&gt; &lt;http://www.SITENAME/MEMBERNAME &lt;http://www.SITENAME/MEMBERNAME&gt;&gt;(s)/TAG(S)/\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; I used regexp\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; http://www.SITENAME/.*/TAG &lt;http://www.SITENAME/.*/TAG&gt; \n&gt; &lt;http://www.SITENAME/.*/TAG &lt;http://www.SITENAME/.*/TAG&gt;&gt;(S)/\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; With URIregExpFilter in FilterDecideRule , but it didn&#39;t work , any\n&gt;  &gt; &gt; suggestion ?\n&gt;  &gt;\n&gt; \n&gt; \n\n"}}