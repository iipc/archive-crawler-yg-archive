{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"d1umPTgzIKTCjsaumNo-fjIinmOB1Pri_oEDB0_18iz32C1kBTZb6aDIWlm1KwuXUqM770m3rv87v1diG4rz_lscp1Vd11-z","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] BroadScope Crawl and out of memory error","postDate":"1113513057","msgId":1729,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyNUVEQzYxLjgwMzA4MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDE2OTkwLjUwNjI3LjkxMzQ4Mi40NTk0MTZAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","referencesHeader":"PGQzbWZ2NytoMnE1QGVHcm91cHMuY29tPiA8MTY5OTAuNTA2MjcuOTEzNDgyLjQ1OTQxNkB0aXBoYXJlcy5iYXNpc3RlY2gubmV0Pg=="},"prevInTopic":1725,"nextInTopic":1732,"prevInTime":1728,"nextInTime":1730,"topicId":1724,"numMessagesInTopic":14,"msgSnippet":"Hi Tom, That is pretty good! When you get a chance could you please check how many queues (hosts) are part of the crawl? Thanks. i.","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 52160 invoked from network); 14 Apr 2005 21:15:50 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m30.grp.scd.yahoo.com with QMQP; 14 Apr 2005 21:15:50 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta5.grp.scd.yahoo.com with SMTP; 14 Apr 2005 21:15:50 -0000\r\nReceived: (qmail 25439 invoked by uid 100); 14 Apr 2005 21:15:45 -0000\r\nReceived: from pauk.archive.org (HELO ?207.241.238.153?) (igor@...@207.241.238.153)\n  by mail-dev.archive.org with SMTP; 14 Apr 2005 21:15:45 -0000\r\nMessage-ID: &lt;425EDC61.8030807@...&gt;\r\nDate: Thu, 14 Apr 2005 14:10:57 -0700\r\nUser-Agent: Mozilla Thunderbird 0.7.3 (Windows/20040803)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;d3mfv7+h2q5@...&gt; &lt;16990.50627.913482.459416@...&gt;\r\nIn-Reply-To: &lt;16990.50627.913482.459416@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-53.8 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] BroadScope Crawl and out of memory error\r\nX-Yahoo-Group-Post: member; u=137477665\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Tom,\n\nThat is pretty good! When you get a chance could you please check how many queues (hosts) are part \nof the crawl?\nThanks.\n\ni.\n\n\n&gt; Broad=scope memory issues is a known problem. First off, see\n&gt; \n&gt; http://crawler.archive.org/faq.html#oome_broadcrawl\n&gt; \n&gt; Secondly, I&#39;d grab a 1.3 snapshot and try using that with the\n&gt; BdbFrontier --- it helps a lot.\n&gt; \n&gt; Finally, not that it is relevant for your specific case, I&#39;ve been\n&gt; running a crawl for the last 7 days over 60 seeds using a set of\n&gt; custom SURT prefixes (including some rather open-ended ones, like\n&gt; &quot;http://(cn,org,&quot;).\n&gt; \n&gt; So far I&#39;ve crawled 4,091,915 documents out of 10,610,687 discovered\n&gt; (so far) for 62 GB of data (I&#39;m just saving HTML), running 75\n&gt; toe-threads with a 512MB heap. This is with a (now 7 day old) snapshot\n&gt; of CVS head. Heritrix is chugging along just fine at around 9.2 docs\n&gt; per second, 158 KB/sec, on an old 666 MHz i686.\n&gt; \n&gt; \n\n\n"}}