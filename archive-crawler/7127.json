{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":479368905,"authorName":"David Stafen","from":"David Stafen &lt;stafend@...&gt;","profile":"stafend","replyTo":"LIST","senderId":"bcn5b8Zf9frQ4OVXEvXTONnUv2zkUaj1zbilAyQZkl_6PXhK0Xj0MZ6MWnMoBr5tEDK98we4jMliyNyQWvAX1Mwoe3bf1Yc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"is &quot;Requires 5 Gigabytes for 1 billion URLs&quot;  true?","postDate":"1303568189","msgId":7127,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDI5NDQ1Ni45MTY3OC5xbUB3ZWIxMjE2MTYubWFpbC5uZTEueWFob28uY29tPg=="},"prevInTopic":0,"nextInTopic":7128,"prevInTime":7126,"nextInTime":7128,"topicId":7127,"numMessagesInTopic":2,"msgSnippet":"Hi to all I m using heritrix 1.14.4 on a system with 16 Gigabyte Ram and i assigned 10 Gigabyte heap with 100 threads to heritrix, it is interesting; up to now","rawEmail":"Return-Path: &lt;stafend@...&gt;\r\nX-Sender: stafend@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 79665 invoked from network); 23 Apr 2011 14:16:30 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m6.grp.sp2.yahoo.com with QMQP; 23 Apr 2011 14:16:30 -0000\r\nX-Received: from unknown (HELO nm2.bullet.mail.ne1.yahoo.com) (98.138.90.65)\n  by mta2.grp.sp2.yahoo.com with SMTP; 23 Apr 2011 14:16:29 -0000\r\nX-Received: from [98.138.90.54] by nm2.bullet.mail.ne1.yahoo.com with NNFMP; 23 Apr 2011 14:16:29 -0000\r\nX-Received: from [98.138.89.234] by tm7.bullet.mail.ne1.yahoo.com with NNFMP; 23 Apr 2011 14:16:29 -0000\r\nX-Received: from [127.0.0.1] by omp1049.mail.ne1.yahoo.com with NNFMP; 23 Apr 2011 14:16:29 -0000\r\nX-Yahoo-Newman-Property: ymail-3\r\nX-Yahoo-Newman-Id: 386334.54816.bm@...\r\nX-Received: (qmail 94878 invoked by uid 60001); 23 Apr 2011 14:16:29 -0000\r\nMessage-ID: &lt;294456.91678.qm@...&gt;\r\nX-YMail-OSG: r6X9PMEVM1ldhu_SfOjSLCwLj.TULR1hq8pQ4uqz7rPlbPV\n oJLbk6RzBTA9lixeNYllUBK2DlqV45Z0tKWptqx9HnppMy3C0NWVYsWiplfz\n Fl_CVcwXt7ykmxpqm96EBZbP5Y2nrXSydG86m6z5qiSo3bAFE5mOPJiwGB1u\n Cq0Y.ETRD7SDrP9wWFzW5dMKceL9FZt5n3rFFvn5YdZj5_6gzIRtdA.6B.BN\n QSgSQoU8ZDzUdiNsC1jQZdLtDPOX42V95.PrxNe6AQkhY.eMicLnGSjd4NMC\n vdlOucEKygpd6thsoevLQ_ZaaNJZgqK5gW4DXIpqZZzMgHf5E88wU\r\nX-Received: from [85.185.163.134] by web121616.mail.ne1.yahoo.com via HTTP; Sat, 23 Apr 2011 07:16:29 PDT\r\nX-Mailer: YahooMailRC/559 YahooMailWebService/0.8.110.299900\r\nDate: Sat, 23 Apr 2011 07:16:29 -0700 (PDT)\r\nTo: archive-crawler@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-1968309455-1303568189=:91678&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Stafen &lt;stafend@...&gt;\r\nSubject: is &quot;Requires 5 Gigabytes for 1 billion URLs&quot;  true?\r\nX-Yahoo-Group-Post: member; u=479368905; y=WT4zEjdlFRoG29tc2LNRfqLZIWT4SisxyCY9HrSNCXdepQ\r\nX-Yahoo-Profile: stafend\r\n\r\n\r\n--0-1968309455-1303568189=:91678\r\nContent-Type: text/plain; charset=us-ascii\r\n\r\n\n\n\n\nHi to all\nI&#39;m using heritrix 1.14.4 on a system with 16 Gigabyte Ram and i assigned 10 \nGigabyte heap with 100 threads to heritrix, it is interesting; up to now i \ndownloaded 30,000,000 web pages and only 60,000,000  URLs are queued . The HDD \nLED is always on and something like swapping is happening, at first i thought it \nhas low memory (heap) but i found a document that reject this theory.\n&quot;Represent URL by 8-byte checksum.  Maintain in-memory hash table of URLs. \nRequires 5 Gigabytes for 1 billion URLs.&quot;\nI have a good  bandwidth (16 MB) but only at the beginning of the crawling \nprocess i could consume all of the bandwidth and now only 6MB is consumed and my \ndownload rate decreased. i have only two decide rules (only URLs in  a specific \ndomain with &quot;TEXT/HTML&quot; content are accepted).  I wanna to crawl 200,000,000 web \npages, but in this situation what should i do? do i need a system with stronger \nresource?\n\r\n--0-1968309455-1303568189=:91678\r\nContent-Type: text/html; charset=us-ascii\r\n\r\n&lt;html&gt;&lt;head&gt;&lt;style type=&quot;text/css&quot;&gt;&lt;!-- DIV {margin:0px;} --&gt;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div style=&quot;font-family:times new roman,new york,times,serif;font-size:12pt&quot;&gt;&lt;br&gt;\n&lt;div style=&quot;font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;&lt;div style=&quot;font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;&lt;div style=&quot;font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;&lt;div style=&quot;font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;&lt;div style=&quot;font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;&lt;br&gt;&lt;div style=&quot;font-family: times new roman,new york,times,serif; font-size: 12pt;&quot;&gt;&lt;div&gt;Hi to all&lt;br&gt;I&#39;m using heritrix 1.14.4 on a system with 16 Gigabyte Ram and i assigned 10 Gigabyte heap with 100 threads to heritrix, it is interesting; up to now i downloaded 30,000,000 web pages and only 60,000,000&nbsp; URLs are queued . The HDD LED is always on and something like swapping is happening, at first i thought it has low memory (heap) but i found a document that reject this theory.&lt;div class=&quot;O&quot;&gt;\n\n&lt;div style=&quot;&quot;&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;&quot;Represent URL by 8-byte checksum.&lt;span style=&quot;&quot;&gt;&nbsp; &lt;/span&gt;Maintain in-memory hash table &lt;/span&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;of URLs.\n&lt;/span&gt;&lt;/div&gt;\n\n&lt;div style=&quot;&quot;&gt;&lt;span style=&quot;font-size: 24pt;&quot;&gt;Requires 5 Gigabytes for 1 billion URLs.&lt;/span&gt;&quot;&lt;br&gt;I have a good&nbsp; bandwidth (16 MB) but only at the beginning of the crawling process i could consume all of the bandwidth and now only 6MB is consumed and my download rate decreased. i have only two decide rules (only URLs in&nbsp; a specific domain with &quot;TEXT/HTML&quot; content are accepted).&nbsp; I wanna to crawl 200,000,000 web pages, but in this situation what should i do? do i need a system with stronger resource?&lt;br&gt;&lt;/div&gt;\n\n&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; \n&lt;/div&gt;&lt;meta http-equiv=&quot;x-dns-prefetch-control&quot; content=&quot;on&quot;&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\r\n--0-1968309455-1303568189=:91678--\r\n\n"}}