{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":189012361,"authorName":"Laurian Gridinoc","from":"&quot;Laurian Gridinoc&quot; &lt;laurian@...&gt;","profile":"lauriangridinoc","replyTo":"LIST","senderId":"PpwGEiq8VFXZFecHgk3COr8FefDxO5wmxmKRvz1Srrhnn_I0_k6_h7BP1F_UYWtGkFiGsBZdNpf-wc8dP3XnnuUtsE1PCjqpVpzDx8s","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Inserting information to MYSQL during crawl","postDate":"1158135428","msgId":3280,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDk3ODJlMzM1MDYwOTEzMDExN3AyMmY2MDZhMW82NjFlZTYwNGI3NDA5YWFlQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PDIwMDYwOTEzMDU0NjU4LjEyMjY1LnFtYWlsQHdlYjUwODAzLm1haWwueWFob28uY29tPg==","referencesHeader":"PDIwMDYwOTEzMDU0NjU4LjEyMjY1LnFtYWlsQHdlYjUwODAzLm1haWwueWFob28uY29tPg=="},"prevInTopic":3279,"nextInTopic":3281,"prevInTime":3279,"nextInTime":3281,"topicId":507,"numMessagesInTopic":19,"msgSnippet":"Hello, ... Then, do something like this: package my.writer; import org.archive.crawler.datamodel.CoreAttributeConstants; import","rawEmail":"Return-Path: &lt;laurian@...&gt;\r\nX-Sender: laurian@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 72050 invoked from network); 13 Sep 2006 08:17:14 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m39.grp.scd.yahoo.com with QMQP; 13 Sep 2006 08:17:14 -0000\r\nReceived: from unknown (HELO nz-out-0102.google.com) (64.233.162.199)\n  by mta8.grp.scd.yahoo.com with SMTP; 13 Sep 2006 08:17:14 -0000\r\nReceived: by nz-out-0102.google.com with SMTP id j2so863461nzf\n        for &lt;archive-crawler@yahoogroups.com&gt;; Wed, 13 Sep 2006 01:17:09 -0700 (PDT)\r\nReceived: by 10.65.139.9 with SMTP id r9mr5134685qbn;\n        Wed, 13 Sep 2006 01:17:09 -0700 (PDT)\r\nReceived: by 10.64.3.18 with HTTP; Wed, 13 Sep 2006 01:17:08 -0700 (PDT)\r\nMessage-ID: &lt;9782e3350609130117p22f606a1o661ee604b7409aae@...&gt;\r\nDate: Wed, 13 Sep 2006 09:17:08 +0100\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;20060913054658.12265.qmail@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;20060913054658.12265.qmail@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;Laurian Gridinoc&quot; &lt;laurian@...&gt;\r\nSubject: Re: [archive-crawler] Inserting information to MYSQL during crawl\r\nX-Yahoo-Group-Post: member; u=189012361; y=VFj3J6ldtBF8vwgM1RnytwHyghjEQVuyCCyGn6J2d_WHxt4UeRNvE9k2\r\nX-Yahoo-Profile: lauriangridinoc\r\n\r\nHello,\n\nOn 13/09/06, Joshi Sudhindra &lt;jsudhindra@...&gt; wrote:\n&gt;  I was told to write a processor in heritrix so that it\n&gt;  should take url and string as input and process the\n&gt;  crawling if page contains the given string then that\n&gt;  link should be written to mysql.Can anybody help me in\n&gt;  this regard?\n&gt;\n&gt;  I am new to java. I donn&#39;t khow where to modify the\n&gt;  heritrix code and configure the same in heritrix.\n&gt;\n&gt;  I don&#39;t want to store the content .i just want to\n&gt;  store the matched link.\n\nThen, do something like this:\n\npackage my.writer;\n\nimport org.archive.crawler.datamodel.CoreAttributeConstants;\nimport org.archive.crawler.datamodel.CrawlURI;\nimport org.archive.crawler.framework.Processor;\nimport org.archive.net.UURI;\n\npublic class SQLWriterProcessor extends Processor implements\nCoreAttributeConstants {\n\n\n    /** Creates a new instance of SQLWriterProcessor */\n    public SQLWriterProcessor (String name) {\n        super(name, &quot;CacheWriter processor. &quot; +\n                    &quot;A writer that writes each URL to DB&quot;);\n        try {\n            //or whatever DB init code you need, you may also put some\ncleanup code in the destructor\n            myDataBaseManager = new MyDataBaseManager();\n            myDataBaseManager.connect(&quot;localhost&quot;, &quot;db&quot;, &quot;user&quot;, &quot;pass&quot;);\n        } catch (SQLException ex) {\n            ex.printStackTrace();\n        }\n    }\n\n    protected void innerProcess(CrawlURI curi) {\n        // process only HTTP 2XX, check also for CrawlURI.isSuccess() docs\n        if ( ! curi.is2XXSuccess() ) {\n            return;\n        }\n        UURI uuri = curi.getUURI(); // Current URI.\n\n        // Only http and https schemes are supported.\n        String scheme = uuri.getScheme();\n        if (    ! &quot;http&quot;.equalsIgnoreCase(scheme)\n             && ! &quot;https&quot;.equalsIgnoreCase(scheme) ) {\n            return;\n        }\n\n       myDataBaseManager.logURI(uuri.toString());\n    } //end of method\n\n} // end of class\n\nThen add this into the classpath, and add a line in\nconf/modules/processor.options like this:\n\nmy.writer.SQLWriterProcessor|SQLWriter\n\nsuch that you&#39;ll have it in the WUI in the pull-down list of processors.\n\nnote that conf/modules/processor.options is in the heritrix jar file,\nyou may need to unpack, modify and repack.\n\nCheers,\n-- \nLaurian Gridinoc, purl.org/net/laur\n\n"}}