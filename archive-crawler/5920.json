{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":406177353,"authorName":"Ignacio Garcia","from":"Ignacio Garcia &lt;igc.csmail@...&gt;","replyTo":"LIST","senderId":"quw6jAS6j-IbzpLNiMJMwh2IScKgc3dP8czRgwvCiZgBXvbypLgGqH_oIfcYc6WxuphviV9OdhjrIvb7tjSiv9qGgVqF2oApADDEWg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Recrawl Issue (Heritrix 2.0.2)","postDate":"1247588448","msgId":5920,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PDE5N2I1YjIxMDkwNzE0MDkyMGkzYzdjNmUzYXZjZGY2OWU2MDFhM2EzZWIyQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":0,"nextInTopic":5925,"prevInTime":5919,"nextInTime":5921,"topicId":5920,"numMessagesInTopic":3,"msgSnippet":"Hello, We are trying to set up dedupe in our crawls right now but we are having some issues launching the Recrawl jobs. We have no problems adding the","rawEmail":"Return-Path: &lt;igc.csmail@...&gt;\r\nReceived: (qmail 11999 invoked from network); 14 Jul 2009 16:23:16 -0000\r\nReceived: from unknown (98.137.34.46)\n  by m5.grp.sp2.yahoo.com with QMQP; 14 Jul 2009 16:23:16 -0000\r\nReceived: from unknown (HELO n46b.bullet.mail.sp1.yahoo.com) (66.163.168.160)\n  by mta3.grp.sp2.yahoo.com with SMTP; 14 Jul 2009 16:23:16 -0000\r\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoogroups.com; s=lima; t=1247588586; bh=iaZ844dI6N10KXKDCVwxZQhtcqhdkN0A5jQe41uriBg=; h=Received:Received:X-Sender:X-Apparently-To:X-Received:X-Received:X-Received:X-Received:MIME-Version:X-Received:Date:Message-ID:To:Content-Type:X-Originating-IP:X-eGroups-Msg-Info:From:Subject:X-Yahoo-Group-Post:X-YGroups-SubInfo:Sender:X-Yahoo-Newman-Property:X-eGroups-Approved-By:X-eGroups-Auth; b=BacOzJlckVKm6NSvupQ2rXbbqaBft7l+ufthA8cFi98vEGpjOANPybqt6D07TIzvMeJqRaH1GMFu+FKLzT0Z7+81sToS1QgBFcodSrqBbyxTM530M4fJfP/TbuePAggz\r\nReceived: from [69.147.65.151] by n46.bullet.mail.sp1.yahoo.com with NNFMP; 14 Jul 2009 16:23:06 -0000\r\nReceived: from [98.137.35.13] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 14 Jul 2009 16:23:06 -0000\r\nX-Sender: igc.csmail@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 25619 invoked from network); 14 Jul 2009 16:21:48 -0000\r\nX-Received: from unknown (69.147.108.201)\n  by m3.grp.re1.yahoo.com with QMQP; 14 Jul 2009 16:21:48 -0000\r\nX-Received: from unknown (HELO mail-qy0-f203.google.com) (209.85.221.203)\n  by mta2.grp.re1.yahoo.com with SMTP; 14 Jul 2009 16:21:48 -0000\r\nX-Received: by qyk41 with SMTP id 41so2854971qyk.29\n        for &lt;archive-crawler@yahoogroups.com&gt;; Tue, 14 Jul 2009 09:20:48 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.224.3.10 with SMTP id 10mr3932822qal.56.1247588448056; Tue, 14 \n\tJul 2009 09:20:48 -0700 (PDT)\r\nDate: Tue, 14 Jul 2009 12:20:48 -0400\r\nMessage-ID: &lt;197b5b210907140920i3c7c6e3avcdf69e601a3a3eb2@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=0015175ca95245b828046eacd29e\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Ignacio Garcia &lt;igc.csmail@...&gt;\r\nSubject: Recrawl Issue (Heritrix 2.0.2)\r\nX-Yahoo-Group-Post: member; u=406177353\r\nX-YGroups-SubInfo: t=0;f=0;g=none;\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Approved-By: stearcorg &lt;steve@...&gt; via web; 14 Jul 2009 16:23:06 -0000\r\n\r\n\r\n--0015175ca95245b828046eacd29e\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\nHello,\n\nWe are trying to set up dedupe in our crawls right now but we are having\nsome issues launching the Recrawl jobs.\nWe have no problems adding the processors needed for the initial crawl:\n\n   1. FetchHistoryProcessor, after the last fetching processor.\n   2. PersistStoreProcessor, after all other processors.\n\nWhen that first job is finished, a grep &quot;uri_history&quot; on the state files\nshows that one of the jdb files matches, so it seems that some information\nis being written to it.\nAfter that, we are creating a new ready job, using the completed initial\ncrawl as the base (so that the state is copied over) and adding the required\nload processor:\n\n   1. PersistLoadProcessor, before any fetch processor\n\nWe also keep the FetchHistory and PersistStore, to have a cumulative history\nof the crawls\n\nHowever, when we try to start this Recrawl job, with all three processors, I\nget the following exception:\n\njavax.management.ReflectionException\n\nnull\n\njavax.management.ReflectionException\n\tat org.archive.settings.jmx.LoggingDynamicMBean.dealWithException(LoggingDynamicMBean.java:77)\n\tat org.archive.settings.jmx.LoggingDynamicMBean.invoke(LoggingDynamicMBean.java:160)\n\n\tat com.sun.jmx.mbeanserver.DynamicMetaDataImpl.invoke(Unknown Source)\n\tat com.sun.jmx.mbeanserver.MetaDataImpl.invoke(Unknown Source)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)\n\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)\n\tat org.archive.openmbeans.annotations.BeanProxy.invoke(BeanProxy.java:87)\n\tat $Proxy9.launchJob(Unknown Source)\n\tat org.archive.crawler.webui.CrawlerArea.launch(CrawlerArea.java:155)\n\n\tat jsp.crawler_005farea.do_005flaunch_jsp._jspService(jsp.crawler_005farea.do_005flaunch_jsp:45)\n\tat org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:809)\n\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:459)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1054)\n\tat org.archive.crawler.webui.AuthFilter.doFilter(AuthFilter.java:79)\n\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1045)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:358)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:231)\n\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:629)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:453)\n\tat org.mortbay.jetty.handler.HandlerList.handle(HandlerList.java:49)\n\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:141)\n\tat org.mortbay.jetty.Server.handle(Server.java:303)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:452)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:721)\n\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:509)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:209)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:349)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:217)\n\n\tat org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:475)\nCaused by: java.lang.IllegalStateException\n\t... 30 more\n\nI don&#39;t know where the problem is, since I am simply adding the Persist Load\nprocessor, and the &quot;sheet&quot; looks good when viewed via the webUI, no errors\nor missing fields.\n\nAs detailed in the Features notes for Heritrix 1.12.0, I am adding the\nprocessors in their places, but I am still getting this error.\nHere is a copy of my global.sheet, in case anyone sees a problem with it:\n\nroot=map, java.lang.Object\n&gt; root:metadata=primary, org.archive.modules.writer.DefaultMetadataProvider\n&gt; root:metadata:description=string, deep seed crawl\n&gt; root:metadata:operator-contact-url=string, http://www.loc.gov/webcapture/\n&gt; root:metadata:robots-honoring-policy=primary,\n&gt; org.archive.modules.net.RobotsHonoringPolicy\n&gt; root:metadata:robots-honoring-policy:type=enum,\n&gt; org.archive.modules.net.RobotsHonoringPolicy$Type-IGNORE\n&gt; root:metadata:robots-honoring-policy:user-agents=list, java.lang.String\n&gt; root:loggerModule=primary,\n&gt; org.archive.crawler.framework.CrawlerLoggerModule\n&gt; root:seeds=primary, org.archive.modules.seeds.SeedModuleImpl\n&gt; root:scope=object, org.archive.modules.deciderules.DecideRuleSequence\n&gt; root:scope:rules=list, org.archive.modules.deciderules.DecideRule\n&gt; root:scope:rules:0=object, org.archive.modules.deciderules.AcceptDecideRule\n&gt; root:scope:rules:1=object,\n&gt; org.archive.modules.deciderules.surt.SurtPrefixedDecideRule\n&gt; root:scope:rules:2=object,\n&gt; org.archive.modules.deciderules.TransclusionDecideRule\n&gt; root:scope:rules:2:max-speculative-hops=int, 1\n&gt; root:scope:rules:2:max-trans-hops=int, 1\n&gt; root:scope:rules:3=object,\n&gt; org.archive.modules.deciderules.TooManyPathSegmentsDecideRule\n&gt; root:scope:rules:3:max-path-depth=int, 1\n&gt; root:scope:rules:4=object,\n&gt; org.archive.modules.deciderules.PrerequisiteAcceptDecideRule\n&gt; root:uriUniqFilter=primary, org.archive.crawler.util.BdbUriUniqFilter\n&gt; root:queue-assignment-policy=primary,\n&gt; org.archive.crawler.frontier.SurtAuthorityQueueAssignmentPolicy\n&gt; root:server-cache=primary, org.archive.modules.net.BdbServerCache\n&gt; root:credential-store=primary,\n&gt; org.archive.modules.credential.CredentialStore\n&gt; root:controller=primary, org.archive.crawler.framework.CrawlControllerImpl\n&gt; root:controller:frontier=primary, org.archive.crawler.frontier.BdbFrontier\n&gt; root:controller:frontier:rules=list,\n&gt; org.archive.modules.canonicalize.CanonicalizationRule\n&gt; root:controller:frontier:rules:0=object,\n&gt; org.archive.modules.canonicalize.LowercaseRule\n&gt; root:controller:frontier:rules:1=object,\n&gt; org.archive.modules.canonicalize.StripUserinfoRule\n&gt; root:controller:frontier:rules:2=object,\n&gt; org.archive.modules.canonicalize.StripWWWNRule\n&gt; root:controller:frontier:rules:3=object,\n&gt; org.archive.modules.canonicalize.StripSessionIDs\n&gt; root:controller:frontier:rules:4=object,\n&gt; org.archive.modules.canonicalize.StripSessionCFIDs\n&gt; root:controller:frontier:rules:5=object,\n&gt; org.archive.modules.canonicalize.FixupQueryStr\n&gt; root:controller:frontier:scope=reference, root:scope\n&gt; root:controller:processors=map, org.archive.modules.Processor\n&gt; root:controller:processors:Preselector=object,\n&gt; org.archive.crawler.prefetch.Preselector\n&gt; root:controller:processors:Preselector:scope=reference, root:scope\n&gt; root:controller:processors:Preprocessor=object,\n&gt; org.archive.crawler.prefetch.PreconditionEnforcer\n&gt; root:controller:processors:Load=object,\n&gt; org.archive.modules.recrawl.PersistLoadProcessor\n&gt; root:controller:processors:DNS=object, org.archive.modules.fetcher.FetchDNS\n&gt; root:controller:processors:HTTP=object,\n&gt; org.archive.modules.fetcher.FetchHTTP\n&gt; root:controller:processors:History=object,\n&gt; org.archive.modules.recrawl.FetchHistoryProcessor\n&gt; root:controller:processors:HTTP:accept-headers=list, java.lang.String\n&gt; root:controller:processors:HTTP:midfetch-rules=object,\n&gt; org.archive.modules.deciderules.DecideRuleSequence\n&gt; root:controller:processors:ExtractorHTTP=object,\n&gt; org.archive.modules.extractor.ExtractorHTTP\n&gt; root:controller:processors:ExtractorHTML=object,\n&gt; org.archive.modules.extractor.ExtractorHTML\n&gt; root:controller:processors:ExtractorCSS=object,\n&gt; org.archive.modules.extractor.ExtractorCSS\n&gt; root:controller:processors:ExtractorJS=object,\n&gt; org.archive.modules.extractor.ExtractorJS\n&gt; root:controller:processors:ExtractorSWF=object,\n&gt; org.archive.modules.extractor.ExtractorSWF\n&gt; root:controller:processors:Archiver=object,\n&gt; org.archive.modules.writer.WARCWriterProcessor\n&gt; root:controller:processors:Archiver:prefix=string, lawsc\n&gt; root:controller:processors:Updater=object,\n&gt; org.archive.crawler.postprocessor.CrawlStateUpdater\n&gt; root:controller:processors:LinksScoper=object,\n&gt; org.archive.crawler.postprocessor.LinksScoper\n&gt; root:controller:processors:LinksScoper:logger-module=auto,\n&gt; root:loggerModule\n&gt; root:controller:processors:LinksScoper:scope=reference, root:scope\n&gt; root:controller:processors:Scheduler=object,\n&gt; org.archive.crawler.postprocessor.FrontierScheduler\n&gt; root:controller:processors:Store=object,\n&gt; org.archive.modules.recrawl.PersistStoreProcessor\n&gt; root:controller:statistics-tracker=object,\n&gt; org.archive.crawler.framework.StatisticsTrackerImpl\n&gt;\n\nWe are using Heritrix 2.0.2, in case that makes a difference, or it is known\nto have problems with the dedupe process.\n\nThank you very much.\n\r\n--0015175ca95245b828046eacd29e\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHello,&lt;br&gt;&lt;br&gt;We are trying to set up dedupe in our crawls right now but we=\r\n are having some issues launching the Recrawl jobs.&lt;br&gt;We have no problems =\r\nadding the processors needed for the initial crawl:&lt;br&gt;&lt;ol&gt;&lt;li&gt;FetchHistory=\r\nProcessor, after the last fetching processor.&lt;/li&gt;\n&lt;li&gt;PersistStoreProcesso=\r\nr, after all other processors.&lt;/li&gt;&lt;/ol&gt;When\nthat first job is finished, a =\r\ngrep &quot;uri_history&quot; on the state files\nshows that one of the jdb f=\r\niles matches, so it seems that some\ninformation is being written to it.&lt;br&gt;=\r\n\nAfter that, we are creating a new ready job, using the completed\ninitial c=\r\nrawl as the base (so that the state is copied over) and adding\nthe required=\r\n load processor:&lt;br&gt;&lt;ol&gt;&lt;li&gt;PersistLoadProcessor, before any fetch processo=\r\nr&lt;/li&gt;&lt;/ol&gt;We also keep the FetchHistory and PersistStore, to have a cumula=\r\ntive history of the crawls&lt;br&gt;&lt;br&gt;However, when we try to start this Recraw=\r\nl job, with all three processors, I get the following exception:&lt;br&gt;\n&lt;br&gt;&lt;p=\r\nre&gt;&lt;h3 style=3D&quot;color: red;&quot;&gt;javax.management.ReflectionException&lt;/h3&gt;&lt;/pre=\r\n&gt;\n\n&lt;pre&gt;null&lt;p&gt;javax.management.ReflectionException&lt;br&gt;\tat org.archive.sett=\r\nings.jmx.LoggingDynamicMBean.dealWithException(LoggingDynamicMBean.java:77)=\r\n&lt;br&gt;\tat org.archive.settings.jmx.LoggingDynamicMBean.invoke(LoggingDynamicM=\r\nBean.java:160)&lt;br&gt;\n&lt;br&gt;\tat com.sun.jmx.mbeanserver.DynamicMetaDataImpl.invo=\r\nke(Unknown Source)&lt;br&gt;\tat com.sun.jmx.mbeanserver.MetaDataImpl.invoke(Unkno=\r\nwn Source)&lt;br&gt;\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.inv=\r\noke(Unknown Source)&lt;br&gt;\n&lt;br&gt;\tat com.sun.jmx.mbeanserver.JmxMBeanServer.invo=\r\nke(Unknown Source)&lt;br&gt;\tat org.archive.openmbeans.annotations.BeanProxy.invo=\r\nke(BeanProxy.java:87)&lt;br&gt;\tat $Proxy9.launchJob(Unknown Source)&lt;br&gt;\tat org.a=\r\nrchive.crawler.webui.CrawlerArea.launch(CrawlerArea.java:155)&lt;br&gt;\n&lt;br&gt;\tat j=\r\nsp.crawler_005farea.do_005flaunch_jsp._jspService(jsp.crawler_005farea.do_0=\r\n05flaunch_jsp:45)&lt;br&gt;\tat org.apache.jasper.runtime.HttpJspBase.service(Http=\r\nJspBase.java:97)&lt;br&gt;\tat javax.servlet.http.HttpServlet.service(HttpServlet.=\r\njava:809)&lt;br&gt;\n&lt;br&gt;\tat org.mortbay.jetty.servlet.ServletHolder.handle(Servle=\r\ntHolder.java:459)&lt;br&gt;\tat org.mortbay.jetty.servlet.ServletHandler$CachedCha=\r\nin.doFilter(ServletHandler.java:1054)&lt;br&gt;\tat org.archive.crawler.webui.Auth=\r\nFilter.doFilter(AuthFilter.java:79)&lt;br&gt;\n&lt;br&gt;\tat org.mortbay.jetty.servlet.S=\r\nervletHandler$CachedChain.doFilter(ServletHandler.java:1045)&lt;br&gt;\tat org.mor=\r\ntbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:358)&lt;br&gt;\tat or=\r\ng.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:231)&lt;br&gt;\n=\r\n&lt;br&gt;\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java=\r\n:629)&lt;br&gt;\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.ja=\r\nva:453)&lt;br&gt;\tat org.mortbay.jetty.handler.HandlerList.handle(HandlerList.jav=\r\na:49)&lt;br&gt;\n&lt;br&gt;\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWr=\r\napper.java:141)&lt;br&gt;\tat org.mortbay.jetty.Server.handle(Server.java:303)&lt;br&gt;=\r\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:452)=\r\n&lt;br&gt;\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(Http=\r\nConnection.java:721)&lt;br&gt;\n&lt;br&gt;\tat org.mortbay.jetty.HttpParser.parseNext(Htt=\r\npParser.java:509)&lt;br&gt;\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpPa=\r\nrser.java:209)&lt;br&gt;\tat org.mortbay.jetty.HttpConnection.handle(HttpConnectio=\r\nn.java:349)&lt;br&gt;\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(Soc=\r\nketConnector.java:217)&lt;br&gt;\n&lt;br&gt;\tat org.mortbay.thread.BoundedThreadPool$Poo=\r\nlThread.run(BoundedThreadPool.java:475)&lt;br&gt;Caused by: java.lang.IllegalStat=\r\neException&lt;br&gt;\t... 30 more&lt;br&gt;&lt;/p&gt;&lt;/pre&gt;I\ndon&#39;t know where the problem =\r\nis, since I am simply adding the Persist\nLoad processor, and the &quot;shee=\r\nt&quot; looks good when viewed via the webUI,\nno errors or missing fields.&lt;=\r\nbr&gt;\n&lt;br&gt;As detailed in the Features notes for Heritrix 1.12.0, I am adding\n=\r\nthe processors in their places, but I am still getting this error.&lt;br&gt;Here =\r\nis a copy of my global.sheet, in case anyone sees a problem with it:&lt;br&gt;&lt;br=\r\n&gt;\n&lt;blockquote style=3D&quot;border-left: 1px solid rgb(204, 204, 204); margin: 0=\r\npt 0pt 0pt 0.8ex; padding-left: 1ex;&quot; class=3D&quot;gmail_quote&quot;&gt;root=3Dmap, jav=\r\na.lang.Object&lt;br&gt;root:metadata=3Dprimary, org.archive.modules.writer.Defaul=\r\ntMetadataProvider&lt;br&gt;\n\nroot:metadata:description=3Dstring, deep seed crawl&lt;=\r\nbr&gt;root:metadata:operator-contact-url=3Dstring, &lt;a href=3D&quot;http://www.loc.g=\r\nov/webcapture/&quot; target=3D&quot;_blank&quot;&gt;http://www.loc.gov/webcapture/&lt;/a&gt;&lt;br&gt;roo=\r\nt:metadata:robots-honoring-policy=3Dprimary, org.archive.modules.net.Robots=\r\nHonoringPolicy&lt;br&gt;\n\nroot:metadata:robots-honoring-policy:type=3Denum, org.a=\r\nrchive.modules.net.RobotsHonoringPolicy$Type-IGNORE&lt;br&gt;root:metadata:robots=\r\n-honoring-policy:user-agents=3Dlist, java.lang.String&lt;br&gt;root:loggerModule=\r\n=3Dprimary, org.archive.crawler.framework.CrawlerLoggerModule&lt;br&gt;\n\nroot:see=\r\nds=3Dprimary, org.archive.modules.seeds.SeedModuleImpl&lt;br&gt;root:scope=3Dobje=\r\nct, org.archive.modules.deciderules.DecideRuleSequence&lt;br&gt;root:scope:rules=\r\n=3Dlist, org.archive.modules.deciderules.DecideRule&lt;br&gt;root:scope:rules:0=\r\n=3Dobject, org.archive.modules.deciderules.AcceptDecideRule&lt;br&gt;\n\nroot:scope=\r\n:rules:1=3Dobject, org.archive.modules.deciderules.surt.SurtPrefixedDecideR=\r\nule&lt;br&gt;root:scope:rules:2=3Dobject, org.archive.modules.deciderules.Transcl=\r\nusionDecideRule&lt;br&gt;root:scope:rules:2:max-speculative-hops=3Dint, 1&lt;br&gt;\n\nro=\r\not:scope:rules:2:max-trans-hops=3Dint, 1&lt;br&gt;root:scope:rules:3=3Dobject, or=\r\ng.archive.modules.deciderules.TooManyPathSegmentsDecideRule&lt;br&gt;root:scope:r=\r\nules:3:max-path-depth=3Dint, 1&lt;br&gt;root:scope:rules:4=3Dobject, org.archive.=\r\nmodules.deciderules.PrerequisiteAcceptDecideRule&lt;br&gt;\n\nroot:uriUniqFilter=3D=\r\nprimary, org.archive.crawler.util.BdbUriUniqFilter&lt;br&gt;root:queue-assignment=\r\n-policy=3Dprimary, org.archive.crawler.frontier.SurtAuthorityQueueAssignmen=\r\ntPolicy&lt;br&gt;root:server-cache=3Dprimary, org.archive.modules.net.BdbServerCa=\r\nche&lt;br&gt;\n\nroot:credential-store=3Dprimary, org.archive.modules.credential.Cr=\r\nedentialStore&lt;br&gt;root:controller=3Dprimary, org.archive.crawler.framework.C=\r\nrawlControllerImpl&lt;br&gt;root:controller:frontier=3Dprimary, org.archive.crawl=\r\ner.frontier.BdbFrontier&lt;br&gt;\n\nroot:controller:frontier:rules=3Dlist, org.arc=\r\nhive.modules.canonicalize.CanonicalizationRule&lt;br&gt;root:controller:frontier:=\r\nrules:0=3Dobject, org.archive.modules.canonicalize.LowercaseRule&lt;br&gt;root:co=\r\nntroller:frontier:rules:1=3Dobject, org.archive.modules.canonicalize.StripU=\r\nserinfoRule&lt;br&gt;\n\nroot:controller:frontier:rules:2=3Dobject, org.archive.mod=\r\nules.canonicalize.StripWWWNRule&lt;br&gt;root:controller:frontier:rules:3=3Dobjec=\r\nt, org.archive.modules.canonicalize.StripSessionIDs&lt;br&gt;root:controller:fron=\r\ntier:rules:4=3Dobject, org.archive.modules.canonicalize.StripSessionCFIDs&lt;b=\r\nr&gt;\n\nroot:controller:frontier:rules:5=3Dobject, org.archive.modules.canonica=\r\nlize.FixupQueryStr&lt;br&gt;root:controller:frontier:scope=3Dreference, root:scop=\r\ne&lt;br&gt;root:controller:processors=3Dmap, org.archive.modules.Processor&lt;br&gt;roo=\r\nt:controller:processors:Preselector=3Dobject, org.archive.crawler.prefetch.=\r\nPreselector&lt;br&gt;\n\nroot:controller:processors:Preselector:scope=3Dreference, =\r\nroot:scope&lt;br&gt;root:controller:processors:Preprocessor=3Dobject, org.archive=\r\n.crawler.prefetch.PreconditionEnforcer&lt;br&gt;root:controller:processors:Load=\r\n=3Dobject, org.archive.modules.recrawl.PersistLoadProcessor&lt;br&gt;\n\nroot:contr=\r\noller:processors:DNS=3Dobject, org.archive.modules.fetcher.FetchDNS&lt;br&gt;root=\r\n:controller:processors:HTTP=3Dobject, org.archive.modules.fetcher.FetchHTTP=\r\n&lt;br&gt;root:controller:processors:History=3Dobject, org.archive.modules.recraw=\r\nl.FetchHistoryProcessor&lt;br&gt;\n\nroot:controller:processors:HTTP:accept-headers=\r\n=3Dlist, java.lang.String&lt;br&gt;root:controller:processors:HTTP:midfetch-rules=\r\n=3Dobject, org.archive.modules.deciderules.DecideRuleSequence&lt;br&gt;root:contr=\r\noller:processors:ExtractorHTTP=3Dobject, org.archive.modules.extractor.Extr=\r\nactorHTTP&lt;br&gt;\n\nroot:controller:processors:ExtractorHTML=3Dobject, org.archi=\r\nve.modules.extractor.ExtractorHTML&lt;br&gt;root:controller:processors:ExtractorC=\r\nSS=3Dobject, org.archive.modules.extractor.ExtractorCSS&lt;br&gt;root:controller:=\r\nprocessors:ExtractorJS=3Dobject, org.archive.modules.extractor.ExtractorJS&lt;=\r\nbr&gt;\n\nroot:controller:processors:ExtractorSWF=3Dobject, org.archive.modules.=\r\nextractor.ExtractorSWF&lt;br&gt;root:controller:processors:Archiver=3Dobject, org=\r\n.archive.modules.writer.WARCWriterProcessor&lt;br&gt;root:controller:processors:A=\r\nrchiver:prefix=3Dstring, lawsc&lt;br&gt;\n\nroot:controller:processors:Updater=3Dob=\r\nject, org.archive.crawler.postprocessor.CrawlStateUpdater&lt;br&gt;root:controlle=\r\nr:processors:LinksScoper=3Dobject, org.archive.crawler.postprocessor.LinksS=\r\ncoper&lt;br&gt;root:controller:processors:LinksScoper:logger-module=3Dauto, root:=\r\nloggerModule&lt;br&gt;\n\nroot:controller:processors:LinksScoper:scope=3Dreference,=\r\n root:scope&lt;br&gt;root:controller:processors:Scheduler=3Dobject, org.archive.c=\r\nrawler.postprocessor.FrontierScheduler&lt;br&gt;root:controller:processors:Store=\r\n=3Dobject, org.archive.modules.recrawl.PersistStoreProcessor&lt;br&gt;\n\nroot:cont=\r\nroller:statistics-tracker=3Dobject, org.archive.crawler.framework.Statistic=\r\nsTrackerImpl&lt;br&gt;&lt;/blockquote&gt;&lt;br&gt;We are using Heritrix 2.0.2, in case that =\r\nmakes a difference, or it is known to have problems with the dedupe process=\r\n.&lt;br&gt;\n\n&lt;br&gt;Thank you very much.\n\r\n--0015175ca95245b828046eacd29e--\r\n\n"}}