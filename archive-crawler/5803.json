{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":396830496,"authorName":"kolnaser","from":"&quot;kolnaser&quot; &lt;kolnaser@...&gt;","profile":"kolnaser","replyTo":"LIST","senderId":"hVTOaPzhW7WjyN_jEkK4B4ht5m9O9bn6aJVDCPxOoSGoW76rsszC7dVmmOeYKhDPt089lUd7Fy8MnTcv8FK1CYhoMUEaxA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to download HTML files from a relative path","postDate":"1240802430","msgId":5803,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGd0Mzg5dStiaTMyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ5RjRENTczLjQwMTA2MDVAZ21haWwuY29tPg=="},"prevInTopic":5802,"nextInTopic":5804,"prevInTime":5802,"nextInTime":5804,"topicId":5795,"numMessagesInTopic":11,"msgSnippet":"Thanks Sergey How can I restrict the crawl to citeulike.org only ? I don t think the write way is to reject all these and more .. rejectPaths .*/pdf_options.* ","rawEmail":"Return-Path: &lt;kolnaser@...&gt;\r\nX-Sender: kolnaser@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 7916 invoked from network); 27 Apr 2009 03:21:12 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.re1.yahoo.com with QMQP; 27 Apr 2009 03:21:12 -0000\r\nX-Received: from unknown (HELO n46b.bullet.mail.sp1.yahoo.com) (66.163.168.160)\n  by mta1.grp.sp2.yahoo.com with SMTP; 27 Apr 2009 03:21:12 -0000\r\nX-Received: from [69.147.65.174] by n46.bullet.mail.sp1.yahoo.com with NNFMP; 27 Apr 2009 03:20:33 -0000\r\nX-Received: from [98.137.34.72] by t12.bullet.mail.sp1.yahoo.com with NNFMP; 27 Apr 2009 03:20:33 -0000\r\nDate: Mon, 27 Apr 2009 03:20:30 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gt389u+bi32@...&gt;\r\nIn-Reply-To: &lt;49F4D573.4010605@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;kolnaser&quot; &lt;kolnaser@...&gt;\r\nSubject: Re: How to download HTML files from a relative path\r\nX-Yahoo-Group-Post: member; u=396830496; y=7gb6roBaA7qnShSMCAqDxdaWDR1bT5xO4k-WypOamtZCn-w\r\nX-Yahoo-Profile: kolnaser\r\n\r\nThanks Sergey\n\nHow can I restrict the crawl to citeulike.org only ? I don&#39;t=\r\n think the write way is to reject all these and more ..\n\nrejectPaths\n.*/pdf=\r\n_options.*\n.*/static.citeulike.org.*\nimages.amazon.com\nhttp://www.google-an=\r\nalytics.com\nhttp://www.springer.com\nhttp://www.annualreviews.org/\nhttp://au=\r\nthormapper.com\napi.recaptcha.net\npagead2.googlesyndication.com\n\nI add in th=\r\ne ARCWriterProcessor , URIRegExpFilter , .*/article/[0-9]*$ , which I think=\r\n will write only URLs containg that path.\n\nHow can I do points 2, 3, 4 ?\n2.=\r\n Add tags to the crawling process in order to harvest more articles.) \n3. B=\r\nut limit to the global tags (not user level tags like\nhttp://www.citeulike.=\r\norg/user/VGreiff/tag/immunology).\n4. Try to avoid duplicate articles in gro=\r\nup/user.\n\nMany Thanks !\n\n\n--- In archive-crawler@yahoogroups.com, Sergey Kh=\r\nenkin &lt;skhenkin@...&gt; wrote:\n&gt;\n&gt; Hm, rather an interesting task.\n&gt; \n&gt; I trie=\r\nd the DecidingScope described below (an excerpt from the job&#39;s \n&gt; oreder.xm=\r\nl):\n&gt; \n&gt;      &lt;newObject name=3D&quot;scope&quot; \n&gt; class=3D&quot;org.archive.crawler.dec=\r\niderules.DecidingScope&quot;&gt;\n&gt;        &lt;boolean name=3D&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n=\r\n&gt;        &lt;string name=3D&quot;seedsfile&quot;&gt;seeds.txt&lt;/string&gt;\n&gt;        &lt;boolean na=\r\nme=3D&quot;reread-seeds-on-config&quot;&gt;true&lt;/boolean&gt;\n&gt;        &lt;newObject name=3D&quot;de=\r\ncide-rules&quot; \n&gt; class=3D&quot;org.archive.crawler.deciderules.DecideRuleSequence&quot;=\r\n&gt;\n&gt;          &lt;map name=3D&quot;rules&quot;&gt;\n&gt;            &lt;newObject name=3D&quot;rejectByD=\r\nefault&quot; \n&gt; class=3D&quot;org.archive.crawler.deciderules.RejectDecideRule&quot;&gt;\n&gt;   =\r\n         &lt;/newObject&gt;\n&gt;            &lt;newObject name=3D&quot;acceptIfPrereq&quot; \n&gt; cl=\r\nass=3D&quot;org.archive.crawler.deciderules.PrerequisiteAcceptDecideRule&quot;&gt;\n&gt;    =\r\n        &lt;/newObject&gt;\n&gt;            &lt;newObject name=3D&quot;acceptHome&quot; \n&gt; class=\r\n=3D&quot;org.archive.crawler.deciderules.MatchesRegExpDecideRule&quot;&gt;\n&gt;            =\r\n  &lt;string name=3D&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n&gt;              &lt;string name=3D&quot;=\r\nregexp&quot;&gt;.*/home&#92;?page=3D.*&lt;/string&gt;\n&gt;            &lt;/newObject&gt;\n&gt;            =\r\n&lt;newObject name=3D&quot;acceptArticles&quot; \n&gt; class=3D&quot;org.archive.crawler.decideru=\r\nles.MatchesRegExpDecideRule&quot;&gt;\n&gt;              &lt;string name=3D&quot;decision&quot;&gt;ACCE=\r\nPT&lt;/string&gt;\n&gt;              &lt;string name=3D&quot;regexp&quot;&gt;.*/article/[0-9]*$&lt;/stri=\r\nng&gt;\n&gt;            &lt;/newObject&gt;\n&gt;          &lt;/map&gt;\n&gt;        &lt;/newObject&gt;\n&gt;    =\r\n  &lt;/newObject&gt;\n&gt; \n&gt; And started with the http://www.citeulike.org/home seed=\r\n.\n&gt; \n&gt; It basically tries to go through pages like\n&gt; http://www.citeulike.o=\r\nrg/home?page=3D1, http://www.citeulike.org/home?page=3D2,\n&gt; ...\n&gt; \n&gt; and li=\r\nke\n&gt; http://www.citeulike.org/user/shung/article/4408244\n&gt; http://www.citeu=\r\nlike.org/user/VGreiff/article/4406467\n&gt; ...\n&gt; \n&gt; The latter ones are what y=\r\nou are looking for.\n&gt; The former ones are also necessary as they contain li=\r\nnks to the articles.\n&gt; \n&gt; It&#39;s not a bad start as overhead (home pages) is =\r\nrather small, you \n&gt; mostly get the articles. But a lot of improvements sho=\r\nuld be made:\n&gt; 1. Reject content under pdf_options directory.\n&gt; 2. Add tags=\r\n to the crawling process in order to harvest more articles.\n&gt; 3. But limit =\r\nto the global tags (not user level tags like \n&gt; http://www.citeulike.org/us=\r\ner/VGreiff/tag/immunology).\n&gt; 4. Try to avoid duplicate articles in group/u=\r\nser.\n&gt; And perhaps a lot more.\n&gt; \n&gt; Regards,\n&gt; Sergey.\n&gt; \n&gt; &gt; First , thank=\r\ns a lot for the reply !\n&gt; &gt; \n&gt; &gt; I am using 1.1.4.3 ( is there any advantag=\r\ne of moving to the new 2.0 ?)\n&gt; &gt; \n&gt; &gt; I want to crawl this site :www.citeu=\r\nlike.org , but the default will \n&gt; &gt; download all html , css, images etc\n&gt; =\r\n&gt; \n&gt; &gt; I just need the HTML pages that are generated from this path for exa=\r\nmple\n&gt; &gt; \n&gt; &gt; http://www.citeulike.org/user/reyez/article/4406466 \n&gt; &gt; &lt;htt=\r\np://www.citeulike.org/user/reyez/article/4406466&gt;\n&gt; &gt; \n&gt; &gt; Adding HTML or H=\r\nTM would generate the page also ?!\n&gt; &gt; http://www.citeulike.org/user/reyez/=\r\narticle/4406466.html \n&gt; &gt; &lt;http://www.citeulike.org/user/reyez/article/4406=\r\n466.html&gt;\n&gt; &gt; http://www.citeulike.org/user/reyez/article/4406466.htm \n&gt; &gt; =\r\n&lt;http://www.citeulike.org/user/reyez/article/4406466.htm&gt;\n&gt; &gt; \n&gt; &gt; So I use=\r\nd/tried\n&gt; &gt; \n&gt; &gt; http://www.citeulike.org/user/.*/article/.* \n&gt; &gt; &lt;http://w=\r\nww.citeulike.org/user/.*/article/.*&gt;\n&gt; &gt; \n&gt; &gt; and\n&gt; &gt; \n&gt; &gt; http://www.citeu=\r\nlike.org/user/.*/article/.* \n&gt; &gt; &lt;http://www.citeulike.org/user/.*/article/=\r\n.*&gt;&#92;.html$\n&gt; &gt; \n&gt; &gt; with DecidingScope and MatchesRegExpDecideRule , and ma=\r\nny other options \n&gt; &gt; but I didn&#39;t succeed. I got some images and some host=\r\ns that are other \n&gt; &gt; than citeulike.org, and some HTML pages that I don&#39;t =\r\nneed.\n&gt; &gt; \n&gt; &gt; So I add this to get only the HTML\n&gt; &gt; \n&gt; &gt; 1. Add the Conte=\r\nntTypeRegExFilter as a midfetch-filter and \n&gt; &gt; write-processor Archiver fi=\r\nlter.\n&gt; &gt; \n&gt; &gt; 2. In the settings for both filters set the if-match-return =\r\nto true, and \n&gt; &gt; use the regexp &#39;(?i)text/html.*&#39;\n&gt; &gt; \n&gt; &gt; 3) MatchesRegEx=\r\npDecideRule\n&gt; &gt; REJECT\n&gt; &gt; \n&gt; &gt; .*(?i)&#92;.(a|ai|aif|aifc|aiff|asc|avi|bcpio|b=\r\nin|bmp|bz2|c|cdf|cgi|cg&#92;\n&gt; &gt; m|class|cpio|cpp?|cpt|csh|css|cxx|dcr|dif|dir|=\r\ndjv|djvu|dll|dmg|dms|doc|dtd|dv|dv&#92;\n&gt; &gt; i|dxr|eps|etx|exe|ez|gif|gram|grxml=\r\n|gtar|h|hdf|hqx|ice|ico|ics|ief|ifb|iges|igs|&#92;\n&gt; &gt; iso|jnlp|jp2|jpe|jpeg|jp=\r\ng|js|kar|latex|lha|lzh|m3u|mac|man|mathml|me|mesh|mid|mi&#92;\n&gt; &gt; di|mif|mov|mo=\r\nvie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|nc|o|oda|ogg|pbm|pct|p&#92;\n&gt; &gt; db=\r\n|pdf|pgm|pgn|pic|pict|pl|png|pnm|pnt|pntg|ppm|ppt|ps|py|qt|qti|qtif|ra|ram|=\r\nras&#92;\n&gt; &gt; |rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|sgml|sh|shar|silo|sit|skd|skm|s=\r\nkp|skt|smi|smi&#92;\n&gt; &gt; l|snd|so|spl|src|srpm|sv4cpio|sv4crc|svg|swf|t|tar|tcl|=\r\ntex|texi|texinfo|tgz|tif|&#92;\n&gt; &gt; tiff|tr|tsv|ustar|vcd|vrml|vxml|wav|wbmp|wbx=\r\nml|wml|wmlc|wmls|wmlsc|wrl|xbm|xht|x&#92;\n&gt; &gt; html|xls|xml|xpm|xsl|xslt|xwd|xyz=\r\n|z|zip)$\n&gt; &gt; \n&gt; &gt; This delayed the crawling process. (any other helpful ide=\r\na ?)\n&gt; &gt; \n&gt; &gt; What I need now is how to download only that HTM/HTML pages f=\r\nrom this path\n&gt; &gt; http://www.citeulike.org/user/.*/article/.* \n&gt; &gt; &lt;http://=\r\nwww.citeulike.org/user/.*/article/.*&gt;&#92;.html$\n&gt; &gt; \n&gt; &gt; ?\n&gt; &gt; \n&gt; &gt; Because I =\r\nam getting some HTML pages from other links , example\n&gt; &gt; http://www.citeul=\r\nike.org/user/birolilu/author/Siu:H \n&gt; &gt; &lt;http://www.citeulike.org/user/biro=\r\nlilu/author/Siu:H&gt;\n&gt; &gt; \n&gt; &gt; I added a list of rejected pages , in the scope=\r\n\n&gt; &gt; \n&gt; &gt; http://www.citeulike.org/user/.*/author/.* \n&gt; &gt; &lt;http://www.citeu=\r\nlike.org/user/.*/author/.*&gt;\n&gt; &gt; http://www.citeulike.org/news &lt;http://www.c=\r\niteulike.org/news&gt;\n&gt; &gt; http://www.citeulike.org/faq &lt;http://www.citeulike.o=\r\nrg/faq&gt;\n&gt; &gt; http://www.citeulike.org/tag/internet \n&gt; &gt; &lt;http://www.citeulik=\r\ne.org/tag/internet&gt;\n&gt; &gt; http://www.citeulike.org/pdf_options &lt;http://www.ci=\r\nteulike.org/pdf_options&gt;\n&gt; &gt; http://www.citeulike.org/groups &lt;http://www.ci=\r\nteulike.org/groups&gt;\n&gt; &gt; http://www.citeulike.org/user/.*/tag/.* \n&gt; &gt; &lt;http:=\r\n//www.citeulike.org/user/.*/tag/.*&gt;\n&gt; &gt; http://www.citeulike.org/howto &lt;htt=\r\np://www.citeulike.org/howto&gt;\n&gt; &gt; http://www.citeulike.org/journals &lt;http://=\r\nwww.citeulike.org/journals&gt;\n&gt; &gt; \n&gt; &gt; But that stop the crawling !\n&gt; &gt; \n&gt; &gt; =\r\nI added the NotMatchesRegExpDecideRule in the ARCWriterProcessor , but \n&gt; &gt;=\r\n that didn&#39;t help aslo .\n&gt; &gt; \n&gt; &gt; --- In archive-crawler@yahoogroups.com \n&gt;=\r\n &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Sergey Khenkin \n&gt; &gt; &lt;skhenki=\r\nn@&gt; wrote:\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Hi,\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; I&#39;m not sure which heritrix vers=\r\nion you are using but in 1.14.3 you can\n&gt; &gt;  &gt; do this using DecidingScope =\r\nand MatchesRegExpDecideRule with regexp like\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; http://www.SITEN=\r\nAME/.*/TAG/.* &lt;http://www.SITENAME/.*/TAG/.*&gt;\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; or maybe\n&gt; &gt;  &gt;=\r\n\n&gt; &gt;  &gt; http://www.SITENAME/.*/TAG/.* &lt;http://www.SITENAME/.*/TAG/.*&gt;&#92;.html=\r\n?$\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; depending on what you want to achieve.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; But a=\r\ns far as I understand it is also important how links to these HTML\n&gt; &gt;  &gt; a=\r\nre gathered during the crawl. Most probably they are found inside other\n&gt; &gt;=\r\n  &gt; HTML documents which won&#39;t be matched by the regexps above. This\n&gt; &gt;  &gt;=\r\n should be addressed somehow.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Regards,\n&gt; &gt;  &gt; Sergey\n&gt; &gt;  &gt;\n&gt;=\r\n &gt;  &gt;\n&gt; &gt;  &gt; &gt; How can I download HTML files only from this path for exampl=\r\ne\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; http://www.SITENAME/MEMBERNAME &lt;http://www.SITENAME/MEM=\r\nBERNAME&gt; \n&gt; &gt; &lt;http://www.SITENAME/MEMBERNAME &lt;http://www.SITENAME/MEMBERNA=\r\nME&gt;&gt;(s)/TAG(S)/\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; I used regexp\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; http://ww=\r\nw.SITENAME/.*/TAG &lt;http://www.SITENAME/.*/TAG&gt; \n&gt; &gt; &lt;http://www.SITENAME/.*=\r\n/TAG &lt;http://www.SITENAME/.*/TAG&gt;&gt;(S)/\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; With URIregExpFilt=\r\ner in FilterDecideRule , but it didn&#39;t work , any\n&gt; &gt;  &gt; &gt; suggestion ?\n&gt; &gt;=\r\n  &gt;\n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}