{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":191387937,"authorName":"pandae667","from":"&quot;pandae667&quot; &lt;aaron667@...&gt;","profile":"pandae667","replyTo":"LIST","senderId":"0MposyPx_WwKUosCAlxZWdWaEo1rI77vPfCpRGlRbPvlgg3sn5xQXc-l5W_bMbA57LXmz1Df8h5MTiITCftJrWd7aIV9","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: problem with ExtractorHTML and robots.txt","postDate":"1153996097","msgId":3116,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVhYTRnMStmZG5jQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGVhN2d1NiszOGNjQGVHcm91cHMuY29tPg=="},"prevInTopic":3113,"nextInTopic":3123,"prevInTime":3115,"nextInTime":3117,"topicId":3113,"numMessagesInTopic":3,"msgSnippet":"Hi, its me once again. I need a little clarification. The filters I can add to an extract-processor - are those applied _before_ applying the extractors (so i","rawEmail":"Return-Path: &lt;aaron667@...&gt;\r\nX-Sender: aaron667@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 98195 invoked from network); 27 Jul 2006 10:28:52 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m40.grp.scd.yahoo.com with QMQP; 27 Jul 2006 10:28:52 -0000\r\nReceived: from unknown (HELO n30.bullet.scd.yahoo.com) (66.94.237.23)\n  by mta9.grp.scd.yahoo.com with SMTP; 27 Jul 2006 10:28:52 -0000\r\nReceived: from [66.218.69.2] by n30.bullet.scd.yahoo.com with NNFMP; 27 Jul 2006 10:28:38 -0000\r\nReceived: from [66.218.66.68] by t2.bullet.scd.yahoo.com with NNFMP; 27 Jul 2006 10:28:20 -0000\r\nDate: Thu, 27 Jul 2006 10:28:17 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;eaa4g1+fdnc@...&gt;\r\nIn-Reply-To: &lt;ea7gu6+38cc@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;pandae667&quot; &lt;aaron667@...&gt;\r\nSubject: Re: problem with ExtractorHTML and robots.txt\r\nX-Yahoo-Group-Post: member; u=191387937; y=jQitiMNWnXMq_ZYR0vVR7tfYveLhSv0D3mIij8IYh7CwW-R2\r\nX-Yahoo-Profile: pandae667\r\n\r\nHi,\n\nits me once again. I need a little clarification.\nThe filters I can ad=\r\nd to an extract-processor - are those applied\n_before_ applying the extract=\r\nors (so i can say to what URIs I want the\nextractor to apply) or are those =\r\napplied _after_ extraction (so I can\ndefine what I want to have extracted).=\r\n\n\nThanks in advance\n  Olaf Freyer\n\n--- In archive-crawler@yahoogroups.com, =\r\n&quot;pandae667&quot; &lt;aaron667@...&gt; wrote:\n&gt;\n&gt; Hi everyone,\n&gt; \n&gt; its me again with y=\r\net a another problem.\n&gt; \n&gt; The host in question this time:\n&gt; http://www.sch=\r\numacher-partner-immo.de\n&gt; \n&gt; The real problem:\n&gt; http://www.schumacher-part=\r\nner-immo.de/robots.txt\n&gt; \n&gt; As you can see the robots.txt directs to some c=\r\nompletly different\n&gt; place (in fact any URL not really present leads there)=\r\n -  and as the\n&gt; result I get is text/html it triggers the ExtracotrHTML.\n&gt;=\r\n Now I end up with dozens of URIs nowhere linked on the page I intended\n&gt; t=\r\no crawl. So anyone having any ideas? I can&#39;t add filters for the\n&gt; Extracto=\r\nrHTML so the only option I&#39;d see so far is changing its code\n&gt; to not extra=\r\nct stuff from a robots.txt. If anyone has a better idea\n&gt; I&#39;d be interested=\r\n to hear.\n&gt; \n&gt; Thanks in advance\n&gt;   Olaf Freyer\n&gt;\n\n\n\n\n\n"}}