{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":325624130,"authorName":"Noah Levitt","from":"Noah Levitt &lt;nlevitt@...&gt;","profile":"nlevitt","replyTo":"LIST","senderId":"B7QV096mDC9nBo9wG6hiQlXXLvWjBdX7zexgG2EPRvyLl4Isk3k_05Qd77cvxxyPZvAkGdLbrY5ZtHAAoE-hiWUDNjKPFRLH","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] H3 - distributed crawling and memory/cpu utilization","postDate":"1318644349","msgId":7353,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFOThFQTdELjEwODA4MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGo3NHMzNCtldWVnQGVHcm91cHMuY29tPg==","referencesHeader":"PGo3NHMzNCtldWVnQGVHcm91cHMuY29tPg=="},"prevInTopic":7351,"nextInTopic":7355,"prevInTime":7352,"nextInTime":7354,"topicId":7351,"numMessagesInTopic":7,"msgSnippet":"Hello David, ... If the crawl has plenty of urls to keep it busy, the limiting factor is typically disk, not cpu. ... Don t know without looking more closely","rawEmail":"Return-Path: &lt;nlevitt@...&gt;\r\nX-Sender: nlevitt@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 96269 invoked from network); 15 Oct 2011 02:05:57 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m5.grp.sp2.yahoo.com with QMQP; 15 Oct 2011 02:05:57 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.224.6)\n  by mta3.grp.sp2.yahoo.com with SMTP; 15 Oct 2011 02:05:57 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 997B068410E7;\n\tFri, 14 Oct 2011 19:05:51 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id nuJ9MNmLgiCr; Fri, 14 Oct 2011 19:05:48 -0700 (PDT)\r\nX-Received: from [208.70.27.155] (desktop-nlevitt.sf.archive.org [208.70.27.155])\n\tby mail.archive.org (Postfix) with ESMTPSA id 85C9768410E5;\n\tFri, 14 Oct 2011 19:05:48 -0700 (PDT)\r\nMessage-ID: &lt;4E98EA7D.1080806@...&gt;\r\nDate: Fri, 14 Oct 2011 19:05:49 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.2.23) Gecko/20110922 Thunderbird/3.1.15\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: david_pane1 &lt;dpane@...&gt;\r\nReferences: &lt;j74s34+eueg@...&gt;\r\nIn-Reply-To: &lt;j74s34+eueg@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Noah Levitt &lt;nlevitt@...&gt;\r\nSubject: Re: [archive-crawler] H3 - distributed crawling and memory/cpu utilization\r\nX-Yahoo-Group-Post: member; u=325624130; y=mu1I-wrc0AXCtwmZoF4hYwsb927XWZaPvxBDMPU2O6uubA\r\nX-Yahoo-Profile: nlevitt\r\n\r\nHello David,\n\nOn 10/12/2011 01:08 PM, david_pane1 wrote:\n&gt; 1) I am using two 8 core 32GB machines for some test crawls.  The machines are connected to the internet on a gigabit connection.  The internal network is also a gigabit.  I am writing the data from the crawler to a NAS which I can copy data to at an average of 70MB/sec transfer rate. I am trying to adjust the memory and maxToeThread settings to get the maximum throughput.  Currently my settings are:\n&gt;\n&gt; JAVA_OPTS=&quot;-Xmx11000M\n&gt; &lt;property name=&quot;maxToeThreads&quot; value=&quot;1200&quot; /&gt;\n&gt;\n&gt; With these values, I am seeing about 50% cpu utilization (about half of the 8 cores) and an average of around 750MB/min or (96Mbps) network activity.  I would like to utilize more of the cpu.  Is this reasonable?\n\nIf the crawl has plenty of urls to keep it busy, the limiting factor is \ntypically disk, not cpu.\n\n&gt; Increasing the maxToeThreads to a higher value than 1200 causes the java application to fall to minimal to no cpu usage and the web interface to be unresponsive.  Does anyone know why this is happening?\n\nDon&#39;t know without looking more closely at the logs and state of the \njava process when that happens. But 1200 threads for an 11000M heap \nseems like a lot, maybe too many depending on the rest of your config. \nThe other big consumers of memory are the bdb cache and the bloom filter \nif you&#39;re using that. By default the bdb cache occupies 60% of available \nheap, so 6600M in your case.\n\nIt sounds like you&#39;re doing a broad crawl, so the bloom filter is \nprobably appropriate for your already-seen check. The default \nBdbUriUniqFilter can be a bottleneck in a large crawl. The bloom filter \noccupies a fixed amount of memory and does not use disk, but is \nprobabilistic, and accepts a small rate of false positives. With default \nsettings, it uses 495M of heap, and has an expected false-positive rate \nof 1-in-4million through the first 125 million urls it sees.\n\nThere&#39;s some old but generally relevant discussion of the bloom filter \nhere: http://tech.groups.yahoo.com/group/archive-crawler/message/3142\n\nI&#39;m not finding existing docs on this so just for the record, this is \nhow you can configure a different sized bloom filter in h3. This example \nwould use\n1.44 * 400,000,000 * 30 / 8 = 2,160,000,000 bytes of heap with an \nexpected false-postive rate of 2^-30 or about 1-in-a-billion through the \nfirst 400 million urls seen. Hopefully I&#39;m getting this math right. :)\n\n&lt;bean id=&quot;uriUniqFilter&quot; \nclass=&quot;org.archive.crawler.util.BloomUriUniqFilter&quot;&gt;\n&lt;property name=&quot;bloomFilter&quot;&gt;\n&lt;bean class=&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&lt;constructor-arg value=&quot;400000000&quot;/&gt;\n&lt;constructor-arg value=&quot;30&quot;/&gt;\n&lt;/bean&gt;\n&lt;/property&gt;\n&lt;/bean&gt;\n\n&gt; How much of the 32GB of memory should I allocate to JAVA_OPTS?\n\nThe more the merrier, as long as other processes have enough room and \nthe system doesn&#39;t start swapping.\n\nThere&#39;s no hard limit on the amount of heap each toe thread can use, and \nheritrix can be vulnerable to websites that do weird stuff. With default \nsettings though, 5-10M per toe thread is a reasonable rule of thumb. \nMost of the time each thread will use much less than that, but sometimes \nthey&#39;ll use more. If you get an OutOfMemoryError then you should reduce \nthe number of toe threads.\n\nSo let&#39;s say you give heritrix a 25G heap, leave the bdb cache at the \ndefault of 60%, and use the default .5G bloom filter. That leaves you \nwith 25GB * (100% - 60%) - .5G = 9.5G for toe threads. At 7.5M per toe \nthread that comes to 1266 of them.\n\nWith this config and hardware I would expect your crawl rate to be \nlimited elsewhere, likely in warc writing, assuming you have that \nenabled. Presumably all these threads will be competing to write to a \nsmall number of disks.\n\n&gt; 2) One of the 2 crawlers stopped crawling due to a congestion ratio of infinity.  What are some ways to overcome this? What can I do to avoid it happening in the future?\n\nI don&#39;t know. Not clear what you mean by &quot;stopped&quot; exactly for one \nthing. If it happens again, maybe gather more information from \nheritrix_out.log, toe thread report, frontier report, top, iostat, \njstack, etc.\n\n&gt; 3) In http://tech.groups.yahoo.com/group/archive-crawler/message/3846\n&gt; Gordon stated:\n&gt;\n&gt; &quot;...\n&gt; HashCrawlMapper looks at the queue key of a URI -- here, the SURT\n&gt; authority part, because of the above choice -- and decides if a URI is\n&gt; handled by the current crawler or one of its siblings. If mapped to a\n&gt; sibling, the URI is dumped to a log rather than crawled locally.\n&gt; Depending on the character of your crawl, you may want to feed these\n&gt; logs to the other crawlers occasionally or it may be OK to ignore them.\n&gt; ...\n&gt; &quot;\n&gt;\n&gt; How does one feed the diverted URIs/logs to a sibling crawler?\n\nA coworker kindly put up this page yesterday: \nhttps://webarchive.jira.com/wiki/display/Heritrix/Multiple+Machine+Crawling \nwhich says, &quot;Crawl operators must set up a process where the the URIs \ncontained in .divert files are copied from each crawler to their \nassigned crawlers and queued into the active crawl (putting the .divert \nfile in the actions directory as a .include should be sufficient).&quot;\n\nNoah\n\n\n&gt; Any help is greatly appreciated.\n&gt;\n&gt; --David\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n\n"}}