{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Wtnj6_uhC1nhEHIJPPgQvVHTLwuZ96bJy_eKWES339eMvNPmaolaGJtvO78Jg-E3d9gV-eF5mALnEDemIqKkGMYlBjbY-_g","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Limit the crawls to e.g. 100 URLs/Host","postDate":"1348106477","msgId":7783,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwNUE3OEVELjYwMTA2MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDUwNTk2M0I3LjMwMzAyQGdteC5kZT4=","referencesHeader":"PGsxcXFyOSthZm9pQGVHcm91cHMuY29tPiA8NTA1MjY3QUQuNjAyMDgwNkBhcmNoaXZlLm9yZz4gPDUwNTk1MzM0LjYwNDA2MDhAZ214LmRlPiA8MzcyMEU0NTItOEY0OS00QkZCLUE0MEYtRDYwQkMyMDkxNDVEQHN0YXRzYmlibGlvdGVrZXQuZGs+IDw1MDU5NUIxOS43MDQwMDAzQGdteC5kZT4gPDIyOUQ2NDcxLURFQ0EtNDQ0Ni1CMEY4LTUxNzkzMjZCRTk0REBzdGF0c2JpYmxpb3Rla2V0LmRrPiA8NTA1OTYzQjcuMzAzMDJAZ214LmRlPg=="},"prevInTopic":7778,"nextInTopic":0,"prevInTime":7782,"nextInTime":7784,"topicId":7755,"numMessagesInTopic":8,"msgSnippet":"You could also look at the optional QuotaEnforcer processor: org.archive.crawler.prefetch.QuotaEnforcer It lets you set a cap by host, server (host:port), or","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 20702 invoked from network); 20 Sep 2012 02:01:20 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m4.grp.sp2.yahoo.com with QMQP; 20 Sep 2012 02:01:20 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta4.grp.sp2.yahoo.com with SMTP; 20 Sep 2012 02:01:20 -0000\r\nX-Received: (qmail 74009 invoked by uid 0); 20 Sep 2012 02:01:18 -0000\r\nX-Received: from 70.36.143.78 (HELO silverbook.local) (70.36.143.78)\n  by relay03.pair.com with SMTP; 20 Sep 2012 02:01:18 -0000\r\nX-pair-Authenticated: 70.36.143.78\r\nMessage-ID: &lt;505A78ED.6010606@...&gt;\r\nDate: Wed, 19 Sep 2012 19:01:17 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:15.0) Gecko/20120907 Thunderbird/15.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;k1qqr9+afoi@...&gt; &lt;505267AD.6020806@...&gt; &lt;50595334.6040608@...&gt; &lt;3720E452-8F49-4BFB-A40F-D60BC209145D@...&gt; &lt;50595B19.7040003@...&gt; &lt;229D6471-DECA-4446-B0F8-5179326BE94D@...&gt; &lt;505963B7.30302@...&gt;\r\nIn-Reply-To: &lt;505963B7.30302@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Limit the crawls to e.g. 100 URLs/Host\r\nX-Yahoo-Group-Post: member; u=137285340; y=KbNKtZzRE6vkeHIyO0SVkasibxKeizJIk9Uih_TDwqrd\r\nX-Yahoo-Profile: gojomo\r\n\r\nYou could also look at the optional &#39;QuotaEnforcer&#39; processor:\n\n  org.archive.crawler.prefetch.QuotaEnforcer\n\nIt lets you set a cap by host, server (host:port), or &#39;group&#39; (which in \nthis context is the same as queue -- so similar the the budgeting, but \nbudgeting can assign different URIs different &#39;costs&#39;, while \nQuotaEnforcer just uses a simple count).\n\nOnce that cap is met, any URI in the category will be aborted with a \nspecial &#39;over-quota&#39; status (S_BLOCKED_BY_QUOTA, appearing in the \ncrawl.log that way). In this way, all URIs that were in the same \ncategory will each come up for their turn to be crawled, be blocked, and \nbe logged to the crawl.log.\n\nAlternatively, if you set the &#39;forceRetire&#39; option, instead of using the \nblock-status it will set a flag on the URI causing whichever queue it \ncame from to be set to &#39;retired&#39; (frozen) status. This essentially holds \nthem in the queue in case you might want to change the quota and resume \nworking through them.\n\nThe quotas can be set to different values for different URI patterns \nusing the &#39;overlay sheets&#39; mechanisms. (You could set a &#39;host&#39; quota of \n100 for all .COM URIs, for example -- but this would mean every .COM URI \nwould have its host count compared against 100, not 100 across all \nmatching URIs over all hosts.)\n\n- Gordon\n\nOn 9/18/12 11:18 PM, Markus Mirsberger wrote:\n&gt;\n&gt;\n&gt; omg ... yes you right.....I forgot that....maxdocuments\n&gt; I just first thought about another requirement where I need the\n&gt; queuetotalbudget...but there I have only one queue/host and I forgot\n&gt; that. Here I can use maxdocuments\n&gt;\n&gt; Thanks alot :)\n&gt; Markus\n&gt;\n&gt;\n&gt;\n&gt; On 09/19/2012 01:02 PM, Bjarne Andersen wrote:\n&gt;&gt; IF one job is only crawling one host and you want to limit that host\n&gt;&gt; in total to a number you should just use the global setting for\n&gt;&gt; limiting the entire job (I Think I remember there is one although I\n&gt;&gt; never used it)\n&gt;&gt; -\n&gt;&gt; Bjarne\n&gt;&gt;\n&gt;&gt; Sendt fra min iPhone\n&gt;&gt;\n&gt;&gt; Den 19/09/2012 kl. 07.42 skrev &quot;Markus Mirsberger&quot;\n&gt;&gt; &lt;markus.mirsberger@... &lt;mailto:markus.mirsberger@...&gt;&gt;:\n&gt;&gt;\n&gt;&gt;&gt; Yes I think that too.\n&gt;&gt;&gt; But with one job I&#39;m only crawling one host and I not follow external\n&gt;&gt;&gt; hosts (with a custom decide rule).\n&gt;&gt;&gt; So in every queue should be the same hostname since I think the\n&gt;&gt;&gt; external hosts should not be moved to the queue when filtered out\n&gt;&gt;&gt; with a decide rule.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Regards,\n&gt;&gt;&gt; Markus\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 09/19/2012 12:23 PM, Bjarne Andersen wrote:\n&gt;&gt;&gt;&gt; IF your queues does not exactly match hostnames the queuetotalbudget\n&gt;&gt;&gt;&gt; does not really make sense in your setup. This setting limits the\n&gt;&gt;&gt;&gt; number of URIs taken off each queue so if a queue can have URIs from\n&gt;&gt;&gt;&gt; more than one host, the limit just means that every URI could be\n&gt;&gt;&gt;&gt; from the same host as long as the total budget is not yet spent. If\n&gt;&gt;&gt;&gt; URIs from the same host gets randomly put into different queues it\n&gt;&gt;&gt;&gt; makes even less sense in your setting\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; AFAIK there is no existing solution for you with heritrix :(\n&gt;&gt;&gt;&gt; You could write your own limiter module off cause that holds an\n&gt;&gt;&gt;&gt; alternative datastructure side by side the current queues\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Best\n&gt;&gt;&gt;&gt; Bjarne Andersen\n&gt;&gt;&gt;&gt; Netarchive.dk &lt;http://Netarchive.dk&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Sendt fra min iPhone\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Den 19/09/2012 kl. 07.08 skrev &quot;Markus Mirsberger&quot;\n&gt;&gt;&gt;&gt; &lt;markus.mirsberger@... &lt;mailto:markus.mirsberger@...&gt;&gt;:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Hello Noah,\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; thanks for your reply. This works well when I only use one thread\n&gt;&gt;&gt;&gt;&gt; for one host.\n&gt;&gt;&gt;&gt;&gt; Unfortunately I am crawling most hosts with parallel queues and\n&gt;&gt;&gt;&gt;&gt; this setting affects every queue.\n&gt;&gt;&gt;&gt;&gt; I thought first .... ok no problem..just part the amount of sites\n&gt;&gt;&gt;&gt;&gt; to the queues .. e.g. if I like to crawl 10.000 URLs with 10\n&gt;&gt;&gt;&gt;&gt; parallel queues so I set queueTotalBudget to 1000.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; This worked in tests with small hosts but now I tried it with a\n&gt;&gt;&gt;&gt;&gt; bigger host and the result is completely different from what I\n&gt;&gt;&gt;&gt;&gt; expected.\n&gt;&gt;&gt;&gt;&gt; With 30 parallelQueues I tried to get a maximum of 250.000 URIs\n&gt;&gt;&gt;&gt;&gt; (out of 2.000.000) from one host. So I set the queueTotalBudgt to\n&gt;&gt;&gt;&gt;&gt; 8334 but the result are only about 55.000 crawled URIs.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Did I use this in a wrong way or do I have to use another setting\n&gt;&gt;&gt;&gt;&gt; when I use parallel queues?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Thanks and regarads,\n&gt;&gt;&gt;&gt;&gt; Markus\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; On 09/14/2012 06:09 AM, Noah Levitt wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Hello Markus,\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; You can set the value of queueTotalBudget to 100 on your frontier.\n&gt;&gt;&gt;&gt;&gt;&gt; Since\n&gt;&gt;&gt;&gt;&gt;&gt; by default each queue corresponds to one host, the effect is what you\n&gt;&gt;&gt;&gt;&gt;&gt; describe.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Noah\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; On 08/31/2012 10:04 AM, mirschi74 wrote:\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; Hi,\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; I have a seed file filled with hosts, but want only crawl e.g.\n&gt;&gt;&gt;&gt;&gt;&gt; 100 URLs from each host.\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; Can you please give me a hint where I can configure that?\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; I think it should be somewhere in the BDBFrontier, but I cant\n&gt;&gt;&gt;&gt;&gt;&gt; find any documentation about that.\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; There is another setting that limits the maxdocuments. But this\n&gt;&gt;&gt;&gt;&gt;&gt; is a global setting and limits the crawls for a jobrun and not\n&gt;&gt;&gt;&gt;&gt;&gt; meant to limit crawls by host.\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; Thanks in advance,\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; Markus\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}