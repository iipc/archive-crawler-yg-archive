{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":194371199,"authorName":"bjhong02","from":"&quot;bjhong02&quot; &lt;bjhong02@...&gt;","profile":"bjhong02","replyTo":"LIST","senderId":"AzoLQoo59WR8w_OF75gZdQajaSfG9PNnh3rdGYPI7zJ9LWfSFA2U5W0rhgQqlant-FEI9bZngam_MizriFp0Gl9q_RXmOA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: what&#39;s wrong with reading robots.txt","postDate":"1096119253","msgId":1035,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNqM3M0bCtidDV2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQxNTM3QjIxLjIwMDAwMDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":1032,"nextInTopic":0,"prevInTime":1034,"nextInTime":1036,"topicId":1004,"numMessagesInTopic":12,"msgSnippet":"... wget and Heritrix are all via the same proxy. In fact, the crawl did not fail, just hangs up. when read robots.txt and get a timeout, it will retry.","rawEmail":"Return-Path: &lt;bjhong02@...&gt;\r\nX-Sender: bjhong02@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 95118 invoked from network); 25 Sep 2004 13:34:49 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m17.grp.scd.yahoo.com with QMQP; 25 Sep 2004 13:34:49 -0000\r\nReceived: from unknown (HELO n9.grp.scd.yahoo.com) (66.218.66.93)\n  by mta3.grp.scd.yahoo.com with SMTP; 25 Sep 2004 13:34:49 -0000\r\nReceived: from [66.218.67.132] by n9.grp.scd.yahoo.com with NNFMP; 25 Sep 2004 13:34:14 -0000\r\nDate: Sat, 25 Sep 2004 13:34:13 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;cj3s4l+bt5v@...&gt;\r\nIn-Reply-To: &lt;41537B21.2000003@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 2348\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.218.66.93\r\nFrom: &quot;bjhong02&quot; &lt;bjhong02@...&gt;\r\nSubject: Re: what&#39;s wrong with reading robots.txt\r\nX-Yahoo-Group-Post: member; u=194371199\r\nX-Yahoo-Profile: bjhong02\r\n\r\n&gt; I&#39;m a little stumped.  Is there something particular about the proxy on \n&gt; your end?  Can you wget/curl/etc. via this proxy without problem (You \n&gt; say you can wget the robots.txt buy you don&#39;t say if this is via the \n&gt; proxy)?  Do you have to use the proxy?  Have you tried crawling from \n&gt; elsewhere where you don&#39;t need a proxy?  Does crawl still fail? (You \n&gt; seemed to be able to crawl www.yahoo.com.cn according to a subsequent \n&gt; email; was that via the proxy?).\n&gt; \n&gt; Yours,\n&gt; St.Ack\n\n\nwget and Heritrix are all via the same proxy.\n\nIn fact, the crawl did not fail, just hangs up. when read robots.txt\nand get a timeout, it will retry. because the retry-delay is 900\nseconds, so the Toethread turns to inactive, and the crawl seems dead.\n But, all (I wait for 5 times) retries get a time-out.\n\nIn case of www.yahoo.com.cn, I checked the log, no robots-reading get\na 200. For the domains get a time-out, the crawl on that domain hangs\nup. for others get a 302, or 404, the crawl continues.\n\nSo, the problem is why it get a time-out. I download the source code,\nand debuged inside. and get some observations.\n\n\nFetchHTTP.java\nvoid innerProcess(CrawlURI curi)\n{\n...\n            rec.getRecordedInput().readFullyOrUntil(getMaxLength(curi),\n                1000 * getTimeout(curi));\n...\n                                                                     \n          }\n\nRecordingInputStream.java\nreadFullyOrUntil(...)\n{\n...\n        while (true) {\n            try {\n                bytesRead = read(buf);       &lt;======\n                if (bytesRead == -1) {\n                    break;\n                }\n                totalBytes += bytesRead;\n                if(Thread.interrupted()) {\n                    throw new InterruptedException(&quot;interrupted during\nIO&quot;);\n                }\n            } catch (SocketTimeoutException e) {\n...\n\n}\n\n----------------\nthe observations\n----------------\nThis time, the seed is www.cas.ac.cn\n\nwhen first execute read(buf), the bytesRead = 54, it&#39;s the length of\nthe robots.txt. but when following the loop to execute read(.) again,\ntime-out happens.\n\nThe very interesting things is, if we do not execute read(.)\nimmediately after the first execution as the above case, but wait for\na while, maybe 1 or 2 minutes, then read(.) will return -1, and succeed.\n\n\nI wonder why this happens, it&#39;s beyond my knowledge.\n\n\n\n"}}