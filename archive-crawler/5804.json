{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":336053106,"authorName":"Sergey Khenkin","from":"Sergey Khenkin &lt;skhenkin@...&gt;","profile":"skhenkin@ymail.com","replyTo":"LIST","senderId":"ZyyRcX6GrOMgnpeaOwCzwKI38qCUoUbCQQtsvVqNO_fJKU9IYoaekPvTePZBMSQbcgOvJe7I5Vc_2K9xNh7pi7WFcvISdALXUUc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: How to download HTML files from a relative path","postDate":"1240825182","msgId":5804,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5RjU3RDVFLjUwODA5MDhAZ21haWwuY29tPg==","inReplyToHeader":"PGd0Mzg5dStiaTMyQGVHcm91cHMuY29tPg==","referencesHeader":"PGd0Mzg5dStiaTMyQGVHcm91cHMuY29tPg=="},"prevInTopic":5803,"nextInTopic":5808,"prevInTime":5803,"nextInTime":5805,"topicId":5795,"numMessagesInTopic":11,"msgSnippet":"Hi, I added a couple of enhancements and now it seems to work fine. The idea is to crawl through http://www.citeulike.org/home http://www.citeulike.org/tag ","rawEmail":"Return-Path: &lt;skhenkin@...&gt;\r\nX-Sender: skhenkin@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 47018 invoked from network); 27 Apr 2009 09:40:37 -0000\r\nX-Received: from unknown (69.147.108.200)\n  by m5.grp.sp2.yahoo.com with QMQP; 27 Apr 2009 09:40:37 -0000\r\nX-Received: from unknown (HELO mail-bw0-f208.google.com) (209.85.218.208)\n  by mta1.grp.re1.yahoo.com with SMTP; 27 Apr 2009 09:40:36 -0000\r\nX-Received: by bwz4 with SMTP id 4so2385844bwz.6\n        for &lt;archive-crawler@yahoogroups.com&gt;; Mon, 27 Apr 2009 02:39:36 -0700 (PDT)\r\nX-Received: by 10.204.31.77 with SMTP id x13mr5159514bkc.6.1240825176184;\n        Mon, 27 Apr 2009 02:39:36 -0700 (PDT)\r\nReturn-Path: &lt;skhenkin@...&gt;\r\nX-Received: from ?192.168.1.3? ([213.227.229.75])\n        by mx.google.com with ESMTPS id d13sm4652818fka.39.2009.04.27.02.39.35\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Mon, 27 Apr 2009 02:39:35 -0700 (PDT)\r\nMessage-ID: &lt;49F57D5E.5080908@...&gt;\r\nDate: Mon, 27 Apr 2009 12:39:42 +0300\r\nUser-Agent: Thunderbird 2.0.0.21 (Windows/20090302)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;gt389u+bi32@...&gt;\r\nIn-Reply-To: &lt;gt389u+bi32@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Sergey Khenkin &lt;skhenkin@...&gt;\r\nSubject: Re: [archive-crawler] Re: How to download HTML files from a relative\n path\r\nX-Yahoo-Group-Post: member; u=336053106; y=auuToDqayd_ircPdhLoi8X549axLq9n3zk9aS9VM2MdoW42kai_71gHm4q_f\r\nX-Yahoo-Profile: skhenkin@...\r\n\r\nHi,\n\nI added a couple of enhancements and now it seems to work fine.\nThe idea is to crawl through\n   http://www.citeulike.org/home\n   http://www.citeulike.org/tag\n   http://www.citeulike.org/user\nand to write only\n   http://www.citeulike.org/user/.../article/...\n\nSo I created a new job and altered its configuration so that:\n\n1. Seed URL is http://www.citeulike.org/home\n\n2. CrawlScope is org.archive.crawler.deciderules.DecidingScope.\nIts submodules are:\n\n   rejectByDefault of type RejectDecideRule\n\n   acceptIfPrereq of type PrerequisiteAcceptDecideRule\n\n   acceptHome of type MatchesRegExpDecideRule\n     decision: ACCEPT\n     regexp: .*/home&#92;?page=.*\n\n   acceptTags of type MatchesRegExpDecideRule\n     decision: ACCEPT\n     regexp: http://www&#92;.citeulike&#92;.org/tag/.*\n\n   acceptUsers of type MatchesRegExpDecideRule\n     decision: ACCEPT\n     regexp: http://www&#92;.citeulike&#92;.org/user/.*\n\n   rejectEverythingUserSpecific of type MatchesRegExpDecideRule\n     decision: REJECT\n     regexp: http://www&#92;.citeulike&#92;.org/user/.*/.*\n\n   acceptArticles of type MatchesRegExpDecideRule\n     decision: ACCEPT\n     regexp: .*/article/[0-9]*$\n\n   rejectPdf of type MatchesRegExpDecideRule\n     decision: REJECT\n     regexp: http://www&#92;.citeulike&#92;.org/pdf_options/.*\n\n   rejectGroups of type MatchesRegExpDecideRule\n     decision: REJECT\n     regexp: http://www&#92;.citeulike&#92;.org/groups/.*\n\n3. Frontier, Preprocessors, Fetchers and Postprocessors are kept default.\n\n4. Extractors are only org.archive.crawler.extractor.ExtractorHTTP and \norg.archive.crawler.extractor.ExtractorHTTP.\n\n5. I used org.archive.crawler.writer.MirrorWriterProcessor Writer but \nyou can go with the default ARCWriterProcessor. Using mirror is just \neasier to experiment.\nI added a submodule here to filter out everything except articles:\n   acceptArticlesOnly\n     of type org.archive.crawler.deciderules.NotMatchesRegExpDecideRule\n     decision: REJECT\n     regexp: .*/article/[0-9]*$\n\nThat&#39;s all. Works nicely for me. Please give it a try.\n\nI wonder if there&#39;s an easy way to export/import jobs between different \nHeritrix instances so that users can communicate more effectively?\n\nRegards,\nSergey\n\n\n&gt; Thanks Sergey\n&gt; \n&gt; How can I restrict the crawl to citeulike.org only ? I don&#39;t think the \n&gt; write way is to reject all these and more ..\n&gt; \n&gt; rejectPaths\n&gt; .*/pdf_options.*\n&gt; .*/static.citeulike.org.*\n&gt; images.amazon.com\n&gt; http://www.google-analytics.com &lt;http://www.google-analytics.com&gt;\n&gt; http://www.springer.com &lt;http://www.springer.com&gt;\n&gt; http://www.annualreviews.org/ &lt;http://www.annualreviews.org/&gt;\n&gt; http://authormapper.com &lt;http://authormapper.com&gt;\n&gt; api.recaptcha.net\n&gt; pagead2.googlesyndication.com\n&gt; \n&gt; I add in the ARCWriterProcessor , URIRegExpFilter , .*/article/[0-9]*$ , \n&gt; which I think will write only URLs containg that path.\n&gt; \n&gt; How can I do points 2, 3, 4 ?\n&gt; 2. Add tags to the crawling process in order to harvest more articles.)\n&gt; 3. But limit to the global tags (not user level tags like\n&gt; http://www.citeulike.org/user/VGreiff/tag/immunology \n&gt; &lt;http://www.citeulike.org/user/VGreiff/tag/immunology&gt;).\n&gt; 4. Try to avoid duplicate articles in group/user.\n&gt; \n&gt; Many Thanks !\n\n"}}