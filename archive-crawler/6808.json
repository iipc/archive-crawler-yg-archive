{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"U1mAtiFHowBGAD0i03xzVu5W79LvzvNSaajlZI0cVVKxeM5ydO9jFhqPpqzrv6KiKjcy65fgVwaXq3aeIScznJcNkFRENiY","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix 3.1 and Continuous Crawling?","postDate":"1289596761","msgId":6808,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDRERBRjU5LjYwNTAxMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDA4OTRGQzEwRUNFODQzRDQ4Qzc5NzQ2MERBNUNCMjZFQGRhdGFjbGlwLmNvbT4=","referencesHeader":"PDA4OTRGQzEwRUNFODQzRDQ4Qzc5NzQ2MERBNUNCMjZFQGRhdGFjbGlwLmNvbT4="},"prevInTopic":6803,"nextInTopic":0,"prevInTime":6807,"nextInTime":6809,"topicId":6803,"numMessagesInTopic":2,"msgSnippet":"... It depends on what you mean by continuous crawling . There s not a firm definition expected to arrive in toto in a single release, but features since 1.12","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 95945 invoked from network); 12 Nov 2010 21:19:24 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.sp2.yahoo.com with QMQP; 12 Nov 2010 21:19:24 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.sp2.yahoo.com with SMTP; 12 Nov 2010 21:19:24 -0000\r\nX-Received: (qmail 28651 invoked from network); 12 Nov 2010 21:19:22 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 12 Nov 2010 21:19:22 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4CDDAF59.6050104@...&gt;\r\nDate: Fri, 12 Nov 2010 13:19:21 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.12) Gecko/20101027 Thunderbird/3.1.6\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;0894FC10ECE843D48C797460DA5CB26E@...&gt;\r\nIn-Reply-To: &lt;0894FC10ECE843D48C797460DA5CB26E@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix 3.1 and Continuous Crawling?\r\nX-Yahoo-Group-Post: member; u=137285340; y=Jeed73U96p9TjaOHER-fjeiapH7rUclLL0PS28jWfXtV\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 11/9/10 2:14 PM, Zach Bailey wrote:\n&gt;\n&gt;\n&gt; I remember reading somewhere that Heritrix 3.1 would support continuous\n&gt; crawling. Is there a preview release of this available anywhere or a\n&gt; place I can see how progress is coming along?\n\nIt depends on what you mean by &#39;continuous crawling&#39;. There&#39;s not a firm \ndefinition expected to arrive in toto in a single release, but features \nsince 1.12 have been adding capabilities towards better support for \ncrawling activities that don&#39;t fall into distinct, standalone repeat \ncrawls.\n\nThe steps so far have been:\n\n- an optional history mechanism for storing previous HTTP headers and \ncontent-checksums, then making followup crawls issue requests and make \nstorage decisions based on that history. (The Persist** and FetchHistory \nprocessors of H1.12 and up.)\n\n- the H3 options for relaunching a job in-place, and splitting the above \nhistory to a separate BDB-directory for easier reuse/portability\n\n- the H3 option for a finished-URI to be scheduled to revisit at a \n(settings/overlay-sensitive) future interval\n\n- H3&#39;s faster and less-serialization-sensitive checkpointing, which has \nthe ultimate goal of supporting a model where crawls never &#39;finish&#39;, but \nare simply put on hold for days/weeks/etc., then resumed to catch-up \nfrom where they left off\n\n- H3&#39;s added options for feeding/force-feeding new &quot;do-crawl&quot; or &quot;don&#39;t \ncrawl&quot; URIs/configuration to a running crawl (chiefly through the \n&#39;action&#39; directory, web UI, or scripting interfaces so far)\n\nSo with some manual effort in your processes, you can approach a \n&#39;continuous&#39; model.\n\nFuture steps are likely to be:\n\n- policies for automatically determining revisit intervals from history \ninformation and other configuration\n\n- enabling the history mechanism and/or entire frontier (of queues and \ndiscovery-information) to use a persistent bulk remote store\n\n- improvements to the relaunch-in-place conventions that leave fewer \nconfusing redundant files\n\n- improvements to the ease-of-use, reliability, and \nrobustness-across-software-upgrades of the checkpointing mechanism\n\nBut, there&#39;s not yet a firm roadmap for these future steps -- it depends \non which projects -- internal to IA or external -- drive development \nrelative to other priorities.\n\n- Gordon @ IA\n\n\n"}}