{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"JbO1y8FgIhxsbXrgFkir9b8zg0dGjoaTwMJYPYfh91tOgHPmDUr74Iv2TpNCJ49lXXlFePyogvX51JX1TRQ57vYDvSU","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Heritrix crawling strategy","postDate":"1320780419","msgId":7394,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFQjk4MjgzLjkwNzA2MDhAY3MuY211LmVkdT4=","inReplyToHeader":"PDRFQjg5OTkzLjgwNDAyMDhAYXJjaGl2ZS5vcmc+","referencesHeader":"PGo4bXRkcisybnI1QGVHcm91cHMuY29tPiA8NEVCODEyRjQuNDA3MDYwOEBhcmNoaXZlLm9yZz4gPDRFQjgyQ0E3LjYwMDA3MDRAY3MuY211LmVkdT4gPDRFQjgzNTA5LjUwMzAyMDlAYXJjaGl2ZS5vcmc+IDw0RUI4NDM4Ri4zMDQwNjA2QGNzLmNtdS5lZHU+IDw0RUI4OTk5My44MDQwMjA4QGFyY2hpdmUub3JnPg=="},"prevInTopic":7393,"nextInTopic":7395,"prevInTime":7393,"nextInTime":7395,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"Thanks for all of this information.  It really helps. Another thing that I have seen when running the small test crawls (setting maxTimeSeconds) is that when","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 52978 invoked from network); 8 Nov 2011 19:27:01 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m16.grp.sp2.yahoo.com with QMQP; 8 Nov 2011 19:27:01 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.96)\n  by mta5.grp.sp2.yahoo.com with SMTP; 8 Nov 2011 19:27:01 -0000\r\nX-Received: from [128.2.209.200] (SAVOY.LTI.CS.CMU.EDU [128.2.209.200])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id pA8JQwSp007398\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tTue, 8 Nov 2011 14:26:58 -0500\r\nMessage-ID: &lt;4EB98283.9070608@...&gt;\r\nDate: Tue, 08 Nov 2011 14:26:59 -0500\r\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:7.0.1) Gecko/20110929 Thunderbird/7.0.1\r\nMIME-Version: 1.0\r\nTo: Gordon Mohr &lt;gojomo@...&gt;\r\nCc: archive-crawler@yahoogroups.com, Noah Levitt &lt;nlevitt@...&gt;\r\nReferences: &lt;j8mtdr+2nr5@...&gt; &lt;4EB812F4.4070608@...&gt; &lt;4EB82CA7.6000704@...&gt; &lt;4EB83509.5030209@...&gt; &lt;4EB8438F.3040606@...&gt; &lt;4EB89993.8040208@...&gt;\r\nIn-Reply-To: &lt;4EB89993.8040208@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2010.4.9.4220\r\nX-SMTP-Spam-Clean: 8% (\n KNOWN_FREEWEB_URI 0.05, ECARD_KNOWN_DOMAINS 0, __BOUNCE_CHALLENGE_SUBJ 0, __BOUNCE_NDR_SUBJ_EXEMPT 0, __CANPHARM_UNSUB_LINK 0, __CP_MEDIA_2_BODY 0, __CP_URI_IN_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __KNOWN_FREEWEB_URI3 0, __KNOWN_FREEWEB_URI4 0, __KNOWN_FREEWEB_URI5 0, __KNOWN_FREEWEB_URI7 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __SANE_MSGID 0, __TO_MALFORMED_2 0, __URI_NO_WWW 0, __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.96\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix crawling strategy\r\nX-Yahoo-Group-Post: member; u=500983475; y=mqW5cUQks14O5SRdan-1maFflhcFnwVg2NeiX3dazKPdbwoTNkGukg\r\nX-Yahoo-Profile: david_pane1\r\n\r\nThanks for all of this information.  It really helps.\n\nAnother thing that I have seen when running the small test crawls \n(setting maxTimeSeconds) is that when the time expires, the crawler \nstarts to stop and the job gets stuck in the &quot;Active: STOPPING&quot; state.\nRate is 0 URIs/sec and Load is 0 active of 0 threads.\n\nSelecting terminate and/or teardown does not help.  I&#39;ve had to force \nkill the crawler in the past, but then the reports are not created.  Do \nyou have any insight on why this is happening?\n\nI currently have a job in the stopping state for the past 8 hrs.\n\n--David\n\nOn 11/7/2011 9:53 PM, Gordon Mohr wrote:\n&gt; On 11/7/11 12:46 PM, David Pane wrote:\n&gt;&gt; Thank you Gordon.\n&gt;&gt;\n&gt;&gt; We do not plan to do repeated crawls, but capture a 1 billion page\n&gt;&gt; dataset to be used for information retrieval research purposes. We would\n&gt;&gt; like to capture as many high quality pages as possible.\n&gt;\n&gt; If the end goal is a &quot;best billion&quot; dataset that is then reused for a\n&gt; long time, you still might want to do that in indirect/incremental steps\n&gt; or recrawls: for example, crawl 2 billion, then pick the best billion.\n&gt; Or crawl 500 million with a naive approach, do an offline PR\n&gt; calculation, then do the &#39;production&#39; 1B crawl biased by the results of\n&gt; the 500 million. Etc...\n&gt;\n&gt;&gt; We would also like to have a blacklist of unwanted sites. This blacklist\n&gt;&gt; would possibly contain millions hosts. Do you have any experience in the\n&gt;&gt; resources needed for this? Would this slow down the speed of the crawl?\n&gt;\n&gt; I&#39;ve not done anything with a blacklist that big. You certainly wouldn&#39;t\n&gt; want to use a list of regexes!\n&gt;\n&gt; The SURT-based DecideRules use a sorted list of prefixes, all in memory,\n&gt; and have roughly O(log n) lookup. Perhaps that would work for your\n&gt; purposes, if you have a big RAM machine; you should do some tests and\n&gt; back-of-the-envelope calculations to check for sure. If the blacklist\n&gt; entries are always hosts, some other hash-based structure might work\n&gt; with even less RAM overhead and O(1) lookup.\n&gt;\n&gt;&gt; In our small crawls that we have been running, we found that although we\n&gt;&gt; have setup for a breadth first crawl, days into the crawl and 10&#39;s of\n&gt;&gt; million of pages crawled, the crawler has still only crawled a small\n&gt;&gt; percentage of seeds (under 10% of a 2 million host seed list - 1200\n&gt;&gt; threads). It appears that most of the seeds are in a separate queue so\n&gt;&gt; would cycling through the queues (balance-replenish-amount to a lower\n&gt;&gt; amount maybe 100) help cover all of the queues?\n&gt;\n&gt; Yes, lowering the balance-replenish-amount will cycle through the queues\n&gt; more quickly, since each will stay in active rotation a shorter amount\n&gt; of time until other queues are given a chance.\n&gt;\n&gt; Another factor which has been previously noted as slowing the progress\n&gt; through a diversity of hosts, in crawls with giant seed lists, is if\n&gt; many of the hosts turn out to be unresponsive.\n&gt;\n&gt; In some cases, being unresponsive means a long 20-second socket-timeout,\n&gt; which occupies the worker thread that whole time. Then, the queue goes\n&gt; into a &#39;long snooze&#39; (default 15 minutes) before retrying. Only after a\n&gt; bunch of retries (default 30) will the URI completely fail out as having\n&gt; failed for too many times over too long. Imagining a worst-case\n&gt; scenario: if your first 54000 seeds were all unresponsive like that,\n&gt; your 1200 threads could be doing nothing but these timeouts for over 7\n&gt; hours.\n&gt;\n&gt; Some possible ways to minimize this effect:\n&gt;\n&gt; � shorten the soTimeoutMs (socket timeout) in FetchHTTP below the\n&gt; default 20 seconds (less time holding a thread)\n&gt;\n&gt; � lengthen the retryDelaySeconds above the default 900 seconds (more\n&gt; time between held sessions\n&gt;\n&gt; � lessen the maxRetries below the default 30 (give up entirely on a URI\n&gt; sooner)\n&gt;\n&gt; � prescreen the URIs outside Heritrix to eliminate those not\n&gt; resolving/running-any-webserver\n&gt;\n&gt; Though, changing those default parameters does make the process less\n&gt; robust against the sort of temporary network/server outages that are\n&gt; common and which you normally wouldn&#39;t want to completely exclude a site\n&gt; from your crawl.\n&gt;\n&gt; Looking at the source now, it also seems like a option in Heritrix 1.0\n&gt; that could push &#39;long snoozes&#39; to the back of the list of waiting queues\n&gt; (preventing a logjam of unresponsive hosts among the active queues) does\n&gt; not appear to have been translated properly into H3, so is now inactive.\n&gt; (The WorkQueueFrontier property snoozeLongMs, a threshold above which\n&gt; snoozes are supposed to result in deactivation to the end of the list of\n&gt; waiting queues -- and thus potentially a much longer snooze -- to give\n&gt; other queues a chance.) Fixing this could improve this behavior, in\n&gt; crawls reaching many unresponsive hosts, as well.\n&gt;\n&gt; - Gordon\n&gt;\n&gt;&gt; --David\n&gt;&gt;\n&gt;&gt; On 11/7/2011 2:44 PM, Gordon Mohr wrote:\n&gt;&gt;&gt; Heritrix is not very accommodating for an OPIC-ordered crawl in the way\n&gt;&gt;&gt; that it is not easy to reorder URIs inside a single queue, once they are\n&gt;&gt;&gt; queued. (You&#39;d have to peel them all off, sort, and re-queue... which\n&gt;&gt;&gt; might offset some or all of whatever ordering benefit you were hoping to\n&gt;&gt;&gt; achieve.)\n&gt;&gt;&gt;\n&gt;&gt;&gt; You can, however, re-prioritize entire queues via the queue-precedence\n&gt;&gt;&gt; mechanism. So you could bias the crawler to spend more early effort on\n&gt;&gt;&gt; sites that are large, or have a larger number of total inlinks, or other\n&gt;&gt;&gt; measures.\n&gt;&gt;&gt;\n&gt;&gt;&gt; If you are performing repeated crawls, you could also use PR\n&gt;&gt;&gt; calculations from a completed earlier crawl to bias the ordering of a\n&gt;&gt;&gt; later crawl (because you would have PR values usable at the moment of\n&gt;&gt;&gt; queueing).\n&gt;&gt;&gt;\n&gt;&gt;&gt; From a quick glance at the paper you reference, it doesn&#39;t seem like\n&gt;&gt;&gt; the magnitude of benefit for incremental-PR-estimation is very large. As\n&gt;&gt;&gt; they attribute to their footnote [11], and other broad-crawl studies\n&gt;&gt;&gt; have also suggested, &quot;even a random strategy can perform well on the\n&gt;&gt;&gt; Web, in the sense that a random walk on the Web is biased towards pages\n&gt;&gt;&gt; with high Pagerank&quot;. For this reason, I wouldn&#39;t be too concerned about\n&gt;&gt;&gt; crawling in PR-order unless you know you often have to cut your crawl\n&gt;&gt;&gt; short and thus strongly suspect more important pages are not being\n&gt;&gt;&gt; crawled... and even in that case, in a repeated crawl series you could\n&gt;&gt;&gt; adjust future crawls to offset this problem.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Also important to note: a strict &#39;breadth-first&#39; crawl isn&#39;t a realistic\n&gt;&gt;&gt; description of any large politeness-limited crawl. Almost immediately in\n&gt;&gt;&gt; any crawl, politeness-to-servers means you may only be a few hops deep\n&gt;&gt;&gt; into big sites, while being arbitrarily deep on other paths that are\n&gt;&gt;&gt; spread over many non-politeness-bottlenecked servers. This certainly\n&gt;&gt;&gt; helps in discovering (and crawling to completeness) new and\n&gt;&gt;&gt; small-URI-count independent servers... though it leaves the risk that\n&gt;&gt;&gt; potentially &#39;important&#39; (by PR) pages, &#39;deep&#39; within large servers, may\n&gt;&gt;&gt; not be reached within some capped time/storage budget.\n&gt;&gt;&gt;\n&gt;&gt;&gt; - Gordon\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 11/7/11 11:08 AM, David Pane wrote:\n&gt;&gt;&gt;&gt; Hi Noah,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; OPIC stands for Online Page Importance Computation.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; This scoring is in Nutch as a plugin\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Here is a paper that shows that OPIC scoring is better than a breadth\n&gt;&gt;&gt;&gt; first crawl.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; &quot;Crawling a country: better strategies than breadth-first for web page\n&gt;&gt;&gt;&gt; ordering&quot;\n&gt;&gt;&gt;&gt; http://dl.acm.org/citation.cfm?id=1062768\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; --David\n&gt;&gt;&gt;&gt; On 11/7/2011 12:18 PM, Noah Levitt wrote:\n&gt;&gt;&gt;&gt;&gt; Hello David,\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; What is OPIC?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Noah\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; On 2011-10-31 12:37 , david_pane1 wrote:\n&gt;&gt;&gt;&gt;&gt;&gt; Is it possible to apply a crawling strategy based OPIC algorithm to a\n&gt;&gt;&gt;&gt;&gt;&gt; Heritrix 3.x crawl?\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; We initially though that we could use the URI precedence, but it\n&gt;&gt;&gt;&gt;&gt;&gt; appears that there isn&#39;t any provision for a URI&#39;s precedence to\n&gt;&gt;&gt;&gt;&gt;&gt; change after initial assignment and as stated here\n&gt;&gt;&gt;&gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/6022 by\n&gt;&gt;&gt;&gt;&gt;&gt; Gordon, depending on how they are queued, they could be crawled\n&gt;&gt;&gt;&gt;&gt;&gt; out of\n&gt;&gt;&gt;&gt;&gt;&gt; order.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Can anyone share some insight on this?\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; --David\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;\n\n"}}