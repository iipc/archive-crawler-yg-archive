{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"JTU7hmX4N3vJLQd9p2LE_StdaAuMKpDS3VYdfFSlc7eJKvYPxGn_j7-0wgy1CTvIbi_hXv-ghsg7yaSUuXKy8tI49D-i1c8","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Checkpointing, StatusCode","postDate":"1150940270","msgId":2961,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0OTlGNDZFLjgwMDA3MDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGU3Ymxzayt0bTdiQGVHcm91cHMuY29tPg==","referencesHeader":"PGU3Ymxzayt0bTdiQGVHcm91cHMuY29tPg=="},"prevInTopic":2957,"nextInTopic":0,"prevInTime":2960,"nextInTime":2962,"topicId":2957,"numMessagesInTopic":2,"msgSnippet":"... They were created for similar purposes, but don t work the same. Recovering from the recovery log is more crude -- it replays the known URL discoveries and","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 59690 invoked from network); 22 Jun 2006 01:37:25 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m15.grp.scd.yahoo.com with QMQP; 22 Jun 2006 01:37:24 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta4.grp.scd.yahoo.com with SMTP; 22 Jun 2006 01:37:22 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id DED0A1415684D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 21 Jun 2006 18:36:53 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 22648-02-25 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 21 Jun 2006 18:36:53 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 7F428141563B5\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 21 Jun 2006 18:36:53 -0700 (PDT)\r\nMessage-ID: &lt;4499F46E.8000709@...&gt;\r\nDate: Wed, 21 Jun 2006 18:37:50 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e7blsk+tm7b@...&gt;\r\nIn-Reply-To: &lt;e7blsk+tm7b@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Checkpointing, StatusCode\r\nX-Yahoo-Group-Post: member; u=137285340; y=2eF4qPkTygwo-DU0nL7AXow2MnUPoIS-_Y-M6yHt4RIK\r\nX-Yahoo-Profile: gojomo\r\n\r\nSamuel wrote:\n&gt; Hello,\n&gt; I have a couple questions about how heritrix works.\n&gt; \n&gt; 1)\n&gt; Does recovey from recover.gz work exactly the same as checkpointing?\n&gt; Im mean, were they created for the same purpose?\n\nThey were created for similar purposes, but don&#39;t work the same.\n\nRecovering from the recovery log is more crude -- it replays the known \nURL discoveries and and completions from the earlier crawl into a \nbrand-new crawl. That will closely reproduce an important part of the \nearlier crawl&#39;s state -- which URIs were waiting and finished -- but not \nall the other running state. So you really have two distinct crawls -- \nthe second just preconfigured to not revisit certain URIs, and begin \nwith certain non-seed URIs in the queues.\n\nThe checkpointing procedure is designed to snapshot the entire state of \nthe crawl at the moment of the checkpoint. So resuming from a checkpoint \nshould behave just like the first crawl sat paused for the entire time \nfrom when the checkpoint was made to when it was resumed. All running \nstate should be preserved, including things (like total size of data \nreceived or ARC name numbering) that are not rebuildable form the \nrecovery log.\n\nThe advantage of the recovery log is that it is updated continuously, \nwhile the advantage of the checkpoint is the maintenance of one \nconsistent virtual &#39;crawl&#39;.\n\n&gt; The reason Im asking this is that Im able to recover from recover.gz\n&gt; after the crawl has finished whereas if I do it by checkpointing\n&gt; (right before crawl finishes), I wouldnt have anything to crawl\n&gt; afterwards, unless I introduce new seeds to it. So, I create jobs\n&gt; based on a recovery everyday, and as the domain Im crawling has been\n&gt; updated on a daily basis, I always have something to crawl, even not\n&gt; introducing new seeds to it.\n\nIn general, recovering from a recovery log will *not* cause URIs to be \nrevisited. However, one idiosyncrasy of the starting-a-new-crawl process \nis that any seeds are scheduled without regard to their already-visited \nstatus in the recovery-log. So for seeds (and seeds only) a recovery-log \nrestart will result in a refetch. Any URIs discovered on those seeds \nthat were already in the recovery-log will not be rescheduled -- but any \nnew URIs that have appeared on the seeds will be properly treated as new.\n\nI think this explains the behavior you&#39;ve seen. I think you could \nsimulate the same behavior with a checkpoint-resume, by resuming the \ncrawl to a paused state, and then importing whatever URIs you want to be \nre-visited (such as the seeds or some other set) using the &#39;force-fetch&#39; \nflag on the import action.\n\n&gt; 2)\n&gt; Im getting some -5000 status code during a crawl and, acording to the\n&gt; user manual, this would only happen if scope was changed during a\n&gt; crawl. I get -5000 only at documents that match a specific word on a\n&gt; URIRegExpFilter I&#39;ve created for an override. Also, these documents\n&gt; that get -5000 do not have parent URIs...aparently, the&#39;re somehow\n&gt; assembled in a script(js). Are you aware of situations that might\n&gt; trigger this error? \n\nWhere did you install the URIRegExpFilter? This would be expected \nbehavior if the URLs were scheduled (put onto waiting queues) at a time \nwhen they were in-scope, but then by the time they came up for crawling, \na change (like your override) has ruled them out-of-scope.\n\nBy &#39;parent&#39; URIs, do you mean &#39;via&#39; URIs, shown on the crawl.log after \nthe URI itself? Every URI except seeds or those imported by the operator \nafter a crawl has begun should have a &#39;via&#39;. If you think you have a \ncase where one is not shown for a URI that was discovered during the \ncrawl, we&#39;d appreciate more details... such as excerpts from the crawl.log.\n\n- Gordon @ IA\n\n"}}