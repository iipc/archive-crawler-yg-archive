{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"hAO_v4K04eWOBQY2bbQLPzHIzrOP0xPd_c-1vC7Ch9LPcJLuRinrItdfYiS0uA4gfp8UT5IQvPcIAK7044E8VCQ9VAdq1Zg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Re : [archive-crawler] Re: questions on how to setup Heritrix 3 on two machines","postDate":"1328119582","msgId":7606,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMjk3RjFFLjUwMzAzMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEzMjgxMTU1NDAuMzU3NTcuWWFob29NYWlsTmVvQHdlYjE3MTYxOS5tYWlsLmlyMi55YWhvby5jb20+","referencesHeader":"PGpnYmplMitmYzd0QGVHcm91cHMuY29tPiA8amdicHVpK29ub2dAZUdyb3Vwcy5jb20+IDwxMzI4MTE1NTQwLjM1NzU3LllhaG9vTWFpbE5lb0B3ZWIxNzE2MTkubWFpbC5pcjIueWFob28uY29tPg=="},"prevInTopic":7605,"nextInTopic":0,"prevInTime":7605,"nextInTime":7607,"topicId":7341,"numMessagesInTopic":9,"msgSnippet":"If you don t already have things set up where the machines can easily access the same file, you should probably just copy the file to both machines. The seeds","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 93874 invoked from network); 1 Feb 2012 18:06:23 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m5.grp.sp2.yahoo.com with QMQP; 1 Feb 2012 18:06:23 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta5.grp.sp2.yahoo.com with SMTP; 1 Feb 2012 18:06:23 -0000\r\nX-Received: (qmail 88614 invoked by uid 0); 1 Feb 2012 18:06:22 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay01.pair.com with SMTP; 1 Feb 2012 18:06:22 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4F297F1E.5030301@...&gt;\r\nDate: Wed, 01 Feb 2012 10:06:22 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: hatem hatem &lt;hatoum13@...&gt;\r\nReferences: &lt;jgbje2+fc7t@...&gt; &lt;jgbpui+onog@...&gt; &lt;1328115540.35757.YahooMailNeo@...&gt;\r\nIn-Reply-To: &lt;1328115540.35757.YahooMailNeo@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: Re : [archive-crawler] Re: questions on how to setup Heritrix\n 3 on two machines\r\nX-Yahoo-Group-Post: member; u=137285340; y=mXU_hzCOeNvqpMA2BB8a7sZQjw40XBXAo3Lbqr9DeqVe\r\nX-Yahoo-Profile: gojomo\r\n\r\nIf you don&#39;t already have things set up where the machines can easily \naccess the same file, you should probably just copy the file to both \nmachines.\n\nThe seeds file is only read once, at the launch of each job, so there&#39;s \nno need for them to actually be reading the exact same file over the \nnetwork.\n\nPopping up a bit, are you sure you need to use two machines? The \nconfiguration is tricky, and so is the (still-manual) cross-feeding of \n&#39;diversion&#39; URIs found by one crawler that should be handled by the \nother. Monitoring the multiple crawlers in two places, and compiling \ncombined reports, is harder. Doing these things will take time away from \npaying close attention (and optimizing the speed/coverage) of a single \ncrawler.\n\nIt&#39;s a lot of trouble to go through to have 2x2GB RAM crawls versus \n1x2GB RAM crawl; I would only be doing it I was sure optimizing one \nmachine (or just letting it run twice as long) wasn&#39;t good enough.\n\nAlso, machines with only 3GB RAM might be problematic for a large crawl \ndue to other configuration matters (CPU/disk/bandwidth), and 4GB of ECC \nRAM costs less than $40.\n\n- Gordon\n\nOn 2/1/12 8:59 AM, hatem hatem wrote:\n&gt;\n&gt;\n&gt; Ok, now to specify the seeds file in the first instance which is :\n&gt; &lt;property name=&quot;localName&quot; value=&quot;0&quot; /&gt;\n&gt; &lt;property name=&quot;crawlerCount&quot; value=&quot;2&quot; /&gt;\n&gt;\n&gt; I should put :\n&gt;\n&gt; &lt;bean id=&quot;seeds&quot; class=&quot;org.archive.modules.seeds.TextSeedModule&quot;&gt;\n&gt; &lt;property name=&quot;textSource&quot;&gt;\n&gt; &lt;bean class=&quot;org.archive.spring.ConfigFile&quot;&gt;\n&gt; &lt;property name=&quot;path&quot; value=&quot;/path/seeds.txt&quot; /&gt;\n&gt; &lt;/bean&gt;\n&gt; &lt;/property&gt;\n&gt; &lt;property name=&#39;sourceTagSeeds&#39; value=&#39;false&#39;/&gt;\n&gt; &lt;/bean&gt;\n&gt;\n&gt; and in the second instance (in the second machine) :\n&gt; &lt;property name=&quot;localName&quot; value=&quot;1&quot; /&gt;\n&gt; &lt;property name=&quot;crawlerCount&quot; value=&quot;2&quot; /&gt;\n&gt;\n&gt; How can I put the path of seeds file (in the first machine @IP : x.x.x.x) :\n&gt;\n&gt; &lt;bean id=&quot;seeds&quot; class=&quot;org.archive.modules.seeds.TextSeedModule&quot;&gt;\n&gt; &lt;property name=&quot;textSource&quot;&gt;\n&gt; &lt;bean class=&quot;org.archive.spring.ConfigFile&quot;&gt;\n&gt; &lt;property name=&quot;path&quot; value=&quot;???&quot; /&gt;\n&gt; &lt;/bean&gt;\n&gt; &lt;/property&gt;\n&gt; &lt;property name=&#39;sourceTagSeeds&#39; value=&#39;false&#39;/&gt;\n&gt; &lt;/bean&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *De :* david_pane1 &lt;dpane@...&gt;\n&gt; *À :* archive-crawler@yahoogroups.com\n&gt; *Envoyé le :* Mercredi 1 février 2012 17h40\n&gt; *Objet :* [archive-crawler] Re: questions on how to setup Heritrix 3 on\n&gt; two machines\n&gt;\n&gt; Yes, they can work together but you must feed both instances the same\n&gt; seed list. It will then decide based on domains which seeds (and\n&gt; discovered URIs) each will crawl.\n&gt;\n&gt; These two properties shows each crawler instance what its name\n&gt; (localName) is and how many crawlers (crawlerCount) it is &quot;sharing&quot; the\n&gt; crawl with.\n&gt;\n&gt; &lt;property name=&quot;localName&quot; value=&quot;1&quot; /&gt;\n&gt; &lt;property name=&quot;crawlerCount&quot; value=&quot;2&quot; /&gt;\n&gt;\n&gt; If one crawler finds URIs that the other crawler is suppose to crawl, it\n&gt; places those URIs in the diversions folder. You have to manually move\n&gt; those diversion files to the other crawler instance (with some\n&gt; modifications to the file before placing them in the actions folder of\n&gt; the other crawler instance).\n&gt;\n&gt; --David\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, &quot;hatoum13&quot; &lt;hatoum13@...&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; Hi Davi,\n&gt;  &gt;\n&gt;  &gt; Thanks for your response,\n&gt;  &gt;\n&gt;  &gt; Apparently you are talking about the same machine (physically one\n&gt; computer).\n&gt;  &gt;\n&gt;  &gt; In my case, i have technical constraints which is a very limited\n&gt; memory in my machines (3 GB for each one) and I want to accelerate the\n&gt; crawl by launching it in two machines (connected in the same network) :\n&gt;  &gt; 1 st machine : instance with 2 GB of memory\n&gt;  &gt; 2 nd machine : instance with 2 GB of memory\n&gt;  &gt; My question is : Can the two instances works together on the same\n&gt; seeds.txt (which contains 12 millions line)?\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, &quot;david_pane1&quot; &lt;dpane@&gt; wrote:\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; After creating a job on the mail page on the Heritrix web\n&gt; interface, under &quot;Add Job Directory&quot;(in example below the jobname is\n&gt; myjob1), place your seed file into the jobname directory.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; From the directory where you installed heritrix, you will see a\n&gt; directory jobs, inside jobs will be a directory named &quot;jobname&quot;&quot;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; .../HeritrixInstallDirectory/jobs/myjob1/seeds.txt\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; each instance should have the identical seed list placed in its\n&gt; jobs directory. Seed list should have a format like this:\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; http://www.domain.com/\n&gt;  &gt; &gt; http://www.domain2.com/\n&gt;  &gt; &gt; http://www.domain3.com/\n&gt;  &gt; &gt; http://www.domain4.com/\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; I hop this helps\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; --David\n&gt;  &gt; &gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, &quot;hatoum13&quot; &lt;hatoum13@&gt; wrote:\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Hi,\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I&#39;m trying to install H3 on 2 different machines (in a local\n&gt; network) to crawl a large seeds file (more than 10 millions URL), I&#39;m\n&gt; faced to little problem, how can I put the same seeds file for both\n&gt; instances?\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I tried to make the file reachable from a web server but the Job\n&gt; can&#39;t be built because of an exception when it&#39;s creating the seed&#39;s bean.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Thanks\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, &quot;david_pane1&quot; &lt;dpane@&gt; wrote:\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; With some effort, tips from Gordan and using the H1 to get an\n&gt; idea of the property names of the HashCrawlMapper, I was able to\n&gt; successfully configure and run a two machine test crawl. I figured I\n&gt; would post this for others to refer to in the future.\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; Using the default crawler-beans.cxml, I added the following (in\n&gt; addition to the necessary user-agent and the seed list changes )\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; I defined the following in crawler-beans.cxml on machine 1:\n&gt;  &gt; &gt; &gt; &gt; &lt;bean id=&quot;hashCrawlMapperProcessor&quot;\n&gt; class=&quot;org.archive.crawler.processor.HashCrawlMapper&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;localName&quot; value=&quot;0&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;diversionDir&quot; value=&quot;diversions&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;checkUri&quot; value=&quot;True&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;checkOutlinks&quot; value=&quot;False&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;rotationDigits&quot; value=&quot;10&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;crawlerCount&quot; value=&quot;2&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; and this definition on machine #2 (only change was local-name\n&gt; value):\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;bean id=&quot;hashCrawlMapperProcessor&quot;\n&gt; class=&quot;org.archive.crawler.processor.HashCrawlMapper&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;localName&quot; value=&quot;1&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;diversionDir&quot; value=&quot;diversions&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;checkUri&quot; value=&quot;True&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;checkOutlinks&quot; value=&quot;False&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;rotationDigits&quot; value=&quot;10&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;crawlerCount&quot; value=&quot;2&quot; /&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; Then I added in call to hashCrawlMapperProcessor in the\n&gt; candidateProcessors chain.\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;!-- now, processors are assembled into ordered CandidateChain\n&gt; bean --&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;bean id=&quot;candidateProcessors&quot;\n&gt; class=&quot;org.archive.modules.CandidateChain&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;processors&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;list&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;!-- apply scoping rules to each individual candidate URI... --&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;ref bean=&quot;candidateScoper&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;!-- check every URI discovered even before it is ever enqueued --&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;ref bean=&quot;hashCrawlMapperProcessor&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;!-- ...then prepare those ACCEPTed to be enqueued to frontier. --&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;ref bean=&quot;preparer&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/list&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/property&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; and a call to hashCrawlMapperProcessor in the FetchChain\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;!-- now, processors are assembled into ordered FetchChain bean --&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;bean id=&quot;fetchProcessors&quot; class=&quot;org.archive.modules.FetchChain&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;property name=&quot;processors&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;list&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;!-- re-check scope, if so enabled... --&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;ref bean=&quot;preselector&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;ref bean=&quot;hashCrawlMapperProcessor&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; .\n&gt;  &gt; &gt; &gt; &gt; .\n&gt;  &gt; &gt; &gt; &gt; .\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; --David\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, &quot;david_pane1&quot; &lt;dpane@&gt; wrote:\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; I am new to using Heritrix 3 and have only limited experience\n&gt; with H1. I would like to setup H3 on two (maybe more in the future)\n&gt; machines for a distributed crawl. I would also like to apply the\n&gt; HashCrawlMapper to the processing chains so the URIs are shared between\n&gt; the two crawlers.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Although there are discussions about multiple machine crawls\n&gt; and the use of HashCrawlMapper, I could not find specifics on the setup\n&gt; of this (i.e even an example crawler-beans.cxml with a default\n&gt; configuration). I understand that both crawlers should have the same\n&gt; configuration. However, how do you assign a crawler/node name to each so\n&gt; that the HashCrawlMapper can assign URIs and each crawler understands\n&gt; which ones to crawl.?\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Additionally, when attempting to define a name bean for the\n&gt; HashCrawlMapper I am unclear on how to identify the available property\n&gt; names and values. (And this is true for any bean that is not clearly\n&gt; defined as the default crawler-beans.cxml.)\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; I understand that the customization of the configuration for\n&gt; optimal performance may take many iterations, but can anyone help me\n&gt; define the initial configuration of a two crawler/machine system which\n&gt; would work as a distributed crawl.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Thank you,\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; -David\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}