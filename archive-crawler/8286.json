{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":475889180,"authorName":"Arian","from":"&quot;Arian&quot; &lt;ariansacct@...&gt;","profile":"ariansacct","replyTo":"LIST","senderId":"i_ggXFr8tTlUWe_tkK4ZsdV4pSYKTD0hX5ttlKFR1_E8G0OJGRfZv3HdEUMBzmRD05aMvnwvB-iQSPy7DJb2Ht9ITI6G","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Heritix doesn&#39;t observe limit on number of documents","postDate":"1375791950","msgId":8286,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGt0cXEwZStwdDM4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDUyMDBCMEY0LjIwOTA0MDlAYXJjaGl2ZS5vcmc+"},"prevInTopic":8285,"nextInTopic":8287,"prevInTime":8285,"nextInTime":8287,"topicId":8278,"numMessagesInTopic":5,"msgSnippet":"And to clear things, I don t want to crawl anything outside the host. Say I want to crawl www.example1.com, and some pages there have outlinks to","rawEmail":"Return-Path: &lt;ariansacct@...&gt;\r\nX-Sender: ariansacct@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 61012 invoked by uid 102); 6 Aug 2013 12:25:52 -0000\r\nX-Received: from unknown (HELO mtaq3.grp.bf1.yahoo.com) (10.193.84.142)\n  by m3.grp.bf1.yahoo.com with SMTP; 6 Aug 2013 12:25:52 -0000\r\nX-Received: (qmail 915 invoked from network); 6 Aug 2013 12:25:52 -0000\r\nX-Received: from unknown (HELO ng4-vm12.bullet.mail.gq1.yahoo.com) (98.136.219.51)\n  by mtaq3.grp.bf1.yahoo.com with SMTP; 6 Aug 2013 12:25:52 -0000\r\nX-Received: from [98.137.0.85] by ng4.bullet.mail.gq1.yahoo.com with NNFMP; 06 Aug 2013 12:25:52 -0000\r\nX-Received: from [10.193.94.45] by tg5.bullet.mail.gq1.yahoo.com with NNFMP; 06 Aug 2013 12:25:51 -0000\r\nDate: Tue, 06 Aug 2013 12:25:50 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;ktqq0e+pt38@...&gt;\r\nIn-Reply-To: &lt;5200B0F4.2090409@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Arian&quot; &lt;ariansacct@...&gt;\r\nSubject: Re: Heritix doesn&#39;t observe limit on number of documents\r\nX-Yahoo-Group-Post: member; u=475889180; y=CaZrpI-tRwEtgtcVis1gKbCeIGfSfyxt07zar4yAbWN9Wl15pQ\r\nX-Yahoo-Profile: ariansacct\r\n\r\nAnd to clear things, I don&#39;t want to crawl anything outside the host. Say I=\r\n want to crawl www.example1.com, and some pages there have outlinks to www.=\r\nexample2.com. Here I provide example1.com as a seed, and want 10 pages (exa=\r\nctly 10, unless there doesn&#39;t exist that many) only from from example1.com.=\r\n\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;=\r\n\n&gt; On 8/3/13 11:02 PM, Arian wrote:\n&gt; &gt; Hi Everyone,\n&gt; &gt;\n&gt; &gt; I&#39;m using Heri=\r\ntrix 3.1.1 to crawl the contents of a number of sites.\n&gt; &gt; I want it to get=\r\n 10 pages from each seed, not including pages from\n&gt; &gt; outlinks of the seed=\r\n. I set the number of documents limit in the\n&gt; &gt; configuration file to 10, =\r\nand it passed that. Set it to 20, passed it\n&gt; &gt; again. If that&#39;s not the ri=\r\nght tweak, where should I set the limit\n&gt; &gt; for it to work? About leaving o=\r\nut external pages, I have 100+ sites\n&gt; &gt; to crawl, so I can&#39;t set regexes m=\r\natching every single URL (and don&#39;t\n&gt; &gt; know if there&#39;s a better way rather=\r\n than that.)\n&gt; &gt;\n&gt; &gt; I&#39;d appreciate any help.\n&gt; \n&gt; It&#39;s unclear what you&#39;ve=\r\n tried. There&#39;s no setting in the standard \n&gt; configuration which would exa=\r\nctly limit the crawl to 10 &quot;pages&quot; &quot;from \n&gt; each seed&quot; and &quot;not including o=\r\nutlinks of the seed&quot;. (Those descriptions \n&gt; could mean a variety of things=\r\n.)\n&gt; \n&gt; (If the crawl seems to be getting URIs from more sites than you \n&gt; =\r\nexpected, you should check out the FAQ entry at:\n&gt; \n&gt; https://webarchive.ji=\r\nra.com/wiki/display/Heritrix/unexpected+offsite+content \n&gt; )\n&gt; \n&gt; The &#39;Craw=\r\nlLimitEnforcer&#39; which you may have seen sets count/size/time \n&gt; limits for =\r\nthe entire crawl, not individual sites.\n&gt; \n&gt; The optional QuotaEnforcer pro=\r\ncessor can be added to the &#39;prefetch&#39; \n&gt; chain, and then can reject URIs af=\r\nter certain configured counts of URIs \n&gt; per hostname/server/&#39;group&#39; are re=\r\nached. (Server =3D hostname+port; \n&gt; &#39;group&#39; is usually the same as server =\r\nunless using some advanced \n&gt; frontier configuration.)\n&gt; \n&gt; QuotaEnforcer i=\r\ns probably what you want... is this what you tried?\n&gt; \n&gt; (An alternate way =\r\nto get a similar effect is by adjusting the \n&gt; &#39;queueTotalBudget&#39; frontier =\r\nsetting, which stops crawling URIs from a \n&gt; queue -- same as &#39;group&#39; above=\r\n -- after a certain amount of effort is \n&gt; expended, with the usual policy =\r\nbeing roughly one URI equals one unit of \n&gt; effort. But, QuotaEnforcer is l=\r\nikely the better approach unless you need \n&gt; to understand the frontier int=\r\nernals.)\n&gt; \n&gt; - Gordon\n&gt;\n\n\n\n"}}