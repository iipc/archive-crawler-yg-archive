{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"TdujUEVAPm_pXXCNxMrp_DRf4ZqsweM-qaY8mlH0xRj_D1TyGcLni_IHVOuc_LiP4A_h-RkSlGGGWKjllWwV5n6Nq6HFZHY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] getting urls..","postDate":"1149189173","msgId":2900,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0N0YzQzM1LjkwMDAzMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ0N0U4NUVBLjIwODAxMDVAYXJjaGl2ZS5vcmc+","referencesHeader":"PGU1bHV0dCs0ajRtQGVHcm91cHMuY29tPiA8NDQ3RTg1RUEuMjA4MDEwNUBhcmNoaXZlLm9yZz4="},"prevInTopic":2895,"nextInTopic":3010,"prevInTime":2899,"nextInTime":2901,"topicId":2892,"numMessagesInTopic":6,"msgSnippet":"Some other wrinkles to consider: ... This would be the ideal solution if you want *all* extracted URLs, whether they are eligible to be crawled (in-scope) or","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 35106 invoked from network); 1 Jun 2006 19:13:08 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m7.grp.scd.yahoo.com with QMQP; 1 Jun 2006 19:13:06 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta3.grp.scd.yahoo.com with SMTP; 1 Jun 2006 19:12:44 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id E8CC814156C39\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  1 Jun 2006 12:12:34 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 06731-02-67 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 1 Jun 2006 12:12:34 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 644E914156C2F\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  1 Jun 2006 12:12:34 -0700 (PDT)\r\nMessage-ID: &lt;447F3C35.9000304@...&gt;\r\nDate: Thu, 01 Jun 2006 12:12:53 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e5lutt+4j4m@...&gt; &lt;447E85EA.2080105@...&gt;\r\nIn-Reply-To: &lt;447E85EA.2080105@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] getting urls..\r\nX-Yahoo-Group-Post: member; u=137285340; y=4-_TypSNDYIoOUGGBkl-8JYRG5Nj17iAF0m6Jv3CNuua\r\nX-Yahoo-Profile: gojomo\r\n\r\nSome other wrinkles to consider:\n\nMichael Stack wrote:\n&gt; callforshadab wrote:\n&gt;&gt; Hi All,\n&gt;&gt; Is there anyway to save links without saving the contents of the\n&gt;&gt; crawl??\n&gt; \n&gt; You could create your own LinksWriter and use it in place of ARCWriter.\n&gt; \n&gt; Your LinkWriter would open a file and per CrawlURI, it&#39;d dump the \n&gt; CrawlURI URI and all its outlinks.  The outlinks are put into the \n&gt; CrawlURI by the extractors.  See LinksScoper for example code on how to \n&gt; get the links out of the CrawlURI: \n&gt; http://crawler.archive.org/xref/org/archive/crawler/postprocessor/LinksScoper.html#135.\n\nThis would be the ideal solution if you want *all* extracted URLs, \nwhether they are eligible to be crawled (in-scope) or not, and whether \nthey have already been previously encountered (&quot;already-seen&quot;/ \n&quot;already-included&quot;) or not.\n\nIf you only want URLs that are actually scheduled for crawling -- \nmeaning they pass the in-scope test and aren&#39;t duplicates -- you could \nalso use the recovery log. Every line beginning &quot;F+&quot; is an URL being \nqueued for crawling; the only duplicates will be URLs that must be \nfetched more than once in a long crawl (robots, DNS, or URLs force-added \nby the operator).\n\n&gt;&gt; I understood that if we are using the bdbfrontier, it stores\n&gt;&gt; all links and the domain queues in *.jdb files. Can i extract all\n&gt;&gt; domains that it has in its list alongwith the urls belonging to that\n&gt;&gt; domain.\n&gt;&gt;\n&gt; You could use bdbje db dump tools to get at the bdbje db content but it \n&gt; wouldn&#39;t be satisfactory.  The Bdb has only the URIs that are in scope \n&gt; and it only contains hashes of URIs, not the URIs themselves.\n\nTo clarify: another database in the BDB environment does contain full \nURLs that are currently queued up for crawling (not just hashes). But, \nthe recover log would be any easier way to get the same info.\n\n- Gordon @ IA\n\n"}}