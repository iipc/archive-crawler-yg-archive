{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"QXcnG4gMiN72N8am7ATEC0sAogFqvgjYNgCInoVXZljXQBF9J6YclMILvelQaxyE7d0HnNd6qeQ2fuY35gm8PaIeUdfLP0Q","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Issue with the max-retries option under BdbFrontier","postDate":"1184010729","msgId":4425,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2OTI5MUU5LjQwMTA2MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ2OTIxRTRFLjgwOTAwMDRAc3RhdHNiaWJsaW90ZWtldC5kaz4=","referencesHeader":"PGY2dDNrcytiZTduQGVHcm91cHMuY29tPiA8NDY5MjFFNEUuODA5MDAwNEBzdGF0c2JpYmxpb3Rla2V0LmRrPg=="},"prevInTopic":4421,"nextInTopic":4433,"prevInTime":4424,"nextInTime":4426,"topicId":4419,"numMessagesInTopic":7,"msgSnippet":"As Igor notes, this behavior is by design . When a URI comes up for crawling, but its host has not been (recently) fetched via a prerequisite DNS URI, or its","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 47547 invoked from network); 9 Jul 2007 19:51:40 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m50.grp.scd.yahoo.com with QMQP; 9 Jul 2007 19:51:40 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta7.grp.scd.yahoo.com with SMTP; 9 Jul 2007 19:51:39 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id F347D14176222\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  9 Jul 2007 12:51:38 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 22227-02-80 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 9 Jul 2007 12:51:38 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id A6A6D1416BFCB\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  9 Jul 2007 12:51:38 -0700 (PDT)\r\nMessage-ID: &lt;469291E9.4010605@...&gt;\r\nDate: Mon, 09 Jul 2007 12:52:09 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (X11/20070604)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f6t3ks+be7n@...&gt; &lt;46921E4E.8090004@...&gt;\r\nIn-Reply-To: &lt;46921E4E.8090004@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Issue with the max-retries option under BdbFrontier\r\nX-Yahoo-Group-Post: member; u=137285340; y=1fBSpBAgHNOsxpYCAyquyrbEgRk52iyn0_tYej7ZbUOy\r\nX-Yahoo-Profile: gojomo\r\n\r\nAs Igor notes, this behavior is &#39;by design&#39;.\n\nWhen a URI comes up for crawling, but its host has not been (recently) \nfetched via a prerequisite DNS URI, or its robots.txt has not been \n(recently) fetched by a prerequisite robots URI, the URI is considered \nto have been tried, and the count increments. The necessary prerequisite \nis scheduled ahead of the URI itself, and the URI is returned for a \nlater retry.\n\nSo any URI on a new host (like seeds) &#39;fails&#39; twice before it succeeds.\n\nThe intent was to treat temporary failure due to DNS/robots information \nbeing stale or unavailable the same as any other transient, \nprobably-recoverable failure. It is a bit confusing, but if changed to \nescape the general retry-ceiling some other mechanism to prevent \nunbounded prerequisite-reschedulings would be necessary.\n\nWe could consider changing this behavior if the current approach is \nunduly limiting, but as we always allow significantly more than 3 \nretries, it hasn&#39;t been a priority.\n\n- Gordon @ IA\n\n\nBjarne Andersen wrote:\n&gt; I&#39;ve had that problem as well - it seems the crawler won&#39;t crawl with a setting for max-retries lower that 3. Could be a bug?\n&gt; \n&gt; best\n&gt; \n\n\n"}}