{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":6903103,"authorName":"Tom Emerson","from":"Tom Emerson &lt;Tree@...&gt;","profile":"tree02139","replyTo":"LIST","senderId":"zWQMd3bqMR5EIOyVJQvLGcXu7FhmJBKLek2pe5hnJlPBfCDbmY2umsHZ0g5UB-CzWjwelhoWzjU37P9jzhqy38JzFI6WEcQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] beginner&#39;s questions...","postDate":"1103547715","msgId":1287,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDE2ODM4LjUyNTQ3LjUzNzA4OS40NDgwNDZAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","inReplyToHeader":"PDVBQkU3ODM0LTUxQzItMTFEOS1COUJELTAwMzA2NUE3MDFDMkBzc2xtaXQudW5pYm8uaXQ+","referencesHeader":"PDVBQkU3ODM0LTUxQzItMTFEOS1COUJELTAwMzA2NUE3MDFDMkBzc2xtaXQudW5pYm8uaXQ+"},"prevInTopic":1286,"nextInTopic":1288,"prevInTime":1286,"nextInTime":1288,"topicId":1286,"numMessagesInTopic":8,"msgSnippet":"... Indeed: we re using Heritrix for exactly this purpose. ... It is, in the latest releases. And you can do not necessarily need significant hardware to run","rawEmail":"Return-Path: &lt;Tree@...&gt;\r\nX-Sender: Tree@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 34087 invoked from network); 20 Dec 2004 13:02:18 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m23.grp.scd.yahoo.com with QMQP; 20 Dec 2004 13:02:18 -0000\r\nReceived: from unknown (HELO mail2.basistech.net) (199.88.205.1)\n  by mta5.grp.scd.yahoo.com with SMTP; 20 Dec 2004 13:02:18 -0000\r\nReceived: from tiphares.basistech.com ([10.1.3.65]) by mail2.basistech.net with Microsoft SMTPSVC(6.0.3790.211);\n\t Mon, 20 Dec 2004 08:01:55 -0500\r\nReceived: by tiphares.basistech.com (Postfix, from userid 5007)\n\tid 99DC138BC88; Mon, 20 Dec 2004 08:01:55 -0500 (EST)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\nMessage-ID: &lt;16838.52547.537089.448046@...&gt;\r\nDate: Mon, 20 Dec 2004 08:01:55 -0500\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;5ABE7834-51C2-11D9-B9BD-003065A701C2@...&gt;\r\nReferences: &lt;5ABE7834-51C2-11D9-B9BD-003065A701C2@...&gt;\r\nX-Mailer: VM 7.18 under Emacs 21.2.1\r\nReturn-Path: tree@...\r\nX-OriginalArrivalTime: 20 Dec 2004 13:01:55.0437 (UTC) FILETIME=[14F511D0:01C4E694]\r\nX-eGroups-Remote-IP: 199.88.205.1\r\nFrom: Tom Emerson &lt;Tree@...&gt;\r\nReply-To: tree@...\r\nSubject: Re: [archive-crawler] beginner&#39;s questions...\r\nX-Yahoo-Group-Post: member; u=6903103\r\nX-Yahoo-Profile: tree02139\r\n\r\nMarco Baroni writes:\n&gt; As a linguist, I have been interested in downloading large amounts of \n&gt; text from the web for various forms of statistical analyses.\n\nIndeed: we&#39;re using Heritrix for exactly this purpose.\n\n&gt; Now, as I should get some funds to buy a few dedicated servers, I \n&gt; would like to get started with some more ambitious crawl, and heritrix \n&gt; looks like the ideal choice.\n\nIt is, in the latest releases. And you can do not necessarily need\nsignificant hardware to run your crawls, though post-processing will\nbe dependent on local processing power.\n\n&gt; 1) I understand that there is a way to stop fetching documents that do \n&gt; not match certain content-types with a filter. Is there a way to insert \n&gt; such a filter via the standard WUI?\n\nYes, you can specify filters at various stages of the process from\nwithin the WUI. In my data crawls I define three different filters in\nthe &quot;Filters&quot; page:\n\n1. crawl-order:scope:exclude-filter:filters\n      URIRegExpFilter\n\n2. fetch-processors:HTTP:midfetch-filters\n      ContentTypeRegExpFilter\n\n3. write-processors:Archiver:filters\n      ContentTypeRegExpFilter\n\nThe first is configured to restrict the set of file extensions that\nare treated as being in-scope:\n\n.*(?i)&#92;.(a|ai|aif|aifc|aiff|asc|au|avi|bcpio|bin|bmp|bz2|c|cdf|cgi|cgm|class|cpio|cpp?|cpt|csh|css|cxx|dcr|dif|dir|djv|djvu|dll|dmg|dms|doc|dtd|dv|dvi|dxr|eps|etx|exe|ez|gif|gram|grxml|gtar|h|hdf|hqx|ice|ico|ics|ief|ifb|iges|igs|iso|jnlp|jp2|jpe|jpeg|jpg|js|kar|latex|lha|lzh|m3u|mac|man|mathml|me|mesh|mid|midi|mif|mov|movie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|nc|o|oda|ogg|pbm|pct|pdb|pdf|pgm|pgn|pic|pict|pl|png|pnm|pnt|pntg|ppm|ppt|ps|py|qt|qti|qtif|ra|ram|ras|rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|sgml|sh|shar|silo|sit|skd|skm|skp|skt|smi|smil|snd|so|spl|src|srpm|sv4cpio|sv4crc|svg|swf|t|tar|tcl|tex|texi|texinfo|tgz|tif|tiff|tr|tsv|ustar|vcd|vrml|vxml|wav|wbmp|wbxml|wml|wmlc|wmls|wmlsc|wrl|xbm|xht|xhtml|xls|xml|xpm|xsl|xslt|xwd|xyz|z|zip)$\n\nNote that this excludes &quot;.au&quot;, which can be problematic if you are\nfetching documents from domains in Australia, since the DNS request\ngets filtered and this is a required prerequisite, so the entire site\ngets filtered.\n\nThe second and third have the same RegExp, and restrict the content to\ntext/html:\n\n(?i)text/html.*\n\nIf you don&#39;t include the write-processor filter then you will end up\nwith an ARC file containing partial content interrupted by the\nmid-fetch filter.\n\nIf you want to download and save other content types then you will\nobviously need to modify these regular expressions appropriately.\n\n&gt; 2) Similarly, is it possible to restrict the documents to be written to \n&gt; the arc files to a certain maximum and minimum size?\n\nNo, I don&#39;t believe so. It would be pretty easy to write a new type of\nfilter and add it to the write-processor. You would not want to do\nthis as a mid-fetch filter though, because not all servers include the\nContent-Length: header.\n\nI filter on this during post-processing: at that point you know both\nthe size of the returned markup and, after removing the markup, the\nsize of the remaining data. I limit my saved data to the extracted\nsize, not the size including markup since I&#39;ve found that in the\ncontemporary web the markup absolutely dwarfs the content in many\ncases.\n\n&gt; 3) In order to extract pure text from the documents, I plan to write \n&gt; scripts that invoke arcdump to get file type and contents out of the \n&gt; arcs, and then call appropriate tools such as pdftotex. Actually, for \n&gt; now I would be quite happy just extracting text from the html files. \n&gt; Before I re-invent the wheel, is there already some tool/module to do \n&gt; this?\n\nI&#39;ve written this a few times, using my libarc C++ library. Currently\nI strip markup with extreme prejudice, run a language/content\nidentifier on the remainder, transcode the original to UTF-8 from the\ndetected encoding, then dump the HTML (*not* the extracted text) to\ndisk that is within my size constraints. At the end of this phase I\nhave hundreds of thousands to millions of HTML files that fit my size\nand language constraints. For my work I need to preserve as much\nlogical structure in the original texts as possible, so simple HTML\nstripping does not work for me. Instead I use an open source tool\ncalled Vilistextum to extract the text from the HTML in all of the\nextracted files.\n\nOnce that is done we have lots of plain text to use for various\nlinguistics work. By way of example I&#39;ve used this technique to\ncollect a corpus of over 2 GB of plain (i.e., post-processed)\nTraditional Chinese text, and the crawl is still going.\n\nWe&#39;ve done the same for about 200-300 MB of post-processed French,\nItalian, Vietnamese, and Tagalog (well, an order or magnitude less\nTagalog).\n\n&gt; 4) Am I right in thinking that heritrix will not download the same url \n&gt; twice (within the same crawling job)?\n\nYes.\n\nThis scratches the surface. For the Tagalog and Vietnamese crawls we\nseeded using techniques described in your BootCat papers, and we&#39;ve\nthought about how to integrate BootCat type processes into Heritrix\nproper, but for now we do it separately using some Java apps utilizing\nthe Google API and our language identifier technology.\n\n    -tree\n\n-- \nTom Emerson                                          Basis Technology Corp.\nSoftware Architect                                 http://www.basistech.com\n  &quot;Beware the lollipop of mediocrity: lick it once and you suck forever&quot;\n\n"}}