{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"rSBgV0Rd43l_2-nC-i2K8qBi4O_ZqwwjhG9OcIkJMQJqz_ecgCupVhJNgScMTxf7pu-X8MzRJ2rS2NzcHQ6t9H4XMw5jr18","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] How to setup a crawl that gathers the hostnames without downloading files","postDate":"1182968229","msgId":4374,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODJBOUE1LjUwMjAyQGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGY1dTI5ZCtwaWxwQGVHcm91cHMuY29tPg==","referencesHeader":"PGY1dTI5ZCtwaWxwQGVHcm91cHMuY29tPg=="},"prevInTopic":4370,"nextInTopic":4375,"prevInTime":4373,"nextInTime":4375,"topicId":4370,"numMessagesInTopic":7,"msgSnippet":"Doing a test crawl that discovers all the hostnames a real crawl would discover would necessarily visit -- and download -- all the same documents. So the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 46493 invoked from network); 27 Jun 2007 18:17:01 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m50.grp.scd.yahoo.com with QMQP; 27 Jun 2007 18:17:01 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta8.grp.scd.yahoo.com with SMTP; 27 Jun 2007 18:17:01 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 84266141569BB\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 27 Jun 2007 11:16:57 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 26203-04-65 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 27 Jun 2007 11:16:57 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 213311403FD5C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 27 Jun 2007 11:16:57 -0700 (PDT)\r\nMessage-ID: &lt;4682A9A5.50202@...&gt;\r\nDate: Wed, 27 Jun 2007 11:17:09 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (X11/20070604)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f5u29d+pilp@...&gt;\r\nIn-Reply-To: &lt;f5u29d+pilp@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] How to setup a crawl that gathers the hostnames\n without downloading files\r\nX-Yahoo-Group-Post: member; u=137285340; y=P8GLZOfICtxGfS29qplDdjb9d3RzaoNlPn21R_dPhLgf\r\nX-Yahoo-Profile: gojomo\r\n\r\nDoing a &#39;test&#39; crawl that discovers all the hostnames a &#39;real&#39; crawl \nwould discover would necessarily visit -- and download -- all the same \ndocuments. So the load on the target servers is the same, and the \nprocess you&#39;ve outlined won&#39;t save any load on the unwanted-local hosts, \nand will double the load on the wanted hosts.\n\nIf you don&#39;t mind the load on the local hosts -- you just don&#39;t want \ntheir content -- you could crawl everything, then post-process the \nmaterial collected (ARC files) to discard the unwanted material, once \nthe IP addresses are known.\n\nReally, though, it sounds like you want a scope DecideRule that acts on \nIPs rather than hostnames. We don&#39;t have one but it would be relatively \nstraightforward to write. The MatchesRegExpDecideRule would be the \nmodel, but instead of comparing the full URI&#39;s String representation, \njust the IP would be compared.\n\nOne gotcha: the first time URIs are considered for scoping, the IP may \nnot be known -- the DNS lookup is a specific step triggered by a URI&#39;s \nfirst attempted fetch. So you&#39;d want to rule-in all URIs with \nnot-yet-known IPs, then one recheckin (Prescoper processor) some would \nbe ruled-out (resulting in -5000 lines in your crawl.log).\n\nShort of writing a new Java DecideRule, this could also be a job for the \nBeanshellDecideRule, which lets you specify a Beanshell-language \n(Java-inspired scripting-language) script file to be run against URIs to \nmake the ACCEPT/REJECT decision.\n\nHope this helps,\n\n- Gordon @ IA\n\nmjjjhjemj wrote:\n&gt; Is this possible?\n&gt; \n&gt; I have a very large crawl that has numerous seeds and may potentially\n&gt; take weeks to complete. I do not want to crawl local sites accessible\n&gt; under a certain IP mask. Where Heritrix will exclude sites based on IP\n&gt; entered into a surts-source-file with decision=&#39;REJECT&#39; these sites\n&gt; are accessed using the typical hostname and not IP and are therefore\n&gt; not excluded.\n&gt; \n&gt; I have been given an exclude list in typical hostname &#39;www.cnn.com&#39;\n&gt; form, but have confirmed that it is not complete and therefore my\n&gt; crawl scope is greater than it should be. This brings me to why I wish\n&gt; to complete a test crawl where the goal is to gather only the lists of\n&gt; URIs traversed, but not gather all the files. I will then parse the\n&gt; crawl.log and do a lookup on the hostname to determine if the IP and\n&gt; canonical name and see if they should be included in the crawl scope.\n&gt; \n&gt; Any help would be greatly appreciated.\n&gt; \n&gt; Thanks,\n&gt; Mike\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}