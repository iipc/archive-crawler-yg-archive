{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"KbqM4WCzY9-R2sYXRJaBYCUv4vg1M2JZncskEi0-AXSY3pWYKYTdT5zKWrlAsah_XV66C-zQZyZg5TmJyMfJhUBcMrlESMEGn-8vA2d6Yg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] RFE: New queue assignment policy","postDate":"1108390222","msgId":1550,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBMDZFQzYwQHNrYXJmdXIuYm9rLmxvY2FsPg==","inReplyToHeader":"PDIwMDUwMjE0MTQ1Ni4wNTM3OS5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4="},"prevInTopic":1549,"nextInTopic":1551,"prevInTime":1549,"nextInTime":1551,"topicId":1545,"numMessagesInTopic":13,"msgSnippet":"Heritrix is still somewhat limited in the size of the crawls. I doubt you would be able to crawl quite that many pages in one crawl. Also, link depth of 7 is","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 70971 invoked from network); 14 Feb 2005 14:10:26 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m21.grp.scd.yahoo.com with QMQP; 14 Feb 2005 14:10:26 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta1.grp.scd.yahoo.com with SMTP; 14 Feb 2005 14:10:26 -0000\r\nReceived: (qmail 15796 invoked by uid 100); 14 Feb 2005 13:54:05 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (kris@...@130.208.152.83)\n  by mail-dev.archive.org with SMTP; 14 Feb 2005 13:54:05 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Mon, 14 Feb 2005 14:10:22 -0000\r\nMessage-ID: &lt;0678DB1968EAC7409CC3D0AB7A11B84A06EC60@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nIn-Reply-To: &lt;200502141456.05379.ck-heritrix@...&gt;\r\nImportance: Normal\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1441\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: *\r\nX-Spam-Status: No, hits=1.3 required=6.5 tests=AWL,DOMAIN_BODY autolearn=no \n\tversion=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] RFE: New queue assignment policy\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\nHeritrix is still somewhat limited in the size of the crawls. I doubt you\nw=\r\nould be able to crawl quite that many pages in one crawl. Also, link depth\n=\r\nof 7 is quite deep on most web sites. Heck, even a link depth of 4 can lead=\r\n\nto crawling the majority of some sites. Of course this will vary from site=\r\n\nto site.\n\nI&#39;d suggest (if feasible) splitting up the scope into a series o=\r\nf smaller,\nindependent crawls. This has the additional advantage of allowin=\r\ng you to\nmonitor each segment much more closely and cutting it off once you=\r\n are\nclearly crawling rubbish.\n\nThe .is domain spans about 11,000 domains a=\r\nnd (roughly) 35 million pages.\nWith the BDB frontier I could run it in one =\r\ncrawl, but I feel I get better\nperformance running it in 4 seperate batches=\r\n.\n\nThis is of course only feasible if you can both easily split the scope u=\r\np\nand cross linkage is not a high priority concern.\n\n- Kris\n\n&gt; -----Origina=\r\nl Message-----\n&gt; From: Christian Kohlschuetter [mailto:ck-heritrix@newsclub=\r\n.de] \n&gt; Sent: 14. febr=FAar 2005 13:56\n&gt; To: archive-crawler@...=\r\nm\n&gt; Subject: Re: [archive-crawler] RFE: New queue assignment policy\n&gt; \n&gt; \n&gt;=\r\n \n&gt; Hey Kris,\n&gt; \n&gt; indeed, I am currently doing _very_ broad crawls :)\n&gt; \n&gt;=\r\n Using Heritrix, I would like to fetch about 60-100 million \n&gt; representati=\r\nve \n&gt; pages in the DMOZ sphere (currently, the link depth is set to \n&gt; 7, w=\r\nhich is \n&gt; just too much for a host-oriented assignment).\n&gt; \n&gt; Perhaps you =\r\nhave an idea how to do such a crawl with Heritrix current \n&gt; abilities? I w=\r\nould be very interested in a quick solution.\n&gt; \n&gt; Best regards,\n&gt; \n&gt; Christ=\r\nian\n&gt; \n&gt; On Monday 14 February 2005 14:31, Kristinn Sigurdsson wrote:\n&gt; &gt; H=\r\ney Christian,\n&gt; &gt;\n&gt; &gt; I&#39;ve got a quick question for you: just how many host=\r\ns/ips are you\n&gt; &gt; crawling??\n&gt; &gt;\n&gt; &gt; I&#39;ve conducted crawls over about 1000-=\r\n1200 domains (maybe a total of\n&gt; &gt; ten-fifty times that many hosts once you=\r\n count offsite \n&gt; images etc.) that\n&gt; &gt; covered well over a million documen=\r\nts (as many as 5 million \n&gt; in fact). And\n&gt; &gt; this was with the much less e=\r\nfficient HostQueuesFrontier.\n&gt; &gt;\n&gt; &gt; I&#39;ve also tested the BDBFrontier (with=\r\n 1GB of heap) running \n&gt; on 11 thousand\n&gt; &gt; domains (and quite a few more h=\r\nosts in total) without \n&gt; running into any\n&gt; &gt; problems. That crawl collect=\r\ned over 2 million documents \n&gt; before I shut it\n&gt; &gt; down.\n&gt; &gt;\n&gt; &gt; So, I&#39;m a=\r\n little surprised by the this. Are you running a \n&gt; strict broad\n&gt; &gt; crawl?=\r\n The only way I could see the number of hosts become \n&gt; an issue within\n&gt; &gt;=\r\n the first 1 million documents would be in a very broad \n&gt; oriented crawl..=\r\n.?\n&gt; &gt;\n&gt; &gt; - Kris\n&gt; &gt;\n&gt; &gt; -----Original Message-----\n&gt; &gt; From: Christian Ko=\r\nhlschuetter [mailto:ck-heritrix@...]\n&gt; &gt; Sent: 14. febr=FAar 2005 1=\r\n3:15\n&gt; &gt; To: archive-crawler@yahoogroups.com\n&gt; &gt; Subject: [archive-crawler]=\r\n RFE: New queue assignment policy\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt;\n&gt; &gt; here&#39;s another fe=\r\nature which I would like to contribute.\n&gt; &gt;\n&gt; &gt; Currently, I am performing =\r\nbroad crawls using \n&gt; BroadScope/BdbFrontier.\n&gt; &gt; However,\n&gt; &gt; due to the n=\r\number of host- or IP-keyed queues, an \n&gt; OutOfMemoryError occurs\n&gt; &gt; very q=\r\nuickly after starting the crawl. One reason for this \n&gt; is the RAM-based\n&gt; =\r\n&gt; bookkeeping of subqueues -- the more queues, the more heap.\n&gt; &gt;\n&gt; &gt; I hav=\r\ne evaded this by writing a BucketQueueAssignmentPolicy \n&gt; class, which\n&gt; &gt; =\r\nproduces a _fixed_ number of subqueues (&quot;buckets&quot;), not one \n&gt; per host or =\r\nper\n&gt; &gt; IP. The queue key is computed by hashing the hostname (or the IP, i=\r\nf\n&gt; &gt; available) modulo N (a fixed number, such as 1000).\n&gt; &gt;\n&gt; &gt; This way,=\r\n I was able to increase the number of fetched \n&gt; pages from ca.\n&gt; &gt; 400,000=\r\n\n&gt; &gt; to 1,000,000. For some other reason, I still get OOMEs, but \n&gt; I think=\r\n that is\n&gt; &gt; caused by a different problem -- the number of queues did \n&gt; n=\r\not grow over the\n&gt; &gt; specified limit.\n&gt; &gt;\n&gt; &gt; Furthermore, I have modified =\r\nAbstractFrontier to be able to choose\n&gt; &gt; arbitrary\n&gt; &gt;\n&gt; &gt; queue assignmen=\r\nt policies and replaced the current \n&gt; &quot;ip-politness&quot; option by\n&gt; &gt; a\n&gt; &gt; s=\r\nelectbox.\n&gt; &gt;\n&gt; &gt; The patch against CVS HEAD is attached.\n&gt; &gt;\n&gt; &gt; Greetings=\r\n,\n&gt; \n&gt; -- \n&gt; Christian Kohlsch=FCtter\n&gt; mailto: ck -at- NewsClub.de\n&gt; \n&gt; \n&gt;=\r\n  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n&gt; \n\n\n"}}