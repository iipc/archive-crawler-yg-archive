{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":125829837,"authorName":"Ahnu Nahki","from":"&quot;Ahnu Nahki&quot; &lt;ahnunahki@...&gt;","profile":"ahnunahki","replyTo":"LIST","senderId":"dkEE0WotAUmdHkmbjCwMi_hBKpPVNUZfmeW6_HwFOcvunkGuchT3AEsqnMnhY8v9BYQYqJJB-Tf_clU1_nrxsTvX0uO-LhWhxdtmlTkubC7t","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Inserting information to MYSQL during crawl","postDate":"1086618898","msgId":514,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNhMXVlaSs2dXY4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQwQzBCQTA4LjIwMTA1MDZAbG9jLmdvdj4="},"prevInTopic":509,"nextInTopic":515,"prevInTime":513,"nextInTime":515,"topicId":507,"numMessagesInTopic":19,"msgSnippet":"... We had considered that aswell initially as a quick way of importing the data into the db. Going the arc route after a crawl. But we have a search engine we","rawEmail":"Return-Path: &lt;ahnunahki@...&gt;\r\nX-Sender: ahnunahki@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 52632 invoked from network); 7 Jun 2004 14:35:27 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m24.grp.scd.yahoo.com with QMQP; 7 Jun 2004 14:35:27 -0000\r\nReceived: from unknown (HELO n28.grp.scd.yahoo.com) (66.218.66.84)\n  by mta2.grp.scd.yahoo.com with SMTP; 7 Jun 2004 14:35:26 -0000\r\nReceived: from [66.218.67.248] by n28.grp.scd.yahoo.com with NNFMP; 07 Jun 2004 14:35:00 -0000\r\nDate: Mon, 07 Jun 2004 14:34:58 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;ca1uei+6uv8@...&gt;\r\nIn-Reply-To: &lt;40C0BA08.2010506@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 1959\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.84\r\nFrom: &quot;Ahnu Nahki&quot; &lt;ahnunahki@...&gt;\r\nSubject: Re: Inserting information to MYSQL during crawl\r\nX-Yahoo-Group-Post: member; u=125829837\r\nX-Yahoo-Profile: ahnunahki\r\n\r\n--- In archive-crawler@yahoogroups.com, Andy Boyko &lt;aboy@l...&gt; wrote:\n&gt; Ahnu Nahki wrote:\n&gt; &gt; Instead of writing to an arc file, Id like to create a method that\n&gt; &gt; takes the URI info, Content, headers, ect into a MYSQL database. Does\n&gt; &gt; anyone have any suggestion on how to do this , where I should look to\n&gt; &gt; place my methods?\n&gt; \n&gt; Are you interested in that specifically to get away from ARC, or more \n&gt; simply because you&#39;re interested in being able to issue queries on the \n&gt; crawl results in interesting/relational ways?  I ask because we&#39;re \n&gt; looking into a slightly different approach - rather than building the \n&gt; database logic into Heritrix, treating the DB import as a \n&gt; post-processing step on the crawl output (ARC files & logs) once the \n&gt; crawl is complete.  I believe Tom Emerson has also talked about \n&gt; populating a DB from ARC files in future versions of his libarc library.\n&gt; \n&gt; By keeping the content in ARCs, you get the ability to leverage the \n&gt; growing number of tools for dealing with the format coming from this \n&gt; community, and if the post-processing approach can work for you, code \n&gt; for populating a DB may be forthcoming from a couple of sources in the \n&gt; near future.\n&gt; \n&gt; Can you describe in more detail what you&#39;re envisioning with your \n&gt; planned DB crawl storage?\n&gt; \n&gt; Regards,\n&gt; Andy Boyko    aboy@l...     Library of Congress\n\nWe had considered that aswell initially as a quick way of importing\nthe data into the db. Going the arc route after a crawl. But we have a\nsearch engine we run that is powered by\nlucene(http://jakarta.apache.org/lucene/docs/index.html). We have our\nown crawler which has done all it can and is not nearly as powerful as\nheritrix. So we want to incorporate heritrix into out environment\nquickly. We make all our indexes off of content stored in the db, and\nit makes alot of sense for us to just have the crawler populate the db\nat runtime than have some post crawl method were we do it. \n\n\n"}}