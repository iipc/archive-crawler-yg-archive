{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"xthGMj83s7YiaiBR6VjAVM0KCkflMu5GIJorMBeD0PUqC_l98svamhxCQ_wIrjRcWJx7Zl-62k6dIoyzODwH0UlhPzpoE0eB","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Best approach question","postDate":"1200941380","msgId":4914,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3OTRFOTQ0LjQwMjAzMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ3OTQ2OTRCLjIwNTAwQHN0YXRzYmlibGlvdGVrZXQuZGs+","referencesHeader":"PEEwNzdERjdDRjc3MzM1NERCMjg1Mjc5MDMwRjZBMUE1MzFCODE5QE9YWUdFTi5zaXJzaS5wdnQ+IDw0NzkxMzAxRC4xMDcwMjA0QGFyY2hpdmUub3JnPiA8NDc5NDY5NEIuMjA1MDBAc3RhdHNiaWJsaW90ZWtldC5kaz4="},"prevInTopic":4912,"nextInTopic":0,"prevInTime":4913,"nextInTime":4915,"topicId":4906,"numMessagesInTopic":6,"msgSnippet":"Hi Bjarne, ... Oops :( Thanks for catching it. Well, on the second look, I think that this approach would not work for all seeds. For example, for these two","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 91994 invoked from network); 21 Jan 2008 18:49:42 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m36.grp.scd.yahoo.com with QMQP; 21 Jan 2008 18:49:42 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta16.grp.scd.yahoo.com with SMTP; 21 Jan 2008 18:49:42 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id B23604802A\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 21 Jan 2008 10:49:42 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id eLwRh8PEtiEP for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 21 Jan 2008 10:49:42 -0800 (PST)\r\nX-Received: from [192.168.1.107] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 2843547DB9\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 21 Jan 2008 10:49:42 -0800 (PST)\r\nMessage-ID: &lt;4794E944.4020304@...&gt;\r\nDate: Mon, 21 Jan 2008 10:49:40 -0800\r\nUser-Agent: Thunderbird 2.0.0.9 (Windows/20071031)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;A077DF7CF773354DB285279030F6A1A531B819@...&gt; &lt;4791301D.1070204@...&gt; &lt;4794694B.20500@...&gt;\r\nIn-Reply-To: &lt;4794694B.20500@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Best approach question\r\nX-Yahoo-Group-Post: member; u=137477665; y=NLJOfCMWXjf-If-CRPuAFO-T8e45zgFsWZ9jgr7mI51XhoA6uw\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Bjarne,\n\n&gt; getPrefixes() in SurtPrefixedDecideRule has private access ;-(\n\nOops :( Thanks for catching it.\n\nWell, on the second look, I think that this approach would not work for \nall seeds. For example, for these two seeds\n\nhttp://example.com/path1/path.txt\nhttp://example.com/path1/path2/path.txt\n\nthere would be only one SURT prefix\n\nhttp://(com,example,)/path1/\n\nsince redundant prefixes, those that are themselves prefixed by other \nset entries, are removed.\n\nSo, maybe a new decide rule is more appropriate approach. But, the idea \nis same. The decide rule should keep a list of allowed &#39;folders&#39; and \neach candidate uri, when reduced to the &#39;folder&#39; form, is checked \nagainst that list.\n\nTake care,\ni.\n\n\n&gt; \n&gt; -\n&gt; Bjarne Andersen\n&gt; \n&gt; Igor Ranitovic wrote:\n&gt;&gt;\n&gt;&gt; I think that it will be OK, but I am not sure where the breaking point\n&gt;&gt; is :) Maybe running a test crawl is a good idea.\n&gt;&gt;\n&gt;&gt; If you want to write a new decide rule you can subclass the\n&gt;&gt; SurtPrefixedDecideRule and override the &#39;evaluate&#39; method.\n&gt;&gt;\n&gt;&gt; Maybe something like this (not tested):\n&gt;&gt;\n&gt;&gt; protected boolean evaluate(Object object) {\n&gt;&gt; if ( (object instanceof CandidateURI) &&\n&gt;&gt; ((Boolean) getUncheckedAttribute(null, ATTR_ALSO_CHECK_VIA))\n&gt;&gt; .booleanValue()) {\n&gt;&gt; if(evaluate(((CandidateURI)object).getVia())) {\n&gt;&gt; return true;\n&gt;&gt; }\n&gt;&gt; }\n&gt;&gt; String candidateSurt;\n&gt;&gt; candidateSurt = SurtPrefixSet.getCandidateSurt(object);\n&gt;&gt;\n&gt;&gt; // Drop everything after the last &#39;/&#39;\n&gt;&gt; candidateSurt = candidateSurt.replaceFirst(&quot;^(.*/)[^/]*$&quot;, &quot;$1&quot;);\n&gt;&gt;\n&gt;&gt; if (candidateSurt == null) {\n&gt;&gt; return false;\n&gt;&gt; }\n&gt;&gt;\n&gt;&gt; // Check if we have exact match\n&gt;&gt; return getPrefixes().contains(candidateSurt);\n&gt;&gt; }\n&gt;&gt;\n&gt;&gt; Take care,\n&gt;&gt; i.\n&gt;&gt;\n&gt;&gt; Travis Jensen wrote:\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Hi,\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Thanks for your reply, Igor.\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Would this still be a preferred way if I have 1500 of these URLs? I\n&gt;&gt;  &gt; would worry about the performance hit of every crawled URL needing to go\n&gt;&gt;  &gt; through an average of 750 regex matches to find the one that will match.\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; tj\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; -----Original Message-----\n&gt;&gt;  &gt; From: archive-crawler@yahoogroups.com \n&gt;&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt;&gt;  &gt; [mailto:archive-crawler@yahoogroups.com \n&gt;&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;] On Behalf Of Igor Ranitovic\n&gt;&gt;  &gt; Sent: Friday, January 18, 2008 12:26 PM\n&gt;&gt;  &gt; To: archive-crawler@yahoogroups.com \n&gt;&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt;&gt;  &gt; Subject: Re: [archive-crawler] Best approach question\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; I don&#39;t think that you need to run separate jobs for each seed.\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; For example, is you have two seeds as:\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; http://www.foo.com/baz/bar2.html &lt;http://www.foo.com/baz/bar2.html&gt;\n&gt;&gt;  &gt; http://www.foo.com/zab/bar2.html &lt;http://www.foo.com/zab/bar2.html&gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; you can have scope as:\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; +http://www.foo.com/baz/bar2.html &lt;http://www.foo.com/baz/bar2.html&gt;\n&gt;&gt;  &gt; +http://www.foo.com/zab/bar2.html &lt;http://www.foo.com/zab/bar2.html&gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; and this decide rule:\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; &lt;newObject name=&quot;seedsCurrentFolderOnly&quot;\n&gt;&gt;  &gt; class=&quot;org.archive.crawler.deciderules.MatchesListRegExpDecideRule&quot;&gt;\n&gt;&gt;  &gt; &lt;string name=&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n&gt;&gt;  &gt; &lt;string name=&quot;list-logic&quot;&gt;OR&lt;/string&gt;\n&gt;&gt;  &gt; &lt;stringList name=&quot;regexp-list&quot;&gt;\n&gt;&gt;  &gt; &lt;string&gt;(?i)http://www.foo.com/baz/.*/ \n&gt;&gt; &lt;http://www.foo.com/baz/.*/&gt;&lt;/string&gt;\n&gt;&gt;  &gt; &lt;string&gt;(?i)http://www.foo.com/zab/.*/ \n&gt;&gt; &lt;http://www.foo.com/zab/.*/&gt;&lt;/string&gt;\n&gt;&gt;  &gt; &lt;/stringList&gt;\n&gt;&gt;  &gt; &lt;/newObject&gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Creating the scope and MatchesListRegExpDecideRule can be done easily\n&gt;&gt;  &gt; with little scripting.\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; I hope this helps.\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Take care,\n&gt;&gt;  &gt; i.\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Travis Jensen wrote:\n&gt;&gt;  &gt;&gt; Hi,\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; I&#39;m looking for some feedback as to a best approach to a crawling\n&gt;&gt;  &gt;&gt; problem I need solve. I have a list of URLs, some of which I only\n&gt;&gt;  &gt; want\n&gt;&gt;  &gt;&gt; to crawl that URL, some of which I want to crawl that whole domain\n&gt;&gt;  &gt;&gt; (&quot;foo.com&quot;), some I want to crawl only that server (&quot;www.foo.com&quot;),\n&gt;&gt;  &gt; some\n&gt;&gt;  &gt;&gt; I want to crawl anything below the given URL, and some I want to crawl\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;&gt; everything in the URL&#39;s folder.\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; The first four are out-of-the-box from my understanding of Heritrix 2,\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;&gt; which is great. The last one doesn&#39;t seem to be, so I&#39;ve been looking\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;&gt; at options on how to implement it. Just to clearly define the\n&gt;&gt;  &gt; problem,\n&gt;&gt;  &gt;&gt; if I&#39;m given a seed URL of:\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; http://www.foo.com/baz/bar.html &lt;http://www.foo.com/baz/bar.html&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; then I want to match:\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; http://www.foo.com/baz/bar2.html, &lt;http://www.foo.com/baz/bar2.html,&gt;\n&gt;&gt;  &gt; http://www.foo.com/baz/somethingelse.html \n&gt;&gt; &lt;http://www.foo.com/baz/somethingelse.html&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; but I don&#39;t want to match:\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; http://www.foo.com/apage.html, &lt;http://www.foo.com/apage.html,&gt;\n&gt;&gt;  &gt; http://www.foo.com/baz/bang/anotherpage.html \n&gt;&gt; &lt;http://www.foo.com/baz/bang/anotherpage.html&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; The two methods I&#39;ve found to do this would be to define a regex of\n&gt;&gt;  &gt; the\n&gt;&gt;  &gt;&gt; matching URL and use a MatchesRegExpDecideRule or to create my own\n&gt;&gt;  &gt;&gt; DecideRule (call it FolderOnlyDecideRule).\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; If I use the MatchesRegExpDecideRule, will I have to create a\n&gt;&gt;  &gt; different\n&gt;&gt;  &gt;&gt; job for each seed URL (because the regex will be different)? Or is it\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;&gt; possible to say &quot;this job uses this seed URL with this regex&quot;?\n&gt;&gt;  &gt; Dealing\n&gt;&gt;  &gt;&gt; with a different job for each URL seems painful.\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; If the FolderOnlyDecideRule is the way to go, can I get some pointers\n&gt;&gt;  &gt; on\n&gt;&gt;  &gt;&gt; how to go about implementing it. I&#39;ve been looking through the\n&gt;&gt;  &gt;&gt; DecideRules and it hasn&#39;t &quot;clicked&quot; yet. :)\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; Thanks.\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt; tj\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;&gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Yahoo! Groups Links\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Yahoo! Groups Links\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;\n&gt;&gt;\n&gt; \n\n\n"}}