{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"x0TGq_hJMQGc6A0Qx4RKbNcTKL1tPfJgQTzWDI6dBNbpPGmYDhblXjTCC17PUpEfvV3GPlxd2aB33H67zLPEId-8_Nc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: questions before we restart the crawl","postDate":"1327125282","msgId":7562,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMUE1MzIyLjEwMjA1MDFAY3MuY211LmVkdT4=","inReplyToHeader":"PDRGMUE0QjlDLjUwOTAwMDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PDRGMTU5NEQwLjIwOTA4MDhAY3MuY211LmVkdT4gPDRGMTVCQjNBLjUwMzA2QGFyY2hpdmUub3JnPiA8NEYxOUIzOTEuMTA3MDQwMUBjcy5jbXUuZWR1PiA8NEYxQTA0QkMuNjAxMDQwMkBjcy5jbXUuZWR1PiA8NEYxQTRCOUMuNTA5MDAwN0BhcmNoaXZlLm9yZz4="},"prevInTopic":7560,"nextInTopic":7566,"prevInTime":7561,"nextInTime":7563,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"Gordon, Thank you for your response.  And I am sorry for the overwhelming amount of information...I think I am a little overwhelmed.... and feeling the ","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 22262 invoked from network); 21 Jan 2012 05:54:49 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m11.grp.sp2.yahoo.com with QMQP; 21 Jan 2012 05:54:49 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.96)\n  by mta6.grp.sp2.yahoo.com with SMTP; 21 Jan 2012 05:54:49 -0000\r\nX-Received: from iMac.local (c-24-131-240-160.hsd1.pa.comcast.net [24.131.240.160])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id q0L5sgjc022650\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tSat, 21 Jan 2012 00:54:42 -0500\r\nMessage-ID: &lt;4F1A5322.1020501@...&gt;\r\nDate: Sat, 21 Jan 2012 00:54:42 -0500\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: Gordon Mohr &lt;gojomo@...&gt;\r\nCc: archive-crawler@yahoogroups.com, Noah Levitt &lt;nlevitt@...&gt;\r\nReferences: &lt;4F1594D0.2090808@...&gt; &lt;4F15BB3A.50306@...&gt; &lt;4F19B391.1070401@...&gt; &lt;4F1A04BC.6010402@...&gt; &lt;4F1A4B9C.5090007@...&gt;\r\nIn-Reply-To: &lt;4F1A4B9C.5090007@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2010.4.9.4220\r\nX-SMTP-Spam-Clean: 8% (\n HTML_NO_HTTP 0.1, BODY_SIZE_10000_PLUS 0, RDNS_BROADBAND 0, RDNS_GENERIC_POOLED 0, RDNS_POOLED 0, RDNS_SUSP 0, RDNS_SUSP_GENERIC 0, RDNS_SUSP_SPECIFIC 0, __BOUNCE_CHALLENGE_SUBJ 0, __BOUNCE_NDR_SUBJ_EXEMPT 0, __CP_MEDIA_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __FRAUD_BADTHINGS 0, __HAS_HTML 0, __HAS_MSGID 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __RDNS_BROADBAND_5 0, __RDNS_POOLED_11 0, __SANE_MSGID 0, __TO_MALFORMED_2 0, __URI_NO_MAILTO 0, __URI_NO_PATH 0, __URI_NO_WWW 0, __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.96\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=500983475; y=b9OVJe1qiuu4L2jYEn19ZFBIbxjFwyqsdBoEvuo3qeNKoNRpoeScGg\r\nX-Yahoo-Profile: david_pane1\r\n\r\nGordon,\n\nThank you for your response.  And I am sorry for the overwhelming amount \nof information...I think I am a little overwhelmed.... and feeling the \npressure.\n\n1) Our Bloom filter configuration:\n\n&lt;bean id=&quot;uriUniqFilter&quot; \nclass=&quot;org.archive.crawler.util.BloomUriUniqFilter&quot;&gt;\n&lt;property name=&quot;bloomFilter&quot;&gt;\n&lt;bean class=&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&lt;constructor-arg value=&quot;400000000&quot;/&gt;\n&lt;constructor-arg value=&quot;30&quot;/&gt;\n&lt;/bean&gt;\n&lt;/property&gt;\n&lt;/bean&gt;\n\n2) We are writing the crawl data to a NAS configured with RAID 6. We did \nsee some problems with disk errors on the NAS earlier in the crawl (late \nDec ). I recently found this out.  We were/are running in a degraded \nraid state - a few of the disks have been replaced and the RAID is being \nrebuilt.  We didn&#39;t see any block device errors in the logs on the NAS \nso the write failures we saw are probably not related to the rebuild. We \ndid see some network hiccups (no outright failures) in the logs.\nSo, this may be the culprit for some of the\n\n3) Yes, we have been cross-feeding URIs.\n\n--David\n\n\nOn 1/21/12 12:22 AM, Gordon Mohr wrote:\n&gt; You&#39;ve provided an overwhelming amount of information and we may be \n&gt; dealing with multiple issues, some of which have roots going back \n&gt; earlier than the diagnostic data we now have available.\n&gt;\n&gt; A few key points of emphasis:\n&gt;\n&gt;  - we&#39;ve not run crawls with 1200 threads before, or on hardware \n&gt; similar to yours, so our experience is only vaguely suggestive\n&gt;\n&gt;  - it&#39;s not the lower thread counts that are the real source of \n&gt; concern; you can even adjust the number of threads mid-crawl. It&#39;s \n&gt; that the error that killed the threads almost certainly left a queue \n&gt; in a &#39;phantom&#39; state where no progress would be made crawling its \n&gt; URIs, each time it happened, on each resume leading to the current state.\n&gt;\n&gt;  - without having understood and fixed whatever software or system \n&gt; problems caused the earliest/most-foundational errors in your crawl, \n&gt; it&#39;s impossible to say how likely they are to recur.\n&gt;\n&gt; With that in mind, I&#39;ll try to provide quick answers to your other \n&gt; questions...\n&gt;\n&gt; On 1/20/12 4:20 PM, David Pane wrote:\n&gt;&gt;\n&gt;&gt; We have collected about 550 million pages along with the images and\n&gt;&gt; supporting documents on our 5 instance crawl that was started Dec. 23rd.\n&gt;&gt; Although we are please with the amount of data we captured to date, we\n&gt;&gt; are very concerned about the state of the Heritrix instances. If fact,\n&gt;&gt; we aren&#39;t very confident that the instances will last until the end of\n&gt;&gt; February. We are now running on a total of over 500 less threads than\n&gt;&gt; the configured 1200 threads/instance.\n&gt;&gt;\n&gt;&gt; 0 - not running right now.\n&gt;&gt; 1 - running on 1198 ( 2 less)\n&gt;&gt; 2 - running on 931 (269 less)\n&gt;&gt; 3 - running on 987 (213 less)\n&gt;&gt; 4 - running on 1170 (30 less)\n&gt;&gt;\n&gt;&gt; Since we are seriously considering throwing away this past month&#39;s work\n&gt;&gt; and starting over, we would like to pick your brain on some strategies\n&gt;&gt; that will help us avoid getting into this situation again. We were\n&gt;&gt; hoping to be done crawling by the end of February so this restart will\n&gt;&gt; put us behind schedule.\n&gt;&gt;\n&gt;&gt; 1) Can we continue from here but with &quot;clean&quot; Heritrix instances?\n&gt;&gt;\n&gt;&gt; Is there a way that we can continue from the this point forward, but\n&gt;&gt; start with Heritrix instances that will not be corrupt due to sever\n&gt;&gt; error? (e.g. using the\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery ) If\n&gt;&gt; so, would you recommend doing this? You mentioned that this could be\n&gt;&gt; time consuming. Each of our instances has downloaded around 170M URIs,\n&gt;&gt; they have over 700M queued URIs, what is your time estimate for\n&gt;&gt; something this large?\n&gt;&gt;\n&gt;&gt; We are willing to sacrifice a few days to get our crawler to a clean\n&gt;&gt; state again so we can crawl for another 30 days at the pace we have been\n&gt;&gt; crawling.\n&gt;\n&gt; You can do a big &#39;frontier-recover&#39; log replay to avoid recrawling the \n&gt; same URIs, and approximate the earlier queue state. Splitting/filters \n&gt; the logs manually beforehand as alluded to in the wiki page can speed \n&gt; this process somewhat... but given the size of all your log-segments \n&gt; that log grooming beforehand is itself likely to be a lengthy process.\n&gt;\n&gt; I don&#39;t think we&#39;ve ever done it with logs of 170M crawled / 870M \n&gt; discovered before, nor on any hardware comparable to yours. So it&#39;s \n&gt; impossible to project its duration in your environment. It&#39;s taken 2-3 \n&gt; days for us on smaller crawls, slower hardware.\n&gt;\n&gt; An added complication is that this older frontier-recover-log replay \n&gt; technique happens in its own thread separate from the checkpointing \n&gt; process, so it is not, itself, accurately checkpointed during the long \n&gt; reload process.\n&gt;\n&gt; At nearly 1B discovered URIs per node, even if you are using the \n&gt; alternate BloomUriUniqFilter, if you are using it at its default size \n&gt; (~500MB) it will now be heavily saturated and thus returning many \n&gt; false-positives causing truly unique URIs to be rejected as \n&gt; duplicates. (If you&#39;re using a significantly larger filter, you may \n&gt; not yet be at a high false-positive rate: you&#39;d have to do the bloom \n&gt; filter math. If you&#39;re still using BdbUriUniqFilter, you&#39;re way way \n&gt; past the point where its disk seeks have usually made it too slow for \n&gt; our purposes.)\n&gt;\n&gt;&gt; 2) What can be done to avoid corrupting the Heritrix instances?\n&gt;&gt;\n&gt;&gt; - What kind of strategies might we take to keep the crawl error free?\n&gt;&gt;\n&gt;&gt; - Do you think the SEVER errors that we have seen are deterministic or\n&gt;&gt; random (e.g., triggered by occasional flaky network conditions, disks,\n&gt;&gt; race conditions, or whatever)?\n&gt;\n&gt; Hard to say. The main thing I could suggest is watch very closely and \n&gt; when a SEVERE error occurs, prioritize diagnosing and resolving the \n&gt; cause while the info is fresh.\n&gt;\n&gt;&gt; - Do you believe that we can reliably backup to the previous checkpoint\n&gt;&gt; if we watch the logs and stop as soon as we see the first SEVER error?\n&gt;&gt; If we do this, do you speculate that the same SEVER will occur again?\n&gt;\n&gt; Resuming from the latest checkpoint before an error believed to \n&gt; corrupt the on-disk state will be the best strategy.\n&gt;\n&gt; If we never figure out the real cause, but run the same software on \n&gt; the same machine, yes, I expect the same problem will recur!\n&gt;\n&gt;&gt; - Is there any reason why a Heritrix instance that is run while binded\n&gt;&gt; to one ip address can&#39;t be resumed binded to a different ip address?\n&gt;\n&gt; Only the web UI to my knowledge binds to a chosen address, and it is \n&gt; common to have it bind to all. I don&#39;t expect the outbound requests \n&gt; would be hurt by a machine changing its IP address while the crawl was \n&gt; running, but I would run a test to be sure if that was an important, \n&gt; expected transition.\n&gt;\n&gt;&gt; 3) Should we configure the crawler with more instances and switch\n&gt;&gt; between them?\n&gt;&gt;\n&gt;&gt; We have seen that we can run a single instance to 100M pages +\n&gt;&gt; supporting images and documents. Perhaps this means that we need 10 or\n&gt;&gt; more instances instead of 5. That raises the possibility of running 2\n&gt;&gt; instances per machine. If we could run 2, or even 4, instances on a\n&gt;&gt; single machine, they would each run half as long.\n&gt;\n&gt; I don&#39;t think the problems as reported are specifically due to one \n&gt; node&#39;s progress growing beyond a certain size, but it might be the \n&gt; case that giant instances are more likely to suffer from, and harder \n&gt; to recover from, single glitches (eg a single disk error). On the \n&gt; other hand, many instances introduce more redundant overhead costs \n&gt; (certain data structures, cross-feeding URIs if you&#39;re doing that, etc.).\n&gt;\n&gt;&gt; - Can you suggest a way to start/stop instances from a script so we can\n&gt;&gt; change between instances automatically?\n&gt;\n&gt; Not a mode I&#39;ve thought much about.\n&gt;\n&gt;&gt; - Have you seen frequent starting / stopping of instances introduce\n&gt;&gt; instability?\n&gt;\n&gt; No... but it might make you notice latent issues sooner.\n&gt;\n&gt;&gt; 4) Crawl slows but restarting seems to improve the speed again.\n&gt;&gt;\n&gt;&gt; We noticed that the all of our instances would initially run at a fast\n&gt;&gt; pace. We would collect an average of 25M + pages/day for 2-3 days and\n&gt;&gt; then the crawl would slow down to 10M pages/day over the next few days.\n&gt;&gt; (these numbers are totals of all 5 instances combined). When we\n&gt;&gt; restarted the instances, the average pages would improve back to 25M +\n&gt;&gt; pages/day. The total crawled numbers (TiB) also reflected the slow down.\n&gt;&gt;\n&gt;&gt; - Is this something that others have experienced as well?\n&gt;\n&gt; I don&#39;t recall hearing other reports of speed boosts after \n&gt; checkpoint-resumes but others may have more experience.\n&gt;\n&gt;&gt; 5) We are capturing tweets from twitter, harvesting the urls and want to\n&gt;&gt; crawl those urls within 1 day of receiving the tweet. Can you recommend\n&gt;&gt; a strategy for doing this with the 5 instances we are running?\n&gt;&gt;\n&gt;&gt; - Do we need to run a separate crawler dedicated to this? If so, can you\n&gt;&gt; suggest a way to crawl out from the tweeted urls but when we get\n&gt;&gt; additional urls from the tweets, quickly change focus to these urls\n&gt;&gt; instead of the ones branching out. When adding urls as seeds, can you\n&gt;&gt; set a high priority to crawl those before the discovered urls? Do you\n&gt;&gt; recommend maybe setting up a specific crawl for these urls and then only\n&gt;&gt; crawl a few hopes from the seeds - injecting the urls from the tweets as\n&gt;&gt; seeds?\n&gt;\n&gt; Dedicating a special script or crawler to URIs that come from such a \n&gt; constrained source (Twitter feeds), or that need to be crawled in a \n&gt; special timeframe, or according to other special limits (fewer hops), \n&gt; could make sense.\n&gt;\n&gt; It would take some customization of the queueing-policy or \n&gt; &#39;precedence&#39; features of Heritrix to allow URIs added mid-crawl to be \n&gt; prioritized above those already discovered and queued. The most simple \n&gt; possible customization might be a UriPrecedencePolicy that takes all \n&gt; zero-hop URIs (which all seeds and most direct-fed URIs would be) and \n&gt; gives them a higher precedence (lower precedence number) than all \n&gt; other URIs.\n&gt;\n&gt;&gt; 6) I think the answer is no for this question, but I will ask it anyway.\n&gt;&gt; If you have a Heritrix instance that is configured for 1200 threads on\n&gt;&gt; one machine, can you recover from a checkpoint from that 1200 thread\n&gt;&gt; configuration on a different machine with an Heritrix instance that is\n&gt;&gt; configured for less threads (e.g. the default 25 threads)?\n&gt;\n&gt; Yes - there&#39;s no need to keep the thread count the same after a \n&gt; resume. None of the checkpoint structures (or usual disk structures) \n&gt; are based on the number of worker threads (&#39;ToeThreads&#39;)... as \n&gt; mentioned above you can even vary the number of threads in a running \n&gt; crawl.\n&gt;\n&gt; - Gordon\n&gt;\n\n"}}