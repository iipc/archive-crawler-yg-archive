{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"tGPvxIvIVh78MAp9BceNmC1vWHBbWgyh0cv2-1ceakQA9Ti3jCdjfk9asdnNREOKuP0ZoAt0sgGk0Lvm0pjxwMJJX0wG3Z4Q","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Heritrix crawling only xml","postDate":"1146512026","msgId":2825,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0NTY2MjlBLjYwNzAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGUydTJiMytkMmZ2QGVHcm91cHMuY29tPg==","referencesHeader":"PGUydTJiMytkMmZ2QGVHcm91cHMuY29tPg=="},"prevInTopic":2824,"nextInTopic":0,"prevInTime":2824,"nextInTime":2826,"topicId":2823,"numMessagesInTopic":3,"msgSnippet":"... The link posted by libsoft in an earlier response is a good place to start.  Change the filters so they look for text/xml and application/xml rather than","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 21420 invoked from network); 1 May 2006 19:33:40 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m22.grp.scd.yahoo.com with QMQP; 1 May 2006 19:33:40 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta8.grp.scd.yahoo.com with SMTP; 1 May 2006 19:33:40 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 7E36214156834;\n\tMon,  1 May 2006 12:33:30 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 06687-01-13; Mon, 1 May 2006 12:33:29 -0700 (PDT)\r\nReceived: from [192.168.0.110] (adsl-71-130-102-78.dsl.pltn13.pacbell.net [71.130.102.78])\n\tby mail.archive.org (Postfix) with ESMTP id 9D27B141566FC;\n\tMon,  1 May 2006 12:33:29 -0700 (PDT)\r\nMessage-ID: &lt;4456629A.6070303@...&gt;\r\nDate: Mon, 01 May 2006 12:33:46 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.1) Gecko/20060130 SeaMonkey/1.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e2u2b3+d2fv@...&gt;\r\nIn-Reply-To: &lt;e2u2b3+d2fv@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix crawling only xml\r\nX-Yahoo-Group-Post: member; u=168599281; y=2FKARHMMn_zz6W7rbLKmUctoF3i5Ieh2gOOSk1ptmdArEzOU59QR7Ubc\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nn8agrin wrote:\n&gt; Is there a way to get heritrix only to crawl XML based content?  I\n&gt; guess preferably, to set it to crawl all content, but only retreive\n&gt; links from xml?\n\nThe link posted by libsoft in an earlier response is a good place to \nstart.  Change the filters so they look for text/xml and application/xml \nrather than text/html.\n\n&gt;\n&gt; I tried playing with Content-Type filter and ExtractXML extractor to\n&gt; see if that would have any effect, but it was hard to tell if it was\n&gt; doing what I wanted.\n\nStudy the crawl.log.  For each URL crawled, if all is working, its \nantecedent -- the 6th column in crawl log -- will be an xml doc (For \ndetail on crawl.log, see 8.2.1 here: \nhttp://crawler.archive.org/articles/user_manual.html#logs).\n\nSt.Ack\n\n"}}