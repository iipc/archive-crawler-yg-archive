{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"uFMmw2_7sq7wMPuzwZ4dGfHJjc9g3RXdLQ5U-N9ghsEcZXyD6malA1xF3COxzG8Gosav5ysYlYcit6m_1iIgsQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] midfetch abort on robots.txt","postDate":"1104466221","msgId":1314,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxRDREMTJELjUwMTAwMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGNyMXBxYys0M2thQGVHcm91cHMuY29tPg==","referencesHeader":"PGNyMXBxYys0M2thQGVHcm91cHMuY29tPg=="},"prevInTopic":1313,"nextInTopic":0,"prevInTime":1313,"nextInTime":1315,"topicId":1312,"numMessagesInTopic":3,"msgSnippet":"... I took a look.  The robots processing happens after the download so I was curious to see what happens in case where robots.txt gets cut off. Well, turns","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 46659 invoked from network); 31 Dec 2004 04:17:50 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m5.grp.scd.yahoo.com with QMQP; 31 Dec 2004 04:17:50 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta2.grp.scd.yahoo.com with SMTP; 31 Dec 2004 04:17:50 -0000\r\nReceived: (qmail 12507 invoked by uid 100); 31 Dec 2004 04:03:18 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 31 Dec 2004 04:03:18 -0000\r\nMessage-ID: &lt;41D4D12D.5010002@...&gt;\r\nDate: Thu, 30 Dec 2004 20:10:21 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.3) Gecko/20041007 Debian/1.7.3-5\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;cr1pqc+43ka@...&gt;\r\nIn-Reply-To: &lt;cr1pqc+43ka@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.8 required=6.5 tests=AWL,CLICK_BELOW autolearn=no \n\tversion=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] midfetch abort on robots.txt\r\nX-Yahoo-Group-Post: member; u=168599281\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nrobeger wrote:\n\n&gt;\n&gt; I noticed in a crawl log that when I have Tom&#39;s text/html midfetch\n&gt; filter set up that robots.txt triggers it (which makes sense as it&#39;s a\n&gt; text/plain), but I&#39;m guessing that heritrix has already processed the\n&gt; file for what it needs rules-wise, correct?\n\nI took a look.  The robots processing happens after the download so I \nwas curious to see what happens in case where robots.txt gets cut off.\n\nWell, turns out theres a bug.  We were downloading content even though \nthe midfetch filter ruled the current fetch should be aborted. We were \naborting only after the download had completed.  This meant that though \nthe robots.txt might be excluded midfetch by say a mimetype filter that \nexcluded text/plain, it&#39;d still be downloaded and come postprocessing \ntime, the supposedly cutoff robots content would be available to the \nrobots rules processor.\n\nI&#39;ve fixed HEAD so that it truely aborts midfetch if the filters rule \nout a download (&#39;[1093614] midfetch abort doesn&#39;t&#39;).\n\nNow, when robots.txt is aborted midfetch, our robots processing code \ngets an empty robots.txt to read and so assumes open access to the \nsite.   This also needs to be fixed.\n\nMeantime, write regexes that allow robots.txt through!\n\nHappy new year.\nSt.Ack\n\n\n&gt; Just wanted to make sure that it wasn&#39;t somehow circumventing\n&gt; robots.txt and I&#39;d end up crawling sites that didn&#39;t want to be crawled.\n&gt;\n&gt; Thanks,\n&gt; Rob.\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; *Yahoo! Groups Sponsor*\n&gt; ADVERTISEMENT\n&gt; click here \n&gt; &lt;http://us.ard.yahoo.com/SIG=129bnnn88/M=294855.5468653.6549235.3001176/D=groups/S=1705004924:HM/EXP=1104526351/A=2455396/R=0/SIG=119u9qmi7/*http://smallbusiness.yahoo.com/domains/&gt; \n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}