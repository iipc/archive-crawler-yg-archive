{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigur√∞sson","from":"=?ISO-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"DH2Zw6LNUhESdHOVQdWiuuHdAfXLYA-Bm2p2uBF9E3T_dwXOtiRFn9H7pQafMo1T5lokOGGIBeyevocjaDGgacgFVxHRbXXr9OtsjboinXhqpNe_vbXCSCfifRT_QQQr","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] 99 % collected ?","postDate":"1083000846","msgId":348,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwOEQ0ODBFLjkwNTA0MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQwOEM2NkNELjgwNzAxMDhAYXJjaGl2ZS5vcmc+","referencesHeader":"PGM2Z2VmYytya3YyQGVHcm91cHMuY29tPiA8NDA4QzY2Q0QuODA3MDEwOEBhcmNoaXZlLm9yZz4="},"prevInTopic":344,"nextInTopic":349,"prevInTime":347,"nextInTime":349,"topicId":342,"numMessagesInTopic":7,"msgSnippet":"... This sounds like a bug we fixed some time after the 0.6.0 release. In more recent versions these numbers all add up. The only way to get an accurate idea","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 24120 invoked from network); 26 Apr 2004 17:34:41 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m25.grp.scd.yahoo.com with QMQP; 26 Apr 2004 17:34:41 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 26 Apr 2004 17:34:41 -0000\r\nReceived: (qmail 29783 invoked by uid 100); 26 Apr 2004 17:28:46 -0000\r\nReceived: from b116-dyn-55.archive.org (HELO archive.org) (kris@...@209.237.240.55)\n  by mail-dev.archive.org with SMTP; 26 Apr 2004 17:28:46 -0000\r\nMessage-ID: &lt;408D480E.9050406@...&gt;\r\nDate: Mon, 26 Apr 2004 10:34:06 -0700\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c6gefc+rkv2@...&gt; &lt;408C66CD.8070108@...&gt;\r\nIn-Reply-To: &lt;408C66CD.8070108@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.1 required=6.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: =?ISO-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;\r\nSubject: Re: [archive-crawler] 99 % collected ?\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\nGordon Mohr wrote:\n\n&gt; kaisa_kaunonen wrote:\n&gt; &gt; Hi all,\n&gt; &gt; when crawling a small number of domains it&#39;s easy to get the\n&gt; &gt; downloaded/discovered ratio to 95-98% in half a days time. But the end\n&gt; &gt; is slow.\n&gt; &gt;\n&gt; &gt; It often looks like there&#39;s one domain left and one thread fetches a\n&gt; &gt; single doc every now and then. When downloaded/discovered stays at 99%\n&gt; &gt; for a day I usually terminate the job and think that a couple of docs\n&gt; &gt; remained somewhere on the way.\n&gt; &gt;\n&gt; &gt; But then I wonder how much remains uncollected and what paths might\n&gt; &gt; have opened up. For one job the rate has been 99% (16209/16241) over\n&gt; &gt; 24 hours. I somehow think this means about 30 docs uncollected. But\n&gt; &gt; &#39;Crawl report&#39; page says\n&gt; &gt;\n&gt; &gt; URIs\n&gt; &gt; Discovered: 16241\n&gt; &gt; Queued: -178 (queue size &lt;&gt; 30)\n&gt; &gt; Finished: Total 16419   Success 16209  Failed 159 Disregarded 51\n\nThis sounds like a bug we fixed some time after the 0.6.0 release. In \nmore recent versions these\nnumbers all add up.\n\nThe only way to get an accurate idea of how many URIs are left where \nthis bug is still a probelm\n is to pause the crawl and use the Inspect Frontier option Gordon \npointed out. It is not affected\nby this bug.\n\n- Kris\n\n&gt; &gt;\n&gt; &gt; It would be nice to see all URIs waiting in queue before terminating\n&gt; &gt; jobs. Frontier report shows some queues but only their top URIs?\n&gt;\n&gt; This is currently possible, but you have to &#39;pause&#39; the crawler first.\n&gt; Once all threads have paused, a new option will appear on the &#39;console&#39;\n&gt; near the other control (stop/terminate/refresh/etc) options: &#39;Inspect\n&gt; frontier URIs&#39;.\n&gt;\n&gt; There, you can supply a (java) regular expression to page through\n&gt; all pending URIs, and also delete URIs by regular expression if\n&gt; necessary.\n&gt;\n&gt; When done, you can resume the crawl after any changes.\n&gt;\n&gt; Before this facility was available, we would rely on watching the\n&gt; end of the crawl-log, and the frontier report. When the tail\n&gt; activity was all repetitive, and the frontier only showed pending\n&gt; URIs on the problem site, we&#39;d wait a while to see if any novelty\n&gt; appeared in the URIs it was crawling but if not, presume all the\n&gt; pending URIs were like the ones recently completed.\n&gt;\n&gt; &gt; ****\n&gt; &gt; I&#39;ve haven&#39;t been caught in loops lately, maybe because I&#39;ve added\n&gt; &gt; some filters to profile. But it&#39;s sometimes hard to guess in advance\n&gt; &gt; what might cause a loop. Couldn&#39;t heritrix monitor itself =&gt; when one\n&gt; &gt; domain starts to fill queues heritrix could move URIs from other\n&gt; &gt; domains upwards and leave the possible loop causing URIs for a later\n&gt; &gt; time?\n&gt;\n&gt; We have started to add to the frontier/queuing classes two facilities\n&gt; for doing this, one at the URI level and another at the hostname\n&gt; (work queue) level. It is not yet functional, and won&#39;t have any\n&gt; interface by the upcoming 1.0 release, but it would work something\n&gt; like:\n&gt;\n&gt;   - If a URI is classified as so suspicious it should not be\n&gt;     fetched without operator approval, it will be added to a\n&gt;     side &#39;frozen&#39; queue for its host. In this way, it will neither\n&gt;     be tried, nor discarded, but held until the operator makes\n&gt;     a decision about it. The UI would highlight the total number\n&gt;     of &#39;frozen&#39; URIs and let the operator view/reschedule/delete\n&gt;     them as appropriate.\n&gt;\n&gt;   - If a host becomes problematic or suspicious -- for example\n&gt;     if all attempts to connect to it fail, or something about\n&gt;     the rate/novelty/pattern of new URIs discovered indicates\n&gt;     serious problems, then its entire queue can be &#39;frozen&#39;, and\n&gt;     an alert provided to the operator. Then, no further attempts\n&gt;     to fetch URIs for that host will occur, but all current and\n&gt;     newly-discovered URIs will continue to be collected, so that\n&gt;     if the operator &#39;unfreezes&#39; the queue progress will continue.\n&gt;\n&gt; The two facilities, while similar, are independent: a particular\n&gt; host&#39;s work queue could be going fine, but then send a few odd\n&gt; URIs off to the &#39;frozen&#39; queue, but continue fetching legitimate-\n&gt; seeming URIs. Then, due to some new error condition, the whole\n&gt; host queue could be &#39;frozen&#39; in place. If the operator &#39;unfroze&#39;\n&gt; the host queue, the individually frozen URIs would still be on\n&gt; hold. Conversely, if the operator approved the individually-frozen\n&gt; URIs, but left the host queue frozen, the newly approved individual\n&gt; URIs would simply wait for the queue to be reactivated.\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt; *Yahoo! Groups Sponsor*\n&gt; ADVERTISEMENT\n&gt; &lt;http://rd.yahoo.com/SIG=12941gkvo/M=291630.4786521.5933964.1261774/D=groups/S=1705004924:HM/EXP=1083029587/A=2072415/R=0/SIG=11thh7ako/*http://www.netflix.com/Default?mqso=60178432&partid=4786521&gt; \n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}