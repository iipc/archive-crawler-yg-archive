{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"M9wyRWDz80JP5n90xUA97kGXv8JUVBEsi9tQ648q9fqLWJzPAeVTJ6vQornlJTnXSJy2sPvH_Xkiyvxu7SM-3CqdQxNbKyioGIqJIAdakQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: home-made crawler instead of off-the-shelf crawler","postDate":"1248110508","msgId":5938,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGg0MjkzYytjbW1pQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGgzcGVvOSs2YzE0QGVHcm91cHMuY29tPg=="},"prevInTopic":5932,"nextInTopic":0,"prevInTime":5937,"nextInTime":5939,"topicId":5932,"numMessagesInTopic":2,"msgSnippet":"The use case you describe is not web crawling, it is just downloading a set of files. While Heritrix could be (using some custom bean shell scripts) configured","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 7065 invoked from network); 20 Jul 2009 17:22:06 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m3.grp.sp2.yahoo.com with QMQP; 20 Jul 2009 17:22:06 -0000\r\nX-Received: from unknown (HELO n39b.bullet.mail.sp1.yahoo.com) (66.163.168.153)\n  by mta1.grp.sp2.yahoo.com with SMTP; 20 Jul 2009 17:22:06 -0000\r\nX-Received: from [69.147.65.171] by n39.bullet.mail.sp1.yahoo.com with NNFMP; 20 Jul 2009 17:21:51 -0000\r\nX-Received: from [98.137.34.72] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 20 Jul 2009 17:21:51 -0000\r\nDate: Mon, 20 Jul 2009 17:21:48 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;h4293c+cmmi@...&gt;\r\nIn-Reply-To: &lt;h3peo9+6c14@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: Re: home-made crawler instead of off-the-shelf crawler\r\nX-Yahoo-Group-Post: member; u=163406187; y=TJuC-pV54buuJ2rKw6DGer0mkW3L_glLRYFbQskB4CG2Gy6P\r\nX-Yahoo-Profile: kristsi25\r\n\r\nThe use case you describe is not web crawling, it is just downloading a set=\r\n of files. While Heritrix could be (using some custom bean shell scripts) c=\r\nonfigured to perform this task, it is overkill.\n\nA key component of a web c=\r\nrawler is that you do not know all the URLs you intend to visit before hand=\r\n. Much of the complexity in web crawling is related to URL discovery and ma=\r\nnaging the discovered URLs etc. \n\nGiven the simple nature of your use case,=\r\n I&#39;d say you are better off with a custom solution (or possibly there is an=\r\n off the self solution other than Heritrix that is a better fit). Using Her=\r\nitrix would be like using a 747 to go the corner store.\n\n- Kris\n\n--- In arc=\r\nhive-crawler@yahoogroups.com, &quot;dzieciou&quot; &lt;mgawinecki@...&gt; wrote:\n&gt;\n&gt; I&#39;ve n=\r\never used your crawler, but I used to write my own simple solutions in Perl=\r\n for crawling and scraping pages. I would like to ask you for a comment on =\r\nthe following issue.  \n&gt; \n&gt; I have the following crawling task:\n&gt; \n&gt; ***\n&gt; =\r\n\n&gt; I have a list of about 3600 URLs for WSDL files I want to download them =\r\nfrom the Internet. \n&gt; \n&gt; For each URL I have a two back-up links for the co=\r\npies of the same file, to be used if the first one fails.\n&gt; \n&gt; Some of thes=\r\ne URLs are hosted under the same domains, so some particular domains shoudn=\r\n&#39;t be overloaded\n&gt; \n&gt; If all URL fails I want to have some logs about the e=\r\nrrors, too.\n&gt; \n&gt; Each WSDL file will be then parsed for extracting differen=\r\nt informations.\n&gt; \n&gt; ***\n&gt; \n&gt; What would be the advantage of learning and u=\r\nsing off-the-shelf crawler like yours instead of writing a simple Perl scri=\r\npt that checks the URL and dumps its content to a file?\n&gt; \n&gt; Best regards,\n=\r\n&gt; Maciej Gawinecki\n&gt;\n\n\n\n"}}