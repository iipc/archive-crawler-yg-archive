{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":526355047,"authorName":"matteo.ceccarello@ymail.com","from":"&quot;matteo.ceccarello@...&quot; &lt;matteo.ceccarello@...&gt;","profile":"matteo.ceccarello@ymail.com","replyTo":"LIST","senderId":"UBzOvgNk4s_EiByK0cf6cHEyJFPU3YSpHO6vEnnj6vKd85hUQsrb_Edu2SF3lfFivc08KMwKpZhUn1w1vLJYnmBy4lnAYSbrLrwfDwdQ6V7bZtcJawok4Yq8wg6Bokz6wvo","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Heritrix 1.14.4 - Hard disk breaking","postDate":"1335777253","msgId":7678,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpubGw1NSsyb2M0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRGOEQwMTA1LjMwMjAwMDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":7654,"nextInTopic":7679,"prevInTime":7677,"nextInTime":7679,"topicId":7646,"numMessagesInTopic":6,"msgSnippet":"Sorry for the late reply!! Thank you for pointing out BloomuriUniqFilter, we will surely try it! Keeping already seen URIs in memory is just wht we need! So,","rawEmail":"Return-Path: &lt;matteo.ceccarello@...&gt;\r\nX-Sender: matteo.ceccarello@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 15508 invoked from network); 30 Apr 2012 09:14:16 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m1.grp.sp2.yahoo.com with QMQP; 30 Apr 2012 09:14:16 -0000\r\nX-Received: from unknown (HELO ng14-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.174)\n  by mta3.grp.sp2.yahoo.com with SMTP; 30 Apr 2012 09:14:15 -0000\r\nX-Received: from [98.137.0.84] by ng14.bullet.mail.gq1.yahoo.com with NNFMP; 30 Apr 2012 09:14:15 -0000\r\nX-Received: from [98.137.34.155] by tg4.bullet.mail.gq1.yahoo.com with NNFMP; 30 Apr 2012 09:14:15 -0000\r\nDate: Mon, 30 Apr 2012 09:14:13 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jnll55+2oc4@...&gt;\r\nIn-Reply-To: &lt;4F8D0105.3020003@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;matteo.ceccarello@...&quot; &lt;matteo.ceccarello@...&gt;\r\nSubject: Re: Heritrix 1.14.4 - Hard disk breaking\r\nX-Yahoo-Group-Post: member; u=526355047; y=wXIj_PYVYY-CYbHx1jmIzMPf8n9R19AUK4AD8qFB9xo4VeJ3obT6YreWnx2emXHaBnFHhayY\r\nX-Yahoo-Profile: matteo.ceccarello@...\r\n\r\nSorry for the late reply!!\nThank you for pointing out BloomuriUniqFilter, w=\r\ne will surely try it! Keeping already seen URIs in memory is just wht we ne=\r\ned! So, thanks again for the clear and exhaustive answer :)\n\nMatteo\n\n--- In=\r\n archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; BdbFr=\r\nontier with the default BdbUriUniqFilter isn&#39;t the most efficient \n&gt; with r=\r\nandom disk seeks, and our rough rule-of-thumb, developed from \n&gt; experience=\r\n on machines that have 4GB-8GB of RAM, is to consider swapping \n&gt; out the B=\r\ndbUriUniqFilter for the BloomUriUniqFilter on crawls expected \n&gt; to grow pa=\r\nst 50 million discovered URIs, and into the hundreds of \n&gt; millions of URIs=\r\n.\n&gt; \n&gt; The Bloom implementation keeps track of all &#39;seen&#39; URIs in memory, \n=\r\n&gt; trading a small false-positive error rate for compactness. The default \n&gt;=\r\n parameters, which can be changed, maintains a 1-in-4-million false \n&gt; posi=\r\ntive rate through about 125 million seen URIs, using about 500MB of \n&gt; RAM.=\r\n It&#39;s a little bit slower than the alternative in the beginning, but \n&gt; nev=\r\ner gets any slower over time. (And even as it grows past its planned \n&gt; siz=\r\ne, its false error rate just creeps up.)\n&gt; \n&gt; Other than that, making sure =\r\nall the crawler&#39;s key disk paths are spread \n&gt; over available independent v=\r\nolumes, as Bjarne and Travis mention, can \n&gt; help somewhat. Depending on ex=\r\nactly what your crawl is like and what \n&gt; bottlenecks you&#39;re seeing, and yo=\r\nur system characteristics, there might \n&gt; be other crawl settings that coul=\r\nd be tweaked for some improvement.\n&gt; \n&gt; Some other possibilities for future=\r\n improvement, especially for the \n&gt; UriUniqFilter component which is subjec=\r\nt to constant growth and random \n&gt; access, would be to blend several of the=\r\n techniques and use a larger \n&gt; in-memory cache for commonly-seen URIs (in =\r\nfront of the \n&gt; BdbUriUniqFilter), which depending on implementation choice=\r\n might \n&gt; outperform the automatic caching provided by the BDB-JE library.\n=\r\n&gt; \n&gt; The best technique I&#39;ve read about uses a lagged sort and merge to bat=\r\nch \n&gt; together the most possible already-seen tests within the fewest \n&gt; se=\r\neks/reads/writes. It&#39;s described in the 2001 &quot;High-Performance Web \n&gt; Crawl=\r\ning&quot; paper by Najork and Heydon, and perhaps improvable by the \n&gt; approach =\r\ndescribed 2008 &quot;IRLbot: Scaling to 6 Billion Pages and Beyond&quot; \n&gt; paper by =\r\nLee, Leonard, Wang, and Loguinov.\n&gt; \n&gt; We tried to maintain the possibility=\r\n of such lagged (non-instant, \n&gt; reordered) unique-testing in the Heritrix =\r\nfrontier design, to allow \n&gt; these techniques to be swapped in. There&#39;s one=\r\n long-ago implementation \n&gt; of a similar approach in the codebase as the &#39;F=\r\nPMergeUriUniqFilter&#39;, but \n&gt; that might need work to function with the curr=\r\nent codebase, or reveal \n&gt; other ordering-sensitive bugs, and it was never =\r\nrigorously benchmarked \n&gt; against the other options inside the Heritrix con=\r\ntext.\n&gt; \n&gt; - Gordon\n&gt; \n&gt; \n&gt; On 4/11/12 10:50 AM, matteo.ceccarello@... wrot=\r\ne:\n&gt; &gt; Hi all,\n&gt; &gt;\n&gt; &gt; We are two students of the University of Padova, Ita=\r\nly. We are attempting a large crawl using the Heritrix crawler (version 1.1=\r\n4.4). The frontier we are using is the bdbfrontier. We noticed that using t=\r\nhis frontier, the crawler is constantly accessing the disk, performing smal=\r\nl reads/writes that\n&gt; &gt;   a) are inefficient in terms of aggregated disk I/=\r\nO\n&gt; &gt;   b) move the disk head a lot, which is not very healthy for the disk=\r\n itself. In fact, we have already lost multiple disks in the process.\n&gt; &gt;\n&gt;=\r\n &gt; Is there some way to avoid this problem?\n&gt; &gt;\n&gt; &gt; Thanks for the help\n&gt; &gt;=\r\n\n&gt; &gt; Matteo Ceccarello,\n&gt; &gt; Alessandro Secco\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --------------=\r\n----------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}