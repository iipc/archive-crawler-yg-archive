{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"B4UBigOLIn4xHXyklzN4Syttqnc7PMcVHfdtYt1Xy3f4TX6QbuHi6yZ9RSHuTgSslKz2YVAzO2wkAzsuBbhTzmUmPRD2Un4","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Extending ARC Re: [archive-crawler] Re: Crawler-guided form/authentication entry","postDate":"1075169964","msgId":271,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwMTVDQUFDLjkwMzA4MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGJ2MzRhNCtpaWNiQGVHcm91cHMuY29tPg==","referencesHeader":"PGJ2MzRhNCtpaWNiQGVHcm91cHMuY29tPg=="},"prevInTopic":263,"nextInTopic":277,"prevInTime":270,"nextInTime":272,"topicId":235,"numMessagesInTopic":8,"msgSnippet":"... A possible index/access mechanism when considering form submissions would be to perform a fuzzy match against the archived data fields. Given a particular","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 58125 invoked from network); 27 Jan 2004 02:19:27 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m11.grp.scd.yahoo.com with QMQP; 27 Jan 2004 02:19:27 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 27 Jan 2004 02:19:26 -0000\r\nReceived: (qmail 10959 invoked by uid 100); 27 Jan 2004 02:17:02 -0000\r\nReceived: from b116-dyn-43.archive.org (HELO archive.org) (gojomo@...@209.237.240.43)\n  by mail-dev.archive.org with SMTP; 27 Jan 2004 02:17:02 -0000\r\nMessage-ID: &lt;4015CAAC.9030808@...&gt;\r\nDate: Mon, 26 Jan 2004 18:19:24 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;bv34a4+iicb@...&gt;\r\nIn-Reply-To: &lt;bv34a4+iicb@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-4.5 required=6.0 tests=AWL,BAYES_00 autolearn=ham \n\tversion=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: Extending ARC Re: [archive-crawler] Re: Crawler-guided form/authentication\n entry\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nsteensc42 wrote:\n&gt; I agree that the safe approach is to include everything into the \n&gt; checksum.\n&gt; From a retrieval point of view this might however pose some problems.\n&gt; How do we locate the correct record in the archive to return as \n&gt; response to a post request?\n&gt; It would be nice if we could convert the post request into a key that \n&gt; we can locate \n&gt; in an index of archived url-objects. If this key is dependent on all \n&gt; request headers\n&gt; it will only work for specific browsers, referrer combinations.\n&gt; I guess the answer here is to accept that the &lt;capture-identifier&gt; \n&gt; can not be\n&gt; used directly for indexing purposes - and leave the retrieval problem \n&gt; to the\n&gt; implementation of the indexing mechanism.\n\nA possible index/access mechanism when considering form submissions\nwould be to perform a fuzzy match against the archived data fields.\n\nGiven a particular request -- URI, query-string, post-data, accept\nheader, user-agent, etc. -- break each of those fields into tokens and\nlook for the closest matches in the archive, in a full-text search\nstyle.\n\nThe top ranked hit would be shown -- with the necessary UI\ndisclaimers that the access tool cannot perfectly simulate\nthe original form, but only provide captures from similar\nsubmissions.\n\n&gt; I would really like the option to:\n&gt; \t- place the metadata record at an arbitrary position relative \n&gt; to the other capture records.\n&gt; \t- create more than one metadata record per capture.\n&gt; \t\n&gt; An example of a usage scenario with metadata records positioned away \n&gt; from the other capture records:\n&gt; \tAfter creation of an ARC file, a (time consuming) \n&gt; postprocessing of some \n&gt; \tof the collected data is performed.\n&gt; \tThe information extracted by this process is written as \n&gt; metadata records appended to \n&gt; \tthe original ARC file.\n\nI understand this goal, and I don&#39;t think there needs to be a\nstrict requirement that the request, response, and metadata records\nare contiguous.\n\nHowever, your postprocessing scenario is more like what we at the\nArchive have usually kept in separate, but similar, &#39;DAT&#39; files.\n\nEven if the exact same &#39;metadata:&#39; labelling and record format are used,\nit could be beneficial to always keep postprocessing files distinct\nfrom original crawl files, perhaps via a different extension or naming\nscheme. (&quot;.marc&quot;?)\n\n&gt; We would like to use this format to record frequent (daily) captures \n&gt; of selected sites.\n&gt; In this scenario we would need to be able to refer to duplicates \n&gt; across crawls.\n&gt; A procedure that ensures that master records are rewritten \n&gt; occasionally would of\n&gt; course need to be implemented.\n\nYes, the ability to refer back to any previous (and reliably stored)\ncapture would be ideal.\n\nPerhaps even: to support diff-encoding of the most recent version,\nwhen it is only a small change from the previous version.\n\n&gt; We have another usage scenario we would like this format to support.\n&gt; Some of the data formats in the archive may become \n&gt; obsolete/unsupported over time. \n&gt; One approach for handling this problem is storage of applications \n&gt; capable of handling/converting\n&gt; these formats. Another approach we want to support is transformation \n&gt; of (important) data at risk\n&gt; to formats still supported. An example might be transformation from \n&gt; one graphic file format (say gif),\n&gt; to another format (say tif). This might be achieved using &quot;transform&quot; \n&gt; to mark a transformed version:\n&gt; \n&gt; transform:http://&lt;url&gt; &lt;dest-ip&gt; &lt;timestamp&gt; &lt;mime&gt; &lt;response-size&gt; \n&gt; &lt;capture-identifier&gt;\n&gt; &lt;DATA - transformed version of original data record&gt;\n&gt; metadata:transform:http://&lt;url&gt; &lt;dest-ip&gt; &lt;timestamp&gt; &lt;mime&gt; \n&gt; &lt;metadata-size&gt; &lt;capture-identifier&gt;\n&gt; &lt;DATA - metadata describing details about the transformation (from \n&gt; format, to format, application, etc)&gt;\n&gt; \n&gt; Each time a url-object is transformed a new pair of (transform, \n&gt; metadata:transform) records\n&gt; are created, allowing sequences of format transformations.\n&gt; \n&gt; Comments?\n\nMakes sense to me. I think we would generally prefer to archive\ntransformation routines that could always be applied on-the-fly, but\nbatch converts into new archived fromats may also be required.\n\nHere again, even if the internal format is identical, I might make\nsuch tranformed content distinguishable from original crawler\noutput by a naming convention. (&quot;.tarc&quot;?)\n\nA new issue: how could we expand the beginning of the ARC format\nto better identify the context (software, project, settings) in\nwhich the following captures were collected?\n\nI&#39;ve been thinking that either (1) the first &#39;filedesc:&#39; record could\nbe expanded to include both a free-form operator comment and\nsome number of standard fields; or (2) multiple pseudo-records,\nlike the first, could describe the ARC or crawl as a whole.\n\nIn particular, I would be willing to put a few K of crawl\ndescriptive info into every ARC, or perhaps full crawl\nsoftware and config info into the first ARC and pointers\nback into all subsequent ARCs.\n\n- Gordon\n\n"}}