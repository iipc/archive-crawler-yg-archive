{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"pygKFrs_w-EPiVhdxIvaQ9wevov9dzy6QNRdaA3oiXx6izvubAXWLpkiXApJ3nHr8M3DEPBHpszufb8AVQs4ksOhJCefmAA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Crawling Ajax based sites using Heritrix","postDate":"1263347921","msgId":6295,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRCNEQyOEQxLjcwODA5MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDk3MzY5NS41NDQzMC5xbUB3ZWI1NDUwNy5tYWlsLnJlMi55YWhvby5jb20+","referencesHeader":"PDk3MzY5NS41NDQzMC5xbUB3ZWI1NDUwNy5tYWlsLnJlMi55YWhvby5jb20+"},"prevInTopic":6287,"nextInTopic":0,"prevInTime":6294,"nextInTime":6296,"topicId":6287,"numMessagesInTopic":2,"msgSnippet":"... None yet, though it s an area of keen interest among many crawling groups. The two approaches usually discussed are either: (1) building a compliant","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 64234 invoked from network); 13 Jan 2010 01:58:44 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m13.grp.re1.yahoo.com with QMQP; 13 Jan 2010 01:58:44 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta2.grp.sp2.yahoo.com with SMTP; 13 Jan 2010 01:58:43 -0000\r\nX-Received: (qmail 98872 invoked from network); 13 Jan 2010 01:58:42 -0000\r\nX-Received: from 71.202.37.94 (HELO ?192.168.23.128?) (71.202.37.94)\n  by relay02.pair.com with SMTP; 13 Jan 2010 01:58:42 -0000\r\nX-pair-Authenticated: 71.202.37.94\r\nMessage-ID: &lt;4B4D28D1.7080905@...&gt;\r\nDate: Tue, 12 Jan 2010 17:58:41 -0800\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;973695.54430.qm@...&gt;\r\nIn-Reply-To: &lt;973695.54430.qm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Crawling Ajax based sites using Heritrix\r\nX-Yahoo-Group-Post: member; u=137285340; y=vwpR54K_Nfhvo0YzVT88mLr-8Xmb3B3Yi9BjNayg3fcH\r\nX-Yahoo-Profile: gojomo\r\n\r\nlucky goyal wrote:\n&gt; I want to crawl Ajax based websites using heritrix. Is there any \n&gt; avalable plugin or another mechanism to crawl such Ajax based websites \n&gt; using heritrix. Any help on this will be really useful.\n\nNone yet, though it&#39;s an area of keen interest among many crawling groups.\n\nThe two approaches usually discussed are either:\n\n(1) building a compliant simulated browser inside the crawler itself, \nsufficient for most JS/AJAX operations\n\n(2) using a captive genuine browser, like Firefox, by remote control on \npages detected as requiring the extra attention, and synthesizing the \nclicks/mouseovers of interest\n\nIn either case, as part of link-extraction the &#39;browser&#39; would be \nprompted to iterate through all the clicks/mouseovers a user might try.\n\nHtmlUnit is an all-Java &#39;headless&#39; simulated browser with reasonable \nJS/DOM support that could be the basis of approach (1). Indeed, it seems \nto be Google&#39;s choice for automated/unit testing of its GWT-build web apps.\n\nThe biggest architectural issue for Heritrix in my mind is how to \nreconcile the fetching of resources needed to execute a page with the \npreexisting fetch-loop.\n\nFor example, page A might require CSS/JS/XML/JSON/etc. resources B, C, D \nfor JS/DOM-sensitive link-extraction to proceed. Should B, C, D be \nfetched through the normal process, and A&#39;s simulated execution only \noccur when all become available? If so, the crawler needs a more \nsophisticated cache for it to reload (for JS/DOM purposes) \nrecently-fetched resources. (And, this isn&#39;t even considering the \nprospect that in-page execution will then decide E, F, G are needed ASAP \n  for execution to continue.)\n\nAlternatively, B/C/D (and then E/F/G) could be fetched immediately, \nout-of-band with regard to the normal processing cycle. But we&#39;d still \nlike to record their exact contents -- as a reference to what was used \nfor link-extraction, especially in our &#39;verbatim archive&#39; model -- and \nwe don&#39;t want them all re-fetched for the next A2, A3, A4 pages visited \n(if reuse is appropriate by HTTP caching rules).\n\nAn efficient and correct approach probably requires what is essentially \nan HTTP-compliant cache alongside (or inside) the JS/DOM-savvy crawler, \nso it gets everything it needs when it needs it, without redundant \noutbound requests or skipping desired processing steps on some of the \nfetched resources. Some of the current simplifying assumptions about a \nURI&#39;s ordered lifecycle -- it&#39;s discovered, it&#39;s queued, it&#39;s \nfetched/analyzed/stored once, it&#39;s done -- may need to be relaxed.\n\nSuggestions of possible approaches, necessary features, and more info \nabout use cases for AJAX/JS/DOM-sensitive crawling are welcome.\n\n- Gordon @ IA\n\n\n\n\n\n"}}