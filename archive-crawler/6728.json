{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"qmh4g98CzFO_z63OAWXeFg21tu3xDTaLpX3YJJ_4qOmCwxDZUUZlev6vnkqT997xL1UqN-57SL4lb_vFixOkvI2uXWVgAUI","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Ideas to make the distributed crawling.","postDate":"1284532582","msgId":6728,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOTA2OTY2LjQwNjAwMDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRDOEZFNkVBLjMwNTA5MDNAZ21haWwuY29tPg==","referencesHeader":"PGk2bzUwaytlaGI3QGVHcm91cHMuY29tPiA8NEM4RkM3QjguMjA3MDUwNEBhcmNoaXZlLm9yZz4gPDRDOEZFNkVBLjMwNTA5MDNAZ21haWwuY29tPg=="},"prevInTopic":6725,"nextInTopic":0,"prevInTime":6727,"nextInTime":6729,"topicId":6719,"numMessagesInTopic":5,"msgSnippet":"You may be prematurely optimizing for scale. Even on modest hardware the crawler can visit tens to hundreds of thousands of URIs in an hour. (With ideal","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 25383 invoked from network); 15 Sep 2010 06:36:46 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m16.grp.re1.yahoo.com with QMQP; 15 Sep 2010 06:36:46 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta3.grp.re1.yahoo.com with SMTP; 15 Sep 2010 06:36:46 -0000\r\nX-Received: (qmail 20642 invoked from network); 15 Sep 2010 06:36:31 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay01.pair.com with SMTP; 15 Sep 2010 06:36:31 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4C906966.4060009@...&gt;\r\nDate: Tue, 14 Sep 2010 23:36:22 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.9) Gecko/20100825 Thunderbird/3.1.3\r\nMIME-Version: 1.0\r\nTo: Mackram Raydan &lt;mackram@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;i6o50k+ehb7@...&gt; &lt;4C8FC7B8.2070504@...&gt; &lt;4C8FE6EA.3050903@...&gt;\r\nIn-Reply-To: &lt;4C8FE6EA.3050903@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Ideas to make the distributed crawling.\r\nX-Yahoo-Group-Post: member; u=137285340; y=52YQEitbmY6DKzTJoqJMhp1NsT4R9ExNn561W5TpKz0S\r\nX-Yahoo-Profile: gojomo\r\n\r\nYou may be prematurely optimizing for scale. Even on modest hardware the \ncrawler can visit tens to hundreds of thousands of URIs in an hour. \n(With ideal hardware and a bit of tweaking, one crawler might reach \nmillions of URIs per hour. For example, the Texas A&M &#39;IRLBot&#39; achieved \na sustained rate of over 6 million URIs/hour in a 41-day, 6 billion URI \ncrawl [1]. We haven&#39;t yet optimized Heritrix to that level -- or even \nrun it on hardware comparable to the IRLbot results -- but it&#39;s \ninstructive of the possibilities.)\n\nThe current rates, while not enough to deep-crawl many sites \n&#39;real-time&#39;, may be enough to get the feed/breaking-news content of all \nsites of interest without requiring more machines.\n\nThe best way to scale out may depend on what resources are bottlenecks \nwhen you hit a single-machine&#39;s limits.\n\nThe design you sketched out offloads network-fetching, but the &#39;master&#39; \nfrontier still needs to keep track of all queues and all discovered \n(&#39;already seen&#39;/&#39;already included&#39;) URIs. The disk seeks related to \nthose data structures are often the critical bottleneck in large crawls, \nso maintaining a single master frontier might still constrain your \nthroughput. (Or not; it depends on a lot of other factors in the &#39;shape&#39; \nof your crawl and systems environment.)\n\nSo, you may want to profile how one crawler machine will fall short of \nyour needs before settling on the appropriate multi-machine strategy.\n\n- Gordon @ IA\n\n[1] http://irl.cs.tamu.edu/people/hsin-tsang/papers/tweb2009.pdf\n\nOn 9/14/10 2:19 PM, Mackram Raydan wrote:\n&gt; First of all thanks for the heads up on the complexity of the Frontier\n&gt; component. To answer you questions:\n&gt;\n&gt; 1-The primary goal is to make a news search portal for a company that I\n&gt; am contracted to help. They want to be able to throw instances at the\n&gt; problem as the number of sites/news subjects they need to look for\n&gt; increases.\n&gt;\n&gt; 2-Why I think one crawler is not enough is because they want to come\n&gt; close to real time logging of info which means that it would be helpful\n&gt; to run things in parallel.\n&gt;\n&gt; 3- No honestly I have not looked at the CrawlMapper sublcass instance so\n&gt; I can not answer to that. I will make a through look into that further\n&gt; tomorrow. Thank you for pointing it out.\n&gt;\n&gt;\n&gt;\n&gt; On 09/14/2010 10:06 PM, Gordon Mohr wrote:\n&gt;&gt; It is the intent of the Heritrix design that any components, including\n&gt;&gt; the frontier implementation, should be swappable. So creating new\n&gt;&gt; frontier implementations is a plausible approach.\n&gt;&gt;\n&gt;&gt; However, the frontier is the most complicated component, and many other\n&gt;&gt; features are dependent on the behavior of current implementations. So,\n&gt;&gt; offering new variants (by subclassing or offering the Frontier\n&gt;&gt; interface) should be done carefully.\n&gt;&gt;\n&gt;&gt; Backing up a bit: what is the primary goal that makes you interested in\n&gt;&gt; a distributed approach? Why is one crawler not enough?\n&gt;&gt;\n&gt;&gt; (I have my own ideas about this but would like to hear yours.)\n&gt;&gt;\n&gt;&gt; Have you thoroughly understood (and run/benchmarked) the ad-hoc\n&gt;&gt; distribution technique, making use of &#39;CrawlMapper&#39; subclass instances,\n&gt;&gt; that&#39;s been described in past list traffic? What is good or bad about\n&gt;&gt; that approach for your purposes?\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; On 9/14/10 8:40 AM, Mackram Raydan wrote:\n&gt;&gt;&gt; So I have spent all day looking at the Heritrix code and the info on\n&gt;&gt;&gt; its architecture. The more I read the more I realized that doing a\n&gt;&gt;&gt; change in the code directly might not be the smartest thing to do as\n&gt;&gt;&gt; it would require a huge set of changes, so I had an idea that I wanted\n&gt;&gt;&gt; to run by the group if possible.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Since Heritrix so easily supports modules I am thinking that I would\n&gt;&gt;&gt; develop 2 new Frontiers that will do the distributed crawling.\n&gt;&gt;&gt;\n&gt;&gt;&gt; The master Frontier (the distributed crawling I am envisioning has a\n&gt;&gt;&gt; central master) will pretty much do the same code done with the\n&gt;&gt;&gt; following exceptions:\n&gt;&gt;&gt; 1- A socket listener which will be able to dispatch new URIs to\n&gt;&gt;&gt; schedule, to mark URIs as finished and to send out new URIs to be\n&gt;&gt;&gt; crawled when requested\n&gt;&gt;&gt;\n&gt;&gt;&gt; The slave Frontier (for each instance of Heritrix) will be the\n&gt;&gt;&gt; simplest form of a Frontier which will do the following:\n&gt;&gt;&gt; 1- When requested for a next URI it will get it send a request for the\n&gt;&gt;&gt; master for a new URI\n&gt;&gt;&gt; 2- When a new URI is discovered and should be scheduled it is sent to\n&gt;&gt;&gt; the master\n&gt;&gt;&gt; 3- When a URI is finished the slave will inform the master of that\n&gt;&gt;&gt; state.\n&gt;&gt;&gt;\n&gt;&gt;&gt; My idea of the above is to keep the basic code of Heritrix unchanged\n&gt;&gt;&gt; and just allow the addition of distributed code through a new module.\n&gt;&gt;&gt; I would love input on the above especially if you think it would not\n&gt;&gt;&gt; work for some reason or the other.\n&gt;&gt;&gt;\n&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, Mackram Raydan&lt;mackram@...&gt;\n&gt;&gt;&gt; wrote:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Hey everyone,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; I have recently been looking into whether or not we can have heritrix\n&gt;&gt;&gt;&gt; run in\n&gt;&gt;&gt;&gt; a distributed manner and had opened a minor issue for it. Gordon was\n&gt;&gt;&gt;&gt; kind\n&gt;&gt;&gt;&gt; enough to point out the list here might be helpful. Also Noah was kind\n&gt;&gt;&gt;&gt; enough to point to the hcc project which as I understand is in the\n&gt;&gt;&gt;&gt; alpha\n&gt;&gt;&gt;&gt; stage. I was considering porting heritrixs to hadoop and would like the\n&gt;&gt;&gt;&gt; opinions of the group. My line of thought was to change the worker\n&gt;&gt;&gt;&gt; threads\n&gt;&gt;&gt;&gt; into map functions and then use reduce to manage the URLs. Arguably\n&gt;&gt;&gt;&gt; one does\n&gt;&gt;&gt;&gt; not really need map/reduce for this and could go more with the\n&gt;&gt;&gt;&gt; approach of\n&gt;&gt;&gt;&gt; the hcc project but honestly I am still going through heritrix code\n&gt;&gt;&gt;&gt; so, l\n&gt;&gt;&gt;&gt; would love the input from people on this.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Thanks and best regards\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Mackram Raydan\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;\n\n"}}