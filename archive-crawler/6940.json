{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264138474,"authorName":"Kenji Nagahashi","from":"Kenji Nagahashi &lt;knagahashi@...&gt;","profile":"kenznag","replyTo":"LIST","senderId":"NcGY0usr0RS6vO5b7PyUEX4hTdiZoZKcXrrd7ot9LgM8BFnVPf9lDW-IThPgL6rgd-cvGNBkUuVk6Rb5A8P6oHCh0Wzd3P-2PcvpTwg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Too many open files Exception","postDate":"1293129754","msgId":6940,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREMTM5ODFBLjIwMTA3MDdAZ21haWwuY29tPg==","inReplyToHeader":"PDREMTJFNEU4LjUwMTA5MDhAYXJjaGl2ZS5vcmc+","referencesHeader":"PEFBTkxrVGk9ZjYwK3QwNDZuelZna3gzcGhodThHcUo1ZjZ3MTlfRDVBa3FSQUBtYWlsLmdtYWlsLmNvbT4gPDREMTI4NzdELjUwMzA4MDhAZ21haWwuY29tPiA8NEQxMkU0RTguNTAxMDkwOEBhcmNoaXZlLm9yZz4="},"prevInTopic":6939,"nextInTopic":6941,"prevInTime":6939,"nextInTime":6941,"topicId":6936,"numMessagesInTopic":5,"msgSnippet":"Hi Gordon, Ah, sorry for jumping in being ignorant of past conversation! But let me say, using low-default open-files limit is not the point of my post. Our","rawEmail":"Return-Path: &lt;knagahashi@...&gt;\r\nX-Sender: knagahashi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 28511 invoked from network); 23 Dec 2010 18:42:39 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m7.grp.sp2.yahoo.com with QMQP; 23 Dec 2010 18:42:39 -0000\r\nX-Received: from unknown (HELO mail-yw0-f47.google.com) (209.85.213.47)\n  by mta3.grp.sp2.yahoo.com with SMTP; 23 Dec 2010 18:42:39 -0000\r\nX-Received: by ywi6 with SMTP id 6so2894781ywi.6\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Dec 2010 10:42:38 -0800 (PST)\r\nX-Received: by 10.90.30.6 with SMTP id d6mr3792928agd.158.1293129758701;\n        Thu, 23 Dec 2010 10:42:38 -0800 (PST)\r\nReturn-Path: &lt;knagahashi@...&gt;\r\nX-Received: from Kenjis-MacBook-Pro.local (router300.sf.archive.org [208.70.27.190])\n        by mx.google.com with ESMTPS id x31sm12768405ana.29.2010.12.23.10.42.36\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Thu, 23 Dec 2010 10:42:37 -0800 (PST)\r\nMessage-ID: &lt;4D13981A.2010707@...&gt;\r\nDate: Thu, 23 Dec 2010 10:42:34 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; ja-JP-mac; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;AANLkTi=f60+t046nzVgkx3phhu8GqJ5f6w19_D5AkqRA@...&gt; &lt;4D12877D.5030808@...&gt; &lt;4D12E4E8.5010908@...&gt;\r\nIn-Reply-To: &lt;4D12E4E8.5010908@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nSubject: Re: [archive-crawler] Too many open files Exception\r\nX-Yahoo-Group-Post: member; u=264138474; y=LTR1I5SuSbkG6yB1fEbVwUEfD5_skSHE63kYGkXmW23Zyw\r\nX-Yahoo-Profile: kenznag\r\n\r\nHi Gordon,\nAh, sorry for jumping in being ignorant of past conversation!\n\nBut let me say, using low-default open-files limit is not the point of \nmy post. Our crawlers have been doing just fine with that low-limit with \n200-300 threads, running for 20+ days, until we encounter too many open \nfiles error last week. We were not sure if it actually hit the limit for \ngood reason, or some other factors were at play. Before raising the \nlimit, I wanted to know what&#39;s really happening inside. What I found was \nsurge in number of DNS UDP sockets, and ~400 files which I don&#39;t know if \nI have control on. I&#39;m sorry if such information is not useful.\n\nWhile it is easy to raise limit to big number, also it is good to know \nthings are in control. For us, slowing down crawl a bit is much better \nthan getting BDB destroyed!\n\nThanks,\nKenji\n\n(12/22/10 9:58 PM), Gordon Mohr wrote:\n&gt; Based on prior exchanges with Adam, I think his problem is other than\n&gt; hitting the low-default 1024 open-files limit, which I believe he&#39;s\n&gt; already raised. (Notably, my understanding is that his crawl ran for\n&gt; weeks with 300 threads before encountering problems -- though perhaps,\n&gt; something has reverted the limit.) I&#39;ve asked him separately for more\n&gt; information that might narrow down the cause.\n&gt;\n&gt; Anyone doing non-trivial crawling should increase their open-files limit\n&gt; before launching Heritrix, as noted in the older H1-based FAQ...\n&gt;\n&gt; http://crawler.archive.org/faq.html#toomanyopenfiles\n&gt;\n&gt; You&#39;ll find similar advice for many other IO-intensive packages -- for\n&gt; example the Lucene, HBase, Hypertable, and other FAQs have similar entries.\n&gt;\n&gt; We should probably add a more prominent warning in the various H1/H3\n&gt; getting started docs, and (if practical) even do a sanity-check on\n&gt; launch that warns people who haven&#39;t already raised this limit.\n&gt;\n&gt; Historically, most crawling machines at IA have had this limit raised to\n&gt; 32768, though it can harmlessly be much larger as well. Kenji, I&#39;m not\n&gt; sure why your machines would have still had the impractically-small\n&gt; default limit.\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt; On 12/22/10 3:19 PM, Kenji Nagahashi wrote:\n&gt;&gt; Hi Adam,\n&gt;&gt;\n&gt;&gt; Coincidentally, I&#39;ve been investigating the same &quot;too many open files&quot;\n&gt;&gt; error in our crawl projects. Based on what I&#39;ve found so far, it appears\n&gt;&gt; Heritrix hit per-process maximum of open file descriptors (1024 in our\n&gt;&gt; case). Following is what I learned from monitoring 5 hour test crawl\n&gt;&gt; (Heritrix 3) - probably the something similar must be happening in your\n&gt;&gt; crawl project, too.\n&gt;&gt;\n&gt;&gt; Heritrix 3 has ~80 always-open file descriptors for JARs, log files,\n&gt;&gt; ARC/WARCs, BDB and listening sockets.\n&gt;&gt;\n&gt;&gt; Each active crawling thread can open up to 3 file descriptors at one\n&gt;&gt; time for fetching and recording HTTP request/response. Our crawler is\n&gt;&gt; configured with max 200 threads, so there can be up to 600 open file\n&gt;&gt; descriptors at one time, but usually much less than that (max 318 in our\n&gt;&gt; case).\n&gt;&gt;\n&gt;&gt; In addition to total 400 open fds above, there are other 4 kinds of file\n&gt;&gt; descriptors, which reached 400 in total during the test. Now it is\n&gt;&gt; pretty close to per-process limit of 1024.\n&gt;&gt;\n&gt;&gt; One of &quot;additional&quot; open descriptors is UDP socket for DNS query. I saw\n&gt;&gt; max 75 sockets open at one time. If DNS server gets overloaded and slows\n&gt;&gt; down, there could be more.\n&gt;&gt;\n&gt;&gt; Also as crawl job proceed, BDB gets bigger and BDB seems to keep more\n&gt;&gt; database files open.\n&gt;&gt;\n&gt;&gt; Summing up, with following factors combined, Heritrix could run out of\n&gt;&gt; file descriptors and start failing severely:\n&gt;&gt;\n&gt;&gt; - many threads\n&gt;&gt; - shallow crawl on many web sites (such crawl can go very fast, too)\n&gt;&gt; - long-running crawl\n&gt;&gt; - slow DNS queries\n&gt;&gt;\n&gt;&gt; Probably crawling with less threads is the easiest (but not so reliable)\n&gt;&gt; way to prevent this problem, but Heritrix could do a kind of throttling\n&gt;&gt; by limiting the number of concurrent DNS queries.\n&gt;&gt;\n&gt;&gt; Kenji @ Internet Archive\n&gt;&gt;\n&gt;&gt; (12/22/10 4:17 AM), Adam Brokeš wrote:\n&gt;&gt;&gt; [Attachment(s)&lt;#TopText&gt; from =?UTF-8?B?QWRhbSBCcm9rZcWh?= included\n&gt;&gt;&gt; below]\n&gt;&gt;&gt;\n&gt;&gt;&gt; Hallo folks,\n&gt;&gt;&gt;\n&gt;&gt;&gt; I am working on domain crawl and I have encounter few strange\n&gt;&gt;&gt; problems. First of all, the crawl has some rapid slowdown (ten times\n&gt;&gt;&gt; slower). This was caused by managing (or comparing) cookies. Well, we\n&gt;&gt;&gt; have decided that disabling cookies for bulk crawl is not big issue.\n&gt;&gt;&gt; After few days, when 4 TB was harvested, heritrix threw too many open\n&gt;&gt;&gt; files exception. I tried to recover, but all my attempts failed. I\n&gt;&gt;&gt; restarted heritrix, build new job based on the old one. Lower the\n&gt;&gt;&gt; toethreads to 250 and started. This order xml is quite the same as I\n&gt;&gt;&gt; used last year for crawling 9.5 TB. After two and half days with 1TB\n&gt;&gt;&gt; crawled heritrix suddenly stop with exception threw on all threads. I\n&gt;&gt;&gt; have attached the exception in the bottom. The limit on the open files\n&gt;&gt;&gt; in the system is set to more than one million. Frankly, I am not sure\n&gt;&gt;&gt; how to track down this problem. Especially when this setting was\n&gt;&gt;&gt; working last year (probably on 1.14.3).\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thank you very much.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Best regards.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Brokes\n&gt;&gt;&gt;\n&gt;&gt;&gt; HW& SW config\n&gt;&gt;&gt;\n&gt;&gt;&gt; 3485312 KB max heap\n&gt;&gt;&gt; 32bit java6 update22\n&gt;&gt;&gt; heritrix 1.14.4\n&gt;&gt;&gt; 4 core Intel(R) Xeon(R) CPU E5420 @ 2.50GHz\n&gt;&gt;&gt; 8 GiB RAM\n&gt;&gt;&gt; 64bit Debian 5.0.6\n&gt;&gt;&gt;\n&gt;&gt;&gt; Exception\n&gt;&gt;&gt;\n&gt;&gt;&gt; Fatal exception in ToeThread #181: (in thread &#39;ToeThread #181: &#39;)\n&gt;&gt;&gt; Exception:\n&gt;&gt;&gt; com.sleepycat.util.RuntimeExceptionWrapper: (JE 3.3.82) fetchTarget of\n&gt;&gt;&gt; 0x32/0x1ed6a7 parent IN=1266390 IN class=com.sleepycat.je.tree.BIN\n&gt;&gt;&gt; lastFullVersion=0x8b2/0x56a066 parent.getDirty()=false state=0\n&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt; Cause: com.sleepycat.je.DatabaseException: (JE 3.3.82) fetchTarget of\n&gt;&gt;&gt; 0x32/0x1ed6a7 parent IN=1266390 IN class=com.sleepycat.je.tree.BIN\n&gt;&gt;&gt; lastFullVersion=0x8b2/0x56a066 parent.getDirty()=false state=0\n&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt; at com.sleepycat.je.tree.IN.fetchTarget(IN.java:1230)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.je.dbi.CursorImpl.searchAndPosition(CursorImpl.java:2103)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchInternal(Cursor.java:1778)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchAllowPhantoms(Cursor.java:1748)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.search(Cursor.java:1615)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.getSearchKey(Cursor.java:1067)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.doGetSearchKey(RangeCursor.java:965)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.getSearchKey(RangeCursor.java:592)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.DataCursor.doGetSearchKey(DataCursor.java:577)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.DataCursor.getSearchKey(DataCursor.java:559)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.getValue(StoredContainer.java:299)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at com.sleepycat.collections.StoredMap.get(StoredMap.java:227)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.getOrUse(ObjectIdentityBdbCache.java:264)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:217)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:75)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.activateInactiveQueue(WorkQueueFrontier.java:732)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.next(WorkQueueFrontier.java:634)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at org.archive.crawler.framework.ToeThread.run(ToeThread.java:147)\n&gt;&gt;&gt;\n&gt;&gt;&gt; Stacktrace: com.sleepycat.util.RuntimeExceptionWrapper: (JE 3.3.82)\n&gt;&gt;&gt; fetchTarget of 0x32/0x1ed6a7 parent IN=1266390 IN\n&gt;&gt;&gt; class=com.sleepycat.je.tree.BIN lastFullVersion=0x8b2/0x56a066\n&gt;&gt;&gt; parent.getDirty()=false state=0\n&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.convertException(StoredContainer.java:466)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.getValue(StoredContainer.java:306)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at com.sleepycat.collections.StoredMap.get(StoredMap.java:227)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.getOrUse(ObjectIdentityBdbCache.java:264)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:217)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.util.ObjectIdentityBdbCache.get(ObjectIdentityBdbCache.java:75)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.activateInactiveQueue(WorkQueueFrontier.java:732)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.crawler.frontier.WorkQueueFrontier.next(WorkQueueFrontier.java:634)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at org.archive.crawler.framework.ToeThread.run(ToeThread.java:147)\n&gt;&gt;&gt; Caused by: com.sleepycat.je.DatabaseException: (JE 3.3.82) fetchTarget\n&gt;&gt;&gt; of 0x32/0x1ed6a7 parent IN=1266390 IN class=com.sleepycat.je.tree.BIN\n&gt;&gt;&gt; lastFullVersion=0x8b2/0x56a066 parent.getDirty()=false state=0\n&gt;&gt;&gt; com.sleepycat.je.log.LogFileNotFoundException: (JE 3.3.82)\n&gt;&gt;&gt; 0x32/0x1ed6a7 (JE 3.3.82) Couldn&#39;t open file\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb:\n&gt;&gt;&gt; /mnt/pole-c/cz2010/crawl2/state/00000032.jdb (Too many open files)\n&gt;&gt;&gt; at com.sleepycat.je.tree.IN.fetchTarget(IN.java:1230)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.je.dbi.CursorImpl.searchAndPosition(CursorImpl.java:2103)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchInternal(Cursor.java:1778)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.searchAllowPhantoms(Cursor.java:1748)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.search(Cursor.java:1615)\n&gt;&gt;&gt; at com.sleepycat.je.Cursor.getSearchKey(Cursor.java:1067)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.doGetSearchKey(RangeCursor.java:965)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.util.keyrange.RangeCursor.getSearchKey(RangeCursor.java:592)\n&gt;&gt;&gt;\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.DataCursor.doGetSearchKey(DataCursor.java:577)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.DataCursor.getSearchKey(DataCursor.java:559)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; com.sleepycat.collections.StoredContainer.getValue(StoredContainer.java:299)\n&gt;&gt;&gt;\n&gt;&gt;&gt; ... 7 more\n&gt;&gt;&gt; --\n&gt;&gt;&gt; Adam Brokeš\n&gt;&gt;&gt; http://www.brokes.net\n&gt;&gt;&gt; adam.brokes@...&lt;mailto:adam.brokes%40gmail.com&gt;\n&gt;&gt;&gt; 42799740\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; ------------------------------------\n&gt;&gt;\n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n\n\n"}}