{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"UgLp-DSgnsIjc-x5HLUfwx05BKnDeJ7JTl8yLqRYc0QXev4a2S4ENEfZrfL-wcsN1zteGLFaafKrE7iH2NFeedIi1fQbMjk","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re:  A mixed bag of issues, thoughts and suggestions (long)","postDate":"1116979186","msgId":1877,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyOTNCRkYyLjEwOTAyMDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGQ3MDliZCtrbmg1QGVHcm91cHMuY29tPg==","referencesHeader":"PGQ3MDliZCtrbmg1QGVHcm91cHMuY29tPg=="},"prevInTopic":1865,"nextInTopic":1879,"prevInTime":1876,"nextInTime":1878,"topicId":1865,"numMessagesInTopic":4,"msgSnippet":"Lots of good stuff for crawler improvements or further discussion! Comments below... ... I agree that at its current level of accuracy, it causes more","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 93283 invoked from network); 24 May 2005 23:59:51 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m26.grp.scd.yahoo.com with QMQP; 24 May 2005 23:59:51 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta4.grp.scd.yahoo.com with SMTP; 24 May 2005 23:59:51 -0000\r\nReceived: (qmail 13814 invoked by uid 100); 24 May 2005 23:59:47 -0000\r\nReceived: from b116-dyn-239.archive.org (HELO ?207.241.238.239?) (gojomo@...@207.241.238.239)\n  by mail-dev.archive.org with SMTP; 24 May 2005 23:59:47 -0000\r\nMessage-ID: &lt;4293BFF2.1090209@...&gt;\r\nDate: Tue, 24 May 2005 16:59:46 -0700\r\nUser-Agent: Mozilla Thunderbird 1.0.2-1.3.2 (X11/20050324)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;d709bd+knh5@...&gt;\r\nIn-Reply-To: &lt;d709bd+knh5@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-65.9 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re:  A mixed bag of issues, thoughts and suggestions (long)\r\nX-Yahoo-Group-Post: member; u=137285340; y=JDvy2cVR4XVTNP-yG2jfUqRXBdsO_1JpJemlUWJYBPjC\r\nX-Yahoo-Profile: gojomo\r\n\r\nLots of good stuff for crawler improvements or further discussion! Comments\nbelow...\n\n--- In archive-crawler@yahoogroups.com, &quot;Kristinn Sigurdsson&quot; &lt;kris@a...&gt; wrote:\n&gt; Some of this is posted here to get some discussion, other bits are mostly\n&gt; pointed at the boys at the Archive. Basically, this is a mixed bag.\n&gt; \n&gt; Remaining time\n&gt; \n&gt; This feature is of negligible use and is more likely to confuse than inform\n&gt; users. The time estimate is only going to give a reasonable figure if there\n&gt; is only one host left (or several hosts with a similar number of documents)\n&gt; and we&#39;re already done with the &#39;discovery&#39; phase. This may be of use\n&gt; towards the end of some focused crawls, but generally this is of little\n&gt; value. Since this value is almost always wrong I believe that it is far more\n&gt; likely to confuse users. I also find it annoying to have another time\n&gt; counter as I occasionally get it mixed up when I&#39;m looking up the duration\n&gt; of the crawl, but that&#39;s a lesser issue.\n&gt; \n&gt; Unless the estimate can be improved significantly (and this requires taking\n&gt; into account the number of queues and their relative sizes and the\n&gt; politeness restrictions for anything approaching a reasonable estimate) I\n&gt; vote to remove this feature.\n\nI agree that at its current level of accuracy, it causes more confusion\nthan convenience. But I&#39;d very much like to have a &#39;best guess&#39; of time\nremaining.\n\nAs you note, this would require more sophisticated estimate calculations\n\nSome ideas:\n\n  - Track rates of both URI discovery and completion, and the change\n    in these rates over time. Early in any crawl, and for some broad\n    crawls, these rates might imply a crawl that never finishes, or\n    a estimate that&#39;s wildly off. In such cases, an estimate could\n    be withheld, or giant &#39;error bars&#39; indicated (&quot;5 days +/- 1 year&quot;).\n    However, later in the crawl, especially after the rate of completion\n    has exceeded the rate of discovery, this method would likely be\n    fairly accurate, even without considering individual queues.\n\n  - Do the same, per each queue. If all queues are active, the predicted\n    time of finish is the latest predicted time for any single queue.\n    If queues spend time rotating in and out of activity, some other\n    estimate inflator will have to be applied. Fringe benefit: such\n    per-queue estimates could be used to prioritize queues that will\n    cannot afford to be out-of-cycle, Confounding factor: queue &#39;total\n    budgets&#39; or operator intervention will often truncate exactly the\n    largest/longest queues because they&#39;re deep into chaff/junk. But\n    then again: viewing a ranked report of queues based on longest-\n    expected-time-of-finish could be very valuable for the operator,\n    even moreso than raw queue size. (Projected times would take into\n    account historical responsiveness and discovery rates at server.)\n\nSince I&#39;d like to see someone delve deeper into this, even if we don&#39;t\nget a chance to at the IA, I&#39;d like to keep the flawed reading visible,\nproperly disclaimered, as an advertisement for needed work.\n\nSo how about we move it further down in the console and label it &quot;naive\nestimate&quot; or some such?\n\n&gt; The &#39;Crawl resuming&#39; at the top of the progress-statistics.log\n&gt; \n&gt; A change to how the crawls are started has led to a CRAWL RESUMING being\n&gt; printed at the top of the progress-statistics.log. Perhaps the events need\n&gt; to be augmented to include a CRAWL STARTING that the\n&gt; progress-statistics.log\n&gt; would ignore. In any case, this needs to be fixed.\n\nYes, I&#39;ve just made a bug:\n\n  progress-statistics.log shows CRAWL RESUMED at start\n  http://sourceforge.net/tracker/index.php?func=detail&aid=1208186&group_id=73833&atid=539099\n\n&gt; Include actual memory used in the progress-statistics.log instead of or in\n&gt; addition to current heap size.\n&gt; \n&gt; Actual memory used is of much greater interest, especially when looking\n&gt; through the log to see memory usage trends.\n\nI made an RFE for this last week:\n\n  add &#39;memory used&#39; to progress-statistics.log\n  http://sourceforge.net/tracker/index.php?func=detail&aid=1204644&group_id=73833&atid=539102\n\n&gt; Default Bdb cache percent to something modest, like 35-40% (OOM errors)\n&gt; \n&gt; The AR module encountered some issues with the default setting. I&#39;m not\n&gt; going to vote for any one value, but I suggest that crawls should always\n&gt; retain a good chunk of memory after the Bdb cache grows to its maximum\n&gt; allowed size.\n\nI believe the BDB default is 60%. On your machine, how much RAM is available,\nhow much heap do you assign Java, and what cache percent did you settle on\nto resolve your issues?\n\n&gt; Get rid of the recovery journal in favor of decent crash resistance\n&gt; via Bdb \n&gt; \n&gt; This is a bigger issue (and I know it&#39;s something you guys want to\n&gt; do). The\n&gt; recovery journal has never (for me) worked as a crash recovery tool.\n&gt; The AR\n&gt; frontier is able to reconstruct itself based on its databases without any\n&gt; trouble. If done right, resuming a crawl could be done in a matter of\n&gt; seconds, instead of hours (or days). I&#39;d vote to make this the #1 priority\n&gt; for 1.6!\n\nIt is a top priority. The recovery journal will stay around for a while as\nan optional backup/debugging aid/adjunct to checkpoint-based recoveries.\n\n&gt; Allow frontiers to provide &#39;servlets&#39; that enables more detailed\n&gt; monitoring\n&gt; and control.\n&gt; \n&gt; I don&#39;t really care how it&#39;s done, but we need to allow modules,\n&gt; especially\n&gt; (or even exclusively) Frontiers, need to have customized control pages!\n\nAgreed. Perhaps these should be packagable as self-contained WARs that\ncan be dropped into the current or future crawler UIs at specific paths.\n\n&gt; Display data, 4GB should be 4.442GB etc.\n&gt; \n&gt; Need to improve how data amounts are condensed, currently the jump from\n&gt; 4095MB to 4GB lacks granularity. I&#39;d be happy to fix this if we could\n&gt; agree\n&gt; on exactly how things are supposed to be. My suggestion: show four\n&gt; characters. So we&#39;d go from 4095 MB to 4GB and then 4.001GB etc. when\n&gt; we hit\n&gt; 40GB we go to 40.01 etc. same for the move from B to KB and KB to MB\n&gt; \n&gt; Additionally, we could have a mouse-over pop up the exact number of bytes.\n\nMy preference:\n  - always at least 3 significant figures, up to 4 when significand is an integer\n  - use largest unit that results in a leading digit &gt;= 1\n\nExamples:\n\n  4.23K\n  1023K\n  1.00M\n  57.3M\n  789M\n  1001M\n  1.10G\n  27.2G\n  989G\n  1019G\n  1.27P\n  etc.\n\nWe should either be pedantic and use &#39;KiB&#39;, &#39;MiB&#39;, &#39;GiB&#39;, &#39;PiB&#39; -- or strategically\nvague like Gnu tools and use just &#39;K&#39;, &#39;M&#39;, &#39;G&#39;, &#39;P&#39;.\n\nA mouseover in web readouts with exact byte counts would be a nice touch.\n\n&gt; Default log lines to show be 40 instead of 50\n&gt; \n&gt; A bit of a nit-pick. I&#39;ve got a screen running at 1024 lines of vertical\n&gt; resolution and 40 lines fill up my screen. Since not many users are likely\n&gt; to have higher resolution (in fact I&#39;d expect the average to be lower) I\n&gt; suggest we amend the default value to 40 (or possibly 35).\n\nI would much prefer it to be significantly larger; I don&#39;t mind scrolling\nto get more info, compared to forcing a reload with new paramters.\n\nIf the problem is that the last few lines are of most interest in &#39;tail&#39;\nviews, we could:\n  - target an #anchor near the bottom\n  - make the default different for &#39;tail&#39; views than in others, where\n    quantity of info in one fetch is more interesting\n  - (most radical idea) adopt bloggish display: most recent on top,\n    even though raw file on disk is in the opposite order\n\n&gt; Link from help to /help/regexpr.jsp\n&gt; \n&gt; I find this page invaluable when adding new regular expressions, especially\n&gt; to crawls in progress. Lets you double check those reg.expr. using the same\n&gt; interpreter. The page has been available for awhile, but either the link was\n&gt; dropped or it never existed. Either way, a link should be added to the\n&gt; help page.\n\nGreat idea, please add at your leisure!\n\n&gt; Cost assignment on -5000/-5001\n&gt; \n&gt; More of a question, do URIs that return with -500X &#39;cost&#39; anything? I&#39;d\n&gt; suggest that they shouldn&#39;t. After all, they aren&#39;t actually crawled. \n\nThis would require a refund of cost deducted, since costs are currently\napplied at the moment an URL comes off its queue. Could you file a bug\nwith an example of how this current behavior has caused a problem or\ninconvenience in a crawl?\n\n&gt; HostnameQueueAssignmentPolicy attaches the port number to the hostname\n&gt; \n&gt; Is this really sensible considering that these are used as the basic\n&gt; politeness units. Different ports, same server. I&#39;d think we&#39;d want to\n&gt; handle it all the same way.\n\nOn the other hand, different ports/same host could quite possibly be a\ndifferent &#39;server&#39; in the sense of an independent process whose reachability\nis not correlated with other ports on the same machine. Essentially, a\nqueue blocks until the top item on it succeeds or has exceeded its\nretries. What if that item is failing repeatedly against a (no-longer-running)\nserver on a nonstandard port, while other URLs that would succeed against\nthe standard port are stuck behind it?\n\nWe could offer both kinds of queue-assignment policy. I think the\nper-port distinction may have been important in a previous Frontier\nbut beside the &#39;stuck behind&#39; problem I just mentioned, all URLs on\nthe same hostname could be coalesced in the BdbFrontier.\n\n&gt; Daily reports? Detailed look at the crawl, printed to a file daily? \n&gt; \n&gt; I was thinking that it might be nice to add a &#39;Daily report&#39; feature\n&gt; to the\n&gt; StatisticsTracker. It would generate a (new) report file at a\n&gt; specified time\n&gt; each day. Information regarding the number of URIs, amount of data,\n&gt; bandwidth usage etc. would be printed, giving a nice summary on the\n&gt; progress\n&gt; so far.\n\nVery good idea; it would also be natural to mail this report somewhere.\nFile an RFE?\n\n&gt; URI reg.expr filter (OR based) that has a list of strings instead of just\n&gt; one.\n&gt; \n&gt; I find myself using a series of URI reg.expr. filters to tackle numerous\n&gt; issues. I was thinking that it might be useful to add a similar filter\n&gt; that\n&gt; contained a list of strings. This would make it a lot easier to add\n&gt; additional filters while a crawl is in progress. I can handle this if you\n&gt; agree it&#39;s a good idea.\n\nGood idea; perhaps make RFE and assign to yourself? For anywhere a Filter\nwould be considered, a DecideRule (or pair, for the &#39;matches&#39; and &#39;not-matches&#39;\nsenses) makes sense too now.\n\n&gt; In help (and perhaps a link from the logs) provide a quick link to the\n&gt; result codes (local, not on web).\n&gt; \n&gt; I find myself looking these up a lot. Having a handy, local reference\n&gt; would\n&gt; be useful. Should include both the HTTP codes and the ones specified by\n&gt; Heritrix.\n\nDefinitely, please add at your convenience.\n\n&gt; Changes to Meta-data, description do not have any effect.\n&gt; \n&gt; This has been around for awhile I think. The only time this is written is\n&gt; when a profile/job is created. Changes seem to have no effect.\n\nI&#39;ve filed a bug:\n\n  changes to metadata description (&more?) have no effect\n  http://sourceforge.net/tracker/index.php?func=detail&aid=1208203&group_id=73833&atid=539099\n\n&gt; Add items crawled to Bdb reports for each queue.\n&gt; \n&gt; The BdbFrontier report should list the number or URIs crawled for each\n&gt; queue. Currently you can sort of calculate this based on the\n&gt; expenditure and\n&gt; average cost (which is of little use of you are using\n&gt; ZeroCostAssignmentPolicy).\n\nGood idea. I think that also applies to CrawlServers. I&#39;ve filed an RFE:\n\n  completed (and other stats) per queue, per crawlserver\n  http://sourceforge.net/tracker/index.php?func=detail&aid=1208205&group_id=73833&atid=539102\n\n&gt; BdbFrontier: Add a log that details when queues are created and move from\n&gt; active to inactive and finally to exhausted.\n&gt; \n&gt; This would make it easier to monitor crawls. Basically when a queue moves\n&gt; between any of the above states a line should be written in the log. This\n&gt; way you could quickly grep through the log and see how that queue is being\n&gt; processed. \n&gt; \n&gt; The log line might look something like this:\n&gt; \n&gt; [time-date] [queue-name] [old state] [new state] [queue-size] [URIs\n&gt; crawled]\n&gt; [total expended] [last cost] [average cost]\n\nGood idea. I filed an RFE with your suggestion:\n\n  WorkQueueFrontier - add log of queue lifecycle\n  http://sourceforge.net/tracker/index.php?func=detail&aid=1208206&group_id=73833&atid=539102\n\n- Gordon @ IA\n\n"}}