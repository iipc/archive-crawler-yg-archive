{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"lB8wi1xUeScBmaBM_WKqBu8kpR2DPhqthAAcS9iucEmWJf3YAV1dVOuM_5abdaCEIW-QIFyiSGbvrgsEr0UgQ5qACWQkNHQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: [Fwd: Second draft of per host settings document]","postDate":"1071098939","msgId":197,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNGRDdBQzNCLjIwNjAyMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDNGRDc4OEJELjYwNzA2MDFAYXJjaGl2ZS5vcmc+","referencesHeader":"PDNGRDc4OEJELjYwNzA2MDFAYXJjaGl2ZS5vcmc+"},"prevInTopic":196,"nextInTopic":198,"prevInTime":196,"nextInTime":198,"topicId":195,"numMessagesInTopic":5,"msgSnippet":"... Originally, the the order file did not change: it was only editted (by hand or the web UI) before the crawl began, and then consulted at the start of a","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 55080 invoked from network); 10 Dec 2003 23:29:05 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m14.grp.scd.yahoo.com with QMQP; 10 Dec 2003 23:29:05 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 10 Dec 2003 23:29:05 -0000\r\nReceived: (qmail 10314 invoked by uid 100); 10 Dec 2003 23:28:20 -0000\r\nReceived: from b116-dyn-43.archive.org (HELO archive.org) (gojomo@...@209.237.240.43)\n  by ia00524.archive.org with SMTP; 10 Dec 2003 23:28:20 -0000\r\nMessage-ID: &lt;3FD7AC3B.2060208@...&gt;\r\nDate: Wed, 10 Dec 2003 15:28:59 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nSubject: Re: [archive-crawler] Re: [Fwd: Second draft of per host settings\n document]\r\nReferences: &lt;3FD788BD.6070601@...&gt;\r\nIn-Reply-To: &lt;3FD788BD.6070601@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-5.9 required=6.0\n\ttests=AWL,BAYES_01,EMAIL_ATTRIBUTION,IN_REP_TO,QUOTED_EMAIL_TEXT,\n\t      REFERENCES,REPLY_WITH_QUOTES,USER_AGENT_MOZILLA_UA\n\tautolearn=ham version=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nMichael Stack wrote:\n&gt; Proposal looks good to me.  Model reminds of me of apache &#39;.htaccess&#39; \n&gt; file scheme where you can insert directory specific config. to override \n&gt; the core httpd.conf.  I like it.  Below are some general \n&gt; comments/questions:\n&gt; \n&gt; + Does the order file change as you do a crawl?  Where is state kept?  \n\nOriginally, the the order file did not change: it was only editted\n(by hand or the web UI) before the crawl began, and then consulted at\nthe start of a crawl to fill in per-instance variables holding chosen\nvalues.\n\nRecently, Kris made several changes:\n  (1) Most places where order settings are used, the order object (or\n      portion thereof) is directly consulted each time the value is\n      needed -- rather than only at startup -- so that changes during\n      a crawl can have an effect.\n  (2) The crawl can be paused in memory, and various crawl-order fields\n      changed during the pause. The updated order is written to disk,\n      and the new values should affect the crawl when it is resumed.\n\n(I&#39;m not sure everything behaves as might be desirable for all the\ncrawler objects which inherit from XMLConfig, and retain direct\nreferences to their &#39;home node&#39; inside the overall order DOM.)\n\nThe state of the running crawl is in memory, and reflected in the various\nlogs, but never yet captured as a consistent/resumable whole to disk.\n\n&gt; What if you want to tweak a setting during a crawl.  Where would that be \n&gt; done?  Would you change the order file or would you instead change UI \n&gt; and its value would be save to a state file subsequently used crawling?  \n\nCurrently, tweaks can only happen via the web UI, while the crawl is\npaused in memory. We could conceivably allow hand editting of\nthe on-disk files -- and then somehow signal the running crawler\nto revisit them.\n\n&gt; Would such a change be one of the &#39;dynamic settings&#39; from the global \n&gt; configuration file saved at the head of the configuration dir \n&gt; (&#39;overrides&#39; for the order file) mentioned in your doc.?\n\nI think most such adjustments to global values would be reflected\nin the global config file, under John&#39;s proposal.\n\n&gt; + Why can&#39;t I set any value?  What if during a crawl I want to up the \n&gt; number of running threads or change the logging levels or the way in \n&gt; which statistics are being gathered (e.g. move from &#39;pedestrian&#39; crawl \n&gt; to debug mode) or even instantiate new processors and change the order \n&gt; in which the processor chain works.  Some settings won&#39;t be changeable \n&gt; mid-crawl and these might fail w/ a polite message (e..g. changing heap \n&gt; size mid-run) but otherwise, I&#39;d suggest no constraint on what can be \n&gt; changed in configuration (Keep in mind I don&#39;t know much about crawler \n&gt; deploys).\n\nChanging some values mid-crawl may imply consequences that are hard\nto define or implement. For example, does changing the paths for\ncrawler output mid-crawl mean the crawler should copy its existing\ndata to the new location? Or simply write new data to the new location?\nAnd if parts of the UI need to consult the full data set to operate,\nmust it remember all old paths?\n\nThus I think the decision on what settings are dynamically changeable\nmust be made on a case-by-case basis. Outside of the (large) class\nof settings which affect individual CrawlURI processing, I expect\nthe default to be &quot;not changeable until need is demonstrated&quot; -- then\nwe can consider the particulars of the situation.\n\n&gt; + I&#39;d imagine in the scheme of things, the reading of a new \n&gt; configuration file into memory, whether per host or per domain, a rare \n&gt; event.  Do you agree?  At what time are the domain/host configuration \n&gt; files read?  On initially crossing into a new domain or on first seeing \n&gt; a host?  Will we ever refresh what we have in mem?\n\nClearly per-host (or per-domain) settings must be read when any URI\nwithin that category is being processed in a way that could be affected\nby those settings. But such per-host or per-domain info will also often\nbe flushed from memory when no such URIs are being processed, and then\nre-read as necessary. So depending on the breadth and scheduling policy\nof a crawl, such files might be read only once or many times.\n\nI suspect only changes of settings through the web UI, or some other\nmanual kick/flush of existing settings, would cause a refresh of\nsettings held in memory.\n\n&gt; + What are the plans for having multiple crawler instances working off \n&gt; the one Frontier instance?  If the frontier makes a CrawlServer to \n&gt; associate w/ a URI, how will we ensure that one crawler instance does \n&gt; one server only (Maybe frontier can do distribution between crawlers).\n\nNot yet considered in depth. It&#39;s quite likely that when independent\ncrawlers are cooperating, they&#39;ll each have a firm idea of which\nremote servers (host:port combos) they are to crawl, and which they\nshould delegate to their siblings.\n\n&gt; + Won&#39;t there only be one instance of the configuration instance in \n&gt; memory? It won&#39;t be duplicated per CrawlerServer -- just a reference to \n&gt; the single instance?  Why then are we worried about XML DOM cost?  Won&#39;t \n&gt; the configuration just be read into an object tree w/ an xml serializer \n&gt; used creating the objects w/ periodic visits to the file on disk to \n&gt; check mod time?  The serializer would do something like, if a value \n&gt; exists at a certain node in the configuration hierarchy, a getter that \n&gt; returns the value at that location in the hierarchy is returned, else, \n&gt; we call super.  Or are we talking of doing per host an aggregation of \n&gt; all xml snippets to write a new configuration file to feed the crawler \n&gt; accessing a particular host?\n\nIf many domains/hosts/servers have their own custom settings, and\neach settings file is XML, and that XML is always loaded into a\ngeneric DOM and then referenced by the CrawlServer instance, the DOM\noverhead might be a concern.\n\n&gt; + Author, date and section numbering make it easier to refer to a \n&gt; document and its sections.\n\nGood point!\n\n&gt; + Later we might use an ldap server implementation behind your \n&gt; &#39;Configuration&#39; reader class to store config.  The hierarchical nature \n&gt; of the configuration w/ a rare write would make it a natural match.\n\nOr, other relational DBs.\n\n&gt; Other comments:\n&gt; \n&gt; + Is CrawlServer class to do w/ crawl accessing a particular server?  \n&gt; Maybe rename it Server?\n\nYes, CrawlServer represents one remote server (host:port) that should\nbe treated as a unit for various reasons during crawling. Like with\n&quot;CrawlURI&quot;, &quot;Crawl&quot; is prepended to help distinguish the particular\nwork-unit here from other senses/versions of the word.\n\n&gt; + I have comments on the xml -- no schema nor dtd and extensive use of \n&gt; element attributes -- but they can go elsewhere.\n\nThe motivation for extensive use of attributes was overall compactness of\nrepresentation and ease of access using short, unique XPaths. We could\nconsider other better or more common XML syntax practices; I look forward\nto additional comments.\n\n- Gordon\n\n\n"}}