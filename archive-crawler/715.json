{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"JksYGhuSdYiSJDUTc2VSRmvGhTNoEtYyps9D7c7RWwQBBzZ74dvSxwJSEWlGLATeNXlaYQHCa4VqXNEG-6QHX6JeTY4JhuwY","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Limit crawling to Markup?","postDate":"1090950605","msgId":715,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxMDY5NUNELjYwNTA1MDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGNlNjJkaitkajN1QGVHcm91cHMuY29tPg==","referencesHeader":"PGNlNjJkaitkajN1QGVHcm91cHMuY29tPg=="},"prevInTopic":714,"nextInTopic":716,"prevInTime":714,"nextInTime":716,"topicId":705,"numMessagesInTopic":11,"msgSnippet":"Hi Rob, If you think that filtering out URLs that don t look like html pages is good enough for your needs than you can just simple add URIRegExp exclude","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 8691 invoked from network); 27 Jul 2004 17:52:45 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m2.grp.scd.yahoo.com with QMQP; 27 Jul 2004 17:52:45 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta5.grp.scd.yahoo.com with SMTP; 27 Jul 2004 17:52:45 -0000\r\nReceived: (qmail 30583 invoked by uid 100); 27 Jul 2004 17:42:17 -0000\r\nReceived: from pauk.archive.org (HELO archive.org) (igor@...@207.241.238.153)\n  by mail-dev.archive.org with SMTP; 27 Jul 2004 17:42:17 -0000\r\nMessage-ID: &lt;410695CD.6050502@...&gt;\r\nDate: Tue, 27 Jul 2004 10:50:05 -0700\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ce62dj+dj3u@...&gt;\r\nIn-Reply-To: &lt;ce62dj+dj3u@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.0 required=7.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Re: Limit crawling to Markup?\r\nX-Yahoo-Group-Post: member; u=137477665\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Rob,\n\nIf you think that filtering out URLs that don&#39;t look like html pages is good enough for your needs \nthan you can just simple add URIRegExp exclude filter:\n^.*(?i)(&#92;&#92;.(bmp|gif|jpe?g|png|tiff|mid|mp2|mp3|mp4|wav|avi|mov|mpeg|ram|rm|smil|wmv|doc|pdf|ppt|swf)$\n\nKeep in mind that are many URLs out there that are not html pages and don&#39;t end with any generic \nsuffix. For example:\nhttp://multi1.rmuk.co.uk/RealMedia/ads/adstream_nx.ads/www.pm.gov.uk/homepage@Top!Top\nis an image. Without examining HTTP Content-Type header it would be extremely hard to write any \ngeneric URL based filters to exclude these non-html pages.\n\nTake care,\ni.\n\n&gt; I guess I&#39;m somewhat confused because I was thinking that the\n&gt; additionalScopeFocus filter could be used for something like this,\n&gt; basically by having it return false instead of true, which I was\n&gt; thinking would cause the URI to be left out of the scope.  Admittedly\n&gt; my initial efforts at leaving out images, audio, etc. this way haven&#39;t\n&gt; worked, but I hadn&#39;t gotten around to asking about it here.  So, now\n&gt; I&#39;m asking.  \n&gt; \n&gt; Thanks,\n&gt; Rob Eger\n&gt; Aptas, Inc.\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; \n&gt;&gt;Ahnu Nahki wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;Is there a way to only have heritrix downloaded all text/html markup\n&gt;&gt;&gt;and disregard altogether images and other binaries. I notice that this\n&gt;&gt;&gt;eats up alot of time on our crawls.\n&gt;&gt;&gt;\n&gt;&gt;&gt;Thanks,\n&gt;&gt;&gt;Ahnu\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;This topic has been discussed variously here on the list.  You might \n&gt;&gt;check the archive.\n&gt;&gt;\n&gt;&gt;In short, not without doing some work.  Lesser work would be a filter \n&gt;&gt;that skipped non-text/html content after download.  More work would\n&gt; \n&gt; be a \n&gt; \n&gt;&gt;check that interrupted crawls mid-download if the type was other than \n&gt;&gt;what you wanted. \n&gt;&gt;\n&gt;&gt;Though sub-optimal, you could try and write a regex that only excludes \n&gt;&gt;resources with URIs that match a particular pattern.\n&gt;&gt;\n&gt;&gt;St.Ack\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n\n\n"}}