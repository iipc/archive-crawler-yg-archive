{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"ya3pdLGK7_HnhwE2uUbk1Gi5wKp0w7uGJGh2SMyF94BzvAAB6zsoPBxQP9EFm66ydscwwczheviMhZutA0T10XAkyhh87ew","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Re: poor performance","postDate":"1245349342","msgId":5894,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBM0E4NURFLjMwNzA2MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGgxZDZwZitmZThnQGVHcm91cHMuY29tPg==","referencesHeader":"PGgxZDZwZitmZThnQGVHcm91cHMuY29tPg=="},"prevInTopic":5892,"nextInTopic":5898,"prevInTime":5893,"nextInTime":5895,"topicId":5890,"numMessagesInTopic":7,"msgSnippet":"... Aha -- that some of the domains could be unresponsive (registered but not running an HTTP server) could be the real culprit. A normal fetchable URI can be","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 1631 invoked from network); 18 Jun 2009 18:22:47 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m4.grp.sp2.yahoo.com with QMQP; 18 Jun 2009 18:22:47 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta1.grp.sp2.yahoo.com with SMTP; 18 Jun 2009 18:22:47 -0000\r\nX-Received: (qmail 93461 invoked from network); 18 Jun 2009 18:22:23 -0000\r\nX-Received: from 70.137.144.169 (HELO ?10.0.10.194?) (70.137.144.169)\n  by relay00.pair.com with SMTP; 18 Jun 2009 18:22:23 -0000\r\nX-pair-Authenticated: 70.137.144.169\r\nMessage-ID: &lt;4A3A85DE.3070605@...&gt;\r\nDate: Thu, 18 Jun 2009 11:22:22 -0700\r\nUser-Agent: Thunderbird 2.0.0.21 (Windows/20090302)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;h1d6pf+fe8g@...&gt;\r\nIn-Reply-To: &lt;h1d6pf+fe8g@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: poor performance\r\nX-Yahoo-Group-Post: member; u=137285340; y=kW_FWPXfgDgR3wVHO1vYkAgy7_vQjO507Jii_OEPrHvo\r\nX-Yahoo-Profile: gojomo\r\n\r\nnukleonrus wrote:\n&gt;&gt; How many seeds are used to start your crawl, and what does the summary \n&gt;&gt; &quot;QUEUES&quot; section of the &quot;frontier report&quot; show (with totals of \n&gt;&gt; active/inactive/retired/exhausted queues) during the slow periods?\n&gt; \n&gt; seeds.txt contains about 22 000 domains, some of them could be unregistered\n\nAha -- that some of the domains could be unresponsive (registered but \nnot running an HTTP server) could be the real culprit.\n\nA normal fetchable URI can be processed in a matter of milliseconds or \nseconds. With a URI that has a resolvable domain but no listening \nserver, a much longer timeout/retry process occurs, which can tie up \nthreads for long periods. If a large proportion of target domains are \nlike this, soon most threads can be stuck waiting for timeouts, rather \nthan collecting content.\n\nSpecifically, what happens is:\n  - the GET is tried, but times out after FetchHTTP&#39;s &#39;sotimeout-ms&#39; \nmilliseconds (default: 20 seconds)\n  - our HTTP library immediately retries 10 times, as given by \nHeritrixHttpMethodRetryHandler (so now: 200 seconds to completely fail)\n  - on the theory such failures are often due to transient server or \nnetwork issues, the queue is then snoozed for a long \n&#39;retry-delay-seconds&#39; period (default: 900 seconds/15 minutes) before \nbeing tried again. (During this time, it takes up no threads.)\n  - it will be retried up to the &#39;max-retries&#39; number of tries (default: \n30). So 30 times over the course of the crawl, a thread will be tied up \nfor 200 seconds rather than the usual &lt;1 second to several seconds.\n  - after the max-retries is exhausted, the URI is considered a failure \n(and so logged in the crawl-log\n\nA similar problem can occur if the remote site answers the request, but \nonly trickles out content in response; FetchHTTP will wait up to \n&#39;timeout-seconds&#39; (default: 1200 seconds/20 minutes) as long as some \ncontent is being received.\n\n(If the domain does not resolve at all, there&#39;s very little problem: the \nDNS lookup fails immediately, and while the URI/queue is set for a retry \nin 15 minutes, threads are never held up on socket timeouts.)\n\nIf your interest is in failing-fast for maximum throughput, you could \neither:\n\n- prescan the seed list to remove completely unresponsive domains; or\n\n- change a number of the default timeouts and retry parameters to tie up \nHeritrix threads less in the presence of many unresponsive sites. For \nexample, set &#39;sotimeout-ms&#39; to just a few seconds; set \n&#39;retry-delay-seconds&#39; to just a few minutes; set &#39;max-retries&#39; to \nsomething much less than 30 (but always more than 3 or 4, which Heritrix \nneeds for some of its internal retry procedures).\n\nOther speed tips:\n  - you can easily use 200 toethreads, maybe more, on a dual opteron \nwith more than 1.5GB of heap\n  - Java 1.6 seems to offer a small but noticeable speedup over 1.5\n  - disk IO is likely to become the limiting factor once these other \nissues are resolved; spreading the key job directories \n(arcs/logs/state/scratch) over independent disks is one way to help a bit\n\n&gt; we are crawling on a one server for now, we would like to set up a cluster of 10-20 servers[what has been done with hcc? links are down]\n\nHCC only provides a single contact point for fanning out control \noperations to multiple crawlers and is unlikely to be supported past \n1.X; it doesn&#39;t help distribute a workload (in either initial \nconfiguration or during a run).\n\nWhen we&#39;ve set up crawls using 2-8 machines, the split and monitoring \nhave been manual; there are previous threads on this list about \ntechniques we and others have used.\n\n- Gordon @ IA\n\n&gt; here is the summary[55 Toethreads, 130KB/s average, crawl has been running for 27 minutes]\n&gt; \n&gt; Frontier\n&gt; 23681 URI queues: 831 active (105 in-process; 499 ready; 227 snoozed); 22638 inactive; 0 ineligible; 0 retired; 212 exhausted [RUN: 0 in, 50 out]\n&gt; Threads\n&gt; 55 threads: 55 ABOUT_TO_BEGIN_PROCESSOR; 46 HTTP, 9 Archiver \n&gt; \n&gt; \n&gt; surprisingly, there are lots of http threads now, but speed remains low\n&gt; \n&gt;\n\n"}}