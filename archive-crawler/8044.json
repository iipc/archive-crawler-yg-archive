{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":496713521,"authorName":"","from":"phillip.lord@...","profile":"philuniquenamelord","replyTo":"LIST","senderId":"wztSlp02oKufO-LXx7PaLSE98KBkwAKkoBq2SALrcZmfXW3rq7xPLOt2L7VXdkd0y-KsZEUIkkfA9al4WmpkUvKJqg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] crawl limits","postDate":"1367485717","msgId":8044,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDg3azNuaGczNTYuZnNmQG5ld2Nhc3RsZS5hYy51az4=","inReplyToHeader":"PENBRlI3Mkg9cEIycEdBPUFFK25YVUcrUFJIMDZobW5hZjB3bXR1ZWg2Vys4VUI0c1pLUUBtYWlsLmdtYWlsLmNvbT4JKEp1YW4gTWFudWVsIENhaWNlZG8gQ2FydmFqYWwncyBtZXNzYWdlIG9mICJXZWQsIDEgTWF5IDIwMTMgMTk6MzI6MzcJLTA0MDAiKQ==","referencesHeader":"PDg3aGFqNHFkNjguZnNmQHplcmczMi5uY2wuYWMudWs+CTxDQUZSNzJIPXBCMnBHQT1BRStuWFVHK1BSSDA2aG1uYWYwd210dWVoNlcrOFVCNHNaS1FAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":8043,"nextInTopic":0,"prevInTime":8043,"nextInTime":8045,"topicId":8013,"numMessagesInTopic":3,"msgSnippet":"Ah, this is really useful. Using squid would also solve another problem we have; I would like to accept or deny URLs based (partly) on their content. We have","rawEmail":"Return-Path: &lt;phillip.lord@...&gt;\r\nX-Sender: phillip.lord@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 95583 invoked by uid 102); 2 May 2013 09:08:49 -0000\r\nX-Received: from unknown (HELO mta1.grp.bf1.yahoo.com) (10.193.84.135)\n  by m10.grp.bf1.yahoo.com with SMTP; 2 May 2013 09:08:49 -0000\r\nX-Received: (qmail 7113 invoked from network); 2 May 2013 09:08:48 -0000\r\nX-Received: from unknown (HELO cheviot22.ncl.ac.uk) (128.240.234.22)\n  by mta1.grp.bf1.yahoo.com with SMTP; 2 May 2013 09:08:48 -0000\r\nX-Received: from smtpauth-vm.ncl.ac.uk ([10.8.233.129])\n\tby cheviot22.ncl.ac.uk with esmtp (Exim 4.63)\n\t(envelope-from &lt;phillip.lord@...&gt;)\n\tid 1UXpVP-0005RL-FN\n\tfor archive-crawler@yahoogroups.com; Thu, 02 May 2013 10:08:47 +0100\r\nX-Received: from localhost (cpc10-benw10-2-0-cust194.16-2.cable.virginmedia.com [77.98.241.195])\n\t(authenticated bits=0)\n\tby smtpauth-vm.ncl.ac.uk (8.13.8/8.13.8) with ESMTP id r4298kAL025292\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 2 May 2013 10:08:47 +0100\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nIn-Reply-To: &lt;CAFR72H=pB2pGA=AE+nXUG+PRH06hmnaf0wmtueh6W+8UB4sZKQ@...&gt;\n\t(Juan Manuel Caicedo Carvajal&#39;s message of &quot;Wed, 1 May 2013 19:32:37\n\t-0400&quot;)\r\nDate: Thu, 02 May 2013 10:08:37 +0100\r\nMessage-ID: &lt;87k3nhg356.fsf@...&gt;\r\nReferences: &lt;87haj4qd68.fsf@...&gt;\n\t&lt;CAFR72H=pB2pGA=AE+nXUG+PRH06hmnaf0wmtueh6W+8UB4sZKQ@...&gt;\r\nUser-Agent: Gnus/5.13 (Gnus v5.13) Emacs/24.3 (gnu/linux)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nX-eGroups-From: phillip.lord@... (Phillip Lord)\r\nFrom: phillip.lord@...\r\nSubject: Re: [archive-crawler] crawl limits\r\nX-Yahoo-Group-Post: member; u=496713521; y=BnJCeid8PfoJvRBHCQuxG7Ek7Fwy0V0-QpXlWMmp2UkdOS4YsymdzHyOExqsfdtatyAhOGCoXbNlIyw\r\nX-Yahoo-Profile: philuniquenamelord\r\n\r\n\nAh, this is really useful. Using squid would also solve another problem\nwe have; I would like to accept or deny URLs based (partly) on their\ncontent. We have another tool (http://greycite.knowledgeblog.org) which\nmakes the decision as to whether the content is acceptable; obviously,\nthough, we don&#39;t want to fetch the content twice. Pushing both heritrix\nour tool behind a single proxy should solve that problem. They can\noperate independently of each other, but in actuality will only be\nretrieving data once.\n\nMany thanks!\n\nPhil\n\nJuan Manuel Caicedo Carvajal &lt;jcaicedo@...&gt; writes:\n&gt; We used Squid as a proxy for Heritrix. Squid has several options to control\n&gt; the bandwidth usage through a feature called &#39;dealy pools&#39;.\n&gt; You can have more information about it here:\n&gt;\n&gt; http://www.tldp.org/HOWTO/Bandwidth-Limiting-HOWTO/install.html\n&gt;\n&gt; And here&#39;s a quick description of how we use it to limit the bandwidth\n&gt; during certain hours of the day:\n&gt;\n&gt; https://github.com/lemurproject/clueweb12pp/wiki/squid\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; On Thu, Apr 18, 2013 at 7:41 AM, &lt;phillip.lord@...&gt; wrote:\n&gt;\n&gt;&gt; **\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Is it possible to limit the number of outgoing calls that Heritrix in\n&gt;&gt; terms of either number per second, or bandwidth. I can see how to do\n&gt;&gt; this per host, but not for the whole crawl.\n&gt;&gt;\n&gt;&gt; Phil\n&gt;&gt;  \n&gt;&gt;\n\n-- \nPhillip Lord,                           Phone: +44 (0) 191 222 7827\nLecturer in Bioinformatics,             Email: phillip.lord@...\nSchool of Computing Science,            http://homepages.cs.ncl.ac.uk/phillip.lord\nRoom 914 Claremont Tower,               skype: russet_apples\nNewcastle University,                   twitter: phillord\nNE1 7RU                                 \n\n"}}