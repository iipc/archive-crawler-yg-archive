{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"5A7D5NY9HefDmFbHwEArsMxwR3Vw9XtPZUywi2TofKlF-ja1mV3KsvTPxYbo5kQYaIcH0aKMJ1f04huHrQbYnjTlWNg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Crawler running with less than configured threads.","postDate":"1326916914","msgId":7533,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMTcyNTMyLjEwNzAxMDlAY3MuY211LmVkdT4=","inReplyToHeader":"PDRGMEI3MzlFLjIwOTA3MDVAYXJjaGl2ZS5vcmc+","referencesHeader":"PDRGMDVFOUFELjkwMTA5MDNAY3MuY211LmVkdT4gPDRGMDZCQUZCLjgwMTA0MDJAYXJjaGl2ZS5vcmc+IDw0RjA3MjM5My4yMDYwNzAwQGNzLmNtdS5lZHU+IDw0RjA5RjU0OC40MDQwNzA1QGFyY2hpdmUub3JnPiA8NEYwQjIwQjkuODA1MDQwMUBjcy5jbXUuZWR1PiA8NEYwQjczOUUuMjA5MDcwNUBhcmNoaXZlLm9yZz4="},"prevInTopic":7503,"nextInTopic":7542,"prevInTime":7532,"nextInTime":7534,"topicId":7493,"numMessagesInTopic":14,"msgSnippet":"Gordon, A couple questions about using the frontier.recover.gz file. 1) In the full recover process, do I need to send all of the frontier.recover.gz.cp00*","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 8647 invoked from network); 18 Jan 2012 20:01:59 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m13.grp.sp2.yahoo.com with QMQP; 18 Jan 2012 20:01:59 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.95)\n  by mta1.grp.sp2.yahoo.com with SMTP; 18 Jan 2012 20:01:59 -0000\r\nX-Received: from [128.2.209.200] (SAVOY.LTI.CS.CMU.EDU [128.2.209.200])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id q0IK1sDJ014974\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tWed, 18 Jan 2012 15:01:54 -0500\r\nMessage-ID: &lt;4F172532.1070109@...&gt;\r\nDate: Wed, 18 Jan 2012 15:01:54 -0500\r\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: Gordon Mohr &lt;gojomo@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;4F05E9AD.9010903@...&gt; &lt;4F06BAFB.8010402@...&gt; &lt;4F072393.2060700@...&gt; &lt;4F09F548.4040705@...&gt; &lt;4F0B20B9.8050401@...&gt; &lt;4F0B739E.2090705@...&gt;\r\nIn-Reply-To: &lt;4F0B739E.2090705@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2011.5.19.222118\r\nX-SMTP-Spam-Clean: 8% (\n BODYTEXTP_SIZE_3000_LESS 0, BODY_SIZE_2000_2999 0, BODY_SIZE_5000_LESS 0, BODY_SIZE_7000_LESS 0, DATE_TZ_NEG_0500 0, __ANY_URI 0, __BOUNCE_CHALLENGE_SUBJ 0, __BOUNCE_NDR_SUBJ_EXEMPT 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __SANE_MSGID 0, __TO_MALFORMED_2 0, __URI_NO_MAILTO 0, __URI_NO_PATH 0, __URI_NO_WWW 0, __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.95\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: Re: Crawler running with less than configured threads.\r\nX-Yahoo-Group-Post: member; u=500983475; y=ZdfbERHQxGnN0AzR1IZOrggBdSIRBqGoTVdk2gz0CB1iUN-Q0CUC7w\r\nX-Yahoo-Profile: david_pane1\r\n\r\nGordon,\n\nA couple questions about using the frontier.recover.gz file.\n\n1) In the full recover process, do I need to send all of the \nfrontier.recover.gz.cp00* files or just the last checkpoint?  I had \nstopped the crawl and restarted on Jan 05 so do I need include the \nfrontier.recover.gz.cp00* checkpoints from the previous run as well?\n\n2) If using the split recover, (and if you need to include all of the \nfrontier.recover.gz.cp00* files) can you concatenate all of the files \ninto one?\n\n--David\n\n\n\n&gt;&gt; If the crawler does crash or needs to be stopped and we cannot recover\n&gt;&gt; from the last checkpoint (or any recent checkpoint) what are our\n&gt;&gt; options? Do we have to start from an old checkpoint and recrawl all of\n&gt;&gt; the previously collected pages and data after that old recoverable\n&gt;&gt; checkpoint?\n&gt;\n&gt; Resuming from a known-good checkpoint is best from the standpoint of\n&gt; perfectly picking up from that self-consistent point.\n&gt;\n&gt; The older &#39;recovery log&#39; technique can approximate the crawl state at\n&gt; other points, at least with respect to URI discovery/completion.\n&gt; Essentially, from the &#39;frontier-recover&#39; log, this process first treats\n&gt; all previously-completed URIs as discovered (loads up the &#39;already-seen&#39;\n&gt; UriUniqFilter). Then, the process reconsiders all URIs discovered in the\n&gt; earlier run(s). Those that were completed get skipped (because of the\n&gt; first step), those that weren&#39;t are reenqueued in vaguely the same order\n&gt; as originally discovered.\n&gt;\n&gt; Unfortunately this process can take hours even in a moderately-sized\n&gt; crawl. For yours it might take days (or weeks), and it&#39;s not itself\n&gt; checkpointable (or optimized in other ways). It doesn&#39;t restore all\n&gt; running state for reporting purposes.\n&gt;\n&gt; If you groom the logs a bit beforehand (for example not even bothering\n&gt; to include lines not needed in each step) you can save some of the work,\n&gt; speeding the process.\n&gt;\n&gt; Some notes on this process that could get you started if you need to use\n&gt; this approach:\n&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt;\n&gt; - Gordon\n\n"}}