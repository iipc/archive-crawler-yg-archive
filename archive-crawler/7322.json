{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":55685036,"authorName":"Christopher M. Miles","from":"&quot;Christopher M. Miles&quot; &lt;twitch@...&gt;","profile":"cmiles74","replyTo":"LIST","senderId":"P6qRu9AqnJrCzxo3es1zu0oaPj3KvxYLn9h5sKZ63Xaf-72zLt-P_Rfr4PUkSxEL53WM5wWPQeweLt9v6H2R9zWn00WtCsvCrUkDASg5ixjwn8e39GY","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Retrieving Non-Text Data from CrawlURI&#39;s ReplayInputStream","postDate":"1316438245","msgId":7322,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFNzc0MEU1LjQwMDA0MDVAbmVydmVzdGFwbGUuY29tPg==","inReplyToHeader":"PDRFNzZCRUU3LjQwOTAwMDBAYXJjaGl2ZS5vcmc+","referencesHeader":"PGo1NXFqdSsyb2oyQGVHcm91cHMuY29tPiA8NEU3NkJFRTcuNDA5MDAwMEBhcmNoaXZlLm9yZz4="},"prevInTopic":7321,"nextInTopic":8454,"prevInTime":7321,"nextInTime":7323,"topicId":7320,"numMessagesInTopic":4,"msgSnippet":"Hi Noah, You were exactly right! I had picked the InputStreamReader when I first started this project and I completely missed the fact that it was reading","rawEmail":"Return-Path: &lt;twitch04@...&gt;\r\nX-Sender: twitch04@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 2191 invoked from network); 19 Sep 2011 13:17:29 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m6.grp.sp2.yahoo.com with QMQP; 19 Sep 2011 13:17:29 -0000\r\nX-Received: from unknown (HELO mail-qw0-f41.google.com) (209.85.216.41)\n  by mta5.grp.sp2.yahoo.com with SMTP; 19 Sep 2011 13:17:29 -0000\r\nX-Received: by qwa26 with SMTP id 26so5879050qwa.0\n        for &lt;archive-crawler@yahoogroups.com&gt;; Mon, 19 Sep 2011 06:17:28 -0700 (PDT)\r\nX-Received: by 10.224.203.5 with SMTP id fg5mr1888641qab.96.1316438248476;\n        Mon, 19 Sep 2011 06:17:28 -0700 (PDT)\r\nReturn-Path: &lt;twitch04@...&gt;\r\nX-Received: from [10.0.5.8] (68-114-83-198.dhcp.oxfr.ma.charter.com. [68.114.83.198])\n        by mx.google.com with ESMTPS id dv6sm20768334qab.13.2011.09.19.06.17.25\n        (version=SSLv3 cipher=OTHER);\n        Mon, 19 Sep 2011 06:17:27 -0700 (PDT)\r\nMessage-ID: &lt;4E7740E5.4000405@...&gt;\r\nDate: Mon, 19 Sep 2011 09:17:25 -0400\r\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:6.0.2) Gecko/20110906 Thunderbird/6.0.2\r\nMIME-Version: 1.0\r\nTo: Noah Levitt &lt;nlevitt@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;j55qju+2oj2@...&gt; &lt;4E76BEE7.4090000@...&gt;\r\nIn-Reply-To: &lt;4E76BEE7.4090000@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Christopher M. Miles&quot; &lt;twitch@...&gt;\r\nSubject: Re: [archive-crawler] Retrieving Non-Text Data from CrawlURI&#39;s ReplayInputStream\r\nX-Yahoo-Group-Post: member; u=55685036; y=L47BCjt9OHeFW6gSx_3XUtgQCijd8_aSlRHK8MbXI7Vn-Ec\r\nX-Yahoo-Profile: cmiles74\r\n\r\nHi Noah,\n\nYou were exactly right! I had picked the InputStreamReader when I first \nstarted this project and I completely missed the fact that it was \nreading characters from my input stream. I changed the code as you \nrecommended and the PDF files are now coming out pristine! Thank you \nvery much. :)\n\nYour point about reading the entire document into a byte array is well \ntaken. Right now the Java Riak client doesn&#39;t provide a method that will \naccept a stream or a reader, it&#39;s looking for a String, array of bytes \nor a POJO.\n\nhttps://github.com/basho/riak-java-client/blob/master/src/main/java/com/basho/riak/client/bucket/DefaultBucket.java\n\nLooking through the code, however, it appears that the Riak client \neventually clones the byte array that gets passed in. I think I can make \nsome changes and at least eliminate the copying.\n\nhttps://github.com/basho/riak-java-client/blob/master/src/main/java/com/basho/riak/client/DefaultRiakObject.java\n\nThanks again\n--\nMiles\n\nOn 09/19/2011 12:02 AM, Noah Levitt wrote:\n&gt; Hello Miles,\n&gt;\n&gt; A possible cause of your problem is that Reader implementations like\n&gt; InputStreamReader assume the input is text. It looks like there&#39;s\n&gt; another IOUtils.toByteArray() method that takes an InputStream, so this\n&gt; might do the trick:\n&gt;\n&gt; byte[] bytesResponse =\n&gt; IOUtils.toByteArray(crawluri.getRecorder().getContentReplayInputStream());\n&gt;\n&gt; If that doesn&#39;t work I&#39;d recommend comparing the stored data with the\n&gt; expected data to try to get an idea of what might be happening. Compare\n&gt; the sizes for starters, and inspect hex dumps side by side.\n&gt;\n&gt; As an aside, if Riak can accept input from a stream, it would be\n&gt; preferable to use that api. You run the risk of exhausting available\n&gt; memory when you load arbitrarily large files into byte arrays.\n&gt;\n&gt; Noah\n&gt;\n&gt;\n&gt; On 2011-09-18 15:16 , cmiles74 wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Hello,\n&gt;&gt;\n&gt;&gt; I&#39;m trying to code-up my own writer that stores the crawled data in\n&gt;&gt; buckets in a Riak cluster instead of writing the data to a WARC file.\n&gt;&gt; This code is mostly working in that HTML and plain-text content are\n&gt;&gt; being correctly stored. Binary content (specifically PDF files) are\n&gt;&gt; not working so well, they are being garbled along the way.\n&gt;&gt;\n&gt;&gt; I&#39;m using the following code to fetch this data from the CrawlURI\n&gt;&gt; instance:\n&gt;&gt;\n&gt;&gt; InputStreamReader reader =\n&gt;&gt; new\n&gt;&gt; InputStreamReader(crawluri.getRecorder().getContentReplayInputStream());\n&gt;&gt; byte[] bytesResponse = IOUtils.toByteArray(reader);\n&gt;&gt; inputstreamResponse.close();\n&gt;&gt;\n&gt;&gt; I&#39;m then storing this data i n Riak as a ByteArray. When I pull the\n&gt;&gt; data back out, it&#39;s clearly garbled. These are PDF files and while\n&gt;&gt; they open in a PDF reader, all of the pages are blank. I&#39;m not sure\n&gt;&gt; where I might be going wrong.\n&gt;&gt;\n&gt;&gt; Does this approach seem reasonable? Any help or feedback would be\n&gt;&gt; greatly appreciated.\n&gt;&gt;\n&gt;&gt; Thank you\n&gt;&gt; --\n&gt;&gt; Miles\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; \n\n"}}