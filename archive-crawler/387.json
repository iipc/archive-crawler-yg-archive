{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","replyTo":"LIST","senderId":"D6IMa9i4SMQ1cwswprc6z0ryQlxXZZx9gzWdZbEp7SBNKSidIJS1GXHEM2RNEnFj0swCoRz19gtfiAfWUv17cw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: submit batch jobs","postDate":"1084466004","msgId":387,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwQTNBMzU0LjEwMjA1QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGM4MDM1YSsxMGJnYkBlR3JvdXBzLmNvbT4=","referencesHeader":"PGM4MDM1YSsxMGJnYkBlR3JvdXBzLmNvbT4="},"prevInTopic":386,"nextInTopic":0,"prevInTime":386,"nextInTime":388,"topicId":372,"numMessagesInTopic":10,"msgSnippet":"... The general notion is that heritrix does crawling only.  How the downloaded content is mined is domain specific and outside of the heritrix purview. That","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 95128 invoked from network); 13 May 2004 16:28:04 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m12.grp.scd.yahoo.com with QMQP; 13 May 2004 16:28:04 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta5.grp.scd.yahoo.com with SMTP; 13 May 2004 16:28:03 -0000\r\nReceived: from archive.org ([192.168.1.105])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id i4DFqaC25124\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 13 May 2004 08:52:36 -0700\r\nMessage-ID: &lt;40A3A354.10205@...&gt;\r\nDate: Thu, 13 May 2004 09:33:24 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7b) Gecko/20040421\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c8035a+10bgb@...&gt;\r\nIn-Reply-To: &lt;c8035a+10bgb@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Remote-IP: 63.203.238.114\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: submit batch jobs\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\npenguinoamante2 wrote:\n\n&gt;How is it more manageable crawling through all the customers URIs and\n&gt;then dealing with all their data mixed up in one file?  What programs\n&gt;exist for querying the ARC file. A common query is show all pages with\n&gt;contact info in the domain of customerX.  \n&gt;  \n&gt;\n\nThe general notion is that heritrix does crawling only.  How the \ndownloaded content is mined is domain specific and outside of the \nheritrix purview.  \n\nThat said, there&#39;s a dearth of publically available tools for mining \narcs.  Basic java readers are available at the pointer supplied in my \nlast mail. \n\nFor your particular application, sounds like you&#39;d need to build an \napplication that read all downloaded arcs, indexed their content (with \nparsers for all of the common mime types), and then against this index \nyou&#39;d run queries, &quot;Return &#39;contact info&#39; for all documents in domain \ncustomerX&quot;, which would return pointers to arc files with offsets.  Much \nof this is well-travelled territories but it&#39;d be a significant undertaking.\n\nWould sample application(s) that did something with the downloaded arcs \nhelp?\n\n&gt;Doesn&#39;t the Way Back Machine use heritrix? What tools do they use for\n&gt;querying the ARC?\n&gt;  \n&gt;\nThe wayback doesn&#39;t use heritrix.  Heritrix doesn&#39;t (yet) do the volumes \nrequired.  Wayback machine is built atop crawls donated by Alexa.  These \ncome to us as ARC files.\n\nTools used by wayback are mentioned at head of ARCWriter \n(http://crawler.archive.org/apidocs/org/archive/io/arc/ARCWriter.html).  \nThey&#39;re here: \nhttp://www.archive.org/web/researcher/tool_documentation.php.  They are \nnot for general public use.\n\nSt.Ack\n\n&gt;So far Heritrix has worked for me so I&#39;m confident about the crawling\n&gt;side.  I&#39;m not so confident about querying ARC files.\n&gt;  \n&gt;\n&gt;Thanks \n&gt;\n&gt;--- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@a...&gt; wrote:\n&gt;  \n&gt;\n&gt;&gt;penguinoamante2 wrote:\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;Yes you guys are right.  When I restart heritrix the pending jobs on\n&gt;&gt;&gt;disk get loaded into the crawler.\n&gt;&gt;&gt;\n&gt;&gt;&gt;Thanks for the tips.  \n&gt;&gt;&gt;\n&gt;&gt;&gt;I should be asking weather this is the feature to have or if there is\n&gt;&gt;&gt;a better way to do what I want to do.  \n&gt;&gt;&gt;\n&gt;&gt;&gt;I want to watch a list of customers sites to make sure that they\n&gt;&gt;&gt;continue to sell what they say they are currently selling, that their\n&gt;&gt;&gt;contact info is in the USA, and that they aren&#39;t doing anything\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;illegal.\n&gt;  \n&gt;\n&gt;&gt;&gt;It seems like launching a batch of jobs one for each customer would do\n&gt;&gt;&gt;the job. However heritrix could just crawl all the customer URIs in\n&gt;&gt;&gt;one job and then another program could split up the data from the ARC\n&gt;&gt;&gt;file into individual customers.\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;Doing the latter sounds more manageable.\n&gt;&gt;\n&gt;&gt;See http://crawler.archive.org/articles/developer_manual.html#arcreader \n&gt;&gt;for a few notes on reading arcs.\n&gt;&gt;\n&gt;&gt;St.Ack\n&gt;&gt;\n&gt;&gt;P.S. Related, currently there is no means of stopping the crawler from \n&gt;&gt;the command line.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt;--- In archive-crawler@yahoogroups.com, &quot;Kristinn Sigurdsson&quot; \n&gt;&gt;&gt;\n&gt;&gt;&gt;&lt;kris@a...&gt; wrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Michael is correct. Jobs are only read from disk during program\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;startup. At\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;other times in memory chaching is used.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;A suitable workaround might by to create a page that accepts (via\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;GET) the\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;parameters needed to construct a new job (this might only be the\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;crawl order\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;xml file�s name) and creates the new job based on it.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;In fact if you go that route you could not write the XML to it&#39;s\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;intended\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;job directory. Instead you would save it somewhere else and create\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;the new\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;job BASED ON your order. This mirrors how jobs are generally\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;created in\n&gt;  \n&gt;\n&gt;&gt;&gt;&gt;Heritrix.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;I believe that ...webapps/admin/jobs/new.jsp would be a good\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;starting point\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;for someone intent on writing this add on :-)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;- Kris\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;-----Original Message-----\n&gt;&gt;&gt;&gt;From: Michael Stack [mailto:stack@a...] \n&gt;&gt;&gt;&gt;Sent: 11. ma� 2004 16:57\n&gt;&gt;&gt;&gt;To: archive-crawler@yahoogroups.com\n&gt;&gt;&gt;&gt;Subject: Re: [archive-crawler] Re: submit batch jobs\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;penguinoamante2 wrote:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;First try was not successful.  I create a directory called\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;batchjob in\n&gt;  \n&gt;\n&gt;&gt;&gt;&gt;&gt;the jobs directoy which contains three files: batchjob.job \n&gt;&gt;&gt;&gt;&gt;job-batchjob.xml and seeds-batchjob.txt.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;batchjob.job contains\n&gt;&gt;&gt;&gt;&gt;20040511155115186\n&gt;&gt;&gt;&gt;&gt;batchjob\n&gt;&gt;&gt;&gt;&gt;Pending\n&gt;&gt;&gt;&gt;&gt;false\n&gt;&gt;&gt;&gt;&gt;false\n&gt;&gt;&gt;&gt;&gt;2\n&gt;&gt;&gt;&gt;&gt;/opt/src/heritrix-0.6.0/jobs/batchjob/job-batchjob.xml\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;job-batchjob.xml contains a ton of xml I think I got the important\n&gt;&gt;&gt;&gt;&gt;tags such as\n&gt;&gt;&gt;&gt;&gt;&lt;name&gt;batchjob&lt;/name&gt;\n&gt;&gt;&gt;&gt;&gt;&lt;string name=&quot;seedsfile&quot;&gt;seeds-batchjob.txt&lt;/string&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;seeds-batchjob has the right seeds.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;After doing this I would expect the job to at least show up as\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;Pending.  \n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;The code that reads the directory is only run on application\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;startup it \n&gt;  \n&gt;\n&gt;&gt;&gt;&gt;looks like.  Restart.  Does it work?\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Here is the pertinent code:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;http://crawler.archive.org/xref-test/org/archive/crawler/admin/CrawlJobHan=\n&gt;&gt;    \n&gt;&gt;\n&gt;d=\n&gt;  \n&gt;\n&gt;&gt;&gt;l\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;er.html#211\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;St.Ack\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;--- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@a...&gt;\n&gt;&gt;&gt;&gt;&gt;wrote:\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;penguinoamante2 wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;       \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;            \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;What are the best practices for submitting a batch of jobs.  I\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;    \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;         \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;              \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;have a\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;list of fqdn&#39;s in a database and I want heritrix to consider each\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;    \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;         \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;              \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;one\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;a seperate job.  The plan so far is to write a script to populate\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;    \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;         \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;              \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;the\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;jobs directory with the right info.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Is this the right way to do it?\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;    \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;         \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;              \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;That sounds right.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;Make sure the crawler is the &#39;Crawling state&#39; so that it&#39;ll just\n&gt;&gt;&gt;&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;       \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;            \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;start \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;the next job soon as its finished the current job.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;Yours,\n&gt;&gt;&gt;&gt;&gt;&gt;St.Ack\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;       \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;            \n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Thanks,\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Sunny\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;    \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;         \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;              \n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;     \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;          \n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Yahoo! Groups Sponsor\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;ADVERTISEMENT\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&lt;http://rd.yahoo.com/SIG=129q30o18/M=295196.4901138.6071305.3001176/D=grou=\n&gt;&gt;    \n&gt;&gt;\n&gt;p=\n&gt;  \n&gt;\n&gt;&gt;&gt;s\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;/S=1705004924:HM/EXP=1084381409/A=2128215/R=0/SIG=10se96mf6/*http:/compani=\n&gt;&gt;    \n&gt;&gt;\n&gt;o=\n&gt;  \n&gt;\n&gt;&gt;&gt;n\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;.yahoo.com&gt; click here\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&lt;http://us.adserver.yahoo.com/l?M=295196.4901138.6071305.3001176/D=groups/=\n&gt;&gt;    \n&gt;&gt;\n&gt;S=\n&gt;  \n&gt;\n&gt;&gt;&gt;=\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;:HM/A=2128215/rand=708464738&gt; \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; _____  \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;*\tTo visit your group on the web, go to:\n&gt;&gt;&gt;&gt;http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;*\tTo unsubscribe from this group, send an email to:\n&gt;&gt;&gt;&gt;archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt; \n&gt;&gt;    \n&gt;&gt;\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; \n&gt;&gt;&gt;&gt;*\tYour use of Yahoo! Groups is subject to the Yahoo! Terms of Service\n&gt;&gt;&gt;&gt;&lt;http://docs.yahoo.com/info/terms/&gt; .\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;        \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;      \n&gt;&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;\n&gt;  \n&gt;\n\n\n"}}