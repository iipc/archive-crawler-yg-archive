{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":181195510,"authorName":"sebastiandelachica","from":"&quot;sebastiandelachica&quot; &lt;sebastian.delachica@...&gt;","profile":"sebastiandelachica","replyTo":"LIST","senderId":"AzshJZIc1Xx0uh_vQIUsnaSrwDlBkdCc1rMbxe5909q7RulhVpJgTqHufACk-IbFKiOt-EjHlhrncv-Yr7Lqjjn6tig-_SkTBgEdohqB20KbQman6TRtZCjSY3L6jg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Max Size Related Configuration","postDate":"1080660645","msgId":295,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGM0YzNyNStjdDVwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQwNjhDMDE2LjcwMTA2QGFyY2hpdmUub3JnPg=="},"prevInTopic":293,"nextInTopic":297,"prevInTime":294,"nextInTime":296,"topicId":289,"numMessagesInTopic":9,"msgSnippet":"Hiya Kris, Thanks for the hint. That is pretty much where I ended up last nite. To clarify, my original intent was to manage xtiple sites from a single crawl","rawEmail":"Return-Path: &lt;sebastian.delachica@...&gt;\r\nX-Sender: sebastian.delachica@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 80375 invoked from network); 30 Mar 2004 15:31:04 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m1.grp.scd.yahoo.com with QMQP; 30 Mar 2004 15:31:04 -0000\r\nReceived: from unknown (HELO n33.grp.scd.yahoo.com) (66.218.66.101)\n  by mta3.grp.scd.yahoo.com with SMTP; 30 Mar 2004 15:31:04 -0000\r\nReceived: from [66.218.67.186] by n33.grp.scd.yahoo.com with NNFMP; 30 Mar 2004 15:30:45 -0000\r\nDate: Tue, 30 Mar 2004 15:30:45 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;c4c3r5+ct5p@...&gt;\r\nIn-Reply-To: &lt;4068C016.70106@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Length: 5828\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.101\r\nFrom: &quot;sebastiandelachica&quot; &lt;sebastian.delachica@...&gt;\r\nSubject: Re: Max Size Related Configuration\r\nX-Yahoo-Group-Post: member; u=181195510\r\nX-Yahoo-Profile: sebastiandelachica\r\n\r\nHiya Kris,\n\nThanks for the hint. That is pretty much where I ended up last =\r\nnite.\n\nTo clarify, my original intent was to manage xtiple sites from a \nsi=\r\nngle crawl order, but to get the size limit per site, I ended \nup &quot;simplify=\r\ning&quot; my approach and limiting each order to a single \nseed. I like the idea=\r\n of controlling limits on # of downloaded  bytes \non a per domain/host/etc.=\r\n basis for xtiple seeds from a single order. \nI hope that concept becomes p=\r\nart of heritrix at some point in the \nfuture.\n\nTo put things into perspecti=\r\nve, my research work is related to the \nDigital Library for Earth Sciences =\r\n(DLESE) http://www.dlese.org. I am \ntrying to automatically tag online educ=\r\national resources with \nNational Science Educational Standards using natura=\r\nl language \nprocessing techniques. The reason for the size limit per site i=\r\ns b/c \nI need to avoid upsetting the community that provides the content.\n\n=\r\nThanks for all the good advice and the Most Excellent Work on \nHeritrix!\n\nS=\r\neb\n\n--- In archive-crawler@yahoogroups.com, Kristinn Sigur=F0sson \n&lt;kris@a.=\r\n..&gt; wrote:\n&gt; Hei Seb.\n&gt; \n&gt; See below\n&gt; \n&gt; sebastiandelachica wrote:\n&gt; \n&gt; &gt; =\r\nMichael,\n&gt; &gt;\n&gt; &gt; Thanks for the prompt reply. Indeed I upgraded to 0.6.0 ov=\r\ner the\n&gt; &gt; weekend: amazing how close a 9 looks to a 6 given enough lack of=\r\n \nsleep\n&gt; &gt;\n&gt; &gt; In English (to the best of my ability), what I am trying to=\r\n do is \nuse\n&gt; &gt; a separate order for each site I need to crawl. Each order =\r\nhence \nhas\n&gt; &gt; a single seed. The purpose is to place an upper bound on the=\r\n \nnumber\n&gt; &gt; of &quot;useful&quot; bytes downloaded for each crawl order (about 100K =\r\nor \n1MB\n&gt; &gt; say). In other words, stop crawling the site once we have \ndown=\r\nloaded\n&gt; &gt; some number of usable bytes. \n&gt; \n&gt; If you are only crawling one =\r\nsite at a time (using DomainScope) the \n&gt; max-bytes-download\n&gt; just the thi=\r\nng for you.  It limits the total amount of data \ndownloaded \n&gt; in one Crawl=\r\nJob. It is\n&gt; only if you are crawling multiple domains in the same job (as =\r\nis \nusual) \n&gt; that you can&#39;t use it\n&gt; for this purpose as it would only cut=\r\n you off once the total from \nall \n&gt; domain hit the limit.\n&gt; \n&gt; Once the li=\r\nmit is hit the current job will end.  At some point we \nmay \n&gt; allow overri=\r\ndes on this\n&gt; setting enabling cutoffs on specific domains but that is well=\r\n into \nthe \n&gt; future.\n&gt; \n&gt; I hope this is of some use to you.\n&gt; \n&gt; - Kris\n&gt;=\r\n \n&gt; &gt; I thought setting a max size limit on\n&gt; &gt; the ARC file would stop log=\r\nging past that point, but I see now \nthat\n&gt; &gt; it means a slightly different=\r\n thing. I tried the number of files\n&gt; &gt; limit and while it works, I am not =\r\nsure it matches my intent as \nsome\n&gt; &gt; files may be noticably shorter than =\r\nothers.\n&gt; &gt;\n&gt; &gt; I am using a DomainScope crawl per the settings in the Prof=\r\nile \nused\n&gt; &gt; to create the Job.\n&gt; &gt;\n&gt; &gt; Based on the information you sent =\r\nme, I need to think about what\n&gt; &gt; might serve my purpose. Thanks for point=\r\ning me at the right code \nin\n&gt; &gt; ARCWriter.\n&gt; &gt;\n&gt; &gt; Seb\n&gt; &gt;\n&gt; &gt; --- In arch=\r\nive-crawler@yahoogroups.com, Michael Stack &lt;stack@a...&gt;\n&gt; &gt; wrote:\n&gt; &gt; &gt; Th=\r\nanks for trying Heritrix Seb.\n&gt; &gt; &gt;\n&gt; &gt; &gt; See below.\n&gt; &gt; &gt;\n&gt; &gt; &gt; sebastiand=\r\nelachica wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;I have been playing around with heritrix for a=\r\n few weeks now \nand I\n&gt; &gt; am\n&gt; &gt; &gt; &gt;in the process of turning it loose on a=\r\n controlled environment \nfor\n&gt; &gt; &gt; &gt;one of my research strands. I am curren=\r\ntly using version \n0.9.0. My\n&gt; &gt; &gt; &gt;objective is to limit the amount of dat=\r\na scooped from a site \nonto\n&gt; &gt; the\n&gt; &gt; &gt; &gt;ARC file. I tried using the HTTP=\r\n Processor max-length-bytes and\n&gt; &gt; the\n&gt; &gt; &gt; &gt;Archiver max-size-bytes.\n&gt; &gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; Do you mean 0.4.0. You say 0.9.0 above.  We jus=\r\nt released 0.6.0 \non\n&gt; &gt; &gt; friday.  Try it if you haven&#39;t already.  Lots of=\r\n fixes and\n&gt; &gt; improvements.\n&gt; &gt; &gt;\n&gt; &gt; &gt; If you&#39;re doing a broad crawl, you=\r\n have the following options\n&gt; &gt; available\n&gt; &gt; &gt; to you:\n&gt; &gt; &gt;\n&gt; &gt; &gt; max-byt=\r\nes-download\n&gt; &gt; &gt; max-document-download\n&gt; &gt; &gt;\n&gt; &gt; &gt; These options are not a=\r\nvailable in a domain scoped crawl which\n&gt; &gt; seems to\n&gt; &gt; &gt; be what it is yo=\r\nu&#39;d like to do.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Tell us more about what it is that you&#39;d like.\n=\r\n&gt; &gt; &gt;\n&gt; &gt; &gt; The max-length-bytes options limits size of a particular \ndownl=\r\noad\n&gt; &gt; only. \n&gt; &gt; &gt; The max-size-bytes  is upper-bound on the size of ARC =\r\nfiles \nwritten\n&gt; &gt; (See\n&gt; &gt; &gt; the code here http://crawler.archive.org/xref=\r\n/index.html). \n&gt; &gt; &lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt; &gt; &gt;\n&gt;=\r\n &gt; &gt; Yours,\n&gt; &gt; &gt; St.Ack\n&gt; &gt; &gt;\n&gt; &gt; &gt; &gt;The Processor max-length-bytes seems =\r\nto work at some level as \nthe\n&gt; &gt; &gt; &gt;requests are reported as length-trunca=\r\nted in the logs, but the\n&gt; &gt; actual\n&gt; &gt; &gt; &gt;HTML files still make it into th=\r\ne archive.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;The Archive max-size-bytes appears to be ignored.=\r\n Looking at \nthe\n&gt; &gt; &gt; &gt;code, it does not seem to be used by the ARCWriterP=\r\nrocessor \nclass\n&gt; &gt; or\n&gt; &gt; &gt; &gt;any other class for that matter...I just star=\r\nting digging \nthrough\n&gt; &gt; the\n&gt; &gt; &gt; &gt;code earlier today.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;I c=\r\nontinue my experimentation and code reading, but figured, \nI&#39;d\n&gt; &gt; &gt; &gt;check=\r\n in to see if I am missing something very obvious or if\n&gt; &gt; anyone\n&gt; &gt; &gt; &gt;h=\r\nad experienced similar behavior.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;Thanks in advance for your =\r\ntime,\n&gt; &gt; &gt; &gt;Seb\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;Yah=\r\noo! Groups Links\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;\n&gt;=\r\n &gt;\n&gt; &gt;\n&gt; &gt; ----------------------------------------------------------------=\r\n--\n------\n&gt; &gt; *Yahoo! Groups Links*\n&gt; &gt;\n&gt; &gt;     * To visit your group on th=\r\ne web, go to:\n&gt; &gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt; &gt; =\r\n       \n&gt; &gt;     * To unsubscribe from this group, send an email to:\n&gt; &gt;    =\r\n   archive-crawler-unsubscribe@yahoogroups.com\n&gt; &gt;       &lt;mailto:archive-cr=\r\nawler-unsubscribe@yahoogroups.com?\nsubject=3DUnsubscribe&gt;\n&gt; &gt;        \n&gt; &gt;  =\r\n   * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt; &gt;       =\r\nService &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; &gt;\n&gt; &gt;\n\n\n"}}