{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"FyWPcM98cBWM5b8pIZWrwfb6v8jY-scp1AAJNn4Yo6I8av0iZ6Pbr9VcPE79w5YpOE5qC3EziUmFG7kh0EeWnTTA_f7CPZ8","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] crawl only seed urls in Heritrix","postDate":"1294251451","msgId":6969,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREMjRCNUJCLjYwNDA4QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDREMjQ3MTdFLjYwNTAxMDZAZ21haWwuY29tPg==","referencesHeader":"PGlmdjF2ZCt0bzd0QGVHcm91cHMuY29tPiA8NEQyMzcxODAuOTA3MDYwOEBhcmNoaXZlLm9yZz4gPDREMjM5MUVBLjgwNjA3MDFAYXJjaGl2ZS5vcmc+IDw0RDIzRjBENS42MDkwMDA5QGdtYWlsLmNvbT4gPDREMjQwRUNDLjEwMzA0MDNAYXJjaGl2ZS5vcmc+IDw0RDI0NzE3RS42MDUwMTA2QGdtYWlsLmNvbT4="},"prevInTopic":6968,"nextInTopic":6972,"prevInTime":6968,"nextInTime":6970,"topicId":6955,"numMessagesInTopic":9,"msgSnippet":"... In addition to Noah s answer, I would note that if you only have a fixed list of 100 URLs to crawl -- never wanting to extract links, or review HTTP","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 78009 invoked from network); 5 Jan 2011 18:17:33 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m11.grp.re1.yahoo.com with QMQP; 5 Jan 2011 18:17:33 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.re1.yahoo.com with SMTP; 5 Jan 2011 18:17:33 -0000\r\nX-Received: (qmail 9884 invoked by uid 0); 5 Jan 2011 18:17:32 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay03.pair.com with SMTP; 5 Jan 2011 18:17:32 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4D24B5BB.60408@...&gt;\r\nDate: Wed, 05 Jan 2011 10:17:31 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: arundudee gmail &lt;arundudee@...&gt;\r\nCc: Noah Levitt &lt;nlevitt@...&gt;, archive-crawler@yahoogroups.com\r\nReferences: &lt;ifv1vd+to7t@...&gt; &lt;4D237180.9070608@...&gt; &lt;4D2391EA.8060701@...&gt; &lt;4D23F0D5.6090009@...&gt; &lt;4D240ECC.1030403@...&gt; &lt;4D24717E.6050106@...&gt;\r\nIn-Reply-To: &lt;4D24717E.6050106@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] crawl only seed urls in Heritrix\r\nX-Yahoo-Group-Post: member; u=137285340; y=Jl62WHYEGbacRFDkQztopSUbNquH5QBuJu4jNJT3sts3\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 1/5/11 5:26 AM, arundudee gmail wrote:\n&gt; Thanks Noah , i was successful in setting up job with below mentioned\n&gt; steps and its working as desired.\n&gt; please can you help on point #2 as well :\n&gt; 2. I have one more question:let say i crawled 100 seeds urls and they\n&gt; are dumped at arc folder.Is there a way i can specify url and i can get\n&gt; response body of that page from dump... or is that possible through any\n&gt; other mechanism.\n\nIn addition to Noah&#39;s answer, I would note that if you only have a fixed \nlist of 100 URLs to crawl -- never wanting to extract links, or review \nHTTP headers, etc. -- then Heritrix may be overkill for your purposes. A \ncommand-line tool like &#39;wget&#39;, &#39;curl&#39;, etc. in a small loop may be plenty.\n\n&gt; I also noticed then when i run job , num of active threads remain zero :\n&gt; &#39;0 active of 50 threads &#39; how can i run all threads to speed up my\n&gt; crawling.\n\nSee this page in the project wiki FAQ:\n\n&quot;Why is crawling slower than expected with a not-very-busy crawling \nmachine?&quot;\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/unexpectedly+slow+crawling+on+idle+crawler\n\n- Gordon @ IA\n\n&gt; Thank you so much\n&gt; Arun\n&gt;\n&gt;\n&gt; On Wednesday 05 January 2011 11:55 AM, Noah Levitt wrote:\n&gt;&gt; You can add and remove decide rules in the Submodules tab, or edit\n&gt;&gt; order.xml directly.\n&gt;&gt;\n&gt;&gt; Noah\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; On 2011-01-04 20:17 , arundudee gmail wrote:\n&gt;&gt;&gt; Gordon ,Noah - Thanks a ton for reply.\n&gt;&gt;&gt;\n&gt;&gt;&gt; 1. I am newbie to heritrix please can you elaborate how to perform\n&gt;&gt;&gt; three steps you mentioned below.I am attaching a screenshot of\n&gt;&gt;&gt; setting page but i don&#39;t know how to remove these rules and where to\n&gt;&gt;&gt; add #3.\n&gt;&gt;&gt; 2. I have one more question:let say i crawled 100 seeds urls and they\n&gt;&gt;&gt; are dumped at arc folder.Is there a way i can specify url and i can\n&gt;&gt;&gt; get response body of that page from dump... or is that possible\n&gt;&gt;&gt; through any other mechanism.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks once again.\n&gt;&gt;&gt; Arun\n&gt;&gt;&gt; On Wednesday 05 January 2011 03:02 AM, Gordon Mohr wrote:\n&gt;&gt;&gt;&gt; One extra note about Noah&#39;s suggestion: it assumes you&#39;re starting\n&gt;&gt;&gt;&gt; from the (recommended) &#39;deciding-default&#39; example scope, rather than\n&gt;&gt;&gt;&gt; using the (deprecated, legacy) &#39;BroadScope&#39; class.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; (I think your previous max-links-hops and max-trans-hops settings\n&gt;&gt;&gt;&gt; *should* have worked, but since Noah&#39;s approach is the better base\n&gt;&gt;&gt;&gt; for the future, getting to the bottom of what might have gone wrong\n&gt;&gt;&gt;&gt; with the BroadScope approach isn&#39;t as useful as moving the\n&gt;&gt;&gt;&gt; &#39;deciding&#39; scope.)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - Gordon @ IA\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; On 1/4/11 11:14 AM, Noah Levitt wrote:\n&gt;&gt;&gt;&gt;&gt; Here&#39;s one way to crawl seeds only, and not even embedded images or\n&gt;&gt;&gt;&gt;&gt; anything, which seems to be what you want. Starting with the\n&gt;&gt;&gt;&gt;&gt; default order.xml...\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; 1. remove the scope rule acceptIfSurtPrefixed\n&gt;&gt;&gt;&gt;&gt; 2. remove the scope rule acceptIfTranscluded\n&gt;&gt;&gt;&gt;&gt; 3. add this scope rule right after rejectByDefault\n&gt;&gt;&gt;&gt;&gt; &lt;newObject name=&quot;acceptIfSeed&quot;\n&gt;&gt;&gt;&gt;&gt; class=&quot;org.archive.crawler.deciderules.SeedAcceptDecideRule&quot;&gt;\n&gt;&gt;&gt;&gt;&gt; &lt;/newObject&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Hope this works for you.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Noah\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; On 2011-01-04 03:57 , arun wrote:\n&gt;&gt;&gt;&gt;&gt;&gt; i am using Heritrix 1.14.4.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; I have around million seed urls and i just want to crawl these\n&gt;&gt;&gt;&gt;&gt;&gt; seed urls only.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Please any one can suggest me how to do that.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; one more problem i am facing is that.. i tried to crawl only seed\n&gt;&gt;&gt;&gt;&gt;&gt; urls uing broadscope and then max-link-hops: 0 and max-trans-hops:\n&gt;&gt;&gt;&gt;&gt;&gt; 0 but its not restricting to seed ulrs it crawls other urls in\n&gt;&gt;&gt;&gt;&gt;&gt; that page too.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Any help will be highly appreciated ..thanks a ton in advance.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n\n"}}