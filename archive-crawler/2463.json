{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"UBBbqjd8cI8MTfKBu7riIZGhF4Lg5leR5Wrq3IDamXB11yRDZuXSQXJ57ocfuTdfi6NhMAUu1IOEAgnJD88me9-Ft5Le4839cQE4kuT5","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Scope Qs Re: [archive-crawler] Re: Large crawl experience (like, 500M links)","postDate":"1135302278","msgId":2463,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGRvZmtxNitpbnRoQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQzQUI0QzkzLjMwMDA0MDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":2461,"nextInTopic":2465,"prevInTime":2462,"nextInTime":2464,"topicId":2391,"numMessagesInTopic":12,"msgSnippet":"... queued up ... How about new URIs discovered through the JMX importUris as non-seed? Let s say I JMX imported this link (http://members.aol.com/joe) as ","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 89800 invoked from network); 23 Dec 2005 01:44:52 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m11.grp.scd.yahoo.com with QMQP; 23 Dec 2005 01:44:52 -0000\r\nReceived: from unknown (HELO n23.bullet.scd.yahoo.com) (66.94.237.52)\n  by mta3.grp.scd.yahoo.com with SMTP; 23 Dec 2005 01:44:51 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.3] by n23.bullet.scd.yahoo.com with NNFMP; 23 Dec 2005 01:44:39 -0000\r\nReceived: from [66.218.66.87] by mailer3.bullet.scd.yahoo.com with NNFMP; 23 Dec 2005 01:44:39 -0000\r\nDate: Fri, 23 Dec 2005 01:44:38 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;dofkq6+inth@...&gt;\r\nIn-Reply-To: &lt;43AB4C93.3000403@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Scope Qs Re: [archive-crawler] Re: Large crawl experience (like, 500M links)\r\nX-Yahoo-Group-Post: member; u=132996324; y=_WkKgST69H-bDjgc7pRr2D1IXDoTfU6V56gyxkaqTm6jIHwskw\r\nX-Yahoo-Profile: joehung302\r\n\r\n\n&gt; \n&gt; Every URI that is discovered by the Extractor processors gets \nqueued=\r\n up\n&gt; inside the originating URI as it continues its processing. \n\nHow abou=\r\nt new URIs discovered through the JMX importUris as non-seed?\nLet&#39;s say I J=\r\nMX imported this link (http://members.aol.com/joe) as \nnon-seed and this li=\r\nnk gets crawled/extracted and the crawler get \ntwo new links\n\nhttp://member=\r\ns.aol.com/joe/kid1.html\nhttp://members.aol.com/joe/kid2.html\n\nSince http://=\r\nmembers.aol.com/joe is not seed, would the crawler \ncontinue to download \n\n=\r\nhttp://members.aol.com/joe/kid1.html\nhttp://members.aol.com/joe/kid2.html\n\n=\r\n&gt; If you are trying new scopes, the otherthing to look into is the\n&gt; Decidi=\r\nngScope. It &#39;unwraps&#39; some of the things bundled together in \nthe\n&gt; classic=\r\n scopes to be separate reorderable &#39;DecideRules&#39;, applied in\n&gt; sequence. As=\r\n a result, you can gain even finer control over what&#39;s\n&gt; included and what =\r\nisn&#39;t.\n&gt; \n\nFrankly I&#39;m willing to try anything that would leads me to a 500=\r\nM \nlinks crawl. Right now it seems to me the most promising methods are \nsp=\r\nlit-crawl technique with SurtPrefixScope and moving uris around. \nI&#39;m hopin=\r\ng someone on this list can shed some light...\n\ncheers,\n-joe\n\n\n\n\n\n"}}