{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"kwFCcVWBd0YUChti0s7ruAwPV2tw9oOWySOAK8asvS275_9Bl3FvjtQEf5bvJJntT5jDzXracRCaN-x9w_mDoLksKh2CcMk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Constructing a web graph","postDate":"1176335887","msgId":4099,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2MUQ3NjBGLjkwNjAyMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGV2am10aCtxMnBwQGVHcm91cHMuY29tPg==","referencesHeader":"PGV2am10aCtxMnBwQGVHcm91cHMuY29tPg=="},"prevInTopic":4098,"nextInTopic":0,"prevInTime":4098,"nextInTime":4100,"topicId":4059,"numMessagesInTopic":8,"msgSnippet":"While I prefer the approaches I mentioned, where you simply log the extracted links before they are uniq filtered from being redundantly scheduled for","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 56118 invoked from network); 11 Apr 2007 23:56:10 -0000\r\nReceived: from unknown (66.218.66.70)\n  by m47.grp.scd.yahoo.com with QMQP; 11 Apr 2007 23:56:10 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta12.grp.scd.yahoo.com with SMTP; 11 Apr 2007 23:56:10 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 263501415FFAC\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 11 Apr 2007 16:55:36 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 18484-04-86 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 11 Apr 2007 16:55:35 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 9C28C1415FF52\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 11 Apr 2007 16:55:35 -0700 (PDT)\r\nMessage-ID: &lt;461D760F.9060202@...&gt;\r\nDate: Wed, 11 Apr 2007 16:58:07 -0700\r\nUser-Agent: Thunderbird 1.5.0.10 (X11/20070306)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;evjmth+q2pp@...&gt;\r\nIn-Reply-To: &lt;evjmth+q2pp@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Constructing a web graph\r\nX-Yahoo-Group-Post: member; u=137285340; y=iwMJ4f8kKi3LrQnH3ozAnsBKVhiXO8E5vcCWtuDdhxux\r\nX-Yahoo-Profile: gojomo\r\n\r\nWhile I prefer the approaches I mentioned, where you simply log the \nextracted links before they are &#39;uniq filtered&#39; from being redundantly \nscheduled for crawling, your comment suggests yet another way your \ndesired effect could be achieved.\n\nYou could make your own subclass of the UriUniqFilter normally used \n(typically BdbUriUniqFilter) which, when it encounters a duplicate URI, \nrather than discarding it marks it up with some additional state.\n\nThen, ensure that as soon as that URI comes off its queue for \nprocessing, it is rejected at that point. You could possibly do this \nthrough a new scope DecideRule that looks for that special flag and \nREJECTs URIs with it set. The rule would be harmless on initial \nscope-checking -- the flag hasn&#39;t been set yet. But on the usual \nscope-rechecking (in Preselector), the URI would be rejected, and appear \nin the crawl.log as you&#39;d like.\n\nThis is a bit wasteful -- scheduling the URI involves more IO and CPU \nthan would just logging it before it is uniq-d out -- but if you prefer \nto have a single crawl.log from which your graph can be completed, it \ncould be an attractive solution.\n\n- Gordon @ IA\n\nlouisleiyu wrote:\n&gt; After I read the documentation, from my understand, heretrix \n&gt; achieves filter of duplicated pages by basically keeping a hash of \n&gt; already encountered urls; thus any newly encountered url is compared \n&gt; against the hash and it matches, it doesn&#39;t get recorded in the log, \n&gt; and it doesn&#39;t give birth to children.\n&gt; \n&gt; i wonder if I can change the writer processor so it DOES get \n&gt; recorded in the log; other than that the rest are the same (repeated \n&gt; usl are still not allowed to give birth to children)\n&gt; \n&gt; Lou\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; \n&gt; wrote:\n&gt;&gt; If I understand correctly, this analysis requires even the \n&gt; redundant \n&gt;&gt; discovered links to be saved somewhere, since they are not \n&gt; scheduled for \n&gt;&gt; redundant visitation within the same crawl.\n&gt;&gt;\n&gt;&gt; One option would be to perform a post-crawl analysis on your ARC \n&gt; files \n&gt;&gt; to re-extract the links.\n&gt;&gt;\n&gt;&gt; Another would be to insert a processor that logs the outlinks \n&gt; before \n&gt;&gt; scoping and/or scheduling have whittled them down.\n&gt;&gt;\n&gt;&gt; The current experimental WARC-writing processor uses the \n&gt; discovered \n&gt;&gt; outlinks as example metadata for the new &#39;metadata&#39; record, so a \n&gt; side \n&gt;&gt; effect of its operation is to save aside the data you want. Both \n&gt; the \n&gt;&gt; format of WARCs and the content of specific records is going to \n&gt; change, \n&gt;&gt; so I can&#39;t recommend depending on the current processor for \n&gt;&gt; functionality, but it may provide a model for other code to save \n&gt; aside \n&gt;&gt; this info.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; Andrea Goethals wrote:\n&gt;&gt;&gt; If I understand the original post correctly - this is something \n&gt; we also \n&gt;&gt;&gt; want\n&gt;&gt;&gt; to implement (logging of &quot;intra&quot;-harvest duplicates not \n&gt; downloaded).\n&gt;&gt;&gt; The heritrix 1.12 supports deduping *between* crawls but older \n&gt; heritrixs\n&gt;&gt;&gt; dedupe *within* crawls already. Ideally we would like to see \n&gt; logging \n&gt;&gt;&gt; options\n&gt;&gt;&gt; for both kinds of deduplication. We want this intra-harvest \n&gt; dedupe logging\n&gt;&gt;&gt; so that we can know all the parents seen for downloaded \n&gt; resources - not \n&gt;&gt;&gt; just\n&gt;&gt;&gt; the one first parent that currently gets logged in crawl.log.\n&gt;&gt;&gt;\n&gt;&gt;&gt; I haven&#39;t yet looked into where the extra logging should go - \n&gt; just \n&gt;&gt;&gt; wanted to\n&gt;&gt;&gt; add to this thread that this is something we want too & are \n&gt; willing to\n&gt;&gt;&gt; implement (if there&#39;s not already a way to log this) because it \n&gt; will effect\n&gt;&gt;&gt; how we implement our harvest q/a and takedown request handling.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Andrea\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 11 Apr 2007 06:12:44 -0700, mbarlotta &lt;barlotta_michael@...&gt; \n&gt; wrote:\n&gt;&gt;&gt;&gt;   &gt; I&#39;ve tried playing around with the setting for herdrix \n&gt; 1.1.2 but I&#39;m\n&gt;&gt;&gt;&gt;&gt; getting nowhere; I&#39;ve read that older version of heredrix \n&gt; does not\n&gt;&gt;&gt;&gt;&gt; have the ability to filter out duplicate pages, so perhaps I \n&gt; should\n&gt;&gt;&gt;&gt;&gt; try older versions of heredrix?\n&gt;&gt;&gt;&gt; The current version of Heritrix does not dedupe pages by \n&gt; default you\n&gt;&gt;&gt;&gt; would have to configure the job with additional processors to \n&gt; get it to\n&gt;&gt;&gt;&gt; do that. If you crawl sites without dedupe you get what ever \n&gt; your\n&gt;&gt;&gt;&gt; decide rules allow.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Read more about it here:\n&gt;&gt;&gt;&gt;\n&gt; http://webteam.archive.org/confluence/display/Heritrix/Feature+Notes+\n&gt; -\n&gt;&gt;&gt;&gt; +1.12.0\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; What are you using to visualize your Web Graph?\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; HTH,\n&gt;&gt;&gt;&gt; Mike\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;  \n&gt;&gt;&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}