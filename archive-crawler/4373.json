{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"ulfWt1q4bpb9b2yUAj9BWGzRxXQ20G_h1hu_nlG68zLWyQvLxquQVes9XzlNwyry5qJvH7GPsj0B6FpmBALDYIBCy6ZWnSU","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling","postDate":"1182966908","msgId":4373,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODJBNDdDLjgwNTA4MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDkyOTI4NS41MDQ4Ny5xbUB3ZWI1MDMwNi5tYWlsLnJlMi55YWhvby5jb20+","referencesHeader":"PDkyOTI4NS41MDQ4Ny5xbUB3ZWI1MDMwNi5tYWlsLnJlMi55YWhvby5jb20+"},"prevInTopic":4368,"nextInTopic":4379,"prevInTime":4372,"nextInTime":4374,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"... Yes; that is the usual case for distributed crawling: each of the crawlers has the exact same initial configuration, *except* for the HashCrawlMapper","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 18933 invoked from network); 27 Jun 2007 17:56:18 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m47.grp.scd.yahoo.com with QMQP; 27 Jun 2007 17:56:18 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta11.grp.scd.yahoo.com with SMTP; 27 Jun 2007 17:56:18 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 5B12814156B06\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 27 Jun 2007 10:54:56 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 25245-07-22 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 27 Jun 2007 10:54:55 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id C9BBE14156ABD\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 27 Jun 2007 10:54:55 -0700 (PDT)\r\nMessage-ID: &lt;4682A47C.8050805@...&gt;\r\nDate: Wed, 27 Jun 2007 10:55:08 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (X11/20070604)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;929285.50487.qm@...&gt;\r\nIn-Reply-To: &lt;929285.50487.qm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=137285340; y=R6EPmY7uRdFC1RxYxTVsSg-BRjB4dZO7mNLs_qRX_a8r\r\nX-Yahoo-Profile: gojomo\r\n\r\nJigar Patel wrote:\n&gt; Thanks a lot Gordon,\n&gt;    \n&gt;   You solved my problem.\n&gt;    \n&gt;   One more thing I want to know that, \n&gt;   Can I use same settings for two different independent machines for distributed crawling ?\n\nYes; that is the usual case for distributed crawling: each of the \ncrawlers has the exact same initial configuration, *except* for the \nHashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to \ndecline to process URIs mapped to the others.\n\nThere is no automatic facility for sharing the settings; you must \nmanually copy the order.xml (and possibly other files) between crawlers. \nAlso, there is no automatic facility for cross-feeding the URIs each \ncrawler rejects. You must watch the crawl.log (or diversions logs from \nthe mapper) and decide if the URIs should be fed to the sibling crawlers.\n\n- Gordon @ IA\n\n&gt;   Please tell me how does it work and coordinate with different machine ?\n&gt;    \n&gt;   Thanks\n&gt;    \n&gt;   Jigar\n&gt; \n&gt; Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;           Jigar Patel wrote:\n&gt;&gt; Presently I am running two heritrix instances on the same machine on \n&gt;&gt; different port...\n&gt; \n&gt; In general, you only want to use distributed crawling, with URIs \n&gt; partitioned across separate cooperating crawlers, to spread a crawl over \n&gt; multiple independent machines. If using a single machine, a single \n&gt; crawler instance will be more efficient.\n&gt; \n&gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule\n&gt;&gt; I added HashCrawlMapper at two places as you suggested\n&gt;&gt; I made same configuration setting and seed file at each place.\n&gt;&gt;\n&gt;&gt; But as I run my job it gives me following error in seed file and \n&gt;&gt; nothing was crawled.\n&gt;&gt;\n&gt;&gt; Heritrix(-5002)-Blocked by custom prefetch processor \n&gt;&gt;\n&gt;&gt; Please let me know why I am getting such error...\n&gt;&gt;\n&gt;&gt; Is anything missing ?\n&gt; \n&gt; This is the expected crawl.log result for URIs that are considered by a \n&gt; crawler, but mapped to be handled by one of the others in the group of \n&gt; crawlers. With a proper configuration, some but not all lines in your \n&gt; crawl.log will have this code.\n&gt; \n&gt; For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and \n&gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; of &#39;2&#39;.\n&gt; \n&gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, but \n&gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the \n&gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the \n&gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt;&gt; Regards,\n&gt;&gt;\n&gt;&gt; Jigar Patel\n&gt;&gt;\n&gt;&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; \n&gt;&gt; wrote:\n&gt;&gt;&gt; nt_bdr wrote:\n&gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n&gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we \n&gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600 \n&gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8 \n&gt;&gt; machines \n&gt;&gt;&gt; (&gt;1 billion URLs fetched).\n&gt;&gt;&gt;\n&gt;&gt;&gt; For background see some previous threads including:\n&gt;&gt;&gt;\n&gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2909\n&gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/3060\n&gt;&gt;&gt;\n&gt;&gt;&gt; Roughly how we do it:\n&gt;&gt;&gt;\n&gt;&gt;&gt; - Use BloomFilterUriUniqFilter with its defaults -- which devotes \n&gt;&gt;&gt; about 500MB to this structure and keeps the false-positive \n&gt;&gt; (mistakenly \n&gt;&gt;&gt; believed to have been previously-scheduled) rate under 1-in-4-\n&gt;&gt; million up \n&gt;&gt;&gt; through 125 million URIs discovered.\n&gt;&gt;&gt;\n&gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+ \n&gt;&gt; heap\n&gt;&gt;&gt; - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in \n&gt;&gt; queues \n&gt;&gt;&gt; named by the reversed-host (com,example,) rather than usual host \n&gt;&gt;&gt; (example.com)\n&gt;&gt;&gt;\n&gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor \n&gt;&gt; chain:\n&gt;&gt;&gt; * Once, immediately before the PreconditionEnforcer. This one \n&gt;&gt; has \n&gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any \n&gt;&gt; scheduled \n&gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)\n&gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has \n&gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any \n&gt;&gt; discovered \n&gt;&gt;&gt; outlinks before they are scheduled.)\n&gt;&gt;&gt;\n&gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a \n&gt;&gt; number 0 \n&gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and \n&gt;&gt; all \n&gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers, \n&gt;&gt; n).\n&gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the \n&gt;&gt; SURT \n&gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI \n&gt;&gt; is \n&gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to \n&gt;&gt; a \n&gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally. \n&gt;&gt;&gt; Depending on the character of your crawl, you may want to feed \n&gt;&gt; these \n&gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore \n&gt;&gt; them.\n&gt;&gt;&gt; The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key \n&gt;&gt; before \n&gt;&gt;&gt; mapping -- used to ensure that all subdomains of example.com are \n&gt;&gt; treated \n&gt;&gt;&gt; the same as example.com for mapping purposes. The first match of \n&gt;&gt; this \n&gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small \n&gt;&gt;&gt; example would be:\n&gt;&gt;&gt;\n&gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n&gt;&gt;&gt;\n&gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything \n&gt;&gt; through \n&gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it \n&gt;&gt; uses \n&gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns \n&gt;&gt; that \n&gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs, \n&gt;&gt; like \n&gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level \n&gt;&gt;&gt; domains.)\n&gt;&gt;&gt;\n&gt;&gt;&gt; - All crawlers are launched with the same configuration, \n&gt;&gt; including \n&gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate. \n&gt;&gt; Seeds \n&gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early \n&gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed \n&gt;&gt; are \n&gt;&gt;&gt; done so by an external process/scripts.\n&gt;&gt;&gt;\n&gt;&gt;&gt; - Gordon @ IA\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt;          \n&gt; \n&gt;  \n&gt; ---------------------------------\n&gt; Food fight? Enjoy some healthy debate\n&gt; in the Yahoo! Answers Food & Drink Q&A.\n\n\n"}}