{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Va4K0-zEP1iRIWfVYO-6LdEQl_jjp7Q7h9dwYXGGYX7WyJqzUjgLpOjOBZ5uEshpLKEKjHp59FUlyd-9H93FdjD9vnOqoi0","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] about bbs&#39;recrawl","postDate":"1300742556","msgId":7068,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREODdDMTlDLjYwMjA0MDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIwMTEwMzEwMTQxNTE0ODYwMzc1OEBnbWFpbC5jb20+","referencesHeader":"PDIwMTEwMzEwMTQxNTE0ODYwMzc1OEBnbWFpbC5jb20+"},"prevInTopic":7059,"nextInTopic":0,"prevInTime":7067,"nextInTime":7069,"topicId":7059,"numMessagesInTopic":2,"msgSnippet":"With the existing duplication-reduction functionality, there are two main ways that you can be sure your recrawls check every URI of interest: (1) Turn off the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 62123 invoked from network); 21 Mar 2011 21:22:30 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m14.grp.re1.yahoo.com with QMQP; 21 Mar 2011 21:22:30 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta2.grp.re1.yahoo.com with SMTP; 21 Mar 2011 21:22:30 -0000\r\nX-Received: (qmail 78388 invoked by uid 0); 21 Mar 2011 21:22:29 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay00.pair.com with SMTP; 21 Mar 2011 21:22:29 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4D87C19C.6020408@...&gt;\r\nDate: Mon, 21 Mar 2011 14:22:36 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.15) Gecko/20110303 Thunderbird/3.1.9\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;mickey.guoyun&quot; &lt;mickey.guoyun@...&gt;\r\nReferences: &lt;201103101415148603758@...&gt;\r\nIn-Reply-To: &lt;201103101415148603758@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] about bbs&#39;recrawl\r\nX-Yahoo-Group-Post: member; u=137285340; y=i1-TmQ7S7D1l11LYzeiXMZtrNwQUDybHE77gNc6C0abf\r\nX-Yahoo-Profile: gojomo\r\n\r\nWith the existing duplication-reduction functionality, there are two \nmain ways that you can be sure your recrawls check every URI of interest:\n\n(1) Turn off the &#39;conditional-GET&#39; (header-sensitive, bandwidth-saving) \nfeatures, so that the crawler is still always fetching (and thus \nlink-extracting) content. In this case, the recrawls essentially \nre-trace the steps of the original crawl. Still, you&#39;ll save storage by \nnot saving duplicate content to disk.\n\n(2) Feed all URIs of interest from the earlier crawls back to the later \ncrawls by crawl design. (For example, as seeds or other mid-crawl URI \nloads.)\n\nThis is mentioned in the &quot;Considerations&quot; section of the H3 \ndocumentation of this feature, though it also applies to using this \nfeature in H1. See:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/Duplication+Reduction+Processors#DuplicationReductionProcessors-Considerationswhendesigningacrawlusingduplicationreduction\n\nA future improvement we&#39;ve discussed but not yet slotted for a \nparticular release would be to offer the ability to automatically \nreconsider all known URIs, or some meaningful subset; or perhaps to \nremember outlink data in crawl-history so that even without re-fetching \nand re-extracting the same content, the same outlinks can be followed in \nlater crawls. Until then, one of the two techniques above should be used.\n\n- Gordon @ IA\n\nOn 3/9/11 10:15 PM, mickey.guoyun wrote:\n&gt;\n&gt;\n&gt; Hi,every one.when i crawl bbs,i found a question,like this:\n&gt; if i want to crawl www.examle.com/bbs/topic1.html,it\n&gt; &lt;http://www.examle.com/bbs/topic1.html,it&gt; have a lot of replies,and\n&gt; these replies distribution in many pages,for example:\n&gt; // url:www.examle.com/bbs/topic1.html\n&gt; &lt;http://www.examle.com/bbs/topic1.html,it&gt;\n&gt; &lt;html&gt;\n&gt; &lt;head&gt;\n&gt; &lt;title&gt;topic&lt;/title&gt;\n&gt; &lt;/head&gt;\n&gt; &lt;body&gt;\n&gt; // topic context\n&gt; // reply context(floor 1)\n&gt; // reply context(floor2)\n&gt; // ...\n&gt; // reply context(flloor 100)\n&gt; // &lt;a href=&#39;&#39;reply context (page 2)&quot;&gt;2&lt;/a&gt;\n&gt; // &lt;a href=&quot;reply context(page 3)&quot;&gt;3&lt;/a&gt;\n&gt; // ...\n&gt; // &lt;a href=&quot;reply context(page 100)&quot;&gt;100&lt;/a&gt;\n&gt; &lt;/body&gt;\n&gt; &lt;/html&gt;\n&gt; At the first time,we crawl this url(www.examle.com/bbs/topic1.html),and\n&gt; we &lt;http://www.examle.com/bbs/topic1.html),and we&gt; can crawl all of its\n&gt; reply urls(&lt;a href=&#39;&#39;reply context (page 2)&quot;&gt;2&lt;/a&gt;,&lt;a href=&#39;&#39;reply\n&gt; context (page 3)&quot;&gt;3&lt;/a&gt;)\n&gt; but when we recrawl use heritrix,if the\n&gt; url(www.examle.com/bbs/topic1.html\n&gt; &lt;http://www.examle.com/bbs/topic1.html&gt;) had processed,and its\n&gt; etag(last-modified-time,length) and context md5 have not modified,so it\n&gt; will tell crawler that it do not need to recrawl.and it&#39;s reply pages(a\n&gt; href=&#39;&#39;reply context (page 2)&quot;&gt;2&lt;/a&gt;) will have no chance to recrawl.but\n&gt; when the url have new reply(it will find in the last reply url),we can\n&gt; not crawl it.\n&gt; And now,i use this way to resolve this problem:\n&gt; i store the url(www.examle.com/bbs/topic1.html)&#39;s\n&gt; &lt;http://www.examle.com/bbs/topic1.html)&#39;s&gt; last reply page url in\n&gt; database.and from this reply url to start recrawl.when i find have new\n&gt; reply page url,i will replace it.and the next time ,from the new reply\n&gt; url to start recrawl.\n&gt; i do not know whether it is a good way to resove this problem,because it\n&gt; will maked a lot of change in Heritrix,any body have other way?\n&gt; 2011-03-10\n&gt; ------------------------------------------------------------------------\n&gt; mickey.guoyun\n&gt;\n&gt;\n&gt; \n\n"}}