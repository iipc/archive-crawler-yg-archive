{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":225011788,"authorName":"Karl Wright","from":"Karl Wright &lt;kwright@...&gt;","profile":"daddywri","replyTo":"LIST","senderId":"6dbUFZ7-97_Y5aXWHNwYofco2MeCCNhKx6_4lS-0qFab_loKeJ8v7MaKc2VbcPFbB_XiByhsFGGSbE5CCQioIQcO3NucPTtxzgw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Politeness proposal, and CrawlHost class","postDate":"1138046600","msgId":2580,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzRDUzNjg4LjQwNTAwMDVAbWV0YWNhcnRhLmNvbT4=","inReplyToHeader":"PDQzRDUyRTdCLjIwOTA1MDVAYXJjaGl2ZS5vcmc+","referencesHeader":"PDQzRDEzRDU5LjgwOTA3MDBAbWV0YWNhcnRhLmNvbT4gPDQzRDUyRTdCLjIwOTA1MDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":2578,"nextInTopic":2581,"prevInTime":2579,"nextInTime":2581,"topicId":2574,"numMessagesInTopic":5,"msgSnippet":"... Actually, that s the assumption that seems to be false. We ve had complaints from webmasters who were being crawled slowly and steadily over a 4 day","rawEmail":"Return-Path: &lt;kwright@...&gt;\r\nX-Sender: kwright@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 52006 invoked from network); 23 Jan 2006 20:05:36 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m22.grp.scd.yahoo.com with QMQP; 23 Jan 2006 20:05:36 -0000\r\nReceived: from unknown (HELO metacarta.com) (65.77.47.18)\n  by mta1.grp.scd.yahoo.com with SMTP; 23 Jan 2006 20:05:36 -0000\r\nReceived: from localhost (silene.metacarta.com [65.77.47.24])\n\tby metacarta.com (Postfix) with ESMTP id A2EAF5181BD\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 23 Jan 2006 15:02:03 -0500 (EST)\r\nReceived: from metacarta.com ([65.77.47.18])\n\tby localhost (silene.metacarta.com [65.77.47.24]) (amavisd-new, port 10024)\n\twith ESMTP id 21530-03 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 23 Jan 2006 15:02:03 -0500 (EST)\r\nReceived: from [192.168.1.104] (146-115-66-62.c3-0.lex-ubr1.sbo-lex.ma.cable.rcn.com [146.115.66.62])\n\tby metacarta.com (Postfix) with ESMTP id AC126518113\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 23 Jan 2006 15:02:02 -0500 (EST)\r\nMessage-ID: &lt;43D53688.4050005@...&gt;\r\nDate: Mon, 23 Jan 2006 15:03:20 -0500\r\nUser-Agent: Mozilla Thunderbird 1.0.2 (Windows/20050317)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;43D13D59.8090700@...&gt; &lt;43D52E7B.2090505@...&gt;\r\nIn-Reply-To: &lt;43D52E7B.2090505@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: by amavisd-new-20030616-p10 (Debian) at metacarta.com\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Karl Wright &lt;kwright@...&gt;\r\nSubject: Re: [archive-crawler] Politeness proposal, and CrawlHost class\r\nX-Yahoo-Group-Post: member; u=225011788; y=yWberJXFtUB0zR6tBI1rweLuDn70LW9Eyngzjk94YAnM6_8\r\nX-Yahoo-Profile: daddywri\r\n\r\nGordon Mohr (archive.org) wrote:\n&gt; Karl Wright wrote:\n&gt; \n&gt;&gt;Hi,\n&gt;&gt;\n&gt;&gt;We encountered a new politeness complaint recently.  Basically, the \n&gt;&gt;proprietor noticed our crawl not because of excessive bandwidth \n&gt;&gt;consumption or even number of hits, but because we just continued to \n&gt;&gt;steadily read documents over 4 days.\n&gt;&gt;\n&gt;&gt;It seems like it might be therefore a good idea to have a new politeness \n&gt;&gt;parameter set which would limit the total fetches in a specified period \n&gt;&gt;of time.  I&#39;m looking for advice on implementing such a politeness feature.\n&gt;&gt;\n&gt;&gt;The problem basically is where the fetch statistics over the last n days \n&gt;&gt;should be kept.  The CrawlHost class seems an obvious choice.  However, \n&gt;&gt;I believe that this class&#39; size is a major determiner of the total \n&gt;&gt;amount of memory Heritrix requires for a crawl.  Is this true?  Or, if I \n&gt;&gt;made CrawlHost significantly larger, should I expect there to be little \n&gt;&gt;impact on the memory requirements of the crawler?\n&gt; \n&gt; \n&gt; CrawlHost instances are managed by the ServerCache -- which uses a\n&gt; SoftReference-based scheme to let unreferenced CrawlHost instances\n&gt; leave main memory, and only brings them back when they are again\n&gt; needed.\n&gt; \n&gt; So, you don&#39;t need to worry too much about their instance sizes. The\n&gt; same goes for CrawlServer instances -- which exist one per host+port\n&gt; (and cache robots info), as opposed to CrawlHosts which exist one per\n&gt; hostname (and cache DNS info).\n&gt; \n&gt; Are you sure the target site&#39;s needs can&#39;t be met simply by increasing\n&gt; the existing politeness delays? While those settings don&#39;t offer a\n&gt; strict documents-per-period quota, they do offer enough flexibility to\n&gt; reduce the number of documents fetched by any desired factor.\n&gt; \n&gt; For example, doubling the &#39;delay-factor&#39;, &#39;min-delay-ms&#39;, and\n&gt; &#39;max-delay-ms&#39; settings should roughly cut in half the number of\n&gt; requests against a site.\n&gt; \n&gt; (I presume that it&#39;s generally better to space requests out evenly, rather\n&gt; than pulse on and off. That is, if you&#39;re going to hit a site 4K times\n&gt; over a four-day period, it&#39;s be better to do 1K a day, roughly 40 every\n&gt; hour, than to do 2K every other day, and 0 on the off days.)\n&gt; \n\nActually, that&#39;s the assumption that seems to be false.\nWe&#39;ve had complaints from webmasters who were being crawled slowly and \nsteadily over a 4 day period.  Although the rate of crawl did not exceed \none document per 38 seconds, and the maximum bandwidth used did not \nexceed 133 bytes per second, we still got complained at, because we did \nnot LOOK like a user - we looked like a crawler.  We concluded that we \nneeded to look more &quot;bursty&quot;, with a random amount between bursts, to \nseem like a real user.\n\n\n&gt; The key method for including alternate information about politeness\n&gt; delays is AbstractFrontier.politenessDelayFor(). Considering your\n&gt; need, this method could be changed to look inside the CrawlURI to\n&gt; see if an earlier step/Processor had already calculated a delay, and\n&gt; use that rather than the default calculation. That would make it\n&gt; easier for custom delays to be effected without editting/overriding\n&gt; the core Frontier code. If you need this change, let us know and I&#39;ll\n&gt; look into the possibility further.\n\nI&#39;d need to figure out the delay based on some time window into the past \n(e.g. 2 days, settable), and the number of fetches over that time \nperiod.  I don&#39;t know of any current data structure in Heritrix where \nthis kind of information can be tracked or attached - any ideas?\n\nThanks,\nKarl\n\n&gt; \n&gt; Hope this helps,\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}