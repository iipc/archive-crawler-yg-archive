{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":418461621,"authorName":"olintocattaneo","from":"&quot;olintocattaneo&quot; &lt;olintocattaneo@...&gt;","profile":"olintocattaneo","replyTo":"LIST","senderId":"JSwQXbStjra8wcroimUAXDmCWF30qoe2S4q2pluFfIiCyMOS0ClVD-limNcw7OAs4M2rSnqVBO5b-y2mCTq3oEj7Jb4wFlCvXRE8R5pkKfUZyg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Question about QueueOverbudgetDecideRule","postDate":"1257165520","msgId":6138,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhjbWpzZytpZXU3QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGhicWM5NCtjc3JlQGVHcm91cHMuY29tPg=="},"prevInTopic":6126,"nextInTopic":6144,"prevInTime":6137,"nextInTime":6139,"topicId":6126,"numMessagesInTopic":3,"msgSnippet":"Replying to myself just in case anyone competent missed this. Olinto","rawEmail":"Return-Path: &lt;olintocattaneo@...&gt;\r\nX-Sender: olintocattaneo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 30192 invoked from network); 2 Nov 2009 12:40:04 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m14.grp.re1.yahoo.com with QMQP; 2 Nov 2009 12:40:04 -0000\r\nX-Received: from unknown (HELO n5-vm6.bullet.mail.sp2.yahoo.com) (67.195.135.101)\n  by mta3.grp.sp2.yahoo.com with SMTP; 2 Nov 2009 12:40:04 -0000\r\nX-Received: from [67.195.134.49] by n5.bullet.mail.sp2.yahoo.com with NNFMP; 02 Nov 2009 12:38:42 -0000\r\nX-Received: from [69.147.65.172] by t2.bullet.mail.sp2.yahoo.com with NNFMP; 02 Nov 2009 12:38:41 -0000\r\nX-Received: from [98.137.34.34] by t14.bullet.mail.sp1.yahoo.com with NNFMP; 02 Nov 2009 12:38:41 -0000\r\nDate: Mon, 02 Nov 2009 12:38:40 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;hcmjsg+ieu7@...&gt;\r\nIn-Reply-To: &lt;hbqc94+csre@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;olintocattaneo&quot; &lt;olintocattaneo@...&gt;\r\nSubject: Re: Question about QueueOverbudgetDecideRule\r\nX-Yahoo-Group-Post: member; u=418461621; y=urlczijTV2BSzgm1YU2CMO4i-EdTvnX8hwTCwqMEs_egVka1TSrCGNc\r\nX-Yahoo-Profile: olintocattaneo\r\n\r\nReplying to myself just in case anyone competent missed this.\n\nOlinto\n\n--- =\r\nIn archive-crawler@yahoogroups.com, &quot;olintocattaneo&quot; &lt;olintocattaneo@...&gt; w=\r\nrote:\n&gt;\n&gt; Hello\n&gt; \n&gt; I&#39;m trying to get QueueOverbudgetDecideRule to work bu=\r\nt I don&#39;t seem to be able to do this. Is this module still functional or ma=\r\nybe I have added it to a wrong place?\n&gt; \n&gt; Here is my order file:\n&gt; http://=\r\nihave.bushiq.com/stuff/order_20091022065616.xml\n&gt; \n&gt; What I want to accompl=\r\nish is that I just want to crawl 5 pages from each host. I tried QuotaEnfor=\r\ncer initially but this module is really inefficient since when it finds new=\r\n links to a host that has reached it&#39;s quota it will still try to check the=\r\nm out from&quot;already-seen&quot; database, will add them to queue if there are none=\r\n and when the queue goes active and it doesn&#39;t find them it will write them=\r\n to log file. This means that the crawling is using unnecessary amount of r=\r\nesources. \n&gt; \n&gt; If I want to crawl 5 pages from each domain it should do ju=\r\nst that - 1. When extracting links from URL check if domain is already in t=\r\nhe already-seen database, if it is check if it has reached quota, when it i=\r\ns not then add the links to queue but if it has then just drop the or write=\r\n them to log file(would be nice to be able to specify this too).\n&gt; \n&gt; I&#39;m t=\r\nhinking that this is not possible right now and although I have spent weeks=\r\n researching this very fine crawler it seems that it is not possible, maybe=\r\n I&#39;m just doing something wrong though. I can achieve this behavior with Mn=\r\nogosearch but compared to Heritrix it is not scalable and flexible enough f=\r\nor me.\n&gt; \n&gt; I&#39;m sure there are other people too who are interested about co=\r\nnfiguring Heritrix this way since limiting URL&#39;s per host/domain is somethi=\r\nng everyone would probably want to do and I&#39;m sure that they are already do=\r\ning this but they might be doing this as inefficiently as me.\n&gt; \n&gt; Regards\n=\r\n&gt; \n&gt; Olinto\n&gt;\n\n\n\n"}}