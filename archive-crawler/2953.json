{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234926651,"authorName":"Frank McCown","from":"&quot;Frank McCown&quot; &lt;fmccown@...&gt;","profile":"mccownf","replyTo":"LIST","senderId":"tBzGFYUnahzJ4x7zkobNsXvKb_esvurkYk2WmBvJvCQClWPfmVJVzCeTkubZAuhn6ReX-YEQ7fUYtYsSogtvYVKWSWNloP1rFW8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Using Heritrix instead of Wget","postDate":"1150840278","msgId":2953,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU3OXFrbStpZnZtQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":2956,"prevInTime":2952,"nextInTime":2954,"topicId":2953,"numMessagesInTopic":11,"msgSnippet":"I’m a phd student at Old Dominion Univ doing research involving web crawling.  I’ve been using wget in my experiments so far, but I am hoping to find a","rawEmail":"Return-Path: &lt;fmccown@...&gt;\r\nX-Sender: fmccown@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 45751 invoked from network); 20 Jun 2006 21:52:04 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m32.grp.scd.yahoo.com with QMQP; 20 Jun 2006 21:52:04 -0000\r\nReceived: from unknown (HELO n20a.bullet.scd.yahoo.com) (66.94.237.49)\n  by mta10.grp.scd.yahoo.com with SMTP; 20 Jun 2006 21:52:04 -0000\r\nReceived: from [66.218.69.3] by n20.bullet.scd.yahoo.com with NNFMP; 20 Jun 2006 21:51:18 -0000\r\nReceived: from [66.218.66.89] by t3.bullet.scd.yahoo.com with NNFMP; 20 Jun 2006 21:51:18 -0000\r\nDate: Tue, 20 Jun 2006 21:51:18 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;e79qkm+ifvm@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;Frank McCown&quot; &lt;fmccown@...&gt;\r\nSubject: Using Heritrix instead of Wget\r\nX-Yahoo-Group-Post: member; u=234926651; y=Vq25MKvLe_EcCG6WUTqFdLNaIgnD61tqvyFxpLsKnS6bnQ\r\nX-Yahoo-Profile: mccownf\r\n\r\nI=E2=80=99m a phd student at Old Dominion Univ doing research involving web=\r\n\ncrawling.  I=E2=80=99ve been using wget in my experiments so far, but I am=\r\n\nhoping to find a crawler that will do a little bit more for me.  Here\nare =\r\nsome points that are important for me:\n\n- run more than job at the same tim=\r\ne (wget and heritrix allow multiple\ninstances to run at the same time)\n- li=\r\nmit the download of a file to 5 MB (wget can=E2=80=99t do this)\n- download =\r\nall links (wget can=E2=80=99t read links to images in CSS, heritrix\ncan als=\r\no find links in PDF, MS Office, etc. files)\n- support for crawler traps (wg=\r\net has no support)\n- delay between requests that takes into account the tim=\r\ne to download\nthe last page (wget only has static delay between requests)\n\n=\r\nThe question is, how much control will Heritrix give me when running\nit fro=\r\nm the command line?  I saw in the FAQ that there is a &quot;cmdline\ncontrol util=\r\nity&quot;.  Can I launch the tool from a script and then be\nnotified that a craw=\r\nl job has successfully completed?  If the crawler\nis stuck in a crawler tra=\r\np or if I just want to see what is left in\nthe crawl frontier, can I launch=\r\n the WUI to check on the status in the\nmiddle of the crawl?\n\nThanks,\nFrank\n=\r\n\n\n\n\n"}}