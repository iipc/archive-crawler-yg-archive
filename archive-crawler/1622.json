{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":215515511,"authorName":"Mike Schwartz","from":"Mike Schwartz &lt;mschwartz@...&gt;","profile":"mfschwartz","replyTo":"LIST","senderId":"_5lRWi-2qq8sqFbAnvs9fqj4gGRmgdVciFG9wLwxjwPWWpkVyHXNiEeit--S6sVT5DR88XuOhaaZSnM3StSp9Z-wyve6UjYru1E","spamInfo":{"isSpam":false,"reason":"0"},"subject":"keeping threads active during crawls","postDate":"1109624744","msgId":1622,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYuMi4wLjE0LjIuMjAwNTAyMjgxNDA1MzAuMDJiM2YxZThAdmhvc3Q2LmF0b21pY3NlcnZlcnMuY29tPg=="},"prevInTopic":0,"nextInTopic":1632,"prevInTime":1621,"nextInTime":1623,"topicId":1622,"numMessagesInTopic":8,"msgSnippet":"hi, I need to run a series of DomainScope crawls.  I notice that each such crawl gets good thread parallelism for quite a while but then gets to a point where","rawEmail":"Return-Path: &lt;mschwartz@...&gt;\r\nX-Sender: mschwartz@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 70874 invoked from network); 28 Feb 2005 21:05:57 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m23.grp.scd.yahoo.com with QMQP; 28 Feb 2005 21:05:56 -0000\r\nReceived: from unknown (HELO vhost6.atomicservers.com) (216.58.160.194)\n  by mta2.grp.scd.yahoo.com with SMTP; 28 Feb 2005 21:05:56 -0000\r\nReceived: from dev4lt.aptas.com (nat1.aptas.com [64.78.237.254] (may be forged))\n\t(authenticated (0 bits))\n\tby vhost6.atomicservers.com (8.11.6/8.11.6) with ESMTP id j1SL5t115477\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 28 Feb 2005 14:05:55 -0700\r\nMessage-Id: &lt;6.2.0.14.2.20050228140530.02b3f1e8@...&gt;\r\nX-Mailer: QUALCOMM Windows Eudora Version 6.2.0.14\r\nDate: Mon, 28 Feb 2005 14:05:44 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nMime-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;=====================_1299545593==.ALT&quot;\r\nX-eGroups-Remote-IP: 216.58.160.194\r\nFrom: Mike Schwartz &lt;mschwartz@...&gt;\r\nSubject: keeping threads active during crawls\r\nX-Yahoo-Group-Post: member; u=215515511\r\nX-Yahoo-Profile: mfschwartz\r\n\r\n\r\n--=====================_1299545593==.ALT\r\nContent-Type: text/plain; charset=&quot;us-ascii&quot;; format=flowed\r\n\r\nhi,\n\nI need to run a series of DomainScope crawls.  I notice that each such \ncrawl gets good thread parallelism for quite a while but then gets to a \npoint where most of what&#39;s left to do is crawling many pages within a small \nnumber of sites (e.g., all the product pages at each of a few sites).  At \nthat point only a few points are active, and make very slow progress \nbecause I only visit each site once every few seconds.\n\nThis problem wouldn&#39;t arise if I were doing a broad-scope crawl, since at \nany point in time there are more sites left to crawl than there are \navailable threads.\n\nDoes anyone have a suggestion how I could keep most/all of the threads \nactive during a sequence of DomainScope crawls?  I could try adding a new \nset of sites when I get down to the state of many pages left within just a \nfew sites, but it seems to me that&#39;s a problem because in essence I&#39;m \nrunning one much larger crawl instead of a set of limited scope crawls - \nand if that crawl gets into a bad state and I have to kill it, I don&#39;t end \nup knowing exactly which parts of which sites have completed being crawled.\n\n(I&#39;ve tried restarting crawls from recover.gz, but haven&#39;t been sucessful \nwith that - I get out-of-memory errors, even when I set the JVM to have 1 \nGB of RAM or more.)\n\nthanks\n  - Mike Schwartz\n    Aptas, Inc. \r\n--=====================_1299545593==.ALT\r\nContent-Type: text/html; charset=&quot;us-ascii&quot;\r\n\r\n&lt;html&gt;\n&lt;body&gt;\n&lt;font size=3&gt;hi,&lt;br&gt;&lt;br&gt;\nI need to run a series of DomainScope crawls.&nbsp; I notice that each\nsuch crawl gets good thread parallelism for quite a while but then gets\nto a point where most of what&#39;s left to do is crawling many pages within\na small number of sites (e.g., all the product pages at each of a few\nsites).&nbsp; At that point only a few points are active, and make very\nslow progress because I only visit each site once every few\nseconds.&lt;br&gt;&lt;br&gt;\nThis problem wouldn&#39;t arise if I were doing a broad-scope crawl, since at\nany point in time there are more sites left to crawl than there are\navailable threads.&lt;br&gt;&lt;br&gt;\nDoes anyone have a suggestion how I could keep most/all of the threads\nactive during a sequence of DomainScope crawls?&nbsp; I could try adding\na new set of sites when I get down to the state of many pages left within\njust a few sites, but it seems to me that&#39;s a problem because in essence\nI&#39;m running one much larger crawl instead of a set of limited scope\ncrawls - and if that crawl gets into a bad state and I have to kill it, I\ndon&#39;t end up knowing exactly which parts of which sites have completed\nbeing crawled.&lt;br&gt;&lt;br&gt;\n(I&#39;ve tried restarting crawls from recover.gz, but haven&#39;t been sucessful\nwith that - I get out-of-memory errors, even when I set the JVM to have 1\nGB of RAM or more.)&lt;br&gt;&lt;br&gt;\nthanks&lt;br&gt;\n&nbsp;- Mike Schwartz&lt;br&gt;\n&nbsp;&nbsp; Aptas, Inc.&lt;/font&gt;&lt;/body&gt;\n&lt;/html&gt;\n\r\n--=====================_1299545593==.ALT--\r\n\n"}}