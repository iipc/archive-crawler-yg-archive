{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264783887,"authorName":"pbaclace","from":"&quot;pbaclace&quot; &lt;pbaclace@...&gt;","profile":"pbaclace","replyTo":"LIST","senderId":"NjwRxfpsIQu11Z_EABSJpoL4KXK0fkU0EJTPj0itnBwF1NZYtYjxLbglCO6ysY8M8ZztezBWAQ7AtVD03YMOD7REV4h7mA","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: lock contention in ServerCache.getServerFor()","postDate":"1235789239","msgId":5704,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdvYThqbisxMDRqcUBlR3JvdXBzLmNvbT4=","inReplyToHeader":"PGduaHBwNSt2NzBrQGVHcm91cHMuY29tPg=="},"prevInTopic":5687,"nextInTopic":5775,"prevInTime":5703,"nextInTime":5705,"topicId":5665,"numMessagesInTopic":8,"msgSnippet":"I uploaded a revised patch (for Heritrix 1.x): concurrent_ServerCache.2.patch for http://webteam.archive.org/jira/browse/HER-1609 The revised patch addresses","rawEmail":"Return-Path: &lt;pbaclace@...&gt;\r\nX-Sender: pbaclace@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 11124 invoked from network); 28 Feb 2009 02:47:19 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m54.grp.scd.yahoo.com with QMQP; 28 Feb 2009 02:47:19 -0000\r\nX-Received: from unknown (HELO n36b.bullet.mail.sp1.yahoo.com) (66.163.168.150)\n  by mta17.grp.scd.yahoo.com with SMTP; 28 Feb 2009 02:47:19 -0000\r\nX-Received: from [69.147.65.173] by n36.bullet.mail.sp1.yahoo.com with NNFMP; 28 Feb 2009 02:47:19 -0000\r\nX-Received: from [98.137.34.35] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 28 Feb 2009 02:47:19 -0000\r\nDate: Sat, 28 Feb 2009 02:47:19 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;goa8jn+104jq@...&gt;\r\nIn-Reply-To: &lt;gnhpp5+v70k@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;pbaclace&quot; &lt;pbaclace@...&gt;\r\nSubject: Re: lock contention in ServerCache.getServerFor()\r\nX-Yahoo-Group-Post: member; u=264783887; y=5I1C8UOUHiRidgFpIRG1xzBl_LFznkQ06Gqq0yuVAUC4qZc\r\nX-Yahoo-Profile: pbaclace\r\n\r\nI uploaded a revised patch (for Heritrix 1.x):\n\n   concurrent_ServerCache.2=\r\n.patch\n\nfor http://webteam.archive.org/jira/browse/HER-1609\n\nThe revised pa=\r\ntch addresses the cache coherency concerns posted by\nGordon and passes the =\r\nCachedBdbMapTest that is performed during build\nwith &quot;maven dist&quot;.  \n\nCache=\r\ndBdbMap was changed fairly heavily, but it was done very\ncarefully to prese=\r\nrve existing behavior by using fine granularity\nlocking.  CachedBdbMap.java=\r\n is now commented quite heavily, which I\nthink is appropriate for a GC-driv=\r\nen ConcurrentMap cache on top of BDB JE.\n\nCrawl pausing was tested and it w=\r\norks fine.  My tests were run on a\nsingle core machine.  Since multicore ex=\r\nercises multi-threaded\nprograms more, it would be good to have someone test=\r\n the patch with a\nmulticore machine and also test checkpoint/recovery which=\r\n is something\nI have not yet had the &quot;pleasure&quot; of using.\n\n(I don&#39;t know if=\r\n this patch will be applicable to heritrix2, but I\ncertainly hope so.)\n\n\nPa=\r\nul Baclace\n\n\n--- In archive-crawler@yahoogroups.com, &quot;pbaclace&quot; &lt;pbaclace@.=\r\n..&gt; wrote:\n&gt;\n&gt; The patch is now available at:\n&gt; \n&gt; http://webteam.archive.o=\r\nrg/jira/browse/HER-1609\n&gt; \n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, =\r\n&quot;pbaclace&quot; &lt;pbaclace@&gt; wrote:\n&gt; &gt;\n&gt; &gt; I will use the following bug report f=\r\nor the proposed patch:\n&gt; &gt; \n&gt; &gt;   http://webteam.archive.org/jira/browse/HE=\r\nR-1609\n&gt; &gt; \n&gt; &gt; The patch is not yet ready; I will post a message here when=\r\n it is.\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; Paul\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; --- In archive-crawler@yahoog=\r\nroups.com, Gordon Mohr &lt;gojomo@&gt; wrote:\n&gt; &gt; &gt;\n&gt; &gt; &gt; ServerCache has been no=\r\nted as a bottleneck before, so this is a\nvery \n&gt; &gt; &gt; welcome result. Can yo=\r\nu post a patch either here or to a JIRA\n&gt; issue for \n&gt; &gt; &gt; others to review=\r\n and test?\n&gt; &gt; &gt; \n&gt; &gt; &gt; As I&#39;d mentioned earlier in our offlist discussion,=\r\n I didn&#39;t think\n&gt; the \n&gt; &gt; &gt; simple caching approach would help much, becau=\r\nse there are already\n&gt; two \n&gt; &gt; &gt; levels of caching (CachedBDBMap&#39;s soft-re=\r\nference object-identity\n&gt; cache, \n&gt; &gt; &gt; and BDB&#39;s byte-array cache) that sh=\r\nould minimize the IO/lock-time\n&gt; when \n&gt; &gt; &gt; reading the same key multiple =\r\ntimes in sequence.\n&gt; &gt; &gt; \n&gt; &gt; &gt; It&#39;s good to see that the deeper lock-untan=\r\ngling offers such a big \n&gt; &gt; &gt; speedup for your crawl.\n&gt; &gt; &gt; \n&gt; &gt; &gt; - Gordo=\r\nn @ IA\n&gt; &gt; &gt; \n&gt; &gt; &gt; pbaclace wrote:\n&gt; &gt; &gt; &gt; The &quot;un-knotting&quot; performance c=\r\nhange worked.  I see a 2X\nspeedup in\n&gt; &gt; &gt; &gt; heritrix v1.14.2:\n&gt; &gt; &gt; &gt; \n&gt; &gt;=\r\n &gt; &gt; * 460KB/sec (from 230KB/sec) network usage\n&gt; &gt; &gt; &gt; * 100% cpu with loa=\r\nd between 15-19 (as reported by &quot;w&quot; in linux)\n&gt; &gt; &gt; &gt; * disk usage at 600KB=\r\n/sec (from 300KB/sec) \n&gt; &gt; &gt; &gt; * number of established HTTP sockets:  25 (a=\r\nn average from 13\n&gt; &gt; netstats)\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Basically, the same run to=\r\nok half the time.  As long as logging\n&gt; is not\n&gt; &gt; &gt; &gt; verbose, the TOE wor=\r\nker threads are blocked on writing the WARC\n&gt; files.\n&gt; &gt; &gt; &gt;  (Actually, th=\r\nis job writes out both WARC and ARC, so it could be\n&gt; &gt; &gt; &gt; improved.) \n&gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; &gt; The high load number might seem scary to some people, but it =\r\njust\n&gt; &gt; &gt; &gt; means the cpu is fully utilized and more cores could help.\n&gt; &gt;=\r\n &gt; &gt; \n&gt; &gt; &gt; &gt; It requires edits to 3 files plus 3 other files need trivial\n=\r\n&gt; changes.\n&gt; &gt; &gt; &gt;  The un-knotting has not yet been tested against: multi-=\r\ncore,\n&gt; &gt; &gt; &gt; multi-processor, checkpointing, and recovery.\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; =\r\n&gt; \n&gt; &gt; &gt; &gt; Paul\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; --- In archive-crawler@yahoogroup=\r\ns.com, &quot;pbaclace&quot; &lt;pbaclace@&gt;\n&gt; wrote:\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; A test run of:\n&gt; &gt;=\r\n &gt; &gt;&gt;   * Heritrix 1.14.2 on an AWS/EC2, small instance, with 100\nworker\n&gt; =\r\n&gt; &gt; &gt;&gt; threads, 1.3M seeds, 900MB heap\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Has the following =\r\nresource utilization stats:\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;   *  230KB/sec of the network=\r\n\n&gt; &gt; &gt; &gt;&gt;   * 100% cpu with load between 7 and 13\n&gt; &gt; &gt; &gt;&gt;   * disk starts =\r\nout at 300KB/sec, and 24 hours later is at\n1MB/sec\n&gt; &gt; &gt; &gt;&gt;   * number of e=\r\nstablished HTTP sockets:  ranges from 1 to 7,\n&gt; &gt; &gt; &gt;&gt; occasional spiking t=\r\no 14\n&gt; &gt; &gt; &gt;&gt;   * Full GC every 10 minutes\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; The limiting r=\r\nesource is the cpu.  A one core machine should\n&gt; &gt; &gt; &gt;&gt; theoretically be ab=\r\nle to saturate either the network or the disk\n&gt; &gt; &gt; &gt;&gt; bandwidth before the=\r\n cpu hits the wall, unless it has heavy lock\n&gt; &gt; &gt; &gt;&gt; contention. \n&gt; &gt; &gt; &gt;&gt;=\r\n\n&gt; &gt; &gt; &gt;&gt; See how many and where threads are waiting in some jstack thread\n=\r\n&gt; &gt; dumps::\n&gt; &gt; &gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.9.threaddump  |=\r\nsort\n|uniq -c\n&gt; &gt; &gt; &gt;&gt;      25         - waiting to lock &lt;0x5c357d90&gt; (a\n&gt; =\r\n&gt; &gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt; &gt; &gt; &gt;&gt;      6=\r\n1         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt; &gt; &gt;&gt; org.archive.crawler.da=\r\ntamodel.ServerCache)\n&gt; &gt; &gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.8.thre=\r\naddump  |sort\n|uniq -c\n&gt; &gt; &gt; &gt;&gt;       7         - waiting to lock &lt;0x5c357d=\r\n90&gt; (a\n&gt; &gt; &gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt; &gt; &gt; =\r\n&gt;&gt;      56         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt; &gt; &gt;&gt; org.archive.c=\r\nrawler.datamodel.ServerCache)\n&gt; &gt; &gt; &gt;&gt; # grep &#39;waiting to lock&#39; /mnt/Heritr=\r\nix.7.threaddump  |sort\n|uniq -c\n&gt; &gt; &gt; &gt;&gt;      31         - waiting to lock =\r\n&lt;0x5c357d90&gt; (a\n&gt; &gt; &gt; &gt;&gt; org.archive.crawler.postprocessor.FrontierSchedule=\r\nr)\n&gt; &gt; &gt; &gt;&gt;      62         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; &gt; &gt; &gt;&gt; org.=\r\narchive.crawler.datamodel.ServerCache)\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Examination of the=\r\n FrontierScheduler lock shows that it is\nheld in\n&gt; &gt; &gt; &gt;&gt; threaddumps 7 and=\r\n 9 by a thread waiting for ServerCache.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Most threads (abo=\r\nut 90) are waiting for a lock on ServerCache\n&gt; in the\n&gt; &gt; &gt; &gt;&gt; method:\n&gt; &gt; =\r\n&gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;   public synchronized CrawlServer getServerFor(String serverK=\r\ney)\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Presumably, a simple name to host/server would be fas=\r\nt, but one\n&gt; &gt; thread\n&gt; &gt; &gt; &gt;&gt; holds the lock while doing a relatively long=\r\n BDB read operation. \n&gt; &gt; &gt; &gt;&gt; Obviously, having disk io block all cache lo=\r\nokups is not optimal,\n&gt; &gt; &gt; &gt;&gt; especially when BDB has a lock per file (Fil=\r\neManager).  In my\ntest\n&gt; &gt; &gt; &gt;&gt; case, the bdb data is 3.6GB and there are a=\r\nbout 360 *.jdb files\n&gt; &gt; in the\n&gt; &gt; &gt; &gt;&gt; job state directory.  If requests =\r\nto getServerFor(String)\nwere not\n&gt; &gt; &gt; &gt;&gt; synchronized, then BDB should be =\r\nable to read from multiple *.jdb\n&gt; &gt; file\n&gt; &gt; &gt; &gt;&gt; at the same time and thr=\r\neads requesting entries cached in\nmemory by\n&gt; &gt; &gt; &gt;&gt; CachedBDBMap would not=\r\n need to wait.  \n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; I think the following high gain=\r\n, small code footprint\nimprovements\n&gt; &gt; &gt; &gt;&gt; would help:\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; =\r\n* superficial thread-local caching (1 affected file)\n&gt; &gt; &gt; &gt;&gt; **  the Serve=\r\nrCache lookups are done in many code locations,\nso it\n&gt; &gt; &gt; &gt;&gt; seems each t=\r\nhread processing a uri might repeatedly do the same\n&gt; &gt; lookup\n&gt; &gt; &gt; &gt;&gt; and=\r\n get stuck waiting\n&gt; &gt; &gt; &gt;&gt; **  a ThreadLocal cache of one key-value pair c=\r\nould be checked\n&gt; before\n&gt; &gt; &gt; &gt;&gt; the Maps in ServerCache.getServerFor(Stri=\r\nng) before\n&gt; synchronizing on\n&gt; &gt; &gt; &gt;&gt; this instance of ServerCache.\n&gt; &gt; &gt; =\r\n&gt;&gt; **  this must not interfere with soft reference tracking, of\ncourse\n&gt; &gt; =\r\n&gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; * deep un-knotting by lock-splitting and enabling more\n&gt; conc=\r\nurrency in\n&gt; &gt; &gt; &gt;&gt; ServerCache, CachedBdbMap, and BDB.\n&gt; &gt; &gt; &gt;&gt; ** drop sy=\r\nnchronization of ServerCache.getServerFor(String)\n&gt; &gt; &gt; &gt;&gt; ** drop synchron=\r\nization of  CachedBdbMap.get(Object)\n&gt; &gt; &gt; &gt;&gt; ** use ConcurrentHashMap for =\r\nCachedBdbMap.memMap\n&gt; &gt; &gt; &gt;&gt; ** drop synchronization of CachedBdbMap.put(K,=\r\nV) and expose\n&gt; &gt; &gt; &gt;&gt; putIfAbsent(K,V) if needed.\n&gt; &gt; &gt; &gt;&gt; *** ServerCache=\r\n.createServerFor(String) loses synchronization\nwhen\n&gt; &gt; &gt; &gt;&gt; ServerCache.ge=\r\ntServerFor(String) drops it.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; My particular crawl=\r\n job exercises the ServerCache more than most\n&gt; &gt; jobs,\n&gt; &gt; &gt; &gt;&gt; but it is =\r\nanalogous to having a very wide, breadth-first crawl. \n&gt; &gt; &gt; &gt;&gt; Characteris=\r\ntics of this performance case are shared by all jobs\n&gt; that\n&gt; &gt; &gt; &gt;&gt; crawl =\r\nmany thousands of hosts.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Since full GC was occurring abou=\r\nt every 10 minutes, the lock\n&gt; &gt; &gt; &gt;&gt; contention was not due to full GC fre=\r\nquency.  A heap histogram\n&gt; showed\n&gt; &gt; &gt; &gt;&gt; about 3700 CrawlServer instance=\r\ns at the end of the run.\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; If this un-knotting can work, th=\r\nere should be substantially\nbetter\n&gt; &gt; &gt; &gt;&gt; disk and network utilization. \n=\r\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt;&gt; Paul\n&gt; &gt; &gt; &gt;&gt;\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;=\r\n \n&gt; &gt; &gt; &gt; ------------------------------------\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; Yahoo! Grou=\r\nps Links\n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt; \n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}