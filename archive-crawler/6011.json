{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":413211004,"authorName":"Minas Abrahamyan","from":"Minas Abrahamyan &lt;minas.subs@...&gt;","profile":"a_minas","replyTo":"LIST","senderId":"Ddazu2_95W786wZOmEmIzMAgLE0zfFZ1g5AomIUDmZZ3MjmEzKDyuy_tmq6FZr4xgGL5pl41z2_CXusAnuY4w1dasqVatsPmJoP5fyfu","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Propose antitraps.txt for seeds.txt and its handling module","postDate":"1251989535","msgId":6011,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PDgyNDIzZDhlMDkwOTAzMDc1MnA5ZDEzN2Njd2ExOTQ3MDBjZWI5MDE3ZTFAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":0,"nextInTopic":6017,"prevInTime":6010,"nextInTime":6013,"topicId":6011,"numMessagesInTopic":2,"msgSnippet":"Hello, I propose to add new deciding rule for easy restricting URLs en mass: 1 Seed URLs give origins of URLs, which are then expanding very broad 2 Among","rawEmail":"Return-Path: &lt;minas.subs@...&gt;\r\nReceived: (qmail 97289 invoked from network); 3 Sep 2009 18:33:42 -0000\r\nReceived: from unknown (98.137.34.45)\n  by m3.grp.re1.yahoo.com with QMQP; 3 Sep 2009 18:33:42 -0000\r\nReceived: from unknown (HELO n41b.bullet.mail.sp1.yahoo.com) (66.163.168.155)\n  by mta2.grp.sp2.yahoo.com with SMTP; 3 Sep 2009 18:33:42 -0000\r\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoogroups.com; s=lima; t=1252002818; bh=tbS994v4wyezYmFK1z70+/LXaC4kRge7A7B2++UoeQA=; h=Received:Received:X-Sender:X-Apparently-To:X-Received:X-Received:X-Received:X-Received:MIME-Version:X-Received:Date:Message-ID:To:Content-Type:X-Originating-IP:X-eGroups-Msg-Info:From:Subject:X-Yahoo-Group-Post:X-Yahoo-Profile:X-YGroups-SubInfo:Sender:X-Yahoo-Newman-Property:X-eGroups-Approved-By:X-eGroups-Auth; b=uW+uzMruMaTcTrk6h9WZ3ES9cU9f8iQrWNZMqas+LOzkI4U3GdILqZXgL/7u9FdVIHumuS76N2/uqJ3S76okzRfSbHRRUFNhSrwpfK4habjPAXw3fL27Bg75BcFps0HC\r\nReceived: from [69.147.65.149] by n41.bullet.mail.sp1.yahoo.com with NNFMP; 03 Sep 2009 18:33:38 -0000\r\nReceived: from [98.137.34.35] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 03 Sep 2009 18:33:38 -0000\r\nX-Sender: minas.subs@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 3205 invoked from network); 3 Sep 2009 14:52:21 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m1.grp.re1.yahoo.com with QMQP; 3 Sep 2009 14:52:21 -0000\r\nX-Received: from unknown (HELO mail-ew0-f163.google.com) (209.85.219.163)\n  by mta1.grp.sp2.yahoo.com with SMTP; 3 Sep 2009 14:52:21 -0000\r\nX-Received: by ewy7 with SMTP id 7so144077ewy.10\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 03 Sep 2009 07:52:16 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.211.154.18 with SMTP id g18mr10689411ebo.70.1251989536030; \n\tThu, 03 Sep 2009 07:52:16 -0700 (PDT)\r\nDate: Thu, 3 Sep 2009 19:52:15 +0500\r\nMessage-ID: &lt;82423d8e0909030752p9d137ccwa194700ceb9017e1@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=001636c597568ebea70472ad87f5\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Minas Abrahamyan &lt;minas.subs@...&gt;\r\nSubject: Propose antitraps.txt for seeds.txt and its handling module\r\nX-Yahoo-Group-Post: member; u=413211004; y=KMEmMlGWkqPDHuRKvUSswEp_7Nb8Z4S92F9mIPwd1Y50sA\r\nX-Yahoo-Profile: a_minas\r\nX-YGroups-SubInfo: t=0;f=16;g=3AF5yMTCicGuSlhkiUYWUaMbEjYsZARyQ4H4E4AxlnhI1okqYvFT9nDe3BqdG7L5ZUE;\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Approved-By: stearcorg &lt;steve@...&gt; via web; 03 Sep 2009 18:33:36 -0000\r\n\r\n\r\n--001636c597568ebea70472ad87f5\r\nContent-Type: text/plain; charset=ISO-8859-1\r\n\r\nHello,\n\nI propose to add new deciding rule for easy restricting URLs en mass:\n\n1 Seed URLs give origins of URLs, which are then expanding very broad\n\n2 Among right URLs are appearing many unwanted URLs: traps, full duplicates,\nand content duplucates (maybe the last case is more interesting for search\nengine crawler applications then for archiving crawler, but anyway, they are\nduplicates too)\n\n3 The problem is solved by applying constraint regexps on URLs flow by\ndeciding rules\nParticularly, common schema is:\n new separate deciding rule, REJECT on MatchesListRegExpDecideRule,\n set as last rule - to not make next rule allow disallowed URL\nRegexps list disallows those trap and duplicate URLs.\nSo do I in my crawler, so does NetachiveSuite, their order.xml of heritrix\n1.12 set up to avoid only traps and only calendar-type ones.\n\nI suppose that any non-trivial crawl should contain such constraints, and\ntaking into account that they will be edited frequentely and added by bricks\nfor any URL, it would be great to have such special mechanism.\n\nMoreover many sites in Internet use commonly spreaded web software\ncomponents, such as forums, blogs, other open source or close software, so\nfinding trap&dup URLs for forum for example on one domain will help to\ncompose trap&dup URLs for same type forum on other domains.\n\nIdeal solution will allow to keep own antitrap/antidup regexps list for\nevery seedURL, and allow easy editing of it.\n\nSo I propose to make such new module.\n\nMy view of implemetation details is to\n* keep antitrap-antidup Regexps in one text file near the seeds.txt\n* Load this list internally into that same MatchesListRegExpDecideRule with\nREJECT on\nmatch and\n* edit them through special page, similar to one of seeds.txt\n  With two more buttons on that page:\n 1 to remove URLs from current Frontier state, when it is in paused state,\nyet;\n Would be better if it show deleted URLs, not like silent behavior of\nDeleteURLs of frontier on console page\n\n 2 and to test URLs matching: in crawl log, in frontier or manually entered\n\nI&#39;m biased to latest stable version of Heritrix, i.e. 2.0.2, but could\nupgrade to any newest version with working automatic checkpointing.\n\n\nAre there any ideas - about implementation or related with general\nconcerns??\n\n\nRegards,\nMinas Abrahamyan\nRnD.am\n\r\n--001636c597568ebea70472ad87f5\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;div&gt;Hello,&lt;br&gt;&lt;br&gt;I propose to add new deciding rule for easy restricting =\r\nURLs en mass:&lt;br&gt;&lt;br&gt;1 Seed URLs give origins of URLs, which are then expan=\r\nding very broad&lt;br&gt;&lt;br&gt;2 Among right URLs are appearing many unwanted URLs:=\r\n traps, full duplicates, and content duplucates (maybe the last case is mor=\r\ne interesting for search engine crawler applications then for archiving cra=\r\nwler, but anyway, they are duplicates too)&lt;br&gt;\n&lt;br&gt;3 The problem is solved =\r\nby applying constraint regexps on URLs flow by deciding rules=A0&lt;br&gt;Particu=\r\nlarly, common schema is:&lt;br&gt;=A0new separate deciding rule, REJECT on Matche=\r\nsListRegExpDecideRule,&lt;br&gt;=A0set as last rule - to not make next rule allow=\r\n disallowed URL=A0&lt;br&gt;\nRegexps list disallows those trap and duplicate URLs=\r\n.&lt;br&gt;So do I in my crawler, so does NetachiveSuite, their order.xml of heri=\r\ntrix 1.12 set up to avoid only traps and only calendar-type ones.&lt;br&gt;&lt;br&gt;I =\r\nsuppose that any non-trivial crawl should contain such constraints, and tak=\r\ning into account that they will be edited frequentely and added by bricks f=\r\nor any URL, it would be great to have such special mechanism.&lt;br&gt;\n&lt;br&gt;Moreo=\r\nver many sites in Internet use commonly spreaded web software components, s=\r\nuch as forums, blogs, other open source or close software, so finding trap&=\r\namp;dup URLs for forum for example on one domain will help to compose trap&=\r\namp;dup URLs for same type forum on other domains.&lt;br&gt;\n&lt;br&gt;Ideal solution w=\r\nill allow to keep own antitrap/antidup regexps list for every seedURL, and =\r\nallow easy editing of it.&lt;br&gt;&lt;br&gt;So I propose to make such new module.&lt;br&gt;&lt;=\r\nbr&gt;My view of implemetation details is to=A0&lt;br&gt;* keep antitrap-antidup Reg=\r\nexps in one text file near the seeds.txt&lt;br&gt;\n* Load this list internally in=\r\nto that same MatchesListRegExpDecideRule with REJECT on=A0&lt;br&gt;match and=A0&lt;=\r\nbr&gt;* edit them through special page, similar to one of seeds.txt&lt;br&gt;=A0 Wit=\r\nh two more buttons on that page:=A0&lt;br&gt;=A01 to remove URLs from current Fro=\r\nntier state, when it is in paused state, yet;=A0&lt;br&gt;\n=A0Would be better if =\r\nit show deleted URLs, not like silent behavior of DeleteURLs of frontier on=\r\n console page&lt;br&gt;&lt;br&gt;=A02 and to test URLs matching: in crawl log, in front=\r\nier or manually entered&lt;br&gt;&lt;br&gt;I&#39;m biased to latest stable version of H=\r\neritrix, i.e. 2.0.2, but could upgrade to any newest version with working a=\r\nutomatic checkpointing.&lt;br&gt;\n&lt;br&gt;&lt;br&gt;Are there any ideas - about implementat=\r\nion or related with general concerns??&lt;br&gt;&lt;br&gt;&lt;br&gt;Regards,&lt;br&gt;Minas Abraham=\r\nyan&lt;br&gt;RnD.am&lt;br&gt;&lt;/div&gt;\n\r\n--001636c597568ebea70472ad87f5--\r\n\n"}}