{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":544837819,"authorName":"rjoberon80","from":"&quot;rjoberon80&quot; &lt;rjoberon@...&gt;","profile":"rjoberon80","replyTo":"LIST","senderId":"rHiU3uLjZ1KstCrVYhcqRjC9D-wwfQOlnh6aWqEMNMp3fsbDLD7Yc4Xa1kTm6Ry_IZNojE2Oz36KgTb0lte5crMcQ1H2YTlRLeJake0","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: strange cycles with crawling speed","postDate":"1351687724","msgId":7847,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGs2cjZuYytpanU3QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDUwOTA1NkIwLjkwNTA2MDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":7846,"nextInTopic":7848,"prevInTime":7846,"nextInTime":7848,"topicId":7845,"numMessagesInTopic":6,"msgSnippet":"Hello, Gordon, thanks for the explanation. I could track down some issues. First, a correction: I got some errors in the plots due to too large numbers in","rawEmail":"Return-Path: &lt;rjoberon@...&gt;\r\nX-Sender: rjoberon@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 56633 invoked from network); 31 Oct 2012 12:48:47 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m7.grp.sp2.yahoo.com with QMQP; 31 Oct 2012 12:48:47 -0000\r\nX-Received: from unknown (HELO ng20-vm5.bullet.mail.gq1.yahoo.com) (98.136.219.252)\n  by mta4.grp.sp2.yahoo.com with SMTP; 31 Oct 2012 12:48:46 -0000\r\nX-Received: from [98.137.0.87] by ng20.bullet.mail.gq1.yahoo.com with NNFMP; 31 Oct 2012 12:48:46 -0000\r\nX-Received: from [98.137.34.72] by tg7.bullet.mail.gq1.yahoo.com with NNFMP; 31 Oct 2012 12:48:45 -0000\r\nDate: Wed, 31 Oct 2012 12:48:44 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;k6r6nc+iju7@...&gt;\r\nIn-Reply-To: &lt;509056B0.9050605@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;rjoberon80&quot; &lt;rjoberon@...&gt;\r\nSubject: Re: strange cycles with crawling speed\r\nX-Yahoo-Group-Post: member; u=544837819; y=_KgZ3Q7r11_rED-Pif4N5p2WfrEZrUrNMHG3WwzozmNkJ6o-SHrmxpm9_AmumgcHLdEWdlst6loJpCQ\r\nX-Yahoo-Profile: rjoberon80\r\n\r\nHello,\n\nGordon, thanks for the explanation. I could track down some issues.=\r\n\n\nFirst, a correction: I got some errors in the plots due to too large numb=\r\ners in progress-statistics.log which mixed up the columns. The correct plot=\r\n is \n\nhttp://www.flickr.com/photos/89400617@N02/8140895553/in/photostream\n\n=\r\nCompare it with the broken plot of the same time:\n\nhttp://www.flickr.com/ph=\r\notos/89400617@N02/8140883970/in/photostream\n\nSo basically, congestion and m=\r\nax-depth were wrong.\n\nSecond, I had trouble re-starting the crawler after t=\r\nhe changes I made to poolMaxActive (see below). In the end, I had to re-sta=\r\nrt from the earlier checkpoint I used yesterday (the one which caused the t=\r\nrouble).\nWhen I did this, I realized that the checkpoint did not include th=\r\ne changes I made to the beans during the run of the crawler (e.g., modifica=\r\ntions of maxToeThreads, maxLengthBytes, maxRetries, MatchesListRegexDecideR=\r\nule, etc.) which is contrary to my intuition of a &quot;checkpoint&quot;. Thus, befor=\r\ne restarting again, I changed the bean config on disk and then started the =\r\ncrawler with the old checkpoint. Now it is running for over an hour and eve=\r\nrything is fine:\n\nhttp://www.flickr.com/photos/89400617@N02/8141415268/in/p=\r\nhotostream\n\nThe speed is still above the one before the checkpoint, which i=\r\ns very good.\n\nMy notes below can now probably be disregarded, since a re-st=\r\nart from the same checkpoint (with some modifications to the configuration)=\r\n now works without problems. So it was very likely caused by the different =\r\n(default!) configuration that was used or by some other issue which we can =\r\nnot find out. (The only difference now should be warcWriter.poolMaxActive=\r\n=3D10 instead of =3D1 before.)\n\n\n--- In archive-crawler@yahoogroups.com, Go=\r\nrdon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; On 10/30/12 7:55 AM, rjoberon80 wrote:\n&gt; =\r\n&gt; While I was writing this post the crawler seems to stabilize and\n&gt; &gt; poss=\r\nibly the problem is solved. Still, I post it to find an\n&gt; &gt; explanation for=\r\n the behavior and maybe for people which observe a\n&gt; &gt; similar behavior.\n&gt; =\r\n&gt;\n&gt; &gt; My crawler is now running for 20 days and has crawled some 3.4TB. We\n=\r\n&gt; &gt; had to buy new disk space and restart it from a checkpoint and since\n&gt; =\r\n&gt; then it does not run smooth any longer. The only change I made\n&gt; &gt; (besid=\r\nes adding the hard disk) was to double the heap from 1GB to\n&gt; &gt; 2GB.\n&gt; &gt;\n&gt; =\r\n&gt; The problem can best be described by having a look at this graph:\n&gt; &gt; htt=\r\np://www.flickr.com/photos/89400617@N02/8138463405/\n&gt; &gt;\n&gt; &gt; There is a cycli=\r\nc change between high throughput and no throughput,\n&gt; &gt; e.g., between\n&gt; &gt;\n&gt;=\r\n &gt; 76.45 URIs/sec (23.28 avg); 8794 KB/sec (2158 avg) 95 threads: 95\n&gt; &gt; AB=\r\nOUT_TO_BEGIN_PROCESSOR; 75 fetchHttp, 14 extractorHtml, 6\n&gt; &gt; warcWriter\n&gt; =\r\n&gt;\n&gt; &gt; and\n&gt; &gt;\n&gt; &gt; 0 URIs/sec (23.28 avg); 0 KB/sec (2159 avg) 95 threads: 9=\r\n5\n&gt; &gt; ABOUT_TO_BEGIN_PROCESSOR; 94 warcWriter, 1 fetchHttp\n&gt; &gt;\n&gt; &gt; (the ave=\r\nrages in brackets are over the last 20 days)\n&gt; \n&gt; The predominance of &#39;warc=\r\nWriter&#39; indicates a traffic-jam at that stage. \n&gt; If you&#39;re writing all WAR=\r\nCs to a single disk, and perhaps even to a \n&gt; single open file at once, the=\r\nn one extra-large (100s of MB, \n&gt; multiple-GB?) response can sometimes caus=\r\ne a pile-up. It will be even \n&gt; worse if the &#39;scratch&#39; and &#39;warc&#39; destinati=\r\nons are the same disk volume.\n\nI am writing onto several disks which are ag=\r\ngregated using Hardware-RAID and LVM. So having them on the same volume sho=\r\nuld not be a problem.\n\n\n&gt; You could use the &#39;threads report&#39; or other JVM s=\r\ntack dumps (via th \n&gt; &#39;jstack&#39; tool or sending SIGQUIT to the JVM process) =\r\nto get a better \n&gt; idea of what threads are up to (or blocked on) during th=\r\ne slow periods. \n&gt; You could use OS tools like &#39;iostat&#39; to get a better ide=\r\na of disk usage \n&gt; around the episodes.\n\nI have now collected several examp=\r\nles of cases where nothing happens. \n\n1) no download occurs, although 150 t=\r\nhreads are in state &quot;fetchHttp&quot;:\n\nTotals\n    44021585 downloaded + 44382854=\r\n queued =3D 88404589 total\n    4.0 TiB crawled (4.0 TiB novel, 0 B dupByHas=\r\nh, 798 B notModified)\nAlerts\n    26 tail alert log... \nRates\n    0 URIs/sec=\r\n (24.47 avg); 0 KB/sec (2369 avg)\nLoad\n    150 active of 150 threads; 20.54=\r\n congestion ratio; 1445771 deepest queue; 2336 average depth\nElapsed\n    20=\r\nd19h47m37s678ms\nThreads\n    150 threads: 150 ABOUT_TO_BEGIN_PROCESSOR; 150 =\r\nfetchHttp \nFrontier\n    RUN - 107485 URI queues: 1959 active (169 in-proces=\r\ns; 1034 ready; 756 snoozed); 17036 inactive; 0 ineligible; 70 retired; 8842=\r\n0 exhausted \nMemory\n    1410506 KiB used; 1797440 KiB current heap; 1864192=\r\n KiB max heap \n\n\n\n\n2) no download occurs, with some threads fetching and so=\r\nme threads writing:\n\nTotals\n    44509900 downloaded + 44911054 queued =3D 8=\r\n9421104 total\n    4.0 TiB crawled (4.0 TiB novel, 0 B dupByHash, 798 B notM=\r\nodified)\nAlerts\n    27 tail alert log... \nRates\n    0 URIs/sec (24.61 avg);=\r\n 0 KB/sec (2400 avg)\nLoad\n    150 active of 150 threads; 15.55 congestion r=\r\natio; 1451870 deepest queue; 2643 average depth\nElapsed\n    20d22h20m44s721=\r\nms\nThreads\n    150 threads: 150 ABOUT_TO_BEGIN_PROCESSOR; 92 warcWriter, 58=\r\n fetchHttp \nFrontier\n    RUN - 107940 URI queues: 2359 active (169 in-proce=\r\nss; 1266 ready; 924 snoozed); 14633 inactive; 0 ineligible; 70 retired; 908=\r\n78 exhausted \nMemory\n    1419372 KiB used; 1690496 KiB current heap; 186419=\r\n2 KiB max heap \n\n\n\n\n3) no download, most threads writing:\n\n\nTotals\n    4451=\r\n5192 downloaded + 44917089 queued =3D 89432431 total\n    4.0 TiB crawled (4=\r\n.0 TiB novel, 0 B dupByHash, 798 B notModified)\nAlerts\n    27 tail alert lo=\r\ng... \nRates\n    0 URIs/sec (24.61 avg); 0 KB/sec (2400 avg)\nLoad\n    150 ac=\r\ntive of 150 threads; 13.38 congestion ratio; 1451920 deepest queue; 2644 av=\r\nerage depth\nElapsed\n    20d22h24m10s166ms\nThreads\n    150 threads: 150 ABOU=\r\nT_TO_BEGIN_PROCESSOR; 129 warcWriter, 21 fetchHttp \nFrontier\n    RUN - 1079=\r\n43 URI queues: 2351 active (169 in-process; 1081 ready; 1101 snoozed); 1463=\r\n6 inactive; 0 ineligible; 70 retired; 90886 exhausted \nMemory\n    1349687 K=\r\niB used; 1864192 KiB current heap; 1864192 KiB max heap\n\n\n\n\n4) regular oper=\r\nation:\n\n\nTotals\n    44106740 downloaded + 44481072 queued =3D 88587962 tota=\r\nl\n    4.0 TiB crawled (4.0 TiB novel, 0 B dupByHash, 798 B notModified)\nAle=\r\nrts\n    26 tail alert log... \nRates\n    67.33 URIs/sec (24.5 avg); 4030 KB/=\r\nsec (2372 avg)\nLoad\n    150 active of 150 threads; 16.4 congestion ratio; 1=\r\n447021 deepest queue; 2439 average depth\nElapsed\n    20d20h7m39s745ms\nThrea=\r\nds\n    150 threads: 150 ABOUT_TO_BEGIN_PROCESSOR; 145 fetchHttp, 5 warcWrit=\r\ner \nFrontier\n    RUN - 107623 URI queues: 2044 active (169 in-process; 932 =\r\nready; 943 snoozed); 16193 inactive; 0 ineligible; 70 retir\ned; 89316 exhau=\r\nsted \nMemory\n    1529773 KiB used; 1864192 KiB current heap; 1864192 KiB ma=\r\nx heap \n\n\n\n\nI have looked into the threads report for case 3), the warcWrit=\r\ner threads look like this\n\n[ToeThread #66: https://www.csn.tu-chemnitz.de/n=\r\news/archiv.pl?lang=3Dde_DE&lang=3Dde_DE&lang=3Den_US&lang=3Den_US&lang=3Den=\r\n_US&id=3D1010\n CrawlURI https://www.csn.tu-chemnitz.de/news/archiv.pl?lang=\r\n=3Dde_DE&lang=3Dde_DE&lang=3Den_US&lang=3Den_US&lang=3Den_US&id=3D1010 RLLL=\r\nLLLLLLLLLL https://www.csn.tu-chemnitz.de/news/archiv.pl?lang=3Dde_DE&lang=\r\n=3Den_US&lang=3Den_US&lang=3Den_US&id=3D1010    0 attempts\n    in processor=\r\n: warcWriter\n    ACTIVE for 42s139ms\n    step: ABOUT_TO_BEGIN_PROCESSOR for=\r\n 7s389ms\nJava Thread State: TIMED_WAITING\nBlocked/Waiting On: NONE\n    sun.=\r\nmisc.Unsafe.park(Native Method)\n    java.util.concurrent.locks.LockSupport.=\r\nparkNanos(LockSupport.java:226)\n    java.util.concurrent.locks.AbstractQueu=\r\nedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2=\r\n081)\n    java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.ja=\r\nva:370)\n    org.archive.io.WriterPool.borrowFile(WriterPool.java:124)\n    o=\r\nrg.archive.modules.writer.WARCWriterProcessor.write(WARCWriterProcessor.jav=\r\na:224)\n    org.archive.modules.writer.WARCWriterProcessor.innerProcessResul=\r\nt(WARCWriterProcessor.java:209)\n    org.archive.modules.Processor.process(P=\r\nrocessor.java:142)\n    org.archive.modules.ProcessorChain.process(Processor=\r\nChain.java:131)\n    org.archive.crawler.framework.ToeThread.run(ToeThread.j=\r\nava:151)\n]\n\n\nHaving a look at the system status \n\nhttp://www.flickr.com/pho=\r\ntos/89400617@N02/8141135647/in/photostream\n\nI can observe a regular increas=\r\ne of IO wait which then suddenly drops. The cycles are however much longer =\r\nthan the cycles I observe for the crawler and are thus probably not related=\r\n.\n\n\n\n&gt; Sometimes even using larger JVM object heaps cause more noticeable \n=\r\n&gt; pauses due to occasional global garbage-collection cycles (though larger =\r\n\n&gt; heaps should usually help with throughput... but by having fewer larger =\r\n\n&gt; full-GCs they stick out more).\n\nThanks, that sounds also interesting. \n\n=\r\n\n&gt; But from the description I suspect concentrating on the available write =\r\n\n&gt; IO for the warcWriter may offer the biggest help: writing multiple WARCs=\r\n \n&gt; at a time to volumes that are independent of each other (using the \n&gt; p=\r\noolMaxActive and storePaths settings of WARCWriterProcessor), and \n&gt; separa=\r\nte from the &#39;scratch&#39; path.\n\nIndeed, that is a good tip! I had poolMaxActiv=\r\ne set to 1 and now increased it to 10. I am still investigating, if this ch=\r\nanges something. (I run into trouble because I have problems to teardown th=\r\ne crawler.)\n\n\n\n&gt; Others have sometimes noticed that a restarted crawl seems=\r\n to have a \n&gt; &#39;faster progress&#39; honeymoon period, but the exact reasons why=\r\n haven&#39;t \n&gt; been isolated.\n\nThis does not seem to be the case. After runnin=\r\ng for more than 16 hours now, the average crawling speed increased.\n\n\nI wil=\r\nl further investigate the issue and keep you updated, if anything changes. =\r\nI hope that increasing the number of warc files helps. \nSince the average c=\r\nrawling speed increases, I might also opt for ignoring the issue.\n\n\nBest re=\r\ngards,\nRobert\n\n\n\n"}}