{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigur√∞sson","from":"=?ISO-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"In6e7uBtAdGTQG26zJtfrighnA7RGoK-AQ2zNnzAMAF7SGUS6BnInNMpSPatdAM_udUWv7crVgfxNGOUbC29nD1A6Ct09PLJXvBFDie-si9856NUJ740QnL4YVm9dnH1","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Max Size Related Configuration","postDate":"1080606742","msgId":293,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwNjhDMDE2LjcwMTA2QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGM0YWVtNit0MDc0QGVHcm91cHMuY29tPg==","referencesHeader":"PGM0YWVtNit0MDc0QGVHcm91cHMuY29tPg=="},"prevInTopic":292,"nextInTopic":295,"prevInTime":292,"nextInTime":294,"topicId":289,"numMessagesInTopic":9,"msgSnippet":"Hei Seb. See below ... If you are only crawling one site at a time (using DomainScope) the max-bytes-download just the thing for you.  It limits the total","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 586 invoked from network); 30 Mar 2004 00:32:46 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m17.grp.scd.yahoo.com with QMQP; 30 Mar 2004 00:32:46 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta6.grp.scd.yahoo.com with SMTP; 30 Mar 2004 00:32:45 -0000\r\nReceived: (qmail 30559 invoked by uid 100); 30 Mar 2004 00:28:00 -0000\r\nReceived: from b116-dyn-55.archive.org (HELO archive.org) (kris@...@209.237.240.55)\n  by mail-dev.archive.org with SMTP; 30 Mar 2004 00:28:00 -0000\r\nMessage-ID: &lt;4068C016.70106@...&gt;\r\nDate: Mon, 29 Mar 2004 16:32:22 -0800\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c4aem6+t074@...&gt;\r\nIn-Reply-To: &lt;c4aem6+t074@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------040102010108050400080706&quot;\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.2 required=6.0 tests=AWL,HTML_MESSAGE,\n\tHTML_TITLE_EMPTY autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: =?ISO-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;\r\nSubject: Re: [archive-crawler] Re: Max Size Related Configuration\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n--------------040102010108050400080706\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nHei Seb.\n\nSee below\n\nsebastiandelachica wrote:\n\n&gt; Michael,\n&gt;\n&gt; Thanks for the prompt reply. Indeed I upgraded to 0.6.0 over the\n&gt; weekend: amazing how close a 9 looks to a 6 given enough lack of sleep\n&gt;\n&gt; In English (to the best of my ability), what I am trying to do is use\n&gt; a separate order for each site I need to crawl. Each order hence has\n&gt; a single seed. The purpose is to place an upper bound on the number\n&gt; of &quot;useful&quot; bytes downloaded for each crawl order (about 100K or 1MB\n&gt; say). In other words, stop crawling the site once we have downloaded\n&gt; some number of usable bytes. \n\nIf you are only crawling one site at a time (using DomainScope) the \nmax-bytes-download\njust the thing for you.  It limits the total amount of data downloaded \nin one CrawlJob. It is\nonly if you are crawling multiple domains in the same job (as is usual) \nthat you can&#39;t use it\nfor this purpose as it would only cut you off once the total from all \ndomain hit the limit.\n\nOnce the limit is hit the current job will end.  At some point we may \nallow overrides on this\nsetting enabling cutoffs on specific domains but that is well into the \nfuture.\n\nI hope this is of some use to you.\n\n- Kris\n\n&gt; I thought setting a max size limit on\n&gt; the ARC file would stop logging past that point, but I see now that\n&gt; it means a slightly different thing. I tried the number of files\n&gt; limit and while it works, I am not sure it matches my intent as some\n&gt; files may be noticably shorter than others.\n&gt;\n&gt; I am using a DomainScope crawl per the settings in the Profile used\n&gt; to create the Job.\n&gt;\n&gt; Based on the information you sent me, I need to think about what\n&gt; might serve my purpose. Thanks for pointing me at the right code in\n&gt; ARCWriter.\n&gt;\n&gt; Seb\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@a...&gt;\n&gt; wrote:\n&gt; &gt; Thanks for trying Heritrix Seb.\n&gt; &gt;\n&gt; &gt; See below.\n&gt; &gt;\n&gt; &gt; sebastiandelachica wrote:\n&gt; &gt;\n&gt; &gt; &gt;I have been playing around with heritrix for a few weeks now and I\n&gt; am\n&gt; &gt; &gt;in the process of turning it loose on a controlled environment for\n&gt; &gt; &gt;one of my research strands. I am currently using version 0.9.0. My\n&gt; &gt; &gt;objective is to limit the amount of data scooped from a site onto\n&gt; the\n&gt; &gt; &gt;ARC file. I tried using the HTTP Processor max-length-bytes and\n&gt; the\n&gt; &gt; &gt;Archiver max-size-bytes.\n&gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt;\n&gt; &gt; Do you mean 0.4.0. You say 0.9.0 above.  We just released 0.6.0 on\n&gt; &gt; friday.  Try it if you haven&#39;t already.  Lots of fixes and\n&gt; improvements.\n&gt; &gt;\n&gt; &gt; If you&#39;re doing a broad crawl, you have the following options\n&gt; available\n&gt; &gt; to you:\n&gt; &gt;\n&gt; &gt; max-bytes-download\n&gt; &gt; max-document-download\n&gt; &gt;\n&gt; &gt; These options are not available in a domain scoped crawl which\n&gt; seems to\n&gt; &gt; be what it is you&#39;d like to do.\n&gt; &gt;\n&gt; &gt; Tell us more about what it is that you&#39;d like.\n&gt; &gt;\n&gt; &gt; The max-length-bytes options limits size of a particular download\n&gt; only. \n&gt; &gt; The max-size-bytes  is upper-bound on the size of ARC files written\n&gt; (See\n&gt; &gt; the code here http://crawler.archive.org/xref/index.html). \n&gt; &lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt; &gt;\n&gt; &gt; Yours,\n&gt; &gt; St.Ack\n&gt; &gt;\n&gt; &gt; &gt;The Processor max-length-bytes seems to work at some level as the\n&gt; &gt; &gt;requests are reported as length-truncated in the logs, but the\n&gt; actual\n&gt; &gt; &gt;HTML files still make it into the archive.\n&gt; &gt; &gt;\n&gt; &gt; &gt;The Archive max-size-bytes appears to be ignored. Looking at the\n&gt; &gt; &gt;code, it does not seem to be used by the ARCWriterProcessor class\n&gt; or\n&gt; &gt; &gt;any other class for that matter...I just starting digging through\n&gt; the\n&gt; &gt; &gt;code earlier today.\n&gt; &gt; &gt;\n&gt; &gt; &gt;I continue my experimentation and code reading, but figured, I&#39;d\n&gt; &gt; &gt;check in to see if I am missing something very obvious or if\n&gt; anyone\n&gt; &gt; &gt;had experienced similar behavior.\n&gt; &gt; &gt;\n&gt; &gt; &gt;Thanks in advance for your time,\n&gt; &gt; &gt;Seb\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;Yahoo! Groups Links\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; \n&gt; &gt; &gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\r\n--------------040102010108050400080706\r\nContent-Type: text/html; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta content=&quot;text/html;charset=ISO-8859-1&quot; http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\nHei Seb.&lt;br&gt;\n&lt;br&gt;\nSee below&lt;br&gt;\n&lt;br&gt;\nsebastiandelachica wrote:&lt;br&gt;\n&lt;blockquote cite=&quot;midc4aem6+t074@...&quot; type=&quot;cite&quot;&gt;&lt;tt&gt;\nMichael,&lt;br&gt;\n  &lt;br&gt;\nThanks for the prompt reply. Indeed I upgraded to 0.6.0 over the &lt;br&gt;\nweekend: amazing how close a 9 looks to a 6 given enough lack of sleep&lt;br&gt;\n  &lt;br&gt;\nIn English (to the best of my ability), what I am trying to do is use &lt;br&gt;\na separate order for each site I need to crawl. Each order hence has &lt;br&gt;\na single seed. The purpose is to place an upper bound on the number &lt;br&gt;\nof &quot;useful&quot; bytes downloaded for each crawl order (about 100K or 1MB &lt;br&gt;\nsay). In other words, stop crawling the site once we have downloaded &lt;br&gt;\nsome number of usable bytes. &lt;/tt&gt;&lt;/blockquote&gt;\nIf you are only crawling one site at a time (using DomainScope) the &lt;tt&gt;max-bytes-download&lt;br&gt;\n&lt;/tt&gt;just the thing for you.&nbsp; It limits the total amount of data\ndownloaded in one CrawlJob. It is&lt;br&gt;\nonly if you are crawling multiple domains in the same job (as is usual)\nthat you can&#39;t use it&lt;br&gt;\nfor this purpose as it would only cut you off once the total from all\ndomain hit the limit.&lt;br&gt;\n&lt;br&gt;\nOnce the limit is hit the current job will end.&nbsp; At some point we may\nallow overrides on this&lt;br&gt;\nsetting enabling cutoffs on specific domains but that is well into the\nfuture.&lt;br&gt;\n&lt;br&gt;\nI hope this is of some use to you.&lt;br&gt;\n&lt;br&gt;\n- Kris&lt;br&gt;\n&lt;blockquote cite=&quot;midc4aem6+t074@...&quot; type=&quot;cite&quot;&gt;&lt;tt&gt;I thought\nsetting a max size limit on &lt;br&gt;\nthe ARC file would stop logging past that point, but I see now that &lt;br&gt;\nit means a slightly different thing. I tried the number of files &lt;br&gt;\nlimit and while it works, I am not sure it matches my intent as some &lt;br&gt;\nfiles may be noticably shorter than others. &lt;br&gt;\n  &lt;br&gt;\nI am using a DomainScope crawl per the settings in the Profile used &lt;br&gt;\nto create the Job.&lt;br&gt;\n  &lt;br&gt;\nBased on the information you sent me, I need to think about what &lt;br&gt;\nmight serve my purpose. Thanks for pointing me at the right code in &lt;br&gt;\nARCWriter. &lt;br&gt;\n  &lt;br&gt;\nSeb&lt;br&gt;\n  &lt;br&gt;\n--- In &lt;a class=&quot;moz-txt-link-abbreviated&quot; href=&quot;mailto:archive-crawler@yahoogroups.com&quot;&gt;archive-crawler@yahoogroups.com&lt;/a&gt;, Michael Stack\n&lt;a class=&quot;moz-txt-link-rfc2396E&quot; href=&quot;mailto:stack@a...&quot;&gt;&lt;stack@a...&gt;&lt;/a&gt; &lt;br&gt;\nwrote:&lt;br&gt;\n&gt; Thanks for trying Heritrix Seb.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; See below.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; sebastiandelachica wrote:&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &gt;I have been playing around with heritrix for a few weeks now\nand I &lt;br&gt;\nam &lt;br&gt;\n&gt; &gt;in the process of turning it loose on a controlled environment\nfor &lt;br&gt;\n&gt; &gt;one of my research strands. I am currently using version\n0.9.0. My &lt;br&gt;\n&gt; &gt;objective is to limit the amount of data scooped from a site\nonto &lt;br&gt;\nthe &lt;br&gt;\n&gt; &gt;ARC file. I tried using the HTTP Processor max-length-bytes\nand &lt;br&gt;\nthe &lt;br&gt;\n&gt; &gt;Archiver max-size-bytes.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&nbsp; &lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; Do you mean 0.4.0. You say 0.9.0 above.&nbsp; We just released 0.6.0 on\n  &lt;br&gt;\n&gt; friday.&nbsp; Try it if you haven&#39;t already.&nbsp; Lots of fixes and &lt;br&gt;\nimprovements.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; If you&#39;re doing a broad crawl, you have the following options &lt;br&gt;\navailable &lt;br&gt;\n&gt; to you:&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; max-bytes-download&lt;br&gt;\n&gt; max-document-download&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; These options are not available in a domain scoped crawl which &lt;br&gt;\nseems to &lt;br&gt;\n&gt; be what it is you&#39;d like to do.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Tell us more about what it is that you&#39;d like.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; The max-length-bytes options limits size of a particular download &lt;br&gt;\nonly.&nbsp; &lt;br&gt;\n&gt; The max-size-bytes&nbsp; is upper-bound on the size of ARC files\nwritten &lt;br&gt;\n(See &lt;br&gt;\n&gt; the code here &lt;a\n href=&quot;http://crawler.archive.org/xref/index.html%29.&quot;&gt;http://crawler.archive.org/xref/index.html).&lt;/a&gt;&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Yours,&lt;br&gt;\n&gt; St.Ack&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &gt;The Processor max-length-bytes seems to work at some level as\nthe &lt;br&gt;\n&gt; &gt;requests are reported as length-truncated in the logs, but the\n  &lt;br&gt;\nactual &lt;br&gt;\n&gt; &gt;HTML files still make it into the archive.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;The Archive max-size-bytes appears to be ignored. Looking at\nthe &lt;br&gt;\n&gt; &gt;code, it does not seem to be used by the ARCWriterProcessor\nclass &lt;br&gt;\nor &lt;br&gt;\n&gt; &gt;any other class for that matter...I just starting digging\nthrough &lt;br&gt;\nthe &lt;br&gt;\n&gt; &gt;code earlier today.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;I continue my experimentation and code reading, but figured,\nI&#39;d &lt;br&gt;\n&gt; &gt;check in to see if I am missing something very obvious or if &lt;br&gt;\nanyone &lt;br&gt;\n&gt; &gt;had experienced similar behavior.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;Thanks in advance for your time,&lt;br&gt;\n&gt; &gt;Seb&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt;Yahoo! Groups Links&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; &lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&nbsp; &lt;br&gt;\n&gt; &gt;&lt;br&gt;\n  &lt;br&gt;\n  &lt;/tt&gt;\n\n&lt;/blockquote&gt;\n&lt;br&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------040102010108050400080706--\r\n\n"}}