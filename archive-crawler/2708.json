{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"vmB2Ht5QQP22wNvILbYKGcOR1_z2lCQD2RhjEp6UX7mnI051OUYa7Eko7Rairsj_CPSfXj5PtzR8mTAOJI8I1g","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] How do I Change crawler behaviour?","postDate":"1140726601","msgId":2708,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzRkUxQjQ5LjIwODAxMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGYxZGExYjhhMDYwMjIzMTEzOGk0MzJkNmQ5ZWlmNjBlMTYwOWE3MjRlNGZjQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGYxZGExYjhhMDYwMjIzMTEzOGk0MzJkNmQ5ZWlmNjBlMTYwOWE3MjRlNGZjQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":2706,"nextInTopic":2710,"prevInTime":2707,"nextInTime":2709,"topicId":2706,"numMessagesInTopic":5,"msgSnippet":"... Possibly though the crawler selftests by crawling a special selftest webapp.  We then do examination of the produced ARC files to see that what was","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60102 invoked from network); 23 Feb 2006 20:33:11 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m26.grp.scd.yahoo.com with QMQP; 23 Feb 2006 20:33:11 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta1.grp.scd.yahoo.com with SMTP; 23 Feb 2006 20:33:11 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k1NJLB126269\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Feb 2006 11:21:11 -0800\r\nMessage-ID: &lt;43FE1B49.2080102@...&gt;\r\nDate: Thu, 23 Feb 2006 12:30:01 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8) Gecko/20051218 SeaMonkey/1.0b\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f1da1b8a0602231138i432d6d9eif60e1609a724e4fc@...&gt;\r\nIn-Reply-To: &lt;f1da1b8a0602231138i432d6d9eif60e1609a724e4fc@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] How do I Change crawler behaviour?\r\nX-Yahoo-Group-Post: member; u=168599281; y=9M2xwf4q789SISxNGO5VBDZdnZcr3wSAMTxD9TqHhk8i1fGaKTrmWR8l\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nThirumalai Veerasamy wrote:\n&gt; Hi,\n&gt;\n&gt;    I wanted to crawl through my entire web application.\n&gt;\n&gt;   The reasons\n&gt;\n&gt;   1. Test for broken link\n&gt;   2. Record the crawling as script and play it again\n&gt;   3. Find out the elapsed time for individual pages\n&gt;\n&gt;   I wrote a custom component using apache httpconnection. Now I wanted\n&gt; to save each web page, so that I can go through individual page and\n&gt; verify whether the page is display what it is supposed to display.\n&gt;\n&gt;   I found archive crawler can be used to save the pages using\n&gt; MirrorWriterProcessor. I also feel that archive crawler is bit heavy\n&gt; app to crawl just 200-400 pages of my application.\n\nPossibly though the crawler &#39;selftests&#39; by crawling a special &#39;selftest&#39; \nwebapp.  We then do examination of the produced ARC files to see that \nwhat was supposed to be downloaded was (and that what was not supposed \nto be downloaded, wasn&#39;t).  You build out the selftest by adding junit \nbased &#39;SelfTests&#39;. Not elegant but works.\n&gt;\n&gt;   I have few more questions before I can go deep into this crawler.\n&gt;\n&gt;   1. Can I skip certain pages?\n&gt;       i I want to crawl only urls in that particular domain.\n&gt; (http://localhost), &lt;http://localhost%29,&gt;\n&gt;       ii. skip urls which satisfies some regular expression.\nYes on above two.  Play with the deciderules in a decidingscope.\n\n&gt;       iii remove some paramter from the url and check whether it is\n&gt; already crawled (the web app add a paramter which doesn&#39;t change the\n&gt; behaviour of the page, but creates different urls for the same page\n&gt; which I don&#39;t want to crawl).\nYou&#39;d have to add a little code to do this.  See processors in the \npostprocessor package.\n\n&gt;    2. Find out the elapsed time for each page.\nTime downloading?  Its set by the FetchHTTP fetcher into the CrawlURI \nwith the key A_FETCH_COMPLETED_TIME.\n\n\n&gt;    3. Save html, images, etc., into a folder only if it is not already\n&gt; downloaded.\n\nWe should not be refetching the same URL twice.  You might want to \nenable the hash of downloaded content and add a bit of code to not write \nto the folder items of the same hash (See the &quot;sha1-content&quot; attribute \nof FetchHTTP).\n\n&gt;    4. Support for proxy\n&gt;\nIts present.  See FetchHTTP.\n\n\nSt.Ack\n\n"}}