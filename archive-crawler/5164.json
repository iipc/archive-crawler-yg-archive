{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":202034705,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"CFWOdp-IyUZl6gXu9-hQZUBdUJL1bMxBDQ_lr0fPC9jo8hUmPvqyKLLBHIDAZdZxJJVdBvm0xryUVm6C95kv","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Distribution using Consistent Hashing scheme","postDate":"1209745398","msgId":5164,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4MUIzRkY2LjYwMjA0MDlAZHVib2NlLm5ldD4=","inReplyToHeader":"PDhGMTE3MjJBMDU2MkJCNEY4MEE2ODBFRDRDRkVEMEQ1MDFDRUVFRDBARVZTQk5HMDIuYWQub2ZmaWNlLmFvbC5jb20+","referencesHeader":"PDhGMTE3MjJBMDU2MkJCNEY4MEE2ODBFRDRDRkVEMEQ1MDFDRUVFRDBARVZTQk5HMDIuYWQub2ZmaWNlLmFvbC5jb20+"},"prevInTopic":5163,"nextInTopic":5165,"prevInTime":5163,"nextInTime":5165,"topicId":5152,"numMessagesInTopic":7,"msgSnippet":"Sounds really great Ankur. How has it been working for you?  What size clusters have you been running with? Out of interest, did you use the CHF from the mg4j","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 81915 invoked from network); 2 May 2008 16:23:33 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m44.grp.scd.yahoo.com with QMQP; 2 May 2008 16:23:33 -0000\r\nX-Received: from unknown (HELO dns.duboce.net) (63.203.238.117)\n  by mta15.grp.scd.yahoo.com with SMTP; 2 May 2008 16:23:33 -0000\r\nX-Received: by dns.duboce.net (Postfix, from userid 1008)\n\tid 28FAEC564; Fri,  2 May 2008 07:55:02 -0700 (PDT)\r\nX-Spam-Checker-Version: SpamAssassin 3.1.4 (2006-07-26) on dns.duboce.net\r\nX-Spam-Level: \r\nX-Spam-Status: No, score=-2.4 required=5.0 tests=AWL,BAYES_00,\n\tFORGED_RCVD_HELO autolearn=ham version=3.1.4\r\nX-Received: from durruti.local (durruti.desk.hq.powerset.com [208.84.6.77])\n\tby dns.duboce.net (Postfix) with ESMTP id 6ABFEC51B\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri,  2 May 2008 07:54:54 -0700 (PDT)\r\nMessage-ID: &lt;481B3FF6.6020409@...&gt;\r\nDate: Fri, 02 May 2008 09:23:18 -0700\r\nUser-Agent: Thunderbird 2.0.0.14 (Macintosh/20080421)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;8F11722A0562BB4F80A680ED4CFED0D501CEEED0@...&gt;\r\nIn-Reply-To: &lt;8F11722A0562BB4F80A680ED4CFED0D501CEEED0@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Distribution using Consistent Hashing scheme\r\nX-Yahoo-Group-Post: member; u=202034705; y=iDuFAUfKbP5_zEophfW0N_uZspBwgc4pwuvD17BKSi8UtKZoxIoGsPJl\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nSounds really great Ankur.\n\nHow has it been working for you?  What size clusters have you been \nrunning with?\n\nOut of interest, did you use the CHF from the mg4j jar or write your own?\n\nHow has JMX been working out for you?  Its OK making the invocation \nthough the payload is small -- just an URL?  (Do you send just an URL or \nan CrawlURI?).  When the (b)UBIcrawler folks were through last, they \ntalked highly of jgroups over UDP as being good for small communications.\n\nWhat would be in the way, if anything, of your migrating the \ndistribution to Heritrix 2.0?\n\nSt.Ack\n\n\nGoel, Ankur wrote:\n&gt;\n&gt; Hi Folks,\n&gt;             We have our own implementation of the crawl farm based on \n&gt; top of Heritrix 1.6. We moved to version 1.13 sometime back.\n&gt; As a part of our work we have extended the heritrix code and \n&gt; implemented a consistent hashing scheme for maintaining politeness\n&gt; and consistent distribution. We would like to contribute some of our \n&gt; distribution related code to the heritrix community.\n&gt;  \n&gt; Let me give a brief idea of what we have done that we would like to \n&gt; contribute.\n&gt;  \n&gt; 1. Each crawl node (called crawl engine in heritrix 2.0)  \n&gt; is responsible for figuring out if it should crawl the \n&gt; recieved/extracted URLs\n&gt; or the right crawler node that should be crawling it and dispatches \n&gt; the URLs over JMX to the appropriate node.\n&gt;  \n&gt; 2. The crawl node uses the services of a &#39;DynamicCrawlMapper&#39; to take \n&gt; care of distribution. This is a regular heritrix &#39;Processor&#39; integrated\n&gt; into heritrix processing chain.\n&gt;  \n&gt; 3. The &#39;DynamicCrawlMapper&#39; uses the services of a &#39;Distributor&#39; that \n&gt; talks to different crawl nodes over JMX and uses consistent hashing\n&gt; scheme to figure the destination node for the URL. If the URL&#39;s hash \n&gt; falls in the bucket of the local node then it is scheduled directly\n&gt; into the local heritrix instance.\n&gt;  \n&gt; 4. The &#39;Distributor&#39; caches the info for all other crawl nodes. So in \n&gt; this environment every crawl node is agnostic of every other crawl node.\n&gt;  \n&gt; If the community thinks this could bring value then we can work \n&gt; towards scrubbing out our custom code and get all licensing/legal issues\n&gt; sorted out to get the code into heritrix.\n&gt;  \n&gt; Thanks\n&gt; -Ankur\n&gt;  \n\n\n"}}