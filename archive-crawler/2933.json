{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"OEVQ2pRwEDyISOr4QBnvCiNCpNIlDUjH8iw-U_HZVc7hX7CbGhsaYJaZRRoOJ4vrjBNjwEty6dEj1PkF-brBjfXHQgqO1nk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Reduce Politeness / Increase Download Rate","postDate":"1149796439","msgId":2933,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0ODg4MDU3LjEwNzA5QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PDQ0ODgzQ0EzLjkwMzAwMDNAYXJjaGl2ZS5vcmc+","referencesHeader":"PGU2ODA3aituaDBhQGVHcm91cHMuY29tPiA8NDQ4ODNDQTMuOTAzMDAwM0BhcmNoaXZlLm9yZz4="},"prevInTopic":2930,"nextInTopic":0,"prevInTime":2932,"nextInTime":2934,"topicId":2923,"numMessagesInTopic":5,"msgSnippet":"... This is a good strategy. The only thing that might behave oddly/wrongly is the scheduling of prerequisites (DNS lookups and robots-fetches). Normally, the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 37988 invoked from network); 8 Jun 2006 19:54:38 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m34.grp.scd.yahoo.com with QMQP; 8 Jun 2006 19:54:38 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta4.grp.scd.yahoo.com with SMTP; 8 Jun 2006 19:54:38 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 9ECA11415628E\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  8 Jun 2006 12:53:26 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 10657-06-49 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 8 Jun 2006 12:53:24 -0700 (PDT)\r\nReceived: from [192.168.1.203] (unknown [67.170.222.19])\n\tby mail.archive.org (Postfix) with ESMTP id 3CE8C14156274\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu,  8 Jun 2006 12:53:24 -0700 (PDT)\r\nMessage-ID: &lt;44888057.10709@...&gt;\r\nDate: Thu, 08 Jun 2006 12:53:59 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e6807j+nh0a@...&gt; &lt;44883CA3.9030003@...&gt;\r\nIn-Reply-To: &lt;44883CA3.9030003@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Reduce Politeness / Increase Download Rate\r\nX-Yahoo-Group-Post: member; u=137285340; y=oH6VItk8wPran3vcP-o1WX0R4wV3XGH6e8onT1C7uLrK\r\nX-Yahoo-Profile: gojomo\r\n\r\nMichael Stack wrote:\n&gt; jcr2102 wrote:\n&gt;&gt;\n&gt;&gt; How difficult would it be for me to modify the BDB frontier that ships\n&gt;&gt; with Heritrix, such that it will support multiple simultaneous\n&gt;&gt; connections to the same host?\n&gt;&gt;\n&gt; \n&gt; For the special case where you own both ends -- crawler and server -- \n&gt; one thing to try would be writing your own QueueAssignmentPolicy: \n&gt; http://crawler.archive.org/apidocs/org/archive/crawler/frontier/QueueAssignmentPolicy.html.  \n&gt; The QueueAssignmentPolicy looks at passed URL -- i.e. CandidateURI -- \n&gt; and makes a ruling on the queue the URL belongs to.\n&gt; \n&gt; Usually queue assignment is based on URL domain -- \n&gt; SurtAuthorityQueueAssignmentPolicy -- or IP (IPQueueAssignmentPolicy).\n&gt; \n&gt; An alternate policy is the BucketQueueAssignmentPolicy: \n&gt; http://crawler.archive.org/xref/org/archive/crawler/frontier/BucketQueueAssignmentPolicy.html.  \n&gt; It was written by Christian Kohlschuetter.  It has hardcoded number of \n&gt; queues -- 1021 -- and then does a hash on the IP to figure which of the \n&gt; queues/buckets the URL belongs in.  You could make your own version of \n&gt; this policy -- one that hashed the URL itself so you have 1021 queues \n&gt; but in your case, the URLs are all of the same host.\n&gt; \n&gt; Using such a policy, you should be able to set hundreds of threads \n&gt; running against the one server.  Perhaps first start with 2 or 3 threads \n&gt; and see how your server holds up.  To make Heritrix see your new policy, \n&gt; you&#39;ll have to add it to the list here: \n&gt; http://crawler.archive.org/xref/org/archive/crawler/frontier/AbstractFrontier.html#264 \n&gt; (We should move this list out to a config. file).\n\nThis is a good strategy. The only thing that might behave oddly/wrongly \nis the scheduling of prerequisites (DNS lookups and robots-fetches).\n\nNormally, the prerequisite is pushed onto the top of the queue, and the \nnormal one-item-from-a-queue/host-at-a-time logic ensures the \nprerequisite either succeeds or fails before regular URLs are tried.\n\nIf URLs from one host are spread over many queues, the DNS and robots \nURLs may not be fetched before the URLs that require them -- causing the \nprerequisities to be redundantly rescheduled or even URL failures.\n\nThis might be fixable by ensuring prerequisites always land in the same \nqueue (by name, aka &#39;classKey&#39;) as the URL that triggers them. DNS and \nrobots might then be redundantly refetched up to the number of queues \nacross which a host is split... but at least URLs wouldn&#39;t fail because \nof transient waits for prerequisites on another queue to finish.\n\nI think ensuring that your QueueAssignmentPolicy&#39;s getClassKey always \nreturns the same value for a prerequisite URI as it does for that URI&#39;s \n&#39;via&#39; (triggering) URI would have the desired effect.\n\n- Gordon @ IA\n\n\n"}}