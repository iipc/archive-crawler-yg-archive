{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"oQiuVhL0nEDFAvZ8-wTp32XywP8UhriC3LMYZ-4r4g4Vj6C_0f812SS2rtOqDxb5rvRPZiPJoCuScvIOM6duSw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Large crawl experience (like, 500M links)","postDate":"1134005154","msgId":2402,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzOTc4QkEyLjIwNzAzMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGRuMnNiaytmMmJvQGVHcm91cHMuY29tPg==","referencesHeader":"PGRuMnNiaytmMmJvQGVHcm91cHMuY29tPg=="},"prevInTopic":2391,"nextInTopic":2403,"prevInTime":2401,"nextInTime":2403,"topicId":2391,"numMessagesInTopic":12,"msgSnippet":"... Not that I know of.  I ve witnessed/heard-of 200-300Million with 2 to 3 machines.  Would be interested in hearing about your experiences. ... Dual opterons","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 43850 invoked from network); 8 Dec 2005 00:34:43 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m2.grp.scd.yahoo.com with QMQP; 8 Dec 2005 00:34:43 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta6.grp.scd.yahoo.com with SMTP; 8 Dec 2005 00:34:42 -0000\r\nReceived: (qmail 8760 invoked by uid 100); 8 Dec 2005 00:30:46 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.8?) (stack@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 8 Dec 2005 00:30:46 -0000\r\nMessage-ID: &lt;43978BA2.2070306@...&gt;\r\nDate: Wed, 07 Dec 2005 17:25:54 -0800\r\nUser-Agent: Debian Thunderbird 1.0.7 (X11/20051017)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;dn2sbk+f2bo@...&gt;\r\nIn-Reply-To: &lt;dn2sbk+f2bo@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-96.1 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 2:12:4:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Large crawl experience (like, 500M links)\r\nX-Yahoo-Group-Post: member; u=168599281; y=zUU8YFz-uTOiX2tRn1H3U49kuHQBlOw5kvf8m40UQa4i9iXMl2RcPKmX\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\njoehung302 wrote:\n\n&gt; Hi,\n&gt;\n&gt; Just wondering if anybody have used heritrix to do large crawling at\n&gt; the scale at around 500M links.\n\nNot that I know of.  I&#39;ve witnessed/heard-of 200-300Million with 2 to 3 \nmachines.  Would be interested in hearing about your experiences.\n\n&gt;\n&gt; I know I probably need to use mutliple instances and split crawl\n&gt; technique mentioned in this forum. I plan to start with URLs from the\n&gt; dmoz.org.\n\n&gt;\n&gt; Any help is greatly appreciated.\n&gt;\nDual opterons with loads of memory -- 3 or 4gigs -- and 3 or 4 big disks \nhelps alot.\n\nSplit your I/O across multiple disks: Logs to one disk, ARCs to another, \netc.\n\nWe&#39;ve not had success with 64-bit JVMs as yet so you&#39;re bound at 2gigs \nas maximum heap size.\n\nUse the bloom filter option for the already-seen in BdbFrontier.  Seems \nto work better when a machine goes above 30-50million.  Bloom becomes \nsaturated at 125million so thats about the upperbound per machine at the \nmoment unless you up the bloom filter size  (but its already big and \nyou&#39;ll start eating into heap the crawler is using going about its other \nbusiness).  Thereafter the rate of false positives -- reports that we&#39;ve \nseen an URL when in fact we haven&#39;t -- starts to increase (Read the \nBloomFilter javadoc for more on its workings).\n\nHard part is balancing the crawl -- having it so all machines crawl at \nsame general rate.  Doing a test crawl just to help you set your \nCrawlMapper settings right has proved useful (CrawlMapper is the \nprocessor you&#39;ll have to add to split the scope between machines).  See \nits javadoc for more on how it works.\n\nCrawlMapper drops URLs that are in scope but that are meant for other \nmachines to a file.  On a period, we&#39;d sort the dropped URLs and then \nuse cmdline-jmxclient to add URLs dropped by one machine to the \nappropriate crawler.\n\nHopefully that&#39;ll do for a start,\nYours,\nSt.Ack\n\n\n\n\n\n\n&gt; cheers,\n&gt; -joe\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}