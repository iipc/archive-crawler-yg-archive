{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","replyTo":"LIST","senderId":"DZRp-3h_9fxxJkCb4qFnlRCvCMcJzbYn0w53Bum7kEnNZbAWzlJCjrXrlqI9Q7Gy9_PJo4Ln-pzbcjZi0e3nWg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Congrats, ARC tools, link extractors, PageRank, ...","postDate":"1091835554","msgId":760,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxMTQxNkEyLjYwODA5MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQxMTNGQTA3LjMwMDAyMDZAbG9jLmdvdj4=","referencesHeader":"PDQxMTNGQTA3LjMwMDAyMDZAbG9jLmdvdj4="},"prevInTopic":759,"nextInTopic":774,"prevInTime":759,"nextInTime":761,"topicId":759,"numMessagesInTopic":3,"msgSnippet":"... Thanks Andy. Igor and I had a chat about your note.  Comments interpolated below. ... Go for it! ... Yes.  Still work to be done on proposal.  Notes","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 10436 invoked from network); 6 Aug 2004 23:45:36 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m10.grp.scd.yahoo.com with QMQP; 6 Aug 2004 23:45:36 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 6 Aug 2004 23:45:36 -0000\r\nReceived: (qmail 10942 invoked by uid 100); 6 Aug 2004 23:35:43 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 6 Aug 2004 23:35:43 -0000\r\nMessage-ID: &lt;411416A2.6080905@...&gt;\r\nDate: Fri, 06 Aug 2004 16:39:14 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.1) Gecko/20040802 Debian/1.7.1-5\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;4113FA07.3000206@...&gt;\r\nIn-Reply-To: &lt;4113FA07.3000206@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.1 required=7.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Congrats, ARC tools, link extractors, PageRank,\n ...\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\nAndy Boyko wrote:\n\n&gt;Congratulations, Heritrixians, on 1.0 -- you&#39;ve done a tremendous amount \n&gt;in a remarkably short time.  As the St.Ack says: &quot;good stuff!&quot;\n&gt;\n&gt;  \n&gt;\nThanks Andy.\n\n\nIgor and I had a chat about your note.  Comments interpolated below.\n\n&gt;So with the code freeze lifted, maybe now&#39;s a good time to combine a \n&gt;couple of the discussions going on on this list.  Ansi&#39;s question about \n&gt;PageRank leads to the topic of DAT files, which historically have been \n&gt;the best (only) source for out-linking information for a crawl, but \n&gt;there isn&#39;t yet an open-source tool that can create DATs from ARCs.\n&gt;  \n&gt;\n&gt;I&#39;ve appended some notes about DAT extraction I made.  If nobody else is \n&gt;actively on the problem, I&#39;d like to take a shot at the second approach, \n&gt;the command-line tool.  We&#39;ve got plenty of ARC readers, after all :)\n&gt;\n&gt;  \n&gt;\nGo for it! \n\n&gt;Things get better for this problem with the next version of the ARC \n&gt;format, discussed here a while back (or on the wiki?), in which ARC \n&gt;records include item metadata.  With that, we can just put the extracted \n&gt;URLs in the item&#39;s header when it&#39;s written.  Is there still work to be \n&gt;done on the new ARC spec?\n&gt;  \n&gt;\nYes.  Still work to be done on proposal.  Notes (unfinished) are being \nkep here: http://crawler.archive.org/cgi-bin/wiki.pl?ArcRevisionProposal.\n\n&lt;-- SNIPPED SWEET SUMMARY OF WHY DAT FILE THAT SHOULD BE SAVED TO A WIKI \nPAGE IF\n      NOT POSTED ELSEWHERE --&gt;\n\n&gt;\n&gt;1. Alter Heritrix to write the extracted links directly into the ARC \n&gt;record, as metadata\n&gt;* Pro: no other tools needed; ARC is self-contained\n&gt;* Pro: implementation is easy; right data is available at ARC writing time\n&gt;* Con: doesn&#39;t help with previously collected content\n&gt;* Con: requires update to ARC format\n&gt;  \n&gt;\n\nAnother thought is that we could make a DAT writer that wrote out DATs \nat crawl time.\n\nBut Igor and I were against this approach because does nought for case \nof old ARCs absent such metadata.  Also with time extractors will \nimprove; old ARCs will only have the links that the extractors of that \nera could see unless they were rewritten.\n\n&gt;2. Write a replacement for av_procarc that reads an ARC and generates a \n&gt;DAT, using the Heritrix extractors to find the links again.\n&gt;* Pro: reuses crawler code, ensuring we find the same URLs crawler did\n&gt;* Pro: works on any previously collected ARCs\n&gt;* Con: the extractors are currently very tied to data structures only \n&gt;available during the crawl, and would need to be refactored to support \n&gt;this use\n&gt;  \n&gt;\n+1 from me and +1 from Igor.\n\nWe saw two possible approaches: The behemoth approach and the basic \napproach.  I&#39;m sure there are others.\n\nThe behemoth would be a heritrix crawler with special frontier that took \nsomething like file:///tmp/IAH-20040810101010-crawl03.archive.org.arc.gz \nfor seeds.  An ARCRecordFetcher would pull the records from the arc and \nthen give it to the configured chain of extractors with an DAT file \nwriter at the end of the chain.\n* Pro: Can use the processing model, settings system, etc., from base \ncrawler.\n* Pro: Would be useful tool debugging and profiling heritrix.\n* Con: Its a behemoth soln. to a small problem.\n\nThe basic approach would be some subclass of ARCReader that took a list \nof extractors and arcs to process on the command line.  The extractors \nwould be the refactored guts of current extractors stripped of the \ncrawler context (i.e. no reference to CrawlURIs, etc.  The crawler \ncontext references would be in wrappers, just as the ARCWriterProcessor \nwraps ARCWriter now).  Per extractor, the subclass of ARCReader would \nreget the ARCRecord till all were done (Or, if more than one, it might \npay to add caching if small or use the subclass our netarchive.dk \ncolleagues have been talking of).\n* Pro: Easier delivery\n\n&gt;3. Read the Heritrix recovery log recovery.gz, which includes links as \n&gt;they are extracted, along with the URL of the item containing them.\n&gt;* Pro: Available immediately after crawl with minimal processing\n&gt;* Con: requires complex correlation with ARC files if full DAT files are \n&gt;desired (versus just the list of links), since the ARC and the recovery \n&gt;log will not necessarily be in the same order (use intermediate DB?)\n&gt;* Con: requires access to the recovery file, only available in the right \n&gt;format with Heritrix 1.0 crawls\n&gt;\n&gt;  \n&gt;\nThis is a nice idea but I think your &#39;Con&#39; above makes it undoable.\n\nSt.Ack\n\n&gt;\n&gt;\n&gt; \n&gt;Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n&gt; \n&gt;\n&gt;  \n&gt;\n\n\n"}}