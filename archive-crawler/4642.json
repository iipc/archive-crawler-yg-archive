{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"J2SljMQMx1PImLEnq6d_gynunc9RIgk1m8CW52IWCyP8qPi5GbcJNfL5xemwASzYAXcM2-UthWKITY2Hhqpd363BLVWWgl4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: java.lang.OutOfMemoryError: GC overhead limit exceeded","postDate":"1193775418","msgId":4642,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3Mjc5MTNBLjIwMDAzMDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGZnNWlhdSs5ODI3QGVHcm91cHMuY29tPg==","referencesHeader":"PGZnNWlhdSs5ODI3QGVHcm91cHMuY29tPg=="},"prevInTopic":4640,"nextInTopic":4644,"prevInTime":4641,"nextInTime":4643,"topicId":4629,"numMessagesInTopic":9,"msgSnippet":"... I know of no problems with that configuration. ... We typically do not run multiple instances per JVM in our crawling, so we usually leave the cache","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 47342 invoked from network); 30 Oct 2007 20:16:57 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m44.grp.scd.yahoo.com with QMQP; 30 Oct 2007 20:16:57 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta16.grp.scd.yahoo.com with SMTP; 30 Oct 2007 20:16:57 -0000\r\nX-Received: (qmail 37805 invoked from network); 30 Oct 2007 20:16:56 -0000\r\nX-Received: from unknown (HELO ?192.168.1.30?) (unknown)\n  by unknown with SMTP; 30 Oct 2007 20:16:56 -0000\r\nX-pair-Authenticated: 76.102.230.209\r\nMessage-ID: &lt;4727913A.2000307@...&gt;\r\nDate: Tue, 30 Oct 2007 13:16:58 -0700\r\nUser-Agent: Thunderbird 2.0.0.6 (Windows/20070728)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;fg5iau+9827@...&gt;\r\nIn-Reply-To: &lt;fg5iau+9827@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: java.lang.OutOfMemoryError: GC overhead\n limit exceeded\r\nX-Yahoo-Group-Post: member; u=137285340; y=PqC7v0wd9qOUiwiHA1gmI3XEy2vOfQIAmGFKcuJBQfz5\r\nX-Yahoo-Profile: gojomo\r\n\r\nhinoglu wrote:\n&gt; hi, sorry i had some more details in the previous mail, \n&gt; but 65k mail limit rip them off. \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt; What version of Heritrix are you using? 1.12.1 has fixes that would be \n&gt; \n&gt; i&#39;m using version 1.12.1 on tomcat-6.0.14, jdk-1.6.0_2, running on\n&gt; linux 2.6.18 with 2 gbs of ram.\n\nI know of no problems with that configuration.\n\n&gt;&gt; Also remember that every crawl creates its own BerkeleyDB-JE \n&gt;&gt; environment, and each such environment tries to limit its cache by \n&gt;&gt; default to using 60% of the total heap. So, if you leave this\n&gt; default in \n&gt;&gt; place, and launch 2 simultaneous crawls, their environments will\n&gt; seek to \n&gt;&gt; limit themselves to 120% of the heap -- almost guaranteed to cause \n&gt;&gt; problems unless the crawls wrap up very quickly.\n&gt; \n&gt; hm i&#39;ve got most of the settings untouched, should&#39;ve read the\n&gt; instructions better :( i&#39;ll try again by setting the bdb heap around \n&gt; 20-25%, but again it seems that: if i have 5-6 instances at the same\n&gt; time(since i have around 20 sites to crawl), then i&#39;ll probably have\n&gt; some troubles. so, what would be the optimum setting for this value?\n\nWe typically do not run multiple instances per JVM in our crawling, so \nwe usually leave the cache percentage at its default (setting &#39;0&#39;, which \nmeans use the BDB default, which would be 60% unless reset with a BDB \nproperty). (In big crawls using an alternate non-BDB UriUniqFilter, we \nmay lower this to 20%.)\n\nSo, we have no recommendation for an optimal value based on experience. \nIf 60% works well for a single crawl, I would avoid having the \npercentages of all active crawls total greater than 60%.\n\n&gt;&gt; The size and number of &#39;settings&#39;-related instances in your &#39;jmap \n&gt;&gt; -histo&#39; output caught my eye. Are you making extensive use of\n&gt; per-domain \n&gt;&gt; overrides?\n&gt; \n&gt; i&#39;m not sure if i got it right, so let me explain my crawling process. \n&gt; i have around 20 wap and web sites to crawl, and for the process i\n&gt;  have created 5 profiles with different user-agent and cookie\n&gt; settings. according to the periodsi, crawling processes are\n&gt;  automatically started with a script that uses the jmx commandline\n&gt;  client.  \n&gt; \n&gt; data per site is around 50mb&#39;s at most, consisting of \n&gt; thousands of tiny files and urls.\n&gt; \n&gt; when the crawling process is ended, the same script destroys the \n&gt; instance related with the process using the destroy command via jmx\n&gt; client.\n&gt; if i get it right, yes it seems i&#39;m making extensive use of per-domain\n&gt; overrides.. any suggestions about a better solution for this scenario?\n\nIf you&#39;re only crawling ~20 main sites, do you have many more than 20 \ndomain overrides? (I wouldn&#39;t be concerned unless it was in the \nthousands, and even then it might not be a problem.)\n\nAnother thing i noted in your &#39;jmap&#39; output was that there were 1918 \ninstances of a number of objects that would normally exist only once per \ncrawl, and should disappear when the crawl is cleanly finished (like \nindividual Processors). This could be peculiar to your pattern of use, \nor a more general bug of post-crawl object retention.\n\nDo your crawls usually finish by themselves or are they stopped by the \noperator/JMX?\n\n- Gordon @ IA\n\n"}}