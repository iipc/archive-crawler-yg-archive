{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"xUYIMv7AAAgOTluNBT-SeIBZHzCH9zHnDd5E_ZcC4X6wDKtmBrJKVxHeJ2YQSn5hZSyq7s7XAXoRInMZl0PCqqOD0E9RQgE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Heretrix downloaded/queued document ratio","postDate":"1124236330","msgId":2113,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzMDI3QzJBLjMwNDAwMDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDYuMi4zLjQuMi4yMDA1MDgxNjA5NTcwNi4wMjU4MmI5OEB2aG9zdDYuYXRvbWljc2VydmVycy5jb20+","referencesHeader":"PDYuMi4zLjQuMi4yMDA1MDgxNTEwMTkyMy4wMjRjYTk4OEB2aG9zdDYuYXRvbWljc2VydmVycy5jb20+IDw0MzAxMDg4My44MDAwNkBhcmNoaXZlLm9yZz4gPDYuMi4zLjQuMi4yMDA1MDgxNTE3MjMwNi4wMjUyNTUyOEB2aG9zdDYuYXRvbWljc2VydmVycy5jb20+IDw0MzAxMzJCQy44MDAwMEBhcmNoaXZlLm9yZz4gPDYuMi4zLjQuMi4yMDA1MDgxNjA5NTcwNi4wMjU4MmI5OEB2aG9zdDYuYXRvbWljc2VydmVycy5jb20+"},"prevInTopic":2112,"nextInTopic":0,"prevInTime":2112,"nextInTime":2114,"topicId":2104,"numMessagesInTopic":6,"msgSnippet":"The -5000 indicates the URI is being ruled out-of-scope before it s even fetched -- which is further confirmed by the lack of any displayed length. So there s","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 42118 invoked from network); 16 Aug 2005 23:52:15 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m32.grp.scd.yahoo.com with QMQP; 16 Aug 2005 23:52:15 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta3.grp.scd.yahoo.com with SMTP; 16 Aug 2005 23:52:15 -0000\r\nReceived: (qmail 28929 invoked by uid 100); 16 Aug 2005 23:52:11 -0000\r\nReceived: from adsl-71-130-102-78.dsl.pltn13.pacbell.net (HELO ?192.168.1.10?) (gojomo@...@71.130.102.78)\n  by mail-dev.archive.org with SMTP; 16 Aug 2005 23:52:11 -0000\r\nMessage-ID: &lt;43027C2A.3040007@...&gt;\r\nDate: Tue, 16 Aug 2005 16:52:10 -0700\r\nUser-Agent: Mozilla Thunderbird 1.0.6-1.1.fc3 (X11/20050720)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;6.2.3.4.2.20050815101923.024ca988@...&gt; &lt;43010883.80006@...&gt; &lt;6.2.3.4.2.20050815172306.02525528@...&gt; &lt;430132BC.80000@...&gt; &lt;6.2.3.4.2.20050816095706.02582b98@...&gt;\r\nIn-Reply-To: &lt;6.2.3.4.2.20050816095706.02582b98@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-66.8 required=7.0 tests=AWL,USER_IN_WHITELIST \n\tautolearn=no version=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heretrix downloaded/queued document ratio\r\nX-Yahoo-Group-Post: member; u=137285340; y=_K7SmVPzlWtuNzr5ST3n6ja6mKIx_zUtdsMoRihCTYUc\r\nX-Yahoo-Profile: gojomo\r\n\r\nThe -5000 indicates the URI is being ruled out-of-scope before\nit&#39;s even fetched -- which is further confirmed by the lack of\nany displayed length. So there&#39;s no content-type header at all\nto check.\n\nI presume that your filters don&#39;t reject such pre-fetch URIs --\nor else I can&#39;t imagine any of your URIs succeeding.\n\nBut, I still suspect something in your scope that rejects some\nURIs, but not all. If there&#39;s no obvious distinction between\nthose seeds that succeed, and those that don&#39;t, the next thing\nI&#39;d try would be a crawl with a single one of the URIs that\nfail, and use a debugger to step through the Preselector\ninnerProcess application of scoping, to see exactly which\nclause/filter is at fault.\n\n- Gordon @ IA\n\nMike Schwartz wrote:\n&gt; hi,\n&gt; \n&gt; Yes, the log entry included below was a seed.  As for scoping: I&#39;m using \n&gt; DomainScope, and I have 3 filters set up to crawl only HTML content \n&gt; (slightly modified version of what&#39;s described at \n&gt; http://groups.yahoo.com/group/archive-crawler/message/1243) as well as a \n&gt; URI regexp filter set to avoid crawling https pages.\n&gt; \n&gt; grepping thru the Heretrix code it looks like &quot;no-type&quot; has to do with \n&gt; mime type, but I checked an ethereal capture of retrieving \n&gt; http://www.interstaterestoration.com/ , and it is definitely returning a \n&gt; mime type.  If it wasn&#39;t that might explain the problem because I have a \n&gt; midfetch-filter that looks at the HTTP response header, looking for \n&gt; (?i)text/html.*)\n&gt; \n&gt; is it possible the midfetch-filter code has a bug that&#39;s mistakenly \n&gt; thinking the mime type isn&#39;t set in the response header?\n&gt; \n&gt; thanks\n&gt;  - Mike\n&gt; \n&gt; At 06:26 PM 8/15/2005, you wrote:\n&gt; \n&gt;&gt; Mike Schwartz wrote:\n&gt;&gt; &gt; thanks for the explanation on this.  I looked at the crawl.log and I \n&gt;&gt; see\n&gt;&gt; &gt; a whole bunch of my seed URLs with an entry like\n&gt;&gt; &gt; 2005-08-15T22:19:21.481Z -5000          -\n&gt;&gt; &gt; \n&gt;&gt; &lt;http://www.interstaterestoration.com/&gt;http://www.interstaterestoration.com/ \n&gt;&gt; - - no-type #095 - - -\n&gt;&gt; &gt;\n&gt;&gt; &gt; does that mean no mime-type known, or ?\n&gt;&gt; &gt;\n&gt;&gt; &gt; something seems pretty wrong, as I could believe an occasional site \n&gt;&gt; gets\n&gt;&gt; &gt; discarded by here 3/4 of my sites are dropping out\n&gt;&gt;\n&gt;&gt; Consulting FetchStatusCodes, -5000 is &quot;S_OUT_OF_SCOPE&quot;, and is only\n&gt;&gt; set by Preselector -- meaning these URIs were scheduled somehow but\n&gt;&gt; then fail to pass scope-testing when first considered for crawling.\n&gt;&gt;\n&gt;&gt; Since there&#39;s no &#39;via&#39; or &#39;hops-path&#39; info, I presume this is a\n&gt;&gt; seed. Have you perhaps set up a scope that does not accept some\n&gt;&gt; of your seeds?\n&gt;&gt;\n&gt;&gt; Two possibly-relevant open issues:\n&gt;&gt;\n&gt;&gt;   Bug [ 1249828 ] -5000 out-of-scope preconditions; -50 failure\n&gt;&gt;\n&gt;&gt; &lt;https://sourceforge.net/tracker/index.php?func=detail&aid=1249828&group_id=73833&atid=539099&gt;https://sourceforge.net/tracker/index.php?func=detail&aid=1249828&group_id=73833&atid=539099 \n&gt;&gt;\n&gt;&gt;   (probably not the cause of your problem, as it affects prerequisites,\n&gt;&gt;   not seeds themselves)\n&gt;&gt;\n&gt;&gt;   Bug [ 1219486 ] no rule for decidingscope to always crawl seeds\n&gt;&gt;\n&gt;&gt; &lt;https://sourceforge.net/tracker/index.php?func=detail&aid=1219486&group_id=73833&atid=539099&gt;https://sourceforge.net/tracker/index.php?func=detail&aid=1219486&group_id=73833&atid=539099 \n&gt;&gt;\n&gt;&gt;   (based on the idea that people might want &#39;seeds&#39; to be definitionally\n&gt;&gt;   in-scope simply because they&#39;re seeds, even if they wouldn&#39;t otherwise\n&gt;&gt;   be ruled-in)\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; &gt; thanks\n&gt;&gt; &gt;  - Mike\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt; At 03:26 PM 8/15/2005, you wrote:\n&gt;&gt; &gt;\n&gt;&gt; &gt;&gt; Mike Schwartz wrote:\n&gt;&gt; &gt;&gt; &gt; hi,\n&gt;&gt; &gt;&gt; &gt;\n&gt;&gt; &gt;&gt; &gt; I notice that Heretrix 1.5.0 (taken from the HEAD last week) outputs\n&gt;&gt; &gt;&gt; &gt; a down-counting demonimator in the downloaded/queued document\n&gt;&gt; &gt;&gt; &gt; ratio.  For example I started a crawl with 2407 seeds, and it\n&gt;&gt; &gt;&gt; &gt; initially grew as new pages were discovered, but after a few minutes\n&gt;&gt; &gt;&gt; &gt; it reported that I was on page 568 of 593... making it hard to get a\n&gt;&gt; &gt;&gt; &gt; sense of what the real % complete is\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; It&#39;s been this way for a while: URIs that were scheduled, but then\n&gt;&gt; &gt;&gt; &#39;disregarded&#39; don&#39;t count as either pending or completed.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; The most notable cause of &#39;disregarded&#39; URIs is those that are\n&gt;&gt; &gt;&gt; precluded by robots.txt rules -- they get scheduled, but are never\n&gt;&gt; &gt;&gt; attempted, and so should neither count as successful, failed, or\n&gt;&gt; &gt;&gt; still-pending.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; If you&#39;ve added other settings which can cause URIs to be\n&gt;&gt; &gt;&gt; ignored after having been initially scheduled -- such as\n&gt;&gt; &gt;&gt; narrowing the scope midcrawl, resulting in URIs that get\n&gt;&gt; &gt;&gt; ruled out-of-scope on rechecking -- that will also cause\n&gt;&gt; &gt;&gt; URIs to be disregarded, and the total &#39;queued&#39; will decrease\n&gt;&gt; &gt;&gt; without the &#39;completed&#39; changing.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; This area of the crawler needs improvement; the current\n&gt;&gt; &gt;&gt; tallies of &#39;percent done&#39; and &#39;estimated time remaining&#39;\n&gt;&gt; &gt;&gt; take no account of the potential for more URIs to be\n&gt;&gt; &gt;&gt; discovered, and so are almost worse-than-useless.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; I think tracking the effective rates of discovery versus\n&gt;&gt; &gt;&gt; completion -- and the rates of change of each -- could\n&gt;&gt; &gt;&gt; offer more meaningful estimates, for both the whole crawl\n&gt;&gt; &gt;&gt; and individual queues. Some notes on possible approaches\n&gt;&gt; &gt;&gt; are on the crawler wiki [*] here:\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; \n&gt;&gt; &lt;&lt;http://crawler.archive.org/cgi-bin/wiki.pl?CrawlAndQueueDynamics&gt;http://crawler.archive.org/cgi-bin/wiki.pl?CrawlAndQueueDynamics&gt;http://crawler.archive.org/cgi-bin/wiki.pl?CrawlAndQueueDynamics \n&gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; But, implementing some of these ideas is not yet a priority\n&gt;&gt; &gt;&gt; for any upcoming releases, so it would take someone interested\n&gt;&gt; &gt;&gt; in this area to contribute some code for the situation to\n&gt;&gt; &gt;&gt; improve in the short term.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; - Gordon @ IA\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; [*] Reminder: the crawler wiki edit password is available on\n&gt;&gt; &gt;&gt; request to any Heritrix users/developers interested in\n&gt;&gt; &gt;&gt; adding/revising the content. Email any IA team member for it.\n&gt;&gt; &gt;&gt; It&#39;s only protected at all because of past problems with crude\n&gt;&gt; &gt;&gt; wiki spam.\n&gt; \n&gt; \n\n"}}