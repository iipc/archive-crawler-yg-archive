{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":294888586,"authorName":"nuggetwheat","from":"&quot;nuggetwheat&quot; &lt;doug@...&gt;","profile":"nuggetwheat","replyTo":"LIST","senderId":"ZOxWENyJ7I4lXiW3SswkuDqQMdwqf31IIdRwD5JVu0Gwzwn6uKS9N7Q_Wmkm3r6Bfy-J3tBxMO4ho0ZXB0dC2LqWpQyuOw","spamInfo":{"isSpam":false,"reason":"6"},"subject":"[ANC] Hadoop hdfs_writer_processor-0.1.2 released","postDate":"1170962243","msgId":3794,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVxZnQwMys5bWkwQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":3793,"nextInTime":3795,"topicId":3794,"numMessagesInTopic":1,"msgSnippet":"You can find it here: http://www.zvents.com/labs/hdfs_writer_processor I ve iterated on this a bit and have used it for a 5 million document crawl with no","rawEmail":"Return-Path: &lt;doug@...&gt;\r\nX-Sender: doug@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 65305 invoked from network); 8 Feb 2007 19:17:36 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m40a.grp.scd.yahoo.com with QMQP; 8 Feb 2007 19:17:36 -0000\r\nReceived: from unknown (HELO n9b.bullet.sp1.yahoo.com) (69.147.64.101)\n  by mta9.grp.scd.yahoo.com with SMTP; 8 Feb 2007 19:17:36 -0000\r\nReceived: from [216.252.122.217] by n9.bullet.sp1.yahoo.com with NNFMP; 08 Feb 2007 19:17:24 -0000\r\nReceived: from [209.73.164.83] by t2.bullet.sp1.yahoo.com with NNFMP; 08 Feb 2007 19:17:24 -0000\r\nReceived: from [66.218.66.92] by t7.bullet.scd.yahoo.com with NNFMP; 08 Feb 2007 19:17:24 -0000\r\nDate: Thu, 08 Feb 2007 19:17:23 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;eqft03+9mi0@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;nuggetwheat&quot; &lt;doug@...&gt;\r\nSubject: [ANC] Hadoop hdfs_writer_processor-0.1.2 released\r\nX-Yahoo-Group-Post: member; u=294888586; y=UfU3GMdI-C-j6UDv2pFTb_dXi3UuPDlaSzz9P9pkx5licGPtcp0\r\nX-Yahoo-Profile: nuggetwheat\r\n\r\nYou can find it here:\n\nhttp://www.zvents.com/labs/hdfs_writer_processor\n\nI&#39;=\r\nve iterated on this a bit and have used it for a 5 million document\ncrawl w=\r\nith no problems.  This latest release also includes some\nexample map-reduce=\r\n programs for processing the crawl output.  I&#39;ve\nincluded the CHANGELOG bel=\r\now.  Feel free to send me your feedback\n(doug at zvents.com).\n\n- Doug\n\nChan=\r\nges in version 0.1.2\n------------------------\n* Improved charset and conten=\r\nt-type extraction\n\n* Added two more map-reduce example programs:\n  \n  1. or=\r\ng.archive.crawler.examples.mapred.CoutMimeTypes - For\n     generating count=\r\ns for each unique Content-type encountered in \n     crawl\n\n  2. org.archive=\r\n.crawler.examples.mapred.HtmlLinkCount - For \n     generating internal and/=\r\nor external link counts\n\n* Changed CountCharsets example to accept multiple=\r\n input directories\n  on command line\n\n\nChanges in version 0.1.1\n-----------=\r\n-------------\n* Fixed bug where open output files were not getting explicit=\r\nly\n  closed and renamed to remove the &quot;.open&quot; extension\n\n* Added HDFSWriter=\r\nDocument class for doing a minimal (efficient)\n  parse of the hdfs-writer-p=\r\nrocessor document format.  Among other\n  things, it fills a HashMap of the =\r\nANVL fields, gives you a pointer\n  to the request, gives you pointers to th=\r\ne response and message body\n  and determines the message body character enc=\r\noding from the response\n  headers as well as by inspecting the document its=\r\nelf for charset\n  specification in the &lt;meta http-equiv... and &lt;?xml ... en=\r\ncoding=3D\n\n* Added an example map-reduce program called\n  org.archive.crawl=\r\ner.examples.mapred.CoutCharsets to demonstrate how\n  to write a map-reduce =\r\nprogram using the output of a Heritrix crawl\n  as input.  This example prog=\r\nram produces counts for all of the \n  unique character encodings (charsets)=\r\n encountered in your crawled \n  documents.\n  The source code for this examp=\r\nle can be found in the file\n  src/java/org/archive/crawler/examples/mapred/=\r\nCountCharsets.java in \n  the source distribution.  See README.txt for how t=\r\no run it.\n\n\n\n"}}