{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Ye6WbcRshkUqIr7_KcoAMdiiW_2HqAUV9YnKyBY0PzWNyIdIxiVO8KupCjuM12949vpKT8HYC9ZEkyO7ihu-N2kjZ7dF7lk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Max Size Related Configuration","postDate":"1080689224","msgId":300,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwNkEwMjQ4LjIwOTAyMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQwNjlGRTI0LjgwNDAzMDNAYXJjaGl2ZS5vcmc+","referencesHeader":"PGM0YzNyNStjdDVwQGVHcm91cHMuY29tPiA8NDA2OUEyRkUuNTA0MDQwM0BhcmNoaXZlLm9yZz4gPDQwNjlGRTI0LjgwNDAzMDNAYXJjaGl2ZS5vcmc+"},"prevInTopic":299,"nextInTopic":301,"prevInTime":299,"nextInTime":301,"topicId":289,"numMessagesInTopic":9,"msgSnippet":"Also, RFE 891986 added a bandwidth-throttle facility, which I believe can be set per host. John Erik, can you say more about this capability? - Gordon","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 65850 invoked from network); 30 Mar 2004 23:27:09 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m2.grp.scd.yahoo.com with QMQP; 30 Mar 2004 23:27:09 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 30 Mar 2004 23:27:08 -0000\r\nReceived: (qmail 14583 invoked by uid 100); 30 Mar 2004 23:22:22 -0000\r\nReceived: from b116-dyn-47.archive.org (HELO archive.org) (gojomo@...@209.237.240.47)\n  by mail-dev.archive.org with SMTP; 30 Mar 2004 23:22:22 -0000\r\nMessage-ID: &lt;406A0248.2090205@...&gt;\r\nDate: Tue, 30 Mar 2004 15:27:04 -0800\r\nUser-Agent: Mozilla Thunderbird 0.5 (X11/20040208)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c4c3r5+ct5p@...&gt; &lt;4069A2FE.5040403@...&gt; &lt;4069FE24.8040303@...&gt;\r\nIn-Reply-To: &lt;4069FE24.8040303@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-0.0 required=6.0 tests=AWL autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Max Size Related Configuration\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nAlso, RFE 891986 added a bandwidth-throttle facility, which I believe can\nbe set per host. John Erik, can you say more about this capability?\n\n- Gordon\n\nIgor Ranitovic wrote:\n&gt; It seems that Sebs concern is not just polities but the number of bytes downloaded from sites.\n&gt; Some ISPs will charge you arm and leg if you exceed given traffic cap in bytes per month.\n&gt; For example, you can buy a plan from a ISP that puts a cap of 100MB of traffic per host per month.\n&gt; If you exceed this cap than you start paying usually for every extra MB of traffic.\n&gt; If you upload a 3-4 MB mp3 song on your site, and it becomes popular, or being crawled often, than \n&gt; the traffic cap can be easily exceeded (read costs money).\n&gt; My suggestion is to write to webmasters of sites of interest and ask for permission to crawled them \n&gt; if possible.\n&gt; \n&gt; Take care.\n&gt; i.\n&gt; \n&gt; Kristinn Sigur�sson wrote:\n&gt; \n&gt; \n&gt;&gt;See below\n&gt;&gt;\n&gt;&gt;sebastiandelachica wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;Hiya Kris,\n&gt;&gt;&gt;\n&gt;&gt;&gt;Thanks for the hint. That is pretty much where I ended up last nite.\n&gt;&gt;&gt;\n&gt;&gt;&gt;To clarify, my original intent was to manage xtiple sites from a\n&gt;&gt;&gt;single crawl order, but to get the size limit per site, I ended\n&gt;&gt;&gt;up &quot;simplifying&quot; my approach and limiting each order to a single\n&gt;&gt;&gt;seed. I like the idea of controlling limits on # of downloaded  bytes\n&gt;&gt;&gt;on a per domain/host/etc. basis for xtiple seeds from a single order.\n&gt;&gt;&gt;I hope that concept becomes part of heritrix at some point in the\n&gt;&gt;&gt;future.\n&gt;&gt;&gt;\n&gt;&gt;&gt;To put things into perspective, my research work is related to the\n&gt;&gt;&gt;Digital Library for Earth Sciences (DLESE) http://www.dlese.org. I am\n&gt;&gt;&gt;trying to automatically tag online educational resources with\n&gt;&gt;&gt;National Science Educational Standards using natural language\n&gt;&gt;&gt;processing techniques. The reason for the size limit per site is b/c\n&gt;&gt;&gt;I need to avoid upsetting the community that provides the content.\n&gt;&gt;\n&gt;&gt;We are also aware of the fact that you can&#39;t overload websites and that \n&gt;&gt;is why the crawler is very\n&gt;&gt;polite. If you look at the settings under &#39;frontier&#39; you will find the \n&gt;&gt;following:\n&gt;&gt;\n&gt;&gt;delay-factor:\n&gt;&gt;max-delay-ms:\n&gt;&gt;min-delay-ms:\n&gt;&gt;min-interval-ms:\n&gt;&gt;\n&gt;&gt;These settings insure that Heritrix does not hammer a site needlessly. \n&gt;&gt;At any given time only\n&gt;&gt;one document is being fetched from the same host. The delay-factor says \n&gt;&gt;that for every X\n&gt;&gt;millisec you spend downloading a document you should wait factor of X \n&gt;&gt;before starting the\n&gt;&gt;next document download from that same host.  max and min delay allow you \n&gt;&gt;to set upper\n&gt;&gt;and lower bounds on this factor.\n&gt;&gt;\n&gt;&gt;Example: Last document took 200 msec and your delay factor is 3. That \n&gt;&gt;means that 600 msec\n&gt;&gt;will have to elapse before Heritrix starts another download.  If \n&gt;&gt;min-delay is set to 800 that will\n&gt;&gt;override and Heritrix will wait 800 msec.\n&gt;&gt;\n&gt;&gt;Even at it&#39;s most aggressive Heritrix will never fetch more then one \n&gt;&gt;doc. at a time and with the\n&gt;&gt;politeness settings found in the Simple profile you definately don&#39;t \n&gt;&gt;need to worry about\n&gt;&gt;upsetting anyone.\n&gt;&gt;\n&gt;&gt;- Kris\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;Thanks for all the good advice and the Most Excellent Work on\n&gt;&gt;&gt;Heritrix!\n&gt;&gt;&gt;\n&gt;&gt;&gt;Seb\n&gt;&gt;&gt;\n&gt;&gt;&gt;--- In archive-crawler@yahoogroups.com, Kristinn Sigur�sson\n&gt;&gt;&gt;&lt;kris@a...&gt; wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Hei Seb.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;See below\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;sebastiandelachica wrote:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;Michael,\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;Thanks for the prompt reply. Indeed I upgraded to 0.6.0 over the\n&gt;&gt;&gt;&gt;&gt;weekend: amazing how close a 9 looks to a 6 given enough lack of\n&gt;&gt;&gt;\n&gt;&gt;&gt;sleep\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;In English (to the best of my ability), what I am trying to do is\n&gt;&gt;&gt;\n&gt;&gt;&gt;use\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;a separate order for each site I need to crawl. Each order hence\n&gt;&gt;&gt;\n&gt;&gt;&gt;has\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;a single seed. The purpose is to place an upper bound on the\n&gt;&gt;&gt;\n&gt;&gt;&gt;number\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;of &quot;useful&quot; bytes downloaded for each crawl order (about 100K or\n&gt;&gt;&gt;\n&gt;&gt;&gt;1MB\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;say). In other words, stop crawling the site once we have\n&gt;&gt;&gt;\n&gt;&gt;&gt;downloaded\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;some number of usable bytes.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;If you are only crawling one site at a time (using DomainScope) the\n&gt;&gt;&gt;&gt;max-bytes-download\n&gt;&gt;&gt;&gt;just the thing for you.  It limits the total amount of data\n&gt;&gt;&gt;\n&gt;&gt;&gt;downloaded\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;in one CrawlJob. It is\n&gt;&gt;&gt;&gt;only if you are crawling multiple domains in the same job (as is\n&gt;&gt;&gt;\n&gt;&gt;&gt;usual)\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;that you can&#39;t use it\n&gt;&gt;&gt;&gt;for this purpose as it would only cut you off once the total from\n&gt;&gt;&gt;\n&gt;&gt;&gt;all\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;domain hit the limit.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Once the limit is hit the current job will end.  At some point we\n&gt;&gt;&gt;\n&gt;&gt;&gt;may\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;allow overrides on this\n&gt;&gt;&gt;&gt;setting enabling cutoffs on specific domains but that is well into\n&gt;&gt;&gt;\n&gt;&gt;&gt;the\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;future.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;I hope this is of some use to you.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;- Kris\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;I thought setting a max size limit on\n&gt;&gt;&gt;&gt;&gt;the ARC file would stop logging past that point, but I see now\n&gt;&gt;&gt;\n&gt;&gt;&gt;that\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;it means a slightly different thing. I tried the number of files\n&gt;&gt;&gt;&gt;&gt;limit and while it works, I am not sure it matches my intent as\n&gt;&gt;&gt;\n&gt;&gt;&gt;some\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;files may be noticably shorter than others.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;I am using a DomainScope crawl per the settings in the Profile\n&gt;&gt;&gt;\n&gt;&gt;&gt;used\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;to create the Job.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;Based on the information you sent me, I need to think about what\n&gt;&gt;&gt;&gt;&gt;might serve my purpose. Thanks for pointing me at the right code\n&gt;&gt;&gt;\n&gt;&gt;&gt;in\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;ARCWriter.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;Seb\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;--- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@a...&gt;\n&gt;&gt;&gt;&gt;&gt;wrote:\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;Thanks for trying Heritrix Seb.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;See below.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;sebastiandelachica wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;I have been playing around with heritrix for a few weeks now\n&gt;&gt;&gt;\n&gt;&gt;&gt;and I\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;am\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;in the process of turning it loose on a controlled environment\n&gt;&gt;&gt;\n&gt;&gt;&gt;for\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;one of my research strands. I am currently using version\n&gt;&gt;&gt;\n&gt;&gt;&gt;0.9.0. My\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;objective is to limit the amount of data scooped from a site\n&gt;&gt;&gt;\n&gt;&gt;&gt;onto\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;the\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;ARC file. I tried using the HTTP Processor max-length-bytes and\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;the\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Archiver max-size-bytes.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;Do you mean 0.4.0. You say 0.9.0 above.  We just released 0.6.0\n&gt;&gt;&gt;\n&gt;&gt;&gt;on\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;friday.  Try it if you haven&#39;t already.  Lots of fixes and\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;improvements.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;If you&#39;re doing a broad crawl, you have the following options\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;available\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;to you:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;max-bytes-download\n&gt;&gt;&gt;&gt;&gt;&gt;max-document-download\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;These options are not available in a domain scoped crawl which\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;seems to\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;be what it is you&#39;d like to do.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;Tell us more about what it is that you&#39;d like.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;The max-length-bytes options limits size of a particular\n&gt;&gt;&gt;\n&gt;&gt;&gt;download\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;only.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;The max-size-bytes  is upper-bound on the size of ARC files\n&gt;&gt;&gt;\n&gt;&gt;&gt;written\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;(See\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;the code here http://crawler.archive.org/xref/index.html). \n&gt;&gt;&gt;\n&gt;&gt;&gt;&lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;Yours,\n&gt;&gt;&gt;&gt;&gt;&gt;St.Ack\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;The Processor max-length-bytes seems to work at some level as\n&gt;&gt;&gt;\n&gt;&gt;&gt;the\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;requests are reported as length-truncated in the logs, but the\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;actual\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;HTML files still make it into the archive.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;The Archive max-size-bytes appears to be ignored. Looking at\n&gt;&gt;&gt;\n&gt;&gt;&gt;the\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;code, it does not seem to be used by the ARCWriterProcessor\n&gt;&gt;&gt;\n&gt;&gt;&gt;class\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;or\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;any other class for that matter...I just starting digging\n&gt;&gt;&gt;\n&gt;&gt;&gt;through\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;the\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;code earlier today.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;I continue my experimentation and code reading, but figured,\n&gt;&gt;&gt;\n&gt;&gt;&gt;I&#39;d\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;check in to see if I am missing something very obvious or if\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;anyone\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;had experienced similar behavior.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Thanks in advance for your time,\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Seb\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;------------------------------------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt;------\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;*Yahoo! Groups Links*\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;    * To visit your group on the web, go to:\n&gt;&gt;&gt;&gt;&gt;      http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt;&gt;&gt;&gt;      \n&gt;&gt;&gt;&gt;&gt;    * To unsubscribe from this group, send an email to:\n&gt;&gt;&gt;&gt;&gt;      archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;&gt;&gt;&gt;      &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?\n&gt;&gt;&gt;\n&gt;&gt;&gt;subject=Unsubscribe&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;      \n&gt;&gt;&gt;&gt;&gt;    * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt;&gt;&gt;&gt;      Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;------------------------------------------------------------------------\n&gt;&gt;*Yahoo! Groups Links*\n&gt;&gt;\n&gt;&gt;    * To visit your group on the web, go to:\n&gt;&gt;      http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt;       \n&gt;&gt;    * To unsubscribe from this group, send an email to:\n&gt;&gt;      archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;      &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;&gt;       \n&gt;&gt;    * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt;      Service &lt;http://docs.yahoo.com/info/terms/&gt;. \n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n\n\n"}}