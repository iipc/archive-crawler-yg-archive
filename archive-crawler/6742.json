{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"7pWs-OeJzawF92yXCUZY9M3wNDJOJkDsVH0C0cB-g7dJ6z8Oouf92heDR5Zb0GN1l3iNklmhgfhfa_aFCTMpDZEm_avOgH8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Re: Simple, single-site crawl &quot;hangs&quot; on last few URIs?","postDate":"1285190401","msgId":6742,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOUE3MzAxLjIwMTAxMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEFBTkxrVGltVUtTZGYwUXZieVc1VzlYel9SLVQxUW1aTHhLR01OVFkwU3RLZkBtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGk9eWh1TVpHRk8xNWs0T2FIN1RSUHllVF9ncj1CaUR6ejlRV0ZOd0BtYWlsLmdtYWlsLmNvbT4gPEFBTkxrVGltVUtTZGYwUXZieVc1VzlYel9SLVQxUW1aTHhLR01OVFkwU3RLZkBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6741,"nextInTopic":0,"prevInTime":6741,"nextInTime":6743,"topicId":6739,"numMessagesInTopic":4,"msgSnippet":"These are ruled ACCEPT by the TransclusionDecideRule, because the kind of outlink-hops that led to them were suggestive that they might be necessary to render","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 42534 invoked from network); 22 Sep 2010 21:20:04 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m4.grp.sp2.yahoo.com with QMQP; 22 Sep 2010 21:20:04 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta1.grp.sp2.yahoo.com with SMTP; 22 Sep 2010 21:20:04 -0000\r\nX-Received: (qmail 22480 invoked from network); 22 Sep 2010 21:20:02 -0000\r\nX-Received: from 188.22.26.105 (HELO silverbook.local) (188.22.26.105)\n  by relay01.pair.com with SMTP; 22 Sep 2010 21:20:02 -0000\r\nX-pair-Authenticated: 188.22.26.105\r\nMessage-ID: &lt;4C9A7301.2010100@...&gt;\r\nDate: Wed, 22 Sep 2010 14:20:01 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.9) Gecko/20100825 Thunderbird/3.1.3\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Zach Bailey &lt;zach.bailey@...&gt;\r\nReferences: &lt;AANLkTi=yhuMZGFO15k4OaH7TRPyeT_gr=BiDzz9QWFNw@...&gt; &lt;AANLkTimUKSdf0QvbyW5W9Xz_R-T1QmZLxKGMNTY0StKf@...&gt;\r\nIn-Reply-To: &lt;AANLkTimUKSdf0QvbyW5W9Xz_R-T1QmZLxKGMNTY0StKf@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Simple, single-site crawl &quot;hangs&quot; on last\n few URIs?\r\nX-Yahoo-Group-Post: member; u=137285340; y=rSWmeU6ZJpk1fTrZg94MSN-MdWQxADN-LZ2lD5iBX2JA\r\nX-Yahoo-Profile: gojomo\r\n\r\nThese are ruled ACCEPT by the TransclusionDecideRule, because the kind \nof outlink-hops that led to them were suggestive that they might be \nnecessary to render the originating page. (For example, FRAME SRC, or \nreferences in Javascript, or in SWF.) I&#39;d also mentioned this rule the \nother day when you asked about the TooManyHopsDecideRule; depending on \nyour needs you could trim the number of &#39;grace&#39; hops (of certain kinds) \ngranted by TransclusionDecideRule, or eliminate it entirely.\n\nAny URL that appears in the crawl.log passed scope-testing at the time \nof its discovery and enqueuing. And, unless you&#39;ve added settings to \nshort-circuit some processing (such as overlaying an alternate false \n&#39;enabled&#39; value on procesors, or adding per-processor rules), every URI \nthat&#39;s fetched goes through all subsequent processing steps; there&#39;s no \nextra scope-rule checking after a URI is fetched.\n\n- Gordon @ IA\n\nOn 9/22/10 12:21 PM, Zach Bailey wrote:\n&gt;\n&gt;\n&gt; I meant to include this in the original email as well, but it slipped my\n&gt; mind. On some of my more recent crawls with the same configuration, I&#39;ve\n&gt; noticed the crawl log displaying entries that I wasn&#39;t expecting: crawl\n&gt; URLs for a completely different domain than the single domain I\n&gt; specified in the seed list.\n&gt;\n&gt; I haven&#39;t removed or moved the SurtPrefixedDecideRule in the scope rule\n&gt; sequence, so I&#39;m curious as to why I&#39;m suddenly seeing these pages show\n&gt; up in my crawl log. Will the crawler attempt to fetch pages outside of\n&gt; the domain, but then not process them since they don&#39;t match the Surt?\n&gt; If that&#39;s the case that is confusing as I would only expect to see\n&gt; successfully processed URIs in the crawl log - not ALL crawl URIs that\n&gt; were being considered.\n&gt;\n&gt; -Zach\n&gt;\n&gt; On Wed, Sep 22, 2010 at 3:05 PM, Zach Bailey &lt;zach.bailey@...\n&gt; &lt;mailto:zach.bailey@...&gt;&gt; wrote:\n&gt;\n&gt;     I&#39;ve been proving out our use of heritrix over the past week or so\n&gt;     and I&#39;ve noticed the following issue when doing a simple,\n&gt;     single-domain crawl.\n&gt;\n&gt;     Here is how I have the crawl configured - it is a mostly vanilla\n&gt;     crawl configuration save for the following modifications:\n&gt;\n&gt;     * metadata.operatorContactUrl specified (obviously)\n&gt;     * seeds.textSource.value set to a single domain\n&gt;     * Scope chain modifications: TooManyHopsDecideRule.maxHops = 3 and\n&gt;     added MatchesFilePatternDecideRule with decision=REJECT and\n&gt;     usePreset=ALL right before the PrerequisiteAcceptDecideRule\n&gt;     * fetch processor modifications: removed extractorJS, extractorCSS,\n&gt;     and extractorSWF\n&gt;\n&gt;     The crawl runs fine and when it gets down to the last couple of URLs\n&gt;     it &quot;hangs&quot; and does not complete. Looking at the job status page I see:\n&gt;\n&gt;     *Totals*\n&gt;         215 downloaded + 4 queued = 219 total\n&gt;         2.5 MiB crawled (2.5 MiB novel, 0 B dup-by-hash, 0 B not-modified)\n&gt;\n&gt;     *Frontier*\n&gt;         17 URI queues: 2 active (0 in-process; 0 ready; 2 snoozed); 0\n&gt;     inactive; 0 ineligible; 0 retired; 15 exhausted [RUN: 0 in, 0 out]\n&gt;\n&gt;     Examining the frontier report I see the following:\n&gt;\n&gt;       -----===== SNOOZED QUEUES =====-----\n&gt;     SNOOZED#0:\n&gt;     Queue ssl, (p1)\n&gt;        2 items\n&gt;         wakes in: 7m25s888ms\n&gt;          last enqueued: dns:ssl\n&gt;            last peeked: dns:ssl\n&gt;         total expended: 4 (total budget: -1)\n&gt;         active balance: 2996\n&gt;         last(avg) cost: 1(1)\n&gt;         totalScheduled fetchSuccesses fetchFailures fetchDisregards fetchResponses robotsDenials successBytes totalBytes fetchNonResponses\n&gt;         2 0 0 0 0 0 0 0 5\n&gt;         SimplePrecedenceProvider\n&gt;         1\n&gt;\n&gt;     SNOOZED#1:\n&gt;     Queue www, (p1)\n&gt;        2 items\n&gt;         wakes in: 7m25s889ms\n&gt;          last enqueued: dns:www\n&gt;            last peeked: dns:www\n&gt;         total expended: 4 (total budget: -1)\n&gt;         active balance: 2996\n&gt;         last(avg) cost: 1(1)\n&gt;         totalScheduled fetchSuccesses fetchFailures fetchDisregards fetchResponses robotsDenials successBytes totalBytes fetchNonResponses\n&gt;         2 0 0 0 0 0 0 0 5\n&gt;         SimplePrecedenceProvider\n&gt;         1\n&gt;\n&gt;     So, it looks like there are some weird items being put into these\n&gt;     queues that don&#39;t belong there and it&#39;s hanging the crawl job? Is\n&gt;     there a configuration option I can tweak to retire these queues or\n&gt;     clear them out after a specified idle period? Or, is this the result\n&gt;     of a misconfiguration somewhere?\n&gt;\n&gt;     Thanks,\n&gt;     -Zach\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}