{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":423327235,"authorName":"tony871209","from":"&quot;tony871209&quot; &lt;tony871209@...&gt;","profile":"tony871209","replyTo":"LIST","senderId":"HRUdKqkogHRIfV2hCjNhlGs3lQz7ghtjcaK6ajbi0PuGP5l63wZWkHUrbck22ajHQ3OUDaF_jc_op3OvPLpaEU52nOu__8zxXT4","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Two issues for crawling","postDate":"1261127502","msgId":6230,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGhnZmgwZStpMmI3QGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":6232,"prevInTime":6229,"nextInTime":6231,"topicId":6230,"numMessagesInTopic":3,"msgSnippet":"how can i a.crawl a URL at most once ( i do not want a update-checking followed with the 2nd time crawl) b.add seeds after crawl launch","rawEmail":"Return-Path: &lt;tony871209@...&gt;\r\nX-Sender: tony871209@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 9622 invoked from network); 18 Dec 2009 09:15:59 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m12.grp.re1.yahoo.com with QMQP; 18 Dec 2009 09:15:59 -0000\r\nX-Received: from unknown (HELO n43d.bullet.mail.sp1.yahoo.com) (66.163.169.157)\n  by mta3.grp.sp2.yahoo.com with SMTP; 18 Dec 2009 09:15:59 -0000\r\nX-Received: from [69.147.65.149] by n43.bullet.mail.sp1.yahoo.com with NNFMP; 18 Dec 2009 09:11:45 -0000\r\nX-Received: from [98.137.34.35] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 18 Dec 2009 09:11:45 -0000\r\nDate: Fri, 18 Dec 2009 09:11:42 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;hgfh0e+i2b7@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;tony871209&quot; &lt;tony871209@...&gt;\r\nSubject: Two issues for crawling\r\nX-Yahoo-Group-Post: member; u=423327235; y=diXwZf9e5XP06xTXQwTCY26GVaESab1IYsBRov17vsjpuZb82w\r\nX-Yahoo-Profile: tony871209\r\n\r\nhow can i \na.crawl a URL at most once ( i do not want a update-checking fol=\r\nlowed with the 2nd time crawl)\nb.add seeds after crawl launch\n\n\n"}}