{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"h_Flzel-g066QkXWIWNreXuq9AxC6bW-Z5ytyKEgeuTnSxGumZ_GF3qN6kRo-xsPUTpSByeU5Ee2Y-IivvORFsNir-XJpEg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Frontier &quot;maxRetries&quot;","postDate":"1291932828","msgId":6887,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDREMDE1NDlDLjQwM0BhcmNoaXZlLm9yZz4=","inReplyToHeader":"PEQ2NzcwRDFCMDJEODRFM0VBNDU2RThCRDAwOEMyRkE1QGRhdGFjbGlwLmNvbT4=","referencesHeader":"PDVGN0RGQTJFNzU5QzQxNzBBQTA2MTU4QzgzQ0NBQjlFQGRhdGFjbGlwLmNvbT4gPDREMDE0NjE4LjYwMDAwMDJAYXJjaGl2ZS5vcmc+IDxENjc3MEQxQjAyRDg0RTNFQTQ1NkU4QkQwMDhDMkZBNUBkYXRhY2xpcC5jb20+"},"prevInTopic":6886,"nextInTopic":6888,"prevInTime":6886,"nextInTime":6888,"topicId":6881,"numMessagesInTopic":5,"msgSnippet":"... That s exactly the cause -- that status leaks out when the URI isn t allowed to be fetched, because its prerequisites aren t finished, but also isn t","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 22608 invoked from network); 9 Dec 2010 22:13:49 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m11.grp.re1.yahoo.com with QMQP; 9 Dec 2010 22:13:49 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta3.grp.re1.yahoo.com with SMTP; 9 Dec 2010 22:13:49 -0000\r\nX-Received: (qmail 14935 invoked by uid 0); 9 Dec 2010 22:13:49 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay02.pair.com with SMTP; 9 Dec 2010 22:13:49 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4D01549C.403@...&gt;\r\nDate: Thu, 09 Dec 2010 14:13:48 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.12) Gecko/20101027 Thunderbird/3.1.6\r\nMIME-Version: 1.0\r\nTo: Zach Bailey &lt;zach.bailey@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;5F7DFA2E759C4170AA06158C83CCAB9E@...&gt; &lt;4D014618.6000002@...&gt; &lt;D6770D1B02D84E3EA456E8BD008C2FA5@...&gt;\r\nIn-Reply-To: &lt;D6770D1B02D84E3EA456E8BD008C2FA5@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: Frontier &quot;maxRetries&quot;\r\nX-Yahoo-Group-Post: member; u=137285340; y=h0PjNqo_EQ3Rya6dQ4o5_ABvDL1uzI49ujf2_RwujyRj\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 12/9/10 1:51 PM, Zach Bailey wrote:\n&gt; OK I&#39;ll give it a shot set to 3. Any idea about the strange crawl status\n&gt; codes? Could those be caused by the retries running out during these\n&gt; robots/dns refreshes you&#39;re talking about?\n\nThat&#39;s exactly the cause -- that status leaks out when the URI isn&#39;t \nallowed to be fetched, because its prerequisites aren&#39;t finished, but \nalso isn&#39;t allowed to be retried, because its retries are exhausted.\n\nA value of 3 should work but leaves no margin for the kinds of momentary \nproblems that are common. Momentary DNS hiccup? Server/router reboot? \nEtc. -- in a large crawl you&#39;ll still have some URIs fail that would \nhave succeeded if tried one more time a few minutes later.\n\n- Gordon @ IA\n\n\n&gt; -Zach\n&gt;\n&gt; On Thursday, December 9, 2010 at 4:11 PM, Gordon Mohr wrote:\n&gt;\n&gt;&gt; The &#39;maxRetries&#39; setting controls how many times a single URI will be\n&gt;&gt; returned to the frontier after a failed processing attempt to later\n&gt;&gt; retrying. But, importantly, some normal operations count as a temporary\n&gt;&gt; failure, so for example even if nothing goes wrong, a URI might get 1-2\n&gt;&gt; retries used up when it gets deferred to allow DNS or robots refresh\n&gt;&gt; fetches to occur. Thus this value should never be below 3 for normal\n&gt;&gt; operation, and if you want any robustness against occasional/transient\n&gt;&gt; network problems, could be significantly higher.\n&gt;&gt;\n&gt;&gt; The default value of &#39;30&#39;, together with the retryDelay of &#39;900&#39; (15\n&gt;&gt; minutes), means that even a network outage (near the crawler, or near\n&gt;&gt; the target server) of 30*15m = ~7.5 hours won&#39;t cause a URI to be marked\n&gt;&gt; as a total failure.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; On 12/9/10 8:40 AM, Zach Bailey wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; Does this property really do what it says? I&#39;m concerned it&#39;s limiting\n&gt;&gt;&gt; the amount of content being downloaded from a seed.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Just to clarify, I&#39;m crawling some domain lists where some domains may\n&gt;&gt;&gt; not even have an A record, and some may not have web servers running on\n&gt;&gt;&gt; them (but they DO resolve) and so I only want to try and connect to\n&gt;&gt;&gt; these domains once, otherwise the crawl runs very slowly as it retries\n&gt;&gt;&gt; over, and over, and over, and over... (with the default timeout of 20\n&gt;&gt;&gt; minutes and max retries = 30 this will take FOREVER)\n&gt;&gt;&gt;\n&gt;&gt;&gt; So I have configured the crawl in the following manner:\n&gt;&gt;&gt;\n&gt;&gt;&gt; FetchHTTP - timeoutSeconds = 5\n&gt;&gt;&gt; frontier - maxRetries = 2\n&gt;&gt;&gt;\n&gt;&gt;&gt; Does this seem to make sense for what I&#39;m doing? What crawl statuses\n&gt;&gt;&gt; should I be looking for to determine domains that a.) don&#39;t have a DNS A\n&gt;&gt;&gt; record (I believe the code for this is -1) and b.) HTTP connect timeout\n&gt;&gt;&gt; (I would expect to see -2, -4, or -8 for this)\n&gt;&gt;&gt;\n&gt;&gt;&gt; The reason I ask is because I&#39;m seeing a lot of -50 crawl statuses which\n&gt;&gt;&gt; the glossary indicates &quot;appearance in logs may be a bug&quot;, so that has me\n&gt;&gt;&gt; a little worried.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Happy Holidays,\n&gt;&gt;&gt; Zach\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; \n&gt;\n\n"}}