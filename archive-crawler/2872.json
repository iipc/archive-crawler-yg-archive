{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":132996324,"authorName":"joehung302","from":"&quot;joehung302&quot; &lt;joe.hung@...&gt;","profile":"joehung302","replyTo":"LIST","senderId":"EYTNRWGqgQvMHppXQqaT9Q9xuHCu4VIV1503r-hiL0L-3XH-FMQrfBd6WDfB6lw863Q3HzRTUExt0H5JfnT1W4r3u0epXqUeemvFLzUa","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Multimachine crawl","postDate":"1148254028","msgId":2872,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU0cXQwYytxMDNrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0NkExRkIxLjUwNTA3MDFAYXJjaGl2ZS5vcmc+"},"prevInTopic":2867,"nextInTopic":0,"prevInTime":2871,"nextInTime":2873,"topicId":2866,"numMessagesInTopic":3,"msgSnippet":"... We ve successfully used the above approach to download roughly 1 billion  web pages using 8 machines over a 3-month period. I would say the multi-machine","rawEmail":"Return-Path: &lt;joe.hung@...&gt;\r\nX-Sender: joe.hung@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 24052 invoked from network); 21 May 2006 23:27:09 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m29.grp.scd.yahoo.com with QMQP; 21 May 2006 23:27:09 -0000\r\nReceived: from unknown (HELO n31a.bullet.scd.yahoo.com) (209.73.160.90)\n  by mta5.grp.scd.yahoo.com with SMTP; 21 May 2006 23:27:09 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.66.59] by n31.bullet.scd.yahoo.com with NNFMP; 21 May 2006 23:27:09 -0000\r\nReceived: from [66.218.66.90] by t8.bullet.scd.yahoo.com with NNFMP; 21 May 2006 23:27:09 -0000\r\nDate: Sun, 21 May 2006 23:27:08 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;e4qt0c+q03k@...&gt;\r\nIn-Reply-To: &lt;446A1FB1.5050701@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;joehung302&quot; &lt;joe.hung@...&gt;\r\nSubject: Re: Multimachine crawl\r\nX-Yahoo-Group-Post: member; u=132996324; y=wg4i_EWtixHNU7RuA4Ls3OhEU3etsbymwuE-aH_45iTuR4vqPg\r\nX-Yahoo-Profile: joehung302\r\n\r\n&gt; \n&gt; Hey Greg.\n&gt; \n&gt; We&#39;re not there yet.   Still a ways to go.\n&gt; \n&gt; The way=\r\n we currently do multimachine crawls is still effectively as \n&gt; outlined he=\r\nre: \n&gt; http://groups.yahoo.com/group/archive-crawler/message/2402 (See also=\r\n \n&gt; messages 2438 and 2543). \n&gt; \n\nWe&#39;ve successfully used the above approac=\r\nh to download roughly 1\nbillion  web pages using 8 machines over a 3-month =\r\nperiod. I would say\nthe multi-machine crawl functionality is there for prac=\r\ntical purposes\nusing split crawl. We stopped at 1 billion because we only b=\r\nudgeted to\ncrawl for 3 months.\n\nI&#39;m not that crazy about the fully-automati=\r\nc multi-machine crawling\nbecuase the manual crawl technique does not take t=\r\noo much effort to\nimplement...if you get it done right.\n\nI&#39;d say the next b=\r\nottleneck of getting heritrix to go over more than a\ncopule billion pages (=\r\nsay 5 B) is to scale up the capacity of already\nseen URL handling. I think =\r\nthe wonderful heritrix team is working on a\nsmarter way as we speak.\n\ncheer=\r\ns,\n-joe\n\n\n\n\n\n\n\n"}}