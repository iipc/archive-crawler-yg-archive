{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurdsson","from":"&quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"_R4Hpx4TXC7ROzz17KM0L54csTfvqMZBa6l8BGTMPs_6ALetfJSveP3nO-7BXI5IWNJP8Ik2sx-D8GsaSd0w6SURkjfnF4bqr1n4AWyc3Q","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [archive-crawler] Duplicate detection in large scale crawls - thinking out loud","postDate":"1131610270","msgId":2352,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBNzg1NjM4QHNrYXJmdXIuYm9rLmxvY2FsPg==","inReplyToHeader":"PDE3MjY2LjEwNDkwLjYyNTAxNy41MzAyNjVAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4="},"prevInTopic":2351,"nextInTopic":0,"prevInTime":2351,"nextInTime":2353,"topicId":2338,"numMessagesInTopic":7,"msgSnippet":"... gathered ... in ... Correct. Although this could be expanded to do mirror detection by also indexing by content hash. I have limited the project (such as","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 96963 invoked from network); 10 Nov 2005 08:14:08 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m32.grp.scd.yahoo.com with QMQP; 10 Nov 2005 08:14:08 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.171)\n  by mta2.grp.scd.yahoo.com with SMTP; 10 Nov 2005 08:14:08 -0000\r\nReceived: (qmail 28863 invoked by uid 100); 10 Nov 2005 08:08:35 -0000\r\nReceived: from forritun-4.bok.hi.is (HELO forritun4) (kris@...@130.208.152.83)\n  by mail-dev.archive.org with SMTP; 10 Nov 2005 08:08:35 -0000\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Thu, 10 Nov 2005 08:11:10 -0000\r\nMessage-ID: &lt;0678DB1968EAC7409CC3D0AB7A11B84A785638@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\n\tboundary=&quot;----=_NextPart_000_0013_01C5E5CE.5073FED0&quot;\r\nX-Priority: 3 (Normal)\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook, Build 10.0.4510\r\nIn-Reply-To: &lt;17266.10490.625017.530265@...&gt;\r\nImportance: Normal\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.2180\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=-77.9 required=7.0 tests=AWL,HTML_20_30,\n\tHTML_FONTCOLOR_BLUE,HTML_MESSAGE,USER_IN_WHITELIST autolearn=no \n\tversion=2.63\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Kristinn Sigurdsson&quot; &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] Duplicate detection in large scale crawls - thinking out loud\r\nX-Yahoo-Group-Post: member; u=163406187; y=CgTDOXbnmxvzcrZPh-bhYBOCBJdgxQ2rgL4b3mhrGI34XogX\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n------=_NextPart_000_0013_01C5E5CE.5073FED0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&gt;&gt; I&#39;m currently doing the 4th complete .is crawl. The amount of data\ngathe=\r\nred\n&gt;&gt; has got me thinking about detecting and eliminating duplicate docume=\r\nnts\nin\n&gt;&gt; subsequent crawls. \n&gt;\n&gt;Just so I&#39;m clear, this is to prevent from=\r\n downloading the same\n&gt;(unchanged) document at the same URL on subsequent c=\r\nrawls, as opposed\n&gt;to making sure you only have one copy of (say) the LINUX=\r\n HOWTO\n&gt;documents in your entire collection? \n \nCorrect. Although this coul=\r\nd be expanded to do mirror detection by also\nindexing by content hash. \nI h=\r\nave limited the project (such as it is) to non text/* documents. Both\nbecau=\r\nse the potential gain there is much greater for less effort and also\nbecaus=\r\ne content hashes are quite unreliable for detecting change in text\ndocument=\r\ns. False negatives only so no data loss, but we&#39;ll never get any\nsignifican=\r\nt savings compared to the effort expended.\n \n&gt;\n&gt;&gt; A basic idea for handling=\r\n duplicate detection. Add a new processor that\n&gt;&gt; maintains its own databas=\r\ne (Berkley DB or other). In this database we\nrecord\n&gt;&gt; the URL fingerprint =\r\n(ideally canonicalized but that is more difficult\nsince\n&gt;&gt; that is done in =\r\nthe frontier at the moment, Michael any thoughts on\nthis?)\n&gt;&gt; and the conte=\r\nnt hash (plus possibly some additional meta-data such as\ntime\n&gt;&gt; of first d=\r\niscovery and last change). This is indexed by the URL\nfingerprint.\n&gt;&gt; The p=\r\nrocessor is applied after the FetchHTTP processor (and only on HTTP\n&gt;&gt; docu=\r\nments). It looks the URI up and compares the content hashes, aborts\n&gt;&gt; furt=\r\nher processing of unchanged documents. The DB is updated as needed.\nAll\n&gt;&gt; =\r\nvery simple and straightforward.\n&gt;\n&gt;This would work fine. I wonder if it wo=\r\nuldn&#39;t make sense to add a\n&gt;special kind of ARC record that notated the exi=\r\nstence of an identical\n&gt;copy elsewhere? \n \nI believe that will be possible =\r\nin the new WARC format (as Stack confirms in\na seperate post. And, yes, tha=\r\nt would make a great deal of sense.\n \n&gt;\n&gt;&gt; However, text/* accounts for ~26=\r\n5GB while everything else ~615 (there are\n&gt;&gt; some rounding errors in this, =\r\nbut the scales are clear). We know that\ntext\n&gt;&gt; compresses well, typically =\r\naround 80%. We also know that &#39;everything\nelse&#39;\n&gt;&gt; means mostly images and =\r\nother, already compressed, material that does not\n&gt;&gt; compress well. Since t=\r\nhis material accounts for 70% of the downloaded\ndata\n&gt;&gt; (and probably even =\r\nmore of the stored, compressed data) but only 30% of\nthe\n&gt;&gt; downloaded docu=\r\nments, it is clear that eliminiating duplicates. \n&gt;\n&gt;This begs the question=\r\n if it wouldn&#39;t make sense to have a single copy\n&gt;of some files that are re=\r\nferred to by multiple URLs. For example, how\n&gt;many copies of various Linux =\r\nISOs do you want to keep around? \n \nI agree (although we do not collect Lin=\r\nux ISO files). However, I think that\nin the long term there is more to be g=\r\nained from avoiding what I&#39;m going to\ncall temporal duplicates as opposed t=\r\no spatial duplicates. It is also very\nquestionable to avoid storing spatial=\r\n duplicates until we can write some\nkind of entry in the ARC saying that th=\r\nis already exists at this other URL.\nWith temporal duplicates we don&#39;t need=\r\n to look far for a copy of the file.\nSo maybe once the new WARC format ente=\r\nrs use we could expand this.\n \nThanks for your thoughts Tom.\n \n-Kris\n \n\r\n------=_NextPart_000_0013_01C5E5CE.5073FED0\r\nContent-Type: text/html;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot;&gt;\n&lt;HTML&gt;&lt;HEAD&gt;=\r\n&lt;TITLE&gt;Message&lt;/TITLE&gt;\n&lt;META http-equiv=3DContent-Type content=3D&quot;text/html=\r\n; charset=3Diso-8859-1&quot;&gt;\n&lt;META content=3D&quot;MSHTML 6.00.2900.2769&quot; name=3DGEN=\r\nERATOR&gt;&lt;/HEAD&gt;\n&lt;BODY&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&gt;&gt; I&#39;m current=\r\nly doing the 4th complete .is \ncrawl. The amount of data gathered&lt;BR&gt;&gt;&g=\r\nt; has got me thinking about \ndetecting and eliminating duplicate documents=\r\n in&lt;BR&gt;&gt;&gt; subsequent crawls. \n&lt;BR&gt;&gt;&lt;BR&gt;&gt;Just so I&#39;m clear, this=\r\n is to prevent from downloading the \nsame&lt;BR&gt;&gt;(unchanged) document at th=\r\ne same URL on subsequent crawls, as \nopposed&lt;BR&gt;&gt;to making sure you only=\r\n have one copy of (say) the LINUX \nHOWTO&lt;BR&gt;&gt;documents in your entire co=\r\nllection?&lt;SPAN \nclass=3D736120108-10112005&gt;&lt;FONT color=3D#0000ff&gt;&nbsp;&lt;/FO=\r\nNT&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial size=3D2&gt;&lt;SPAN \nclass=3D736=\r\n120108-10112005&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D=\r\n#0000ff size=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;Correct. Although this c=\r\nould be expanded to do mirror \ndetection by also indexing by content hash.&=\r\nnbsp;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT face=3DArial color=3D#0000ff size=3D2&gt;=\r\n&lt;SPAN class=3D736120108-10112005&gt;I have \nlimited the project (such as it is=\r\n) to non text/* documents. Both because the \npotential gain there is much g=\r\nreater for less effort and also because content \nhashes are quite unreliabl=\r\ne for detecting change in text documents. False \nnegatives only so no data =\r\nloss, but we&#39;ll never get any significant savings \ncompared to the effort e=\r\nxpended.&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;FONT size=3D2&gt;&lt;=\r\nSPAN \nclass=3D736120108-10112005&gt;&nbsp;&lt;/SPAN&gt;&lt;BR&gt;&gt;&lt;BR&gt;&gt;&gt; A basic =\r\nidea for \nhandling duplicate detection. Add a new processor that&lt;BR&gt;&gt;&gt=\r\n; maintains its \nown database (Berkley DB or other). In this database we re=\r\ncord&lt;BR&gt;&gt;&gt; the \nURL fingerprint (ideally canonicalized but that is mo=\r\nre difficult \nsince&lt;BR&gt;&gt;&gt; that is done in the frontier at the moment,=\r\n Michael any \nthoughts on this?)&lt;BR&gt;&gt;&gt; and the content hash (plus pos=\r\nsibly some \nadditional meta-data such as time&lt;BR&gt;&gt;&gt; of first discover=\r\ny and last \nchange). This is indexed by the URL fingerprint.&lt;BR&gt;&gt;&gt; Th=\r\ne processor is \napplied after the FetchHTTP processor (and only on HTTP&lt;BR&gt;=\r\n&gt;&gt; documents). \nIt looks the URI up and compares the content hashes, =\r\naborts&lt;BR&gt;&gt;&gt; further \nprocessing of unchanged documents. The DB is up=\r\ndated as needed. All&lt;BR&gt;&gt;&gt; \nvery simple and straightforward.&lt;BR&gt;&gt;&lt;=\r\nBR&gt;&gt;This would work fine. I wonder \nif it wouldn&#39;t make sense to add a&lt;B=\r\nR&gt;&gt;special kind of ARC record that \nnotated the existence of an identica=\r\nl&lt;BR&gt;&gt;copy elsewhere?&lt;SPAN \nclass=3D736120108-10112005&gt;&lt;FONT \ncolor=3D#0=\r\n000ff&gt;&nbsp;&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT face=\r\n=3DArial&gt;&lt;FONT size=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/F=\r\nONT&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;FONT color=3D#0000ff=\r\n size=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;I believe that will be possible=\r\n in the new WARC format \n(as Stack confirms in a seperate post. And, yes, t=\r\nhat would make a great deal of \nsense.&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;D=\r\nIV&gt;&lt;FONT&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;FONT size=3D2&gt;&lt;SPAN \nclass=3D736120108-1=\r\n0112005&gt;&nbsp;&lt;/SPAN&gt;&lt;BR&gt;&gt;&lt;BR&gt;&gt;&gt; However, text/* \naccounts for ~26=\r\n5GB while everything else ~615 (there are&lt;BR&gt;&gt;&gt; some \nrounding errors=\r\n in this, but the scales are clear). We know that \ntext&lt;BR&gt;&gt;&gt; compres=\r\nses well, typically around 80%. We also know that \n&#39;everything else&#39;&lt;BR&gt;&gt=\r\n;&gt; means mostly images and other, already compressed, \nmaterial that doe=\r\ns not&lt;BR&gt;&gt;&gt; compress well. Since this material accounts \nfor 70% of t=\r\nhe downloaded data&lt;BR&gt;&gt;&gt; (and probably even more of the \nstored, comp=\r\nressed data) but only 30% of the&lt;BR&gt;&gt;&gt; downloaded documents, \nit is c=\r\nlear that eliminiating duplicates. &lt;BR&gt;&gt;&lt;BR&gt;&gt;This begs the question \n=\r\nif it wouldn&#39;t make sense to have a single copy&lt;BR&gt;&gt;of some files that a=\r\nre \nreferred to by multiple URLs. For example, how&lt;BR&gt;&gt;many copies of va=\r\nrious \nLinux ISOs do you want to keep around?&lt;SPAN class=3D736120108-101120=\r\n05&gt;&lt;FONT \ncolor=3D#0000ff&gt;&nbsp;&lt;/FONT&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;=\r\n/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;FONT size=3D2&gt;&lt;SPAN \nclass=3D736=\r\n120108-10112005&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;=\r\n&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;FONT color=3D#0000ff size=3D2&gt;&lt;SPAN \nclass=3D7361=\r\n20108-10112005&gt;I agree (although we do not collect Linux ISO files). \nHowev=\r\ner, I think that in the long term there is more to be gained from avoiding =\r\n\nwhat I&#39;m going to call&nbsp;temporal duplicates as opposed to spatial \ndup=\r\nlicates. It is also very questionable to avoid storing spatial duplicates \n=\r\nuntil we can write some kind of entry in the ARC saying that this already e=\r\nxists \nat this &lt;STRONG&gt;other&lt;/STRONG&gt; URL. With temporal duplicates we don&#39;=\r\nt&nbsp;need \nto look far for&nbsp;a copy of the file.&nbsp;So maybe once th=\r\ne new WARC format \nenters use we could expand this.&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/F=\r\nONT&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;FONT color=3D#0000ff=\r\n size=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;&lt;/SPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/F=\r\nONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;FONT color=3D#0000ff =\r\nsize=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;Thanks for your thoughts \nTom.&lt;/=\r\nSPAN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT&gt;&lt;FONT face=3DArial=\r\n&gt;&lt;FONT color=3D#0000ff size=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;&lt;/SPAN&gt;&lt;/=\r\nFONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&nbsp;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;=\r\n&lt;FONT color=3D#0000ff size=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;-Kris&lt;/SPA=\r\nN&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/DIV&gt;\n&lt;DIV&gt;&lt;FONT&gt;&lt;FONT&gt;&lt;FONT face=3DArial&gt;&lt;F=\r\nONT size=3D2&gt;&lt;SPAN \nclass=3D736120108-10112005&gt;&nbsp;&lt;/SPAN&gt;&lt;/DIV&gt;&lt;/FONT&gt;&lt;/=\r\nFONT&gt;&lt;/FONT&gt;&lt;/FONT&gt;&lt;/BODY&gt;&lt;/HTML&gt;\n\r\n------=_NextPart_000_0013_01C5E5CE.5073FED0--\r\n\n"}}