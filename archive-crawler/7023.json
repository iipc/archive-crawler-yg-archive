{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264138474,"authorName":"Kenji Nagahashi","from":"Kenji Nagahashi &lt;knagahashi@...&gt;","profile":"kenznag","replyTo":"LIST","senderId":"yyd8lejodqaOw-UXydmqoHj07bcJU8vF2ZKSV0vGQobwLjUTCE21gTikIAVJ5lPeygemwSfXn3qxWmrPNL2wZBcLAsxdSoPI505HCN4","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix Frontier","postDate":"1297719436","msgId":7023,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENTlBMDhDLjQwMjAwQGdtYWlsLmNvbT4=","inReplyToHeader":"PEM5N0EzNDg5LjIyNDklbXVyYWxpa3BAYW1hem9uLmNvbT4=","referencesHeader":"PEM5N0EzNDg5LjIyNDklbXVyYWxpa3BAYW1hem9uLmNvbT4="},"prevInTopic":7014,"nextInTopic":7108,"prevInTime":7022,"nextInTime":7024,"topicId":7005,"numMessagesInTopic":11,"msgSnippet":"Hi Murali, Thanks for sharing your ideas. Please see my replies below: ... Unfortunately, at this point, source code is the only documentation better than my","rawEmail":"Return-Path: &lt;knagahashi@...&gt;\r\nX-Sender: knagahashi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 53432 invoked from network); 14 Feb 2011 21:40:54 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m11.grp.re1.yahoo.com with QMQP; 14 Feb 2011 21:40:54 -0000\r\nX-Received: from unknown (HELO mail-qw0-f41.google.com) (209.85.216.41)\n  by mta3.grp.re1.yahoo.com with SMTP; 14 Feb 2011 21:40:54 -0000\r\nX-Received: by qwa26 with SMTP id 26so3384046qwa.0\n        for &lt;archive-crawler@yahoogroups.com&gt;; Mon, 14 Feb 2011 13:40:54 -0800 (PST)\r\nX-Received: by 10.224.67.72 with SMTP id q8mr3694901qai.222.1297719441729;\n        Mon, 14 Feb 2011 13:37:21 -0800 (PST)\r\nReturn-Path: &lt;knagahashi@...&gt;\r\nX-Received: from Kenjis-MacBook-Pro.local (router300.sf.archive.org [208.70.27.190])\n        by mx.google.com with ESMTPS id g32sm2092340qck.46.2011.02.14.13.37.18\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Mon, 14 Feb 2011 13:37:19 -0800 (PST)\r\nMessage-ID: &lt;4D59A08C.40200@...&gt;\r\nDate: Mon, 14 Feb 2011 13:37:16 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; ja-JP-mac; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;C97A3489.2249%muralikp@...&gt;\r\nIn-Reply-To: &lt;C97A3489.2249%muralikp@...&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix Frontier\r\nX-Yahoo-Group-Post: member; u=264138474; y=rqTX6dFPmB3HXjnrEs0hHPpTDNcfnW7xPf8Iuhef7Et3Fw\r\nX-Yahoo-Profile: kenznag\r\n\r\nHi Murali,\nThanks for sharing your ideas. Please see my replies below:\n\n(2/10/11 11:04 AM), Krishna, Murali wrote:\n&gt; Thanks Kenji,\n&gt; This is great. Do we have any documentation on the prototype and what is\n&gt; the timeline that you are looking at?\n\nUnfortunately, at this point, source code is the only &quot;documentation&quot; \nbetter than my hand-drawn, out-of-date diagrams X-)\nWe plan to give prototype a try in a week or so, in our own crawl \nproject too see if this is a terrible idea or not. Currently no real \ntimeline beyond that.\n\n&gt; It would be good if you can share the design.\n\nI wanted to leverage whatever in current Heriitrix 3 codebase that \nworks. So in my design, most of the work like politeness control is \nstill done by Heritrix. Central controller is more like a simple &quot;URL \nrepository&quot;. I&#39;m sure you&#39;ll find tons of well-thought-out code that you \ncan base your implementation on.\n\nSo, let me explain my design in a bit more detail. Steps of crawl cycle \nis as follows:\n\n1. controller has a set of URLs; they are grouped into working sets\n2. crawler sends a &quot;give-me-URLs&quot; request to the controller. controller \nassigns a working set to the crawler and returns a batch of URLs from \nthe working set (each working set has zero or one crawler assigned at \nany time)\n3. crawler does its job with those URLs (manage politeness, consult \nrobots.txt, fetch content, extract links, etc.) - all what Heritrix is \ngood at\n4. crawler sends &quot;discovered&quot; URLs to the controller. controller \ndiscards those URLs already &quot;seen&quot;, schedules the rest to working sets\n5. crawler sends &quot;finished&quot; notification for the URL to the controller. \nthis message also carries information about fetched resources, such as \nLast-Modified date and content digest, for future de-duplication \npurposes. &quot;finished&quot; message is sent in a batch for performance reasons.\n\nnot implemented, but in step 4, crawler should perform &quot;local&quot; seen \ncheck as well, because this is a very heavy operation as Gordon noted. \nlocal seen checks can reduce controller workload significantly, my \nexperiment shows. This is in fact a area of a big challenge.\n\nLoad balancing/reliability part is not really implemented yet, but I&#39;m \nthinking about using crawler&#39;s &quot;pulling URLs&quot; and &quot;finished&quot; requests as \na heartbeat signal. that is, if there&#39;s no pulling or finishing for \ncertain amount of time, controller considers crawler is in trouble \ncrawling, and re-dispatches the working set to other crawler. for this \npurpose, URLs pulled are kept in the controller until it receives \n&quot;finished crawl&quot; notification from the crawler, and they&#39;ll be reset to \n&quot;not-being-crawled&quot; state upon re-dispatch. (It is yet to be seen how \nwell this works) There&#39;s a chance of duplicate crawls, but I consider it \nis not a big issue.\n\nRegards,\nKenji\n\n&gt; My rough design based on\n&gt; our use case is\n&gt; 1. Have the queues (grouped by politeness, one host per queue?) in\n&gt; centralized storage(distributed frontier, *DF*).\n&gt; 2. The ‘frontier stub’ on the heritrix worker box will register to the\n&gt; service with it&#39;s &#39;id&#39; and service will take care of assigning queues to\n&gt; workers.\n&gt; 3. Based on the number of workers, DF will assign the queues.\n&gt; 4. Workers will send periodic hearbeats to DF (Zookeeper?). This is for\n&gt; reliability, queues should be automatically reassigned.\n&gt; 5. As we up/down number of worker boxes, queues should be reassigned\n&gt; automatically (pending urls from checkpoint).\n&gt; 6. Should we get urls in batches from DF to worker, when should we fetch\n&gt; again? Some Toe threads in worker might still be working on urls and\n&gt; some might have finished?\n&gt; 7. How do we checkpoint in DF storage? Should we inform each crawled url\n&gt; to DF or in batches ? May be it is fine recrawling a batch incase of\n&gt; worker failure. Idea is keep the previous sent batch in locked state in\n&gt; DF and delete the batch when worker checkpoint with the batch id. Remove\n&gt; the lock incase of no heartbeat.\n&gt; 8. Newly detected urls has to be sent back to DF for scheduling. DF will\n&gt; send it to right queue. Same thing has to be done for ‘refresh’\n&gt;\n&gt; Definitely, these is far from completion and I would have missed lot of\n&gt; aspects of frontier in the design and would need your help/suggestions.\n&gt;\n&gt;    1. how is robots.txt scheduling handled?\n&gt;    2. frontier metrics ?\n&gt;\n&gt;\n&gt; Thanks,\n&gt; Murali\n&gt;\n&gt; On 2/10/11 11:44 PM, &quot;Kenji Nagahashi&quot; &lt;knagahashi@...&gt; wrote:\n&gt;\n&gt;&gt;  Hi Krishna,\n&gt;&gt;\n&gt;&gt;  I&#39;m an engineer working on web-wide crawl at Internet Archive.\n&gt;&gt;  As Gordon said, We&#39;re strongly interested in the architecture you\n&gt;&gt;  described, and in fact developing a Heritrix-3 based prototype right\n&gt;&gt;  now. For your information, current approach is:\n&gt;&gt;\n&gt;&gt;  - a simple central control server, backed by distributed storage, that\n&gt;&gt;  does &quot;seen&quot; check, de-deplication and URL scheduling\n&gt;&gt;  - Heritrix 3 configured with custom UniqUriFilter and a disposition\n&gt;&gt;  processor talking with the central server over HTTP\n&gt;&gt;\n&gt;&gt;  currently it requires small modification to WorkQueueFrontier that\n&gt;&gt;  allows it to efficiently &quot;pull&quot; more URIs from the central server when\n&gt;&gt;  it run out of &quot;ready&quot; queues. So it&#39;s unlikely that it will be\n&gt;&gt;  compatible with the next release of Heritrix 3, although I&#39;d be happy to\n&gt;&gt;  make it a part of future releases.\n&gt;&gt;\n&gt;&gt;  I really welcome discussions on this topic.\n&gt;&gt;\n&gt;&gt;  Thanks,\n&gt;&gt;  Kenji Nagahashi\n&gt;&gt;\n&gt;&gt;  (2/8/11 3:56 PM), Gordon Mohr wrote:\n&gt;&gt; &gt; On 2/8/11 5:55 AM, Krishna, Murali wrote:\n&gt;&gt; &gt;&gt; Thanks Gordon for the detailed response.\n&gt;&gt; &gt;&gt; We don&#39;t have all the urls upfront, it is a continuous stream of urls\n&gt;&gt; &gt;&gt; and we don&#39;t want to wait for the previous heritrix jobs to finish.\n&gt;&gt; &gt;&gt; Essentially, we want to schedule them as and when is possible (honoring\n&gt;&gt; &gt;&gt; politeness). Also some of the urls have higher priority for crawling.\n&gt;&gt; &gt;\n&gt;&gt; &gt; If you want your prioritization to also take effect within the standard\n&gt;&gt; &gt; Heritrix frontier queues, there&#39;s a (seldom-used) pair of features,\n&gt;&gt; &gt; queue and URI &#39;precedence&#39; values, which affect (1) how inactive queues\n&gt;&gt; &gt; are sorted while waiting for their chance to be active; (2) where URIs\n&gt;&gt; &gt; are inserted into individual queues (if all the other factors which\n&gt;&gt; &gt; affect whether they are pushed-near-the-top or queued-to-the-back are\n&gt;&gt; &gt; equal).\n&gt;&gt; &gt;\n&gt;&gt; &gt;&gt; The problem with BDB frontier is that it is tied to the box and is a\n&gt;&gt; &gt;&gt; reliability concern if the machine goes down. We are thinking of\n&gt;&gt; &gt; having the\n&gt;&gt; &gt;&gt; urls in reliable queue service in a different cluster and make the\n&gt;&gt; &gt; heritrix\n&gt;&gt; &gt;&gt; read from that queue. This makes heritrix instance stateless (the\n&gt; crawled\n&gt;&gt; &gt;&gt; content goes to another cluster) and easy to replace with another\n&gt; box. Of\n&gt;&gt; &gt;&gt; course this calls for a new checkpointing mechanism outside the box.\n&gt;&gt; &gt;\n&gt;&gt; &gt; We&#39;ve considered possibilities for this as well, and some sort of\n&gt;&gt; &gt; remote/distributed frontier is likely in the future for Heritrix, though\n&gt;&gt; &gt; nothing is definitively scheduled/prioritized for an upcoming release.\n&gt;&gt; &gt;\n&gt;&gt; &gt; Some of the range of possibilities could include:\n&gt;&gt; &gt;\n&gt;&gt; &gt; - Replacing the current BDB-JE binary key-value store in the standard\n&gt;&gt; &gt; frontier with a remote/distributed alternative. This might be most\n&gt;&gt; &gt; straightforward, though a few issues that don&#39;t come up with the local\n&gt;&gt; &gt; store would have to be addressed, including: (1) new latencies/overhead;\n&gt;&gt; &gt; (2) multiple crawl processes sharing the same store, (3) what recovery\n&gt;&gt; &gt; from arbitrary crashes might be possible.\n&gt;&gt; &gt;\n&gt;&gt; &gt; - Still using the traditional default frontier locally, but pull URIs in\n&gt;&gt; &gt; batches from the shared/remote store. (This might not require any\n&gt;&gt; &gt; rewriting of the local frontier; just some other module that reports\n&gt;&gt; &gt; results up, and pulls the right batches of new URIs down.) The shared\n&gt;&gt; &gt; URI-queues/history info could be very different in its representations,\n&gt;&gt; &gt; checkpointing, etc.\n&gt;&gt; &gt;\n&gt;&gt; &gt; - Replacing the usual local frontier with an alternative which operates\n&gt;&gt; &gt; in a whole new way with the remote queues/store, just implementing the\n&gt;&gt; &gt; minimal expected frontier behavior. We want the code to support this\n&gt;&gt; &gt; possibility, but there are almost certainly hidden assumptions in other\n&gt;&gt; &gt; parts of the code that the frontier behaves like our standard one, which\n&gt;&gt; &gt; would need some clean up when tested by this new approach. And, much of\n&gt;&gt; &gt; the existing timing/politeness/ordering behavior would have to be\n&gt;&gt; &gt; reimplementing in broadly-similar ways... though perhaps much of the\n&gt;&gt; &gt; existing design could be mirrored albeit with remote queues/sets.\n&gt;&gt; &gt;\n&gt;&gt; &gt;&gt; I understand it is an overkill to use heritrix, but In future, we might\n&gt;&gt; &gt;&gt; need depth crawl which we can easily implement by scheduling the newly\n&gt;&gt; &gt;&gt; detected urls back into the reliable queue service. We are just\n&gt; trying to\n&gt;&gt; &gt;&gt; leverage the politeness, threading and pluggable processor frameworks of\n&gt;&gt; &gt;&gt; heritrix.\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; Thoughts?\n&gt;&gt; &gt;\n&gt;&gt; &gt; I better understand your motivations and they seem reasonable. I&#39;ll be\n&gt;&gt; &gt; very interested to hear any architectural directions you take, and can\n&gt;&gt; &gt; discuss. If either the code, or simply the changes that make Heritrix\n&gt;&gt; &gt; better able to rely on remote frontier functionality, can be contributed\n&gt;&gt; &gt; back, it will likely be of interest to the IA and other Heritrix users\n&gt;&gt; &gt; as well!\n&gt;&gt; &gt;\n&gt;&gt; &gt; - Gordon @ IA\n&gt;&gt; &gt;\n&gt;&gt; &gt;&gt; Thanks,\n&gt;&gt; &gt;&gt; Murali\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; On 2/8/11 1:56 PM, &quot;Gordon Mohr&quot;&lt;gojomo@...\n&gt;&gt; &gt; &lt;mailto:gojomo%40archive.org&gt; &lt;mailto:gojomo%40archive.org&gt;&gt;&gt; wrote:\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;&gt; On 2/7/11 3:17 AM, Krishna, Murali wrote:\n&gt;&gt; &gt;&gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;&gt; Hi all,\n&gt;&gt; &gt;&gt;&gt;&gt; We have a list of urls to be crawled, essentially just a fetch and\n&gt; some\n&gt;&gt; &gt;&gt;&gt;&gt; processing. Assume that the list can be huge and run into\n&gt; billions. So,\n&gt;&gt; &gt;&gt;&gt;&gt; we are thinking of writing a new Frontier which will accomplish this.\n&gt;&gt; &gt;&gt;&gt;&gt; Will have multiple heritrix worker boxes, each of the worker¹s\n&gt; frontier\n&gt;&gt; &gt;&gt;&gt;&gt; will get one portion of the centralized url repository (distributed\n&gt;&gt; &gt;&gt;&gt;&gt; storage) and schedule them for crawling.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; You probably won&#39;t need a new Frontier for this; the default frontier\n&gt;&gt; &gt;&gt;&gt; (BdbFrontier) should work for tens to even hundreds of millions of\n&gt;&gt; &gt;&gt;&gt; queued URIs per node.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; I have more confidence in H3 for loading millions of seed URIs at\n&gt;&gt; &gt;&gt;&gt; startup (which should be even more efficient in H3 SVN TRUNK and the\n&gt;&gt; &gt;&gt;&gt; next H3 release), though you could also feed them in smaller\n&gt; batches via\n&gt;&gt; &gt;&gt;&gt; the H1 JMX interface, or in batches via the &#39;action&#39; directory\n&gt; mechanism\n&gt;&gt; &gt;&gt;&gt; in H3.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; If you simply have a large static list of URIs to crawl -- and don&#39;t\n&gt;&gt; &gt;&gt;&gt; need link-extraction and any other running analysis/reporting --\n&gt;&gt; &gt;&gt;&gt; Heritrix may be overkill for your purposes.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;&gt; 1. Can we achieve this by extending the WorkQueueFrontier ? I couldn¹t\n&gt;&gt; &gt;&gt;&gt;&gt; find much documentation on how WQF handles politeness. I am\n&gt; thinking of\n&gt;&gt; &gt;&gt;&gt;&gt; grouping the urls into workqueue based on politeness requirement, will\n&gt;&gt; &gt;&gt;&gt;&gt; it automatically take care of politeness if I group correctly? Can I\n&gt;&gt; &gt;&gt;&gt;&gt; configure crawl-delay per WorkQueue?\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; You can control what is crawled -- whether outlinks from your starting\n&gt;&gt; &gt;&gt;&gt; URIs are followed, for example, to get inline resources or other linked\n&gt;&gt; &gt;&gt;&gt; pages -- by customizing the scoping rules. If grouping URIs by hostname\n&gt;&gt; &gt;&gt;&gt; into queues is insufficient, you can implement a new\n&gt;&gt; &gt;&gt;&gt; QueueAssignmentPolicy. In H3, politeness delays per URI --\n&gt; affecting the\n&gt;&gt; &gt;&gt;&gt; queue from which the URI came -- are configured outside the\n&gt; frontier, in\n&gt;&gt; &gt;&gt;&gt; the DispositionProcessor. So lots of behavioral customization doesn&#39;t\n&gt;&gt; &gt;&gt;&gt; require reimplementing or specializing the frontier.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; The queues are the units of politeness: by default, only one URI from a\n&gt;&gt; &gt;&gt;&gt; queue will be in-process at a time. When a URI finishes, a configurable\n&gt;&gt; &gt;&gt;&gt; pause (see the minDelay, maxDelay, delayFactor, and\n&gt;&gt; &gt;&gt;&gt; respectCrawlDelayUpToSeconds settings on DispositionProcessor in H3) is\n&gt;&gt; &gt;&gt;&gt; applied to that queue before any other URIs are tried. Note that URI\n&gt;&gt; &gt;&gt;&gt; domain-lookup/connectivity failures cause the same URI to be pushed\n&gt; back\n&gt;&gt; &gt;&gt;&gt; atop the queue, a longer (frontier retryDelaySeconds) pause to be\n&gt; taken,\n&gt;&gt; &gt;&gt;&gt; and multiple (frontier maxRetries) attempts to be made, before the next\n&gt;&gt; &gt;&gt;&gt; URI is tried. This means you usually do not want URIs on different\n&gt;&gt; &gt;&gt;&gt; hosts, where one host might be unreachable, mixed in the same queue --\n&gt;&gt; &gt;&gt;&gt; one failure will delay them all -- unless you also knock the\n&gt;&gt; &gt;&gt;&gt; retries/retryDelay way down.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; You can use the settings &#39;sheet overlay&#39; (aka &#39;overrides&#39; in H1) to set\n&gt;&gt; &gt;&gt;&gt; different politeness values for different URIs by host or other\n&gt;&gt; &gt;&gt;&gt; patterns; the queue then takes on the delay of the URI that was just\n&gt;&gt; &gt;&gt;&gt; offered/completed.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; You should also look at previous list traffic about HashCrawlMapper for\n&gt;&gt; &gt;&gt;&gt; ideas on splitting the URI space, and the BloomUriUniqFilter as an\n&gt;&gt; &gt;&gt;&gt; option for an all in-memory URI-already-seen filter that may be\n&gt;&gt; &gt;&gt;&gt; appropriate for larger crawls.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;&gt; 2. What are inactive queues, retired queues and getURIList here?\n&gt;&gt; &gt; (sorry,\n&gt;&gt; &gt;&gt;&gt;&gt; couldn¹t find doc)\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; &#39;inactive&#39; queues are those that are not yet being considered to keep a\n&gt;&gt; &gt;&gt;&gt; thread busy. All those queues that are &#39;active&#39; round-robin to provide\n&gt;&gt; &gt;&gt;&gt; URIs to available threads, until the queue exhausts its &#39;session&#39;\n&gt;&gt; &gt;&gt;&gt; budget; then it goes to the back of all &#39;inactive&#39; queues. If a thread\n&gt;&gt; &gt;&gt;&gt; can&#39;t be kept busy with an available &#39;active&#39; queue, then the top\n&gt;&gt; &gt;&gt;&gt; &#39;inactive&#39; queue is activated. The intent is for the crawler to\n&gt;&gt; &gt;&gt;&gt; intensely focus on some queues for a while -- hoping to finish them, or\n&gt;&gt; &gt;&gt;&gt; at least get a big batch of URIs with as little time-skew as\n&gt; possible ­­\n&gt;&gt; &gt;&gt;&gt; but then rotate others into activity eventually. (The &#39;budgeting&#39; and\n&gt;&gt; &gt;&gt;&gt; URI &#39;cost&#39; parameters affect this cycle.)\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; &#39;retired&#39; queues have already offered up URIs whose total &#39;cost&#39;\n&gt; exceeds\n&gt;&gt; &gt;&gt;&gt; their &#39;totalBudget&#39;, and so they continue to collect newly-discovered\n&gt;&gt; &gt;&gt;&gt; URIs, but will never b considered for &#39;active&#39; rotation (unless you\n&gt;&gt; &gt;&gt;&gt; raise their &#39;totalBudget&#39;). You probably don&#39;t need this feature, and\n&gt;&gt; &gt;&gt;&gt; won&#39;t notice any &#39;retired&#39; queues unless you set a &#39;totalBudget&#39; and\n&gt;&gt; &gt;&gt;&gt; nonzero URI cost policy.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; I don&#39;t know what you mean by &quot;getURIList&quot;.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;&gt; 3. How does checkpointing work, I want to restart from the last\n&gt; crawled\n&gt;&gt; &gt;&gt;&gt;&gt; state. Is there a callback to do the frequent checkpointing for\n&gt;&gt; &gt;&gt;&gt;&gt; WorkQueueFrontier¹s implementations.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; Checkpointing tries to save the whole crawl state at requested points.\n&gt;&gt; &gt;&gt;&gt; You can set it to automatically checkpoint at a certain interval.\n&gt; In H1,\n&gt;&gt; &gt;&gt;&gt; it&#39;s via a system property; see Checkpointer.initialize(). In H3, it&#39;s\n&gt;&gt; &gt;&gt;&gt; CheckpointService&#39;s checkpointIntervalMinutes property. In H1, the\n&gt; crawl\n&gt;&gt; &gt;&gt;&gt; must reach a full pause for a checkpoint to occur; with long downloads\n&gt;&gt; &gt;&gt;&gt; and connection timeouts, this can mean a slowdown in the tens of\n&gt;&gt; &gt;&gt;&gt; minutes. In H3, a checkpoint requires a much narrower lock, so may only\n&gt;&gt; &gt;&gt;&gt; take a few seconds or a minute or two, and long network fetches may\n&gt;&gt; &gt;&gt;&gt; continue during the checkpoint.\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; In both cases, you need to retain the &#39;state&#39; directory files\n&gt;&gt; &gt;&gt;&gt; (.JDB/.DEL) corresponding to the checkpoints from which you might want\n&gt;&gt; &gt;&gt;&gt; to resume. (If not needed for a checkpoint, you can freely delete the\n&gt;&gt; &gt;&gt;&gt; .DELs.) Resuming from an earlier checkpoint may foul any other\n&gt;&gt; &gt;&gt;&gt; subsequent-but-unused checkpoints (though future checkpoints should be\n&gt;&gt; &gt;&gt;&gt; fine).\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; The checkpointing system has always been a bit rough but I have more\n&gt;&gt; &gt;&gt;&gt; confidence in its flexibility and speed in H3. I would not yet count on\n&gt;&gt; &gt;&gt;&gt; it for perfect resumability in a large crawl without more experience\n&gt;&gt; &gt;&gt;&gt; using it. If not using checkpoints, or checkpoints fail for some\n&gt; reason,\n&gt;&gt; &gt;&gt;&gt; an approximation of a frontier&#39;s state at the time of a crash can be\n&gt;&gt; &gt;&gt;&gt; recreated from the &#39;frontier recovery log&#39; also kept by the crawler.\n&gt;&gt; &gt;&gt;&gt; (Not all running stats/state can be reconstructed, but essentially all\n&gt;&gt; &gt;&gt;&gt; the same pending URIs will be reenqueued.)\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; I believe there&#39;s a JMX call in H1 to request a checkpoint, and in H3\n&gt;&gt; &gt;&gt;&gt; it&#39;s just one of the web UI/web service calls easy to trigger by a\n&gt;&gt; &gt; web hit:\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Heri&gt;\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Heri&gt;&gt;&gt;\n&gt; t\n&gt;&gt; &gt;\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Her\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Her&gt;\n&gt;&gt; &gt; it&gt;\n&gt;&gt; &gt;&gt;&gt; rix3.0APIGuide-CheckpointJob\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; Hope this helps!\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; - Gordon @ IA\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; ------------------------------------\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; ------------------------------------\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt; Yahoo! Groups Links\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;&gt;\n&gt;&gt; &gt;\n&gt;&gt; &gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  ------------------------------------\n&gt;&gt;\n&gt;&gt;  Yahoo! Groups Links\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt; \n\n\n"}}