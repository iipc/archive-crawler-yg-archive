{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"4Rv-SsdjQP2gycTgcWe98OhCPSvp7oYeQhj0FxPlbvaik9R9yiPoBI4IT_OyLiabFpIK_bzroTTpJ6JsWfD7RtA0o2trLVpa","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Advice needed on how to (properly) structure new Heritrix modify and delete functionality","postDate":"1153250372","msgId":3080,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0QkQzNDQ0LjkwNTAzMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDIyMzguMTMwLjIwOC4xNTIuODAuMTE1MzIzMTY1Mi5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg==","referencesHeader":"PDIyMzguMTMwLjIwOC4xNTIuODAuMTE1MzIzMTY1Mi5zcXVpcnJlbEBtYWlsLmFyY2hpdmUub3JnPg=="},"prevInTopic":3078,"nextInTopic":3084,"prevInTime":3079,"nextInTime":3081,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"We ve been talking here at the Archive on how to broach the topics under discussion here: duplicate/near-duplicate detection, change-detection, junk-avoidance,","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 11008 invoked from network); 18 Jul 2006 19:19:15 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m21.grp.scd.yahoo.com with QMQP; 18 Jul 2006 19:19:15 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta1.grp.scd.yahoo.com with SMTP; 18 Jul 2006 19:19:15 -0000\r\nReceived: from [192.168.1.106] ([192.168.1.106])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k6II2xk05385;\n\tTue, 18 Jul 2006 11:02:59 -0700\r\nMessage-ID: &lt;44BD3444.9050306@...&gt;\r\nDate: Tue, 18 Jul 2006 12:19:32 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.4) Gecko/20060516 SeaMonkey/1.0.2\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;2238.130.208.152.80.1153231652.squirrel@...&gt;\r\nIn-Reply-To: &lt;2238.130.208.152.80.1153231652.squirrel@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Advice needed on how to (properly) structure\n new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=168599281; y=af9lHR9a1_q0OGU3utHmu-QxUCm4U5wxM7dNnSen0ixg-Iv32-tHLQ0n\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nWe&#39;ve been talking here at the Archive on how to broach the topics under \ndiscussion here: duplicate/near-duplicate detection, change-detection, \njunk-avoidance, etc.  Here are a few notes on what we&#39;ve been thinking.  \nThey pertain to Karl and Kris&#39;s discussion so I&#39;ll add them in here \nunder the same subject (Implementation is probably too far off to solve \nKarl&#39;s needs).\n\nAs per Kris&#39;s experience, crawl-time duplicate/change detection is \nhard.  We&#39;re thinking, like Kris, that at first, we&#39;ll do this by \npost-processing crawls.  The product of post-processing will be \ndistilled data-structures that the crawler can do fast lookups against \nas its crawling to ask such questions as: &quot;Is this a duplicate?&quot;; &quot;Is \nthis a near duplicate?&quot;; &quot;Is this page junk?&quot;; &quot;Is this page \n&#39;important&#39;?&quot;; and &quot;Has this page changed?&quot;.  Or, more likely, we&#39;ll \naggregate all URL evaluation &#39;plugins&#39; and a crawler will simply ask, \n&quot;Should I crawl this?&quot; and get back an &#39;evaluation&#39; that falls somewhere \nbetween 0 and 1 with a configurable threshold in the crawler saying what \nlevel of evaluations it&#39;ll pursue.\n\nWe&#39;re thinking we need to build a pluggable bulk-processing \ninfrastructure to do the post-crawl analysis and we&#39;ll need to build a \nfast lookup service to field crawl-time queries.\n\nFor the bulk-processing platform, hadoop -- lucene.apache.org/hadoop -- \nand parts of nutch are what we&#39;re considering.  We&#39;ve experience using \nboth in our nutchwax, building full-text indices out of ARCs.  For \nexample, evaluating exact duplicates, mirror-detection, and \nrate-of-change detection could be done as mapreduce jobs written to run \non hadoop where the ingest is the Heritrix crawl.log.  Also, Nutch has \nmapreduce jobs we&#39;ll want to  model/exploit.  Nutch has jobs to strip \nmarkup from HTML (or PDF, etc.), extract anchor text, calculate inlinks, \nand it keeps a running database of all URLs ever seen each of which \ncould serve as input to mapreduce jobs that evaluate &#39;importance&#39;, \n&#39;change&#39;, or whether or not a page is &#39;junk&#39;.  Nutch won&#39;t work for us \nout of the box -- for instance, we&#39;ll need to make the extractors used \nby Heritrix sync with the extractors used by Nutch in post-processing \nand we&#39;ll need to add notions of &#39;history&#39; to the crawl database -- but \nits a start.\n\nFor the lookup service, we&#39;re thinking it&#39;ll have to be able to service \na cluster of crawlers.  Bdbje probably won&#39;t be fast enough.  Kris&#39;s \nlucene experiment looks interesting.  We also need to look at hashing \nExternal Memory Alogrithms similar to the one discussed in the Mercator \npaper.\n\nBlue-skying it, if we could agree on a bulk processing platform, such as \nhadoop, and if we could agree on how to package the fast-lookups -- e.g. \nall implement a Lookup Interface that takes a (shrunken?) CrawlURI \nreturning an object that had a &#39;score&#39; and &#39;rationale&#39; -- then it seems \nlike we could collaborate/split-the-work.\n\nSt.Ack\n\n"}}