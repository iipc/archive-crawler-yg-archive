{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":204413223,"authorName":"lists@graemes.com","from":"&quot;lists@...&quot; &lt;lists@...&gt;","profile":"GraemeSeaton","replyTo":"LIST","senderId":"xoITIqbygY4vpuEI2uukaD6rHQLz0cGytFg9N5HY0O3PJ_NCcVVmg7XiRp9pdqc57enjcS-qOA2DbvmJqluS-mRRh9rFSSlObyNQ-HeP","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] How to setup a crawl that gathers the hostnames without downloading files","postDate":"1182990109","msgId":4377,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODJGRjFELjgwMTAyMDhAZ3JhZW1lcy5jb20+","inReplyToHeader":"PDg5MmY2YTM0MDcwNjI3MTE1OXA2ODZhMGY4NXE2NTk1MGUyMGFjYzk1MzAwQG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGY1dTI5ZCtwaWxwQGVHcm91cHMuY29tPiA8NDY4MkE5QTUuNTAyMDJAYXJjaGl2ZS5vcmc+IDw4OTJmNmEzNDA3MDYyNzExNTlwNjg2YTBmODVxNjU5NTBlMjBhY2M5NTMwMEBtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":4375,"nextInTopic":4378,"prevInTime":4376,"nextInTime":4378,"topicId":4370,"numMessagesInTopic":7,"msgSnippet":"Or alternatively, tweak the write processing chain so that you don t actually write any content and then process the crawl log file? Regards, G.","rawEmail":"Return-Path: &lt;lists@...&gt;\r\nX-Sender: lists@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 86015 invoked from network); 28 Jun 2007 00:22:01 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m55.grp.scd.yahoo.com with QMQP; 28 Jun 2007 00:22:01 -0000\r\nReceived: from unknown (HELO graemes.com) (83.105.18.153)\n  by mta13.grp.scd.yahoo.com with SMTP; 28 Jun 2007 00:21:59 -0000\r\nReceived: (qmail 28516 invoked by uid 453); 28 Jun 2007 00:21:50 -0000\r\nReceived: from eddie.graemes.com (HELO eddie.graemes.com) (192.168.7.12)\n    by graemes.com (qpsmtpd/0.32) with ESMTP; Thu, 28 Jun 2007 01:21:50 +0100\r\nMessage-ID: &lt;4682FF1D.8010208@...&gt;\r\nDate: Thu, 28 Jun 2007 01:21:49 +0100\r\nUser-Agent: Thunderbird 2.0.0.4 (X11/20070615)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f5u29d+pilp@...&gt; &lt;4682A9A5.50202@...&gt; &lt;892f6a340706271159p686a0f85q65950e20acc95300@...&gt;\r\nIn-Reply-To: &lt;892f6a340706271159p686a0f85q65950e20acc95300@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------010008010104010802090607&quot;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;lists@...&quot; &lt;lists@...&gt;\r\nSubject: Re: [archive-crawler] How to setup a crawl that gathers the hostnames\n without downloading files\r\nX-Yahoo-Group-Post: member; u=204413223; y=bw-vF9tsZS_GarZvcPhsaCkHLGPmXws9ghBwg-DvccxMFfGbaflH\r\nX-Yahoo-Profile: GraemeSeaton\r\n\r\n\r\n--------------010008010104010802090607\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nOr alternatively, tweak the write processing chain so that you don&#39;t \nactually write any content and then process the crawl log file?\nRegards,\nG.\n\nmikej wrote:\n&gt;\n&gt; Gordon,\n&gt;\n&gt; Thanks for the info. I was hoping there was a way to quickly gather a \n&gt; list of hostnames for a large crawl without pulling down all the data. \n&gt; I was hoping there was a way to download 1 byte and then move on. Our \n&gt; problem is the individuals we are working with do not seem to be able \n&gt; to come up with an accurate exclusion list and so I am doing a \n&gt; run-around approach to gather a hostname list and then determine if \n&gt; they should have excluded or included the host.\n&gt;\n&gt; Thanks,\n&gt; Mike\n&gt;\n&gt; On 6/27/07, *Gordon Mohr* &lt;gojomo@... \n&gt; &lt;mailto:gojomo@...&gt;&gt; wrote:\n&gt;\n&gt;     Doing a &#39;test&#39; crawl that discovers all the hostnames a &#39;real&#39; crawl\n&gt;     would discover would necessarily visit -- and download -- all the\n&gt;     same\n&gt;     documents. So the load on the target servers is the same, and the\n&gt;     process you&#39;ve outlined won&#39;t save any load on the unwanted-local\n&gt;     hosts,\n&gt;     and will double the load on the wanted hosts.\n&gt;\n&gt;     If you don&#39;t mind the load on the local hosts -- you just don&#39;t want\n&gt;     their content -- you could crawl everything, then post-process the\n&gt;     material collected (ARC files) to discard the unwanted material, once\n&gt;     the IP addresses are known.\n&gt;\n&gt;     Really, though, it sounds like you want a scope DecideRule that\n&gt;     acts on\n&gt;     IPs rather than hostnames. We don&#39;t have one but it would be\n&gt;     relatively\n&gt;     straightforward to write. The MatchesRegExpDecideRule would be the\n&gt;     model, but instead of comparing the full URI&#39;s String representation,\n&gt;     just the IP would be compared.\n&gt;\n&gt;     One gotcha: the first time URIs are considered for scoping, the IP\n&gt;     may\n&gt;     not be known -- the DNS lookup is a specific step triggered by a\n&gt;     URI&#39;s\n&gt;     first attempted fetch. So you&#39;d want to rule-in all URIs with\n&gt;     not-yet-known IPs, then one recheckin (Prescoper processor) some\n&gt;     would\n&gt;     be ruled-out (resulting in -5000 lines in your crawl.log).\n&gt;\n&gt;     Short of writing a new Java DecideRule, this could also be a job\n&gt;     for the\n&gt;     BeanshellDecideRule, which lets you specify a Beanshell-language\n&gt;     (Java-inspired scripting-language) script file to be run against\n&gt;     URIs to\n&gt;     make the ACCEPT/REJECT decision.\n&gt;\n&gt;     Hope this helps,\n&gt;\n&gt;     - Gordon @ IA\n&gt;\n&gt;     mjjjhjemj wrote:\n&gt;     &gt; Is this possible?\n&gt;     &gt;\n&gt;     &gt; I have a very large crawl that has numerous seeds and may potentially\n&gt;     &gt; take weeks to complete. I do not want to crawl local sites accessible\n&gt;     &gt; under a certain IP mask. Where Heritrix will exclude sites based\n&gt;     on IP\n&gt;     &gt; entered into a surts-source-file with decision=&#39;REJECT&#39; these sites\n&gt;     &gt; are accessed using the typical hostname and not IP and are therefore\n&gt;     &gt; not excluded.\n&gt;     &gt;\n&gt;     &gt; I have been given an exclude list in typical hostname\n&gt;     &#39;www.cnn.com &lt;http://www.cnn.com&gt;&#39;\n&gt;     &gt; form, but have confirmed that it is not complete and therefore my\n&gt;     &gt; crawl scope is greater than it should be. This brings me to why I\n&gt;     wish\n&gt;     &gt; to complete a test crawl where the goal is to gather only the\n&gt;     lists of\n&gt;     &gt; URIs traversed, but not gather all the files. I will then parse the\n&gt;     &gt; crawl.log and do a lookup on the hostname to determine if the IP and\n&gt;     &gt; canonical name and see if they should be included in the crawl scope.\n&gt;     &gt;\n&gt;     &gt; Any help would be greatly appreciated.\n&gt;     &gt;\n&gt;     &gt; Thanks,\n&gt;     &gt; Mike\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt; Yahoo! Groups Links\n&gt;     &gt;\n&gt;     &gt;\n&gt;     &gt;\n&gt;\n&gt;\n&gt;  \n\r\n--------------010008010104010802090607\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta content=&quot;text/html;charset=ISO-8859-1&quot; http-equiv=&quot;Content-Type&quot;&gt;\n&lt;/head&gt;\n&lt;body bgcolor=&quot;#ffffff&quot; text=&quot;#000000&quot;&gt;\nOr alternatively, tweak the write processing chain so that you don&#39;t\nactually write any content and then process the crawl log file?&lt;br&gt;\nRegards,&lt;br&gt;\nG.&lt;br&gt;\n&lt;br&gt;\nmikej wrote:\n&lt;blockquote\n cite=&quot;mid:892f6a340706271159p686a0f85q65950e20acc95300@...&quot;\n type=&quot;cite&quot;&gt;&lt;!-- Network content --&gt;\n\n  &lt;div id=&quot;ygrp-text&quot;&gt;\n  &lt;p&gt;Gordon,&lt;br&gt;\n  &lt;br&gt;\nThanks for the info. I was hoping there was a way to quickly gather a\nlist of hostnames for a large crawl without pulling down all the data.\nI was hoping there was a way to download 1 byte and then move on. Our\nproblem is the individuals we are working with do not seem to be able\nto come up with an accurate exclusion list and so I am doing a\nrun-around approach to gather a hostname list and then determine if\nthey should have excluded or included the host.\n  &lt;br&gt;\n  &lt;br&gt;\nThanks,&lt;br&gt;\nMike&lt;br&gt;\n  &lt;br&gt;\n  &lt;/p&gt;\n  &lt;div&gt;&lt;span class=&quot;gmail_quote&quot;&gt;On 6/27/07, &lt;b\n class=&quot;gmail_sendername&quot;&gt;Gordon Mohr&lt;/b&gt; &lt;&lt;a moz-do-not-send=&quot;true&quot;\n href=&quot;mailto:gojomo@...&quot;&gt;gojomo@archive.&lt;wbr&gt;org&lt;/a&gt;&gt; wrote:&lt;/span&gt;\n  &lt;blockquote class=&quot;gmail_quote&quot;\n style=&quot;border-left: 1px solid rgb(204, 204, 204);&quot;&gt;\n    &lt;div style=&quot;background-color: rgb(255, 255, 255);&quot;&gt;\n    &lt;div&gt;\n    &lt;div&gt;\n    &lt;div&gt;\n    &lt;p&gt;Doing a &#39;test&#39; crawl that discovers all the hostnames a &#39;real&#39;\ncrawl &lt;br&gt;\nwould discover would necessarily visit -- and download -- all the same &lt;br&gt;\ndocuments. So the load on the target servers is the same, and the &lt;br&gt;\nprocess you&#39;ve outlined won&#39;t save any load on the unwanted-local\nhosts, &lt;br&gt;\nand will double the load on the wanted hosts.&lt;br&gt;\n    &lt;br&gt;\nIf you don&#39;t mind the load on the local hosts -- you just don&#39;t want &lt;br&gt;\ntheir content -- you could crawl everything, then post-process the &lt;br&gt;\nmaterial collected (ARC files) to discard the unwanted material, once &lt;br&gt;\nthe IP addresses are known.&lt;br&gt;\n    &lt;br&gt;\nReally, though, it sounds like you want a scope DecideRule that acts on\n    &lt;br&gt;\nIPs rather than hostnames. We don&#39;t have one but it would be relatively\n    &lt;br&gt;\nstraightforward to write. The MatchesRegExpDecide&lt;wbr&gt;Rule would be the\n    &lt;br&gt;\nmodel, but instead of comparing the full URI&#39;s String representation, &lt;br&gt;\njust the IP would be compared.&lt;br&gt;\n    &lt;br&gt;\nOne gotcha: the first time URIs are considered for scoping, the IP may &lt;br&gt;\nnot be known -- the DNS lookup is a specific step triggered by a URI&#39;s &lt;br&gt;\nfirst attempted fetch. So you&#39;d want to rule-in all URIs with &lt;br&gt;\nnot-yet-known IPs, then one recheckin (Prescoper processor) some would &lt;br&gt;\nbe ruled-out (resulting in -5000 lines in your crawl.log).&lt;br&gt;\n    &lt;br&gt;\nShort of writing a new Java DecideRule, this could also be a job for\nthe &lt;br&gt;\nBeanshellDecideRule&lt;wbr&gt;, which lets you specify a Beanshell-language &lt;br&gt;\n(Java-inspired scripting-language) script file to be run against URIs\nto &lt;br&gt;\nmake the ACCEPT/REJECT decision.&lt;br&gt;\n    &lt;br&gt;\nHope this helps,&lt;br&gt;\n    &lt;br&gt;\n- Gordon @ IA&lt;span class=&quot;q&quot;&gt;&lt;br&gt;\n    &lt;br&gt;\nmjjjhjemj wrote:&lt;br&gt;\n&gt; Is this possible?&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; I have a very large crawl that has numerous seeds and may\npotentially&lt;br&gt;\n&gt; take weeks to complete. I do not want to crawl local sites\naccessible&lt;br&gt;\n&gt; under a certain IP mask. Where Heritrix will exclude sites based\non IP&lt;br&gt;\n&gt; entered into a surts-source-&lt;wbr&gt;file with decision=&#39;REJECT&#39; these\nsites&lt;br&gt;\n&gt; are accessed using the typical hostname and not IP and are\ntherefore&lt;br&gt;\n&gt; not excluded.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; I have been given an exclude list in typical hostname &#39;&lt;a\n moz-do-not-send=&quot;true&quot; href=&quot;http://www.cnn.com&quot; target=&quot;_blank&quot;&gt;www.cnn.com&lt;/a&gt;&#39;&lt;br&gt;\n&gt; form, but have confirmed that it is not complete and therefore my&lt;br&gt;\n&gt; crawl scope is greater than it should be. This brings me to why I\nwish&lt;br&gt;\n&gt; to complete a test crawl where the goal is to gather only the\nlists of&lt;br&gt;\n&gt; URIs traversed, but not gather all the files. I will then parse the&lt;br&gt;\n&gt; crawl.log and do a lookup on the hostname to determine if the IP\nand&lt;br&gt;\n&gt; canonical name and see if they should be included in the crawl\nscope.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Any help would be greatly appreciated.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Thanks,&lt;br&gt;\n&gt; Mike&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n    &lt;/span&gt;&gt; Yahoo! Groups Links&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &lt;br&gt;\n    &lt;br&gt;\n    &lt;/p&gt;\n    &lt;/div&gt;\n    &lt;span width=&quot;1&quot; style=&quot;color: white;&quot;&gt;&lt;/span&gt;&lt;/div&gt;\n    &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/blockquote&gt;\n  &lt;/div&gt;\n  &lt;br&gt;\n  &lt;/div&gt;\n\n&lt;!--End group email --&gt;&lt;/blockquote&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------010008010104010802090607--\r\n\n"}}