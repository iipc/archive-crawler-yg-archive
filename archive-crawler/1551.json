{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":214587980,"authorName":"Christian Kohlschuetter","from":"Christian Kohlschuetter &lt;ck-heritrix@...&gt;","replyTo":"LIST","senderId":"e2jMJ0kLJG-8YkU6e7ENzQmbAicbXwr0ECfhoeE8O_MfQM84_xva09VQ9OMEH8DpsTqlZ88H8_FqdqLLCFX73pVKyu711kveP6GWRvyh-UnkEuQ9we-bWA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] RFE: New queue assignment policy (repost)","postDate":"1108391475","msgId":1551,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDUwMjE0MTUzMS4xNTc0Mi5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4=","inReplyToHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBMDZFQzYwQHNrYXJmdXIuYm9rLmxvY2FsPg==","referencesHeader":"PDA2NzhEQjE5NjhFQUM3NDA5Q0MzRDBBQjdBMTFCODRBMDZFQzYwQHNrYXJmdXIuYm9rLmxvY2FsPg=="},"prevInTopic":1550,"nextInTopic":1557,"prevInTime":1550,"nextInTime":1552,"topicId":1545,"numMessagesInTopic":13,"msgSnippet":"Yes, I think this is a viable solution in most cases. In my scenario, however, I am really interested in linkage (inter-host/inter-partition), especially for","rawEmail":"Return-Path: &lt;ck-heritrix@...&gt;\r\nX-Sender: ck-heritrix@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 12361 invoked from network); 14 Feb 2005 14:32:17 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m24.grp.scd.yahoo.com with QMQP; 14 Feb 2005 14:32:17 -0000\r\nReceived: from unknown (HELO mail.newsclub.de) (130.75.2.42)\n  by mta3.grp.scd.yahoo.com with SMTP; 14 Feb 2005 14:32:17 -0000\r\nReceived: (qmail 22970 invoked by uid 2002); 14 Feb 2005 14:32:10 -0000\r\nReceived: from ck-heritrix@... by nhf3.rrzn.uni-hannover.de by uid 207 with qmail-scanner-1.21 \n (clamscan: 0.67. spamassassin: 2.63.  Clear:RC:0(130.75.87.112):SA:0(-2.6/5.0):. \n Processed in 2.394772 secs); 14 Feb 2005 14:32:10 -0000\r\nX-Spam-Status: No, hits=-2.6 required=5.0\r\nReceived: from pc112.l3s.uni-hannover.de (HELO mail.newsclub.de) (webmail@...@130.75.87.112)\n  by nhf3.rrzn.uni-hannover.de with RC4-MD5 encrypted SMTP; 14 Feb 2005 14:32:08 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nDate: Mon, 14 Feb 2005 15:31:15 +0100\r\nUser-Agent: KMail/1.7.2\r\nReferences: &lt;0678DB1968EAC7409CC3D0AB7A11B84A06EC60@...&gt;\r\nIn-Reply-To: &lt;0678DB1968EAC7409CC3D0AB7A11B84A06EC60@...&gt;\r\nOrganization: NewsClub\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n  charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nMessage-Id: &lt;200502141531.15742.ck-heritrix@...&gt;\r\nX-eGroups-Remote-IP: 130.75.2.42\r\nFrom: Christian Kohlschuetter &lt;ck-heritrix@...&gt;\r\nSubject: Re: [archive-crawler] RFE: New queue assignment policy (repost)\r\nX-Yahoo-Group-Post: member; u=214587980\r\n\r\nYes, I think this is a viable solution in most cases.\nIn my scenario, howev=\r\ner, I am really interested in linkage \n(inter-host/inter-partition), especi=\r\nally for PageRank computation.\nA partitioned crawl probably skews the web&#39;s=\r\n actual link structure towards \nlocal links.\n\nWith a high link-depth in com=\r\nbination with host/IP/bucket round-robin \nsub-queues, I maintain a fairly g=\r\nood coverage of the whole scope, while not \nrunning to deep into a single h=\r\nost&#39;s link structure.\n(By the way, with link depth, I mean max-hops, not pa=\r\nth-depth)\n\nOne big disadvantage of my current BdbFrontier-approach is of co=\r\nurse the \nslow-down which occurs after about 500.000 pages (goes down from =\r\nabout 80 \npages/sec. to 20). As far as I can tell, it&#39;s caused by continous=\r\n page-seen \nlookups, which is log(n), but I might be wrong. Any ideas?\n\n\nCh=\r\nristian\n\nOn Monday 14 February 2005 15:10, Kristinn Sigurdsson wrote:\n&gt; Her=\r\nitrix is still somewhat limited in the size of the crawls. I doubt you\n&gt; wo=\r\nuld be able to crawl quite that many pages in one crawl. Also, link depth\n&gt;=\r\n of 7 is quite deep on most web sites. Heck, even a link depth of 4 can lea=\r\nd\n&gt; to crawling the majority of some sites. Of course this will vary from s=\r\nite\n&gt; to site.\n&gt;\n&gt; I&#39;d suggest (if feasible) splitting up the scope into a =\r\nseries of smaller,\n&gt; independent crawls. This has the additional advantage =\r\nof allowing you to\n&gt; monitor each segment much more closely and cutting it =\r\noff once you are\n&gt; clearly crawling rubbish.\n&gt;\n&gt; The .is domain spans about=\r\n 11,000 domains and (roughly) 35 million pages.\n&gt; With the BDB frontier I c=\r\nould run it in one crawl, but I feel I get better\n&gt; performance running it =\r\nin 4 seperate batches.\n&gt;\n&gt; This is of course only feasible if you can both =\r\neasily split the scope up\n&gt; and cross linkage is not a high priority concer=\r\nn.\n&gt;\n&gt; - Kris\n&gt;\n&gt; &gt; -----Original Message-----\n&gt; &gt; From: Christian Kohlschu=\r\netter [mailto:ck-heritrix@...]\n&gt; &gt; Sent: 14. febr=FAar 2005 13:56\n&gt;=\r\n &gt; To: archive-crawler@yahoogroups.com\n&gt; &gt; Subject: Re: [archive-crawler] R=\r\nFE: New queue assignment policy\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Hey Kris,\n&gt; &gt;\n&gt; &gt; indeed, I=\r\n am currently doing _very_ broad crawls :)\n&gt; &gt;\n&gt; &gt; Using Heritrix, I would =\r\nlike to fetch about 60-100 million\n&gt; &gt; representative\n&gt; &gt; pages in the DMOZ=\r\n sphere (currently, the link depth is set to\n&gt; &gt; 7, which is\n&gt; &gt; just too m=\r\nuch for a host-oriented assignment).\n&gt; &gt;\n&gt; &gt; Perhaps you have an idea how t=\r\no do such a crawl with Heritrix current\n&gt; &gt; abilities? I would be very inte=\r\nrested in a quick solution.\n&gt; &gt;\n&gt; &gt; Best regards,\n&gt; &gt;\n&gt; &gt; Christian\n&gt; &gt;\n&gt; &gt;=\r\n On Monday 14 February 2005 14:31, Kristinn Sigurdsson wrote:\n&gt; &gt; &gt; Hey Chr=\r\nistian,\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;ve got a quick question for you: just how many hosts/=\r\nips are you\n&gt; &gt; &gt; crawling??\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;ve conducted crawls over about 1=\r\n000-1200 domains (maybe a total of\n&gt; &gt; &gt; ten-fifty times that many hosts on=\r\nce you count offsite\n&gt; &gt;\n&gt; &gt; images etc.) that\n&gt; &gt;\n&gt; &gt; &gt; covered well over =\r\na million documents (as many as 5 million\n&gt; &gt;\n&gt; &gt; in fact). And\n&gt; &gt;\n&gt; &gt; &gt; t=\r\nhis was with the much less efficient HostQueuesFrontier.\n&gt; &gt; &gt;\n&gt; &gt; &gt; I&#39;ve a=\r\nlso tested the BDBFrontier (with 1GB of heap) running\n&gt; &gt;\n&gt; &gt; on 11 thousan=\r\nd\n&gt; &gt;\n&gt; &gt; &gt; domains (and quite a few more hosts in total) without\n&gt; &gt;\n&gt; &gt; r=\r\nunning into any\n&gt; &gt;\n&gt; &gt; &gt; problems. That crawl collected over 2 million doc=\r\numents\n&gt; &gt;\n&gt; &gt; before I shut it\n&gt; &gt;\n&gt; &gt; &gt; down.\n&gt; &gt; &gt;\n&gt; &gt; &gt; So, I&#39;m a littl=\r\ne surprised by the this. Are you running a\n&gt; &gt;\n&gt; &gt; strict broad\n&gt; &gt;\n&gt; &gt; &gt; c=\r\nrawl? The only way I could see the number of hosts become\n&gt; &gt;\n&gt; &gt; an issue =\r\nwithin\n&gt; &gt;\n&gt; &gt; &gt; the first 1 million documents would be in a very broad\n&gt; &gt;=\r\n\n&gt; &gt; oriented crawl...?\n&gt; &gt;\n&gt; &gt; &gt; - Kris\n&gt; &gt; &gt;\n&gt; &gt; &gt; -----Original Message-=\r\n----\n&gt; &gt; &gt; From: Christian Kohlschuetter [mailto:ck-heritrix@...]\n&gt;=\r\n &gt; &gt; Sent: 14. febr=FAar 2005 13:15\n&gt; &gt; &gt; To: archive-crawler@yahoogroups.c=\r\nom\n&gt; &gt; &gt; Subject: [archive-crawler] RFE: New queue assignment policy\n&gt; &gt; &gt;\n=\r\n&gt; &gt; &gt;\n&gt; &gt; &gt; Hi,\n&gt; &gt; &gt;\n&gt; &gt; &gt; here&#39;s another feature which I would like to co=\r\nntribute.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Currently, I am performing broad crawls using\n&gt; &gt;\n&gt; &gt;=\r\n BroadScope/BdbFrontier.\n&gt; &gt;\n&gt; &gt; &gt; However,\n&gt; &gt; &gt; due to the number of host=\r\n- or IP-keyed queues, an\n&gt; &gt;\n&gt; &gt; OutOfMemoryError occurs\n&gt; &gt;\n&gt; &gt; &gt; very qui=\r\nckly after starting the crawl. One reason for this\n&gt; &gt;\n&gt; &gt; is the RAM-based=\r\n\n&gt; &gt;\n&gt; &gt; &gt; bookkeeping of subqueues -- the more queues, the more heap.\n&gt; &gt; =\r\n&gt;\n&gt; &gt; &gt; I have evaded this by writing a BucketQueueAssignmentPolicy\n&gt; &gt;\n&gt; &gt;=\r\n class, which\n&gt; &gt;\n&gt; &gt; &gt; produces a _fixed_ number of subqueues (&quot;buckets&quot;),=\r\n not one\n&gt; &gt;\n&gt; &gt; per host or per\n&gt; &gt;\n&gt; &gt; &gt; IP. The queue key is computed by=\r\n hashing the hostname (or the IP, if\n&gt; &gt; &gt; available) modulo N (a fixed num=\r\nber, such as 1000).\n&gt; &gt; &gt;\n&gt; &gt; &gt; This way, I was able to increase the number=\r\n of fetched\n&gt; &gt;\n&gt; &gt; pages from ca.\n&gt; &gt;\n&gt; &gt; &gt; 400,000\n&gt; &gt; &gt; to 1,000,000. Fo=\r\nr some other reason, I still get OOMEs, but\n&gt; &gt;\n&gt; &gt; I think that is\n&gt; &gt;\n&gt; &gt;=\r\n &gt; caused by a different problem -- the number of queues did\n&gt; &gt;\n&gt; &gt; not gr=\r\now over the\n&gt; &gt;\n&gt; &gt; &gt; specified limit.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Furthermore, I have modi=\r\nfied AbstractFrontier to be able to choose\n&gt; &gt; &gt; arbitrary\n&gt; &gt; &gt;\n&gt; &gt; &gt; queu=\r\ne assignment policies and replaced the current\n&gt; &gt;\n&gt; &gt; &quot;ip-politness&quot; optio=\r\nn by\n&gt; &gt;\n&gt; &gt; &gt; a\n&gt; &gt; &gt; selectbox.\n&gt; &gt; &gt;\n&gt; &gt; &gt; The patch against CVS HEAD is=\r\n attached.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Greetings,\n&gt; &gt;\n&gt; &gt; --\n&gt; &gt; Christian Kohlsch=FCtter\n&gt;=\r\n &gt; mailto: ck -at- NewsClub.de\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt;\n&gt; Yaho=\r\no! Groups Links\n&gt;\n&gt;\n&gt;\n\n-- \nChristian Kohlsch=FCtter\nmailto: ck -at- NewsClu=\r\nb.de\n\n"}}