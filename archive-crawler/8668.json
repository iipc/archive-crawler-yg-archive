{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"uCgvdPC-Vyeqi_alMWbw-Tz0Ad-vAi7sHZBu9K2Y9E8NbZ1i7UsAVS62jZxI8JeiGmugfRns67Qxh2k5WVDENcElyHGMPW8","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix 3.2.0 deduplication [6 Attachments]","postDate":"1423263226","msgId":8668,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU0RDU0NUZBLjUwMzA2MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDE0ODE3MDA5NDMuNTI1MTgzLjE0MjMyMjcwMzQ4NzIuSmF2YU1haWwueWFob29AbWFpbC55YWhvby5jb20+","referencesHeader":"PDkxRDEyNTFEOUM2ODhCNEQ4NDQ4RDRGRjUwMDAzMzFEMkFENEMwNDNAVjhCLUVYQ0hBTkdFMDEuYWQuYmwudWs+IDwxNDgxNzAwOTQzLjUyNTE4My4xNDIzMjI3MDM0ODcyLkphdmFNYWlsLnlhaG9vQG1haWwueWFob28uY29tPg=="},"prevInTopic":8667,"nextInTopic":8669,"prevInTime":8667,"nextInTime":8669,"topicId":8664,"numMessagesInTopic":10,"msgSnippet":"It can be helpful in such cases, when learning/testing a feature for the 1st time, to make the crawls toy-sized. That is, crawl just a single URI that you","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 51057 invoked by uid 102); 6 Feb 2015 22:53:48 -0000\r\nX-Received: from unknown (HELO mtaq3.grp.bf1.yahoo.com) (10.193.84.142)\n  by m1.grp.bf1.yahoo.com with SMTP; 6 Feb 2015 22:53:48 -0000\r\nX-Received: (qmail 6488 invoked from network); 6 Feb 2015 22:53:48 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (98.139.245.165)\n  by mtaq3.grp.bf1.yahoo.com with SMTP; 6 Feb 2015 22:53:48 -0000\r\nX-Received: (qmail 27240 invoked by uid 0); 6 Feb 2015 22:53:47 -0000\r\nX-Received: from 70.36.143.121 (HELO probook.local) (70.36.143.121)\n  by relay02.pair.com with SMTP; 6 Feb 2015 22:53:47 -0000\r\nX-pair-Authenticated: 70.36.143.121\r\nMessage-ID: &lt;54D545FA.5030606@...&gt;\r\nDate: Fri, 06 Feb 2015 14:53:46 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;91D1251D9C688B4D8448D4FF5000331D2AD4C043@...&gt; &lt;1481700943.525183.1423227034872.JavaMail.yahoo@...&gt;\r\nIn-Reply-To: &lt;1481700943.525183.1423227034872.JavaMail.yahoo@...&gt;\r\nContent-Type: text/plain; charset=utf-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nSubject: Re: [archive-crawler] Heritrix 3.2.0 deduplication [6 Attachments]\r\nX-Yahoo-Group-Post: member; u=137285340; y=s6_JP2yMN-dCm0ubrZx-vbfdGJU9ebM1wqrAfLg4R8x_\r\nX-Yahoo-Profile: gojomo\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\n\r\nIt can be helpful in such cases, when learning/testing a feature for the \n1st time, to make the crawls toy-sized. That is, crawl just a single URI \nthat you believe would be eligible for duplication-savings. Only when \nyou&#39;re sure the configuration works for the single URI would you then \nexpand to a more realistic crawl.\n\nFor example, you could try a crawl of just the static resource:\n\nhttp://localhost:8081/artJQWeb/resources/core/main.css\n\nIn your first crawl, working with no prior state/history, you&#39;d expect \nthis URI to have a normal log-line. In a second crawl that&#39;s set up \nproperly to consult the first crawl&#39;s digest-history, it should appear \nas a detected duplicate (and then only be written in the WARC as a \n&#39;revisit&#39; record).\n\nUntil you have this working for one URI, review your configuration, and \nmake sure the 2nd crawl is properly consulting the history-info from the \nfirst.\n\n- Gordon\n\nOn 2/6/15 4:50 AM, Sandip Dev devsandip2511@... [archive-crawler] \nwrote:\n&gt; [Attachment(s) &lt;#TopText&gt; from Sandip Dev included below]\n&gt;\n&gt; Roger,\n&gt; I deleted the state directory and history directory within my job folder\n&gt; and then ran the crawl twice .\n&gt; I am attaching the crawl log and crawl report for run 1 and 2 in case\n&gt; you want to have a look. The crawl logs seems to be very much the same\n&gt; for both runs. Do you think it looks ok to you as the warc size remains\n&gt; same  for both crawl?\n&gt;\n&gt; I have some other jobs as well which crawled this website so do I need\n&gt; to delete the state directory for those jobs as well ?\n&gt;\n&gt;\n&gt; Thanks Sandip\n&gt;\n&gt;\n&gt; On Friday, 6 February 2015 4:59 PM, &quot;Coram, Roger&quot; &lt;Roger.Coram@...&gt;\n&gt; wrote:\n&gt;\n&gt;\n&gt; Hi Sandip,\n&gt; If, having remove the legacy beans, you’re seeing “warcRevisit:digest”\n&gt; in the log this sounds like it’s already deduplicating from an earlier\n&gt; crawl—did you delete Heritrix’s state directory before testing? If not,\n&gt; Heritrix will try and deduplicate based on the earlier, stored metadata.\n&gt; Roger\n&gt; *From:*Sandip Dev [mailto:devsandip2511@...]\n&gt; *Sent:* 06 February 2015 11:24\n&gt; *To:* Coram, Roger\n&gt; *Subject:* Re: [archive-crawler] Heritrix 3.2.0 deduplication\n&gt; Hi Roger,\n&gt; Thanks a lot for  your reply.\n&gt; I tried testing this on a small web app I created.\n&gt; As suggested I removed the Legacy config&#39;s and only kept the\n&gt; URL-Agnostic Duplication Reduction .\n&gt; I can also see the warcRevisit:digest  records in the crawl log and it\n&gt; generated a 312 kb warc file which played back fine.\n&gt; I ran the crawl once more without changing anything on the site I was\n&gt; expecting the size of the warc to reduce as I had not changed anything\n&gt; on the site and as all the contents were already present in the first\n&gt; warc file. but the size of the archieve still remained 312 kb.\n&gt; Is my assumption not correct  ?\n&gt; Thanks Sandip\n&gt; On Friday, 6 February 2015 4:43 PM, &quot;Sandip Dev devsandip2511@...\n&gt; [archive-crawler]&quot; &lt;archive-crawler@yahoogroups.com&gt; wrote:\n&gt; Hi Roger,\n&gt; Thanks a lot for  your reply.\n&gt; I tried testing this on a small web app I created.\n&gt; As suggested I removed the Legacy config&#39;s and only kept the\n&gt; URL-Agnostic Duplication Reduction .\n&gt; I can also see the warcRevisit:digest  records in the crawl log and it\n&gt; generated a 312 kb warc file which played back fine.\n&gt; I ran the crawl once more without changing anything on the site I was\n&gt; expecting the size of the warc to reduce as I had not changed anything\n&gt; on the site and as all the contents were already present in the first\n&gt; warc file. but the size of the archieve still remained 312 kb.\n&gt; Is my assumption not correct  ?\n&gt; Thanks Sandip\n&gt; On Friday, 6 February 2015 3:55 PM, &quot;&#39;Coram, Roger&#39; Roger.Coram@...\n&gt; [archive-crawler]&quot; &lt;archive-crawler@yahoogroups.com&gt; wrote:\n&gt; Hi Sandip,\n&gt; As far as I’m aware you should definitely be using only one of the\n&gt; “URL-Agnostic Duplication Reduction” or the “Legacy…” version. Although\n&gt; they largely do the same thing they behave differently (effectively the\n&gt; newer version keys on the checksum, the older keys on the URL+checksum).\n&gt; At first glance your config. looks correct so it might be the inclusion\n&gt; of both the above causing the issue. During a crawl, if you watch your\n&gt; crawl.log file, you should see entries with annotations like\n&gt; “warcRevisit:digest” in the last field—that indicates deduplication is\n&gt; working.\n&gt; Roger\n&gt; *From:*archive-crawler@yahoogroups.com\n&gt; [mailto:archive-crawler@yahoogroups.com]\n&gt; *Sent:* 06 February 2015 04:07\n&gt; *To:* archive-crawler@yahoogroups.com\n&gt; *Subject:* [archive-crawler] Heritrix 3.2.0 deduplication [1 Attachment]\n&gt; *[Attachment(s)\n&gt; &lt;https://in-mg61.mail.yahoo.com/neo/launch?.rand=b829oo386s801#TopText&gt;\n&gt; from Sandip Dev included below]*\n&gt; Hi,\n&gt; I had already posted my questions earlier regarding the issues that I\n&gt; have been facing the deduplication.\n&gt; Can you please let me know if this is the correct forum where I should\n&gt; post my questions ?\n&gt; I am trying to achieve incremental crawling using Heritrix 3.2.0.\n&gt; However it seems it is always browsing all the URL&#39;s even if there is no\n&gt; change.\n&gt; I am attaching crawler-beans.xml for the configuration that I am doing.\n&gt; Can anyone suggest what I might be doing wrong ?\n&gt; Is the spring bean I have put the configurations for both\n&gt;\n&gt;   * URL-Agnostic Duplication Reduction\n&gt;   * Legacy Duplication Reduction Configuration\n&gt;\n&gt; Can I use both the features at the same time in the config ? I see the\n&gt; archive size remains the same across multiple crawls even if there is no\n&gt; change . How do I check if the the deduplication is working ?\n&gt; Any help will be greatly appreciated as I am desperately tryinjg to get\n&gt; this work :)\n&gt; Thanks\n&gt; Sandip Dev\n&gt;\n&gt;\n&gt; ******************************************************************************************************************\n&gt; Experience the British Library online at www.bl.uk &lt;http://www.bl.uk/&gt;\n&gt; The British Library’s latest Annual Report and Accounts :\n&gt; www.bl.uk/aboutus/annrep/index.html\n&gt; &lt;http://www.bl.uk/aboutus/annrep/index.html&gt;\n&gt; Help the British Library conserve the world&#39;s knowledge. Adopt a Book.\n&gt; www.bl.uk/adoptabook &lt;http://www.bl.uk/adoptabook&gt;\n&gt; The Library&#39;s St Pancras site is WiFi - enabled\n&gt; *****************************************************************************************************************\n&gt; The information contained in this e-mail is confidential and may be\n&gt; legally privileged. It is intended for the addressee(s) only. If you are\n&gt; not the intended recipient, please delete this e-mail and notify the\n&gt; postmaster@... &lt;mailto:postmaster@...&gt;: The contents of this e-mail\n&gt; must not be disclosed or copied without the sender&#39;s consent.\n&gt; The statements and opinions expressed in this message are those of the\n&gt; author and do not necessarily reflect those of the British Library. The\n&gt; British Library does not take any responsibility for the views of the\n&gt; author.\n&gt; *****************************************************************************************************************\n&gt;\n&gt; Think before you print\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}