{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"&quot;Gordon Mohr&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"87ibG-rbiGrrCwCinOrudMMkFDTWc79ikQQ5KHUp2aFgMFKCK2ERWK1t8Tdo6etWCSKRVUNjxFKGRV-gFH1k3PV2HYp9IrwbCA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Testing web garden...","postDate":"1055980262","msgId":78,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAwNjEwMWMzMzVmNSRmMTg2MzUzMCQ0OGYwZWRkMUBXT1JLU1RBVElPTjIxPg==","referencesHeader":"PFBpbmUuTE5YLjQuMzMuMDMwNjE4MTYyNTMzMC4xNzUzNS0xMDAwMDBAaG9tZXNlcnZlci5hcmNoaXZlLm9yZz4="},"prevInTopic":0,"nextInTopic":81,"prevInTime":77,"nextInTime":79,"topicId":78,"numMessagesInTopic":2,"msgSnippet":"[moving discussion to arcive-crawler@yahoogroups.com] Looks good! I think we ll want to split up the tests into at least 3 non-overlapping (not cross-linked)","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 66571 invoked from network); 19 Jun 2003 00:01:53 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m14.grp.scd.yahoo.com with QMQP; 19 Jun 2003 00:01:53 -0000\r\nReceived: from unknown (HELO mail.archive.org) (209.237.232.56)\n  by mta2.grp.scd.yahoo.com with SMTP; 19 Jun 2003 00:01:53 -0000\r\nReceived: from WORKSTATION21 (b116-dyn-72.archive.org [209.237.240.72])\n\tby mail.archive.org (8.12.8/8.10.2) with SMTP id h5INCQlQ012225\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 18 Jun 2003 16:12:41 -0700\r\nMessage-ID: &lt;006101c335f5$f1863530$48f0edd1@WORKSTATION21&gt;\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nReferences: &lt;Pine.LNX.4.33.0306181625330.17535-100000@...&gt;\r\nSubject: Re: Testing web garden...\r\nDate: Wed, 18 Jun 2003 16:51:02 -0700\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2800.1158\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1165\r\nFrom: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\n[moving discussion to arcive-crawler@yahoogroups.com]\n\nLooks good!\n\nI think we&#39;ll want to split up the tests into at least 3 non-overlapping\n(not cross-linked) sets:\n\n (1) The easy: stuff every crawler must handle, in a clear way,\n     and we expect our crawler to handle soon. (Basic HTML and \n     embedded images.)\n\n (2) The hard: stuff we&#39;d like to handle, perhaps very difficult\n     (eg javascript which builds a URI string), but ultimately\n     with a clearly right end result. \n\n (3) The indeterminate: stuff that can only be handled by making\n     some policy decision which is not inherently right or wrong,\n     such as determining when we give up on a URI/site/path.\n     (The infinites fit here.)\n\nI like the infinitelybroad script. Some form of it would be useful for \ninternal speed profiling, too. \n\nIt would be ideal if its synthesized target hosts/URIs were \ndeterministic, so that two crawler visits entering at the same\nplace would be reproduceable/comparable (up to the point where\nthey gave up). \n\nAlso, if &quot;infinite&quot; is only up to &quot;www99999999&quot;, we can visit all\nthose. :) 100 million pages isn&#39;t so many considering our eventual \ngoals!\n\n- Gordon\n\n----- Original Message ----- \nFrom: &quot;Parker Thompson&quot; &lt;parkert@...&gt;\nTo: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\nCc: &lt;igor@...&gt;; &lt;jma@...&gt;\nSent: Wednesday, June 18, 2003 4:29 PM\nSubject: Re: Testing web garden...\n\n\n&gt; Gordon,\n&gt; \n&gt; I&#39;ve started filling in some pages that we can use to test the crawl.  I\n&gt; copied ideas but not content from the searchtools pages, and we may be\n&gt; able to copy content pending a request I made to the maintainers of that\n&gt; site.\n&gt; \n&gt; Additionally, I wrote a perl script that I believe we can use to do the\n&gt; infintely broad/duplicate content test.  It&#39;s in the newtest/ directory:\n&gt; \n&gt;   http://crawl08.archive.org/newtest/\n&gt; \n&gt; but is non-functional there.  We&#39;ll need to set up a host with wildcard\n&gt; aliasing.  To see this in action check out:\n&gt; \n&gt;  http://parkert.com/infinitelybroad.cgi\n&gt; \n&gt; Please take a look at the framework and let me know if there&#39;s anything \n&gt; fundementally wrong with it, any top/second-level categories I&#39;ve \n&gt; omitted, etc.  \n&gt; \n&gt; Thanks,\n&gt; \n&gt; pt.\n&gt; -- \n&gt; Parker Thompson\n&gt; The Internet Archive\n&gt; 510.541.0125\n&gt; \n&gt; On Wed, 18 Jun 2003, Gordon Mohr wrote:\n&gt; \n&gt; |The test area at...\n&gt; |\n&gt; |http://crawl08.archive.org/index-2.html\n&gt; |\n&gt; |...came from the www.searchtools.com website, and covers a bunch\n&gt; |of the things we&#39;ll need to cover. \n&gt; |\n&gt; |I don&#39;t think we want to use the exact copy pages, though, \n&gt; |because (1) we may not have permission to do so, and since \n&gt; |we&#39;ll eventually want to have our test suite be CVS-versioned \n&gt; |and open-sourced, we need to be confident of our ability \n&gt; |to use/redistribute it; 2) some of their listed tests aren&#39;t yet \n&gt; |implemented; and (3) we&#39;ll want to add a lot more before we&#39;re \n&gt; |done. \n&gt; |\n&gt; |That site also has a useful &quot;Crawler Checklist&quot; which\n&gt; |covers some other issues we&#39;ll want to test for:\n&gt; |\n&gt; |http://www.searchtools.com/robots/robot-checklist.html\n&gt; |\n&gt; |So, Parker, until Judy&#39;s back it&#39;d be great for you to start \n&gt; |our own master index page, linking off to individual point tests \n&gt; |(inspired by those from search tools, funnellback, and elsewhere), \n&gt; |and devise a strategy for keeping this in CVS -- probably as\n&gt; |another module.\n&gt; |\n&gt; |Some free-form ideas on things to test are at:\n&gt; |\n&gt; |http://crawler.archive.org/cgi-bin/wiki.pl/wiki.pl?WebGarden\n&gt; |\n&gt; |- Gordon\n&gt; |\n&gt;\n\n"}}