{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":275305800,"authorName":"Leo Dagum","from":"&quot;Leo Dagum&quot; &lt;leo_dagum@...&gt;","profile":"leo_dagum","replyTo":"LIST","senderId":"tZgayCOxXmX7WAzjXK-e1uWFo1CuHXuRxniNO4rZEaUv-WV2Iim-vl09clDWDO9el4H0WVQU3Jtj3QKHzYDut9fI_BgCn2iQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"RE: [archive-crawler] semantics of queue-total-budget","postDate":"1195088494","msgId":4688,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"inReplyToHeader":"PDQ3M0I2REY1LjEwOTAxMDFAYXJjaGl2ZS5vcmc+"},"prevInTopic":4686,"nextInTopic":4702,"prevInTime":4687,"nextInTime":4691,"topicId":4685,"numMessagesInTopic":4,"msgSnippet":"Thanks for the explanation!  I retried the crawl monitoring more closely, but was unable to reproduce the situation where 90% of my queues became exhausted in","rawEmail":"Return-Path: &lt;leo_dagum@...&gt;\r\nX-Sender: leo_dagum@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 58217 invoked from network); 15 Nov 2007 01:01:37 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m56.grp.scd.yahoo.com with QMQP; 15 Nov 2007 01:01:37 -0000\r\nX-Received: from unknown (HELO smtp119.plus.mail.sp1.yahoo.com) (69.147.95.82)\n  by mta18.grp.scd.yahoo.com with SMTP; 15 Nov 2007 01:01:37 -0000\r\nX-Received: (qmail 48340 invoked from network); 15 Nov 2007 01:01:37 -0000\r\nX-Received: from unknown (HELO foxtrot) (leo_dagum@209.213.209.230 with login)\n  by smtp119.plus.mail.sp1.yahoo.com with SMTP; 15 Nov 2007 01:01:36 -0000\r\nX-YMail-OSG: Ou7uZbAVM1l0eCfeBIejaq1Fm8BYpwTbfHc6KyiJQq9KuWyYtbFqo7VrroZhAnI0xnhj8E_JCMYPBrc18VDCeBHby1J74Tog72RTL9Ka5hV9DTuck4bj.J.fFINT\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Wed, 14 Nov 2007 17:01:34 -0800\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;us-ascii&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Mailer: Microsoft Office Outlook, Build 11.0.5510\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.3198\r\nThread-Index: AcgnCKRQ1f8uk4ttTDSwLAHmnKIPzwAGXJrQ\r\nIn-Reply-To: &lt;473B6DF5.1090101@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Leo Dagum&quot; &lt;leo_dagum@...&gt;\r\nSubject: RE: [archive-crawler] semantics of queue-total-budget\r\nX-Yahoo-Group-Post: member; u=275305800; y=Ft0r2KXic4VyFRfdwAFL9nwS-jsTVcuvOSnSKz3_9qm6th16\r\nX-Yahoo-Profile: leo_dagum\r\n\r\nThanks for the explanation!  I retried the crawl monitoring more closely,\nbut was unable to reproduce the situation where 90% of my queues became\nexhausted in short order.  I&#39;m now getting the behavior I expected.\n\nA related question: can you explain the different cost-policies?  I\nunderstand zero-cost and unit-cost, but wag-cost and anti-calendar-cost are\na mystery to me.\n\nThanks,\n\n- leo\n\n-----Original Message-----\nFrom: archive-crawler@yahoogroups.com\n[mailto:archive-crawler@yahoogroups.com] On Behalf Of Gordon Mohr\nSent: Wednesday, November 14, 2007 1:52 PM\nTo: archive-crawler@yahoogroups.com\nSubject: Re: [archive-crawler] semantics of queue-total-budget\n\nLeo Dagum wrote:\n&gt; I&#39;m trying to configure a crawl across a broad number of sites but \n&gt; fetching only a restricted number of pages from each site. \n&gt;  Specifically,  I&#39;d like to get only 20pages from each site across about \n&gt; 200k sites that are provided through a seed list.  I set q-t-b to 20 and \n&gt; UnitCostAssignment but did not get the behavior I expected. \n&gt; \n&gt;  \n&gt; \n&gt; I&#39;m using HostnameQueueAsssingment policy, so I expected 200k queues to \n&gt; be created and that they get exhausted as they reached their 20 unit \n&gt; budget.  However what I saw was ~180k queues almost immediately marked \n&gt; as exhausted and the crawler busy on just 20k queues.  My understanding \n&gt; is that once a queue is exhausted it does not get scheduled again.  Is \n&gt; that correct?  Or will the exhausted queues get reactivated at some point?\n\n\n&#39;Exhausted&#39; means there was nothing left to crawl -- the queue is empty. \nSuch queues will come back if new URIs are discovered and added to the \nqueue.\n\nQueues which go over their budget are instead &#39;retired&#39;. They still have \nthe URIs that were waiting, and still get newly discovered URIs -- they \njust aren&#39;t eligible to provide URIs for crawling. The idea, however, is \nthat you could go in and up the total-budget, and these queues would be \nunretired.\n\nSo I could be wrong, but suspect that for the 180k queues that instantly \nbecame exhausted, there is either no/very-little content or some other \nerror (restrictive scope?) is preventing any URIs from being added to \nthose queues.\n\n- Gordon @ IA\n\n\n \nYahoo! Groups Links\n\n\n\n\n\n"}}