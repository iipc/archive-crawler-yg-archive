{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"gzRR6P46E4C9HxmzCKER01GfvRNK-RiXJqyDsdwUCxpgyOg1zK0ygOGDA6OtWivxBfQ2yxuGmAZh6LkdATDTzULasm518S8","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Suggested Heap for Crawling Hundreds of Thousands - Millions of URLs?","postDate":"1284587444","msgId":6733,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOTEzRkI0LjYwODA5MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEFBTkxrVGk9THVpdEpwUEtqRlJXNkFqa1BIRmtjOGgyZG1NOUN4OWh4QkhZX0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGk9THVpdEpwUEtqRlJXNkFqa1BIRmtjOGgyZG1NOUN4OWh4QkhZX0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6732,"nextInTopic":0,"prevInTime":6732,"nextInTime":6734,"topicId":6732,"numMessagesInTopic":2,"msgSnippet":"... The default BdbFrontier uses data structures provided by BerkeleyDB-JE that spill over to disk, so its size is *not* limited by memory. Still, more memory","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 71763 invoked from network); 15 Sep 2010 21:50:47 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m7.grp.sp2.yahoo.com with QMQP; 15 Sep 2010 21:50:47 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta2.grp.sp2.yahoo.com with SMTP; 15 Sep 2010 21:50:47 -0000\r\nX-Received: (qmail 1691 invoked from network); 15 Sep 2010 21:50:45 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay01.pair.com with SMTP; 15 Sep 2010 21:50:45 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4C913FB4.6080904@...&gt;\r\nDate: Wed, 15 Sep 2010 14:50:44 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.9) Gecko/20100825 Thunderbird/3.1.3\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Zach Bailey &lt;zach.bailey@...&gt;\r\nReferences: &lt;AANLkTi=LuitJpPKjFRW6AjkPHFkc8h2dmM9Cx9hxBHY_@...&gt;\r\nIn-Reply-To: &lt;AANLkTi=LuitJpPKjFRW6AjkPHFkc8h2dmM9Cx9hxBHY_@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Suggested Heap for Crawling Hundreds of Thousands\n - Millions of URLs?\r\nX-Yahoo-Group-Post: member; u=137285340; y=vuyB3vU_15K4R2TB3cw6uXrzAAGOafnlcaKQ99d1LlUq\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 9/15/10 11:25 AM, Zach Bailey wrote:\n&gt; Some general hardware and JVM sizing questions -\n&gt;\n&gt; What is the suggested approach to indexing hundreds of thousands -\n&gt; millions of URLs? Some things I was thinking about:\n&gt;\n&gt; 1.) Memory - since the Frontier URIs are held in memory, the heap needs\n&gt; to be rather massive - on the order of a couple gigabytes. Are there any\n&gt; alternative frontiers that don&#39;t use in-memory stores? In general heaps\n&gt; above a couple gigabytes are tough to maintain due to JVM GC issues.\n\nThe default BdbFrontier uses data structures provided by BerkeleyDB-JE \nthat spill over to disk, so its size is *not* limited by memory.\n\nStill, more memory helps, since BDB-JE keeps as much of itself in its \nassigned cache space as possible. BDB-JE&#39;s default is to use 60% of the \ntotal heap for its cache; we&#39;d only recommend changing that if either:\n\n(a) you move some functionality back into all-memory datastructures (ie \nreplace BdbUriUniqFilter with BloomUriUniqFilter), in which case you \nmight *decrease* the BDB-JE cache percentage to make more space for \nother things.\n\n(b) you have tons of heap space, in which case everything other than \nBDB-JE doesn&#39;t need the other 40%, whereas more cache always helps the \nFrontier. In this case (rare) you&#39;d increase the allocation to BDB-JE \ncache.\n\nUnless the memory is needed by other processes on the same machine, I \nwould make the heap as large as physical memory allows, with some \nheadroom for the JVM&#39;s non-heap memory use. (We often use 2300-2700m on \na 4g machine.) Larger heaps can mean longer GC pauses, but they should \nstill be less frequent with a larger heap, meaning overall throughput \ngoes up (which is usually of paramount importance in a crawler, where a \nfew pauses don&#39;t present user-responsiveness problems).\n\n&gt; 2.) Disk Space - for the initial run I really only care about HTML\n&gt; content - no images, JS, CSS, etc. I have turned off the JS/CSS/SWF\n&gt; extractors but the crawl still seems to be pulling down images. Is this\n&gt; because the ExtractorHtml returns &lt;img src&gt; tags as indexable? Is there\n&gt; a way to disable that behavior? I really only care about links to other\n&gt; pages, not images.\n\nYes, the extractors find all outlinks -- including those to \npresentational elements.\n\nA scope rule that rejects URIs likely to be uninteresting types (by last \nfew characters -- &#39;file extension&#39; -- of the URI path) will eliminate \nmost of these, and unless you care about the occasional URI that ends \n&quot;.gif&quot; but returns HTML, that&#39;s probably OK for you.\n\nYou can additionally add rules to the FetchHTTP for cancelling fetches \nas soon as you see a Content-Type header you don&#39;t like -- but the \nsituations where you&#39;re getting non-textual content from a URI without a \nfile-extension hint may be so rare you don&#39;t care.\n\nThere&#39;s one other bit of info a scope rule *could* use to eliminate \nunwanted presentational elements, the CrawlURI&#39;s &#39;viaContext&#39;, an \nXPath-like fragment that (when possible) tries to describe where the \nlink was found. For example, an outlink from an IMG SRC attribute will \nhave a viaContext like &#39;img&#92;@src&#39;. While it would be very simple to make \na DecideRule that consults this for wanted/unwanted patterns, either in \nJava or via the ScriptedDecideRule, we haven&#39;t provided one.\n\n&gt; Once I have this issue tackled I don&#39;t believe disk space should be an\n&gt; issue at all considering how well raw text compresses.\n&gt;\n&gt; 3.) Bottlenecks - in general assuming the internet connection is fast\n&gt; enough, am I going to be I/O bound or CPU bound? What optimizations\n&gt; could be explored at that point?\n\nDepends on your machine, of course, but as the crawl gets larger and \nmore queue/already-seen operations need to touch the disk, those seeks \nare likely to become the limit. One option at that point is to use the \nBloomUriUniqFilter -- then all alreadySeen checks occur in memory, but \nhave a small false-positive rate (that grows beyond an intended cap \nafter you hit certain expected-sizes). See the comments of related \nclasses for more details.\n\nAt that point you could also consider ways to split the crawl over \nmultiple machines; several prior threads talk about the ad-hoc ways \npeople have done that, over 3-12 machines.\n\nThere&#39;s lots of potential for more single-machine code optimization, as \nwell, especially with regard to the &#39;alreadyIncluded&#39; (UriUniqFilter) \nstructure, but after years an even a number of billion-plus page crawls, \nthose haven&#39;t yet been urgent for our operations.\n\n&gt; Thanks again for any insight or experiences doing a large crawl like this.\n\nYW, good luck!\n\n- Gordon @ IA\n\n\n&gt; -Zach\n&gt;\n&gt;\n&gt; \n\n"}}