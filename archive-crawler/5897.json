{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":272799769,"authorName":"Gina Jones","from":"&quot;Gina Jones&quot; &lt;gjon@...&gt;","profile":"gmj2053","replyTo":"LIST","senderId":"7kiWCSX4D2BoGsyn83IBMWI-5yhbFlqVcYP6lzyiA8bjcblEdIHjeq-_UaOVBMnsatOHBlq2UbQavP18wOfBw6_u","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] twitter crawling","postDate":"1245441651","msgId":5897,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBM0JCNjMzMDIwMDAwMTIwMDNCOEU2N0BudGd3Z2F0ZS5sb2MuZ292Pg==","inReplyToHeader":"PDRBM0FCMThCLjMwNzAyMDVAYXJjaGl2ZS5vcmc+","referencesHeader":"PDRBM0EzOUVGMDIwMDAwMTIwMDNCOEJDNEBudGd3Z2F0ZS5sb2MuZ292PiA8NEEzQUIxOEIuMzA3MDIwNUBhcmNoaXZlLm9yZz4="},"prevInTopic":5896,"nextInTopic":0,"prevInTime":5896,"nextInTime":5898,"topicId":5893,"numMessagesInTopic":4,"msgSnippet":"Relevant tweets as best as possible. I am certainly hoping that the division isn t expecting 100% but I want to get more than what we would get with the weekly","rawEmail":"Return-Path: &lt;gjon@...&gt;\r\nX-Sender: gjon@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 78505 invoked from network); 19 Jun 2009 20:01:45 -0000\r\nX-Received: from unknown (69.147.108.202)\n  by m1.grp.sp2.yahoo.com with QMQP; 19 Jun 2009 20:01:45 -0000\r\nX-Received: from unknown (HELO ntgwgate.loc.gov) (140.147.137.18)\n  by mta3.grp.re1.yahoo.com with SMTP; 19 Jun 2009 20:01:45 -0000\r\nX-Received: from LCHub-MTA by ntgwgate.loc.gov\n\twith Novell_GroupWise; Fri, 19 Jun 2009 16:01:19 -0400\r\nMessage-Id: &lt;4A3BB63302000012003B8E67@...&gt;\r\nX-Mailer: Novell GroupWise Internet Agent 7.0.3 \r\nDate: Fri, 19 Jun 2009 16:00:51 -0400\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nReferences: &lt;4A3A39EF02000012003B8BC4@...&gt;\n &lt;4A3AB18B.3070205@...&gt;\r\nIn-Reply-To: &lt;4A3AB18B.3070205@...&gt;\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=US-ASCII\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Gina Jones&quot; &lt;gjon@...&gt;\r\nSubject: Re: [archive-crawler] twitter crawling\r\nX-Yahoo-Group-Post: member; u=272799769; y=7gians-SzjEITfadl7WJ9L5FZ9UbL8V5RxIOWVHg4yK3pQ\r\nX-Yahoo-Profile: gmj2053\r\n\r\nRelevant tweets as best as possible. I am certainly hoping that the divisio=\r\nn isn&#39;t expecting 100% but I want to get more than what we would get with t=\r\nhe weekly crawl that you are doing for us.  It&#39;s easy to check to see what =\r\nthe date range for the 100 pages are because you can check that in a browse=\r\nr.\n \nI did run a crawl removing transclusion rule, that wasn&#39;t the problem.=\r\n  Had 35K queued in minutes.   But yes, would prefer the entirety of the pa=\r\nge.\n   \nI think the issue is my surting for the addtional pages. When I tak=\r\ne that surt out I have a very narrowly scoped crawl.\nhttp://(com,twitter,se=\r\narch,)/search?max_id=3D \nFor some reason, twitter returns a page for that s=\r\ntring.  \nand I need to use that string if I try to do the date range spec.\n=\r\nI don&#39;t see a way to get more results on a page and I can&#39;t specify to the =\r\nhour, only to the day.\nI got no results for \nsotomayor since:2009-06-18-01 =\r\nuntil:2009-06-19-02\n\nI think shortterm, we&#39;ll do a chron job and get the fi=\r\nrst search page frequently.  I&#39;m committed elsewhere next week, but will ge=\r\nt back on it after I get back.    I don&#39;t see any other rules that I could =\r\napply that would make sense but I&#39;ll take a harder look and maybe we&#39;ll get=\r\n 3.0 beta in our testlab and see.\n \n\n\nSo, your example is exactly how I wou=\r\nld envision the archive to work\n&lt;snip&gt;\nIf (for example) you get &lt;http://sea=\r\nrch.twitter.com/search?q=3Dsotomayor&gt; \neach night at midnight, then later u=\r\nsers of the archive via the Wayback \nwould see those daily captures, and st=\r\narting from any of them, be able \nto page through the &#39;older&#39; links -- beca=\r\nuse paging from any one crawl \nwould use consistent &#39;max_id&#39; values.\n&lt;/snip=\r\n&gt;\nOne day ful text, not yet.... maybe we can do something else with these t=\r\nweets.\n\nThanks for that idea about the misspelling, added that in. \n\nThanks=\r\n again Steve and Gordon.  \ngina\n\n\n\n&gt;&gt;&gt; Gordon Mohr &lt;gojomo@...&gt; 6/1=\r\n8/2009 5:28 PM &gt;&gt;&gt;\nGina Jones wrote:\n&gt; I am trying to crawl twitter to get =\r\na search query\n&gt; http://search.twitter.com/search?q=3Dsotomayor \n&gt; as part =\r\nof a new collection that the Library of Congress is doing on the supreme co=\r\nurt nominations.  Intent is to do a daily crawl to capture the tweets about=\r\n sotomayor.\n\nAre you just interested in a sampling, or are you hoping to ca=\r\npture \nevery relevant tweet during the collection period?\n\n(If your chief a=\r\nim was complete coverage, you might want to crawl more \nthan once per day, =\r\nbut make the number of &#39;pages&#39; you dig dependent on \nwhether you&#39;ve started=\r\n getting repeats from your last capture. At the \nextreme, this could involv=\r\ne a custom Beanshell script to analyze in-page \ncontent before following th=\r\ne &#39;older&#39; link.)\n\n&gt; Twitter provides up to 100 pages of what has been tweet=\r\ned about the query term.  Unfortunately, the max-id for the additional 99 p=\r\nages always start with the number of the latest tweet on the fly so I can&#39;t=\r\n really specify that because I have to be concerned with users accessing it=\r\n via wayback as part of the collection.\n\nI&#39;m not sure why the tweet-number-=\r\nin-URL is an issue. Can you explain in \nmore detail?\n\nIf (for example) you =\r\nget &lt;http://search.twitter.com/search?q=3Dsotomayor&gt; \neach night at midnigh=\r\nt, then later users of the archive via the Wayback \nwould see those daily c=\r\naptures, and starting from any of them, be able \nto page through the &#39;older=\r\n&#39; links -- because paging from any one crawl \nwould use consistent &#39;max_id&#39;=\r\n values.\n\n\n\nIf you offered full-text search, there would be a chance for us=\r\ners to \njump directly to deeper results pages, when those pages appear in s=\r\nearch \nresults.\n\n&gt; \n&gt; I am trying to maximize the efficiency of this crawl,=\r\n limiting the extraneous stuff.\n&gt; \n&gt; My surt prefix associations are\n&gt; http=\r\n://(com,twitter,search,)/search?max_id=3D \n&gt; http://(com,twitter,search,)/s=\r\nearch?q=3Dsotomayor \n&gt; \n&gt; Using the standard global sheet for deep seed cra=\r\nwl.\n&gt; \n&gt; Is this the best way to do this crawl?  And yes, Gordon, Heritrix =\r\nrel. 2.\n&gt; \n&gt; suggestions on the sheet settings?\n&gt; I am getting a lot of gar=\r\nbage.\n\nThe standard advice regarding H2 applies: unless you need to use or =\r\n\nlearn the new prioritization features, H1.14.x is still recommended for \np=\r\nroduction crawls.\n\nWhat sort of garbage are you getting? Since Twitter does=\r\nn&#39;t include \nuser-contributed HTML, and everything inline on their site \n(i=\r\nmages/js/css/etc) is from their servers, I would think your scope (as \nI un=\r\nderstand it) wouldn&#39;t stray from material necessary to render the \nsearch r=\r\nesults page.\n\nMy colleague Steve&#39;s suggestion, to eliminate the \nTransclusi=\r\nonDecideRule, is a standard thing to consider when crawls are \nwandering fr=\r\nom the focal sites. (Or, similarly, tightening the \nhop-limits used by the =\r\nTransclusionDecideRule.) However, since I think \nyou care about rendering f=\r\nidelity in the Wayback, and the Twitter result \npage HTML is already tightl=\r\ny controlled by Twitter, you probably want to \nkeep it. (For example, it ap=\r\npears Twitter users&#39; avatar images come from \ns3.amazonaws.com, and those w=\r\nill only be fetched if you leave the \nTransclusionDecideRule in.)\n\nVarious =\r\nother thoughts triggered by your project:\n\n- Duplication in repeated querie=\r\ns may not be that big a deal; tweets are \nsmall, and since a real user, que=\r\nrying each day, would see some \nduplicates it&#39;s fair for the archive to ref=\r\nlect that as well.\n\n- It seems like misspellings of &#39;Sotomayor&#39; are common;=\r\n you might want \nto grab &#39;Sotomayer&#39; (at least) too.\n\n- Often a Tweet inclu=\r\ndes a cryptic shortened-link, and can only be \nunderstood after visiting th=\r\ne link. So this might be a place to do a \n&quot;plus-one&quot; crawl, intentionally v=\r\nisiting all the pages on other sites \nlinked from the Twitter results. Of c=\r\nourse, this means a lot more \n&#39;garbage&#39; -- indeed could swamp the Twitter r=\r\nesults -- but could make \nthe archive much more complete/comprehendable.\n\nH=\r\nope this helps,\n\n- Gordon @ IA\n\n"}}