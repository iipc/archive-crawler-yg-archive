{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"x8EIl29w-iCjaBHgEOP1roVWw6aCzHJGrVBgwtGWMNlXrHon7EVhWlGhBghe_BIZyGtE_XBc84bIa1o7dQ18TLMvZXwQdhc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling","postDate":"1182887000","msgId":4364,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODE2QzU4LjYwMjAwMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGY1cjE1YytlaW41QGVHcm91cHMuY29tPg==","referencesHeader":"PGY1cjE1YytlaW41QGVHcm91cHMuY29tPg=="},"prevInTopic":4361,"nextInTopic":4368,"prevInTime":4363,"nextInTime":4365,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"... In general, you only want to use distributed crawling, with URIs partitioned across separate cooperating crawlers, to spread a crawl over multiple","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 70538 invoked from network); 26 Jun 2007 19:43:13 -0000\r\nReceived: from unknown (66.218.66.70)\n  by m53.grp.scd.yahoo.com with QMQP; 26 Jun 2007 19:43:13 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta12.grp.scd.yahoo.com with SMTP; 26 Jun 2007 19:43:13 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 62F0A141600C6\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 26 Jun 2007 12:43:09 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 16375-06-77 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 26 Jun 2007 12:43:08 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id 9F3A914156D22\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 26 Jun 2007 12:43:08 -0700 (PDT)\r\nMessage-ID: &lt;46816C58.6020000@...&gt;\r\nDate: Tue, 26 Jun 2007 12:43:20 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (X11/20070604)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f5r15c+ein5@...&gt;\r\nIn-Reply-To: &lt;f5r15c+ein5@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=137285340; y=dnn1_VQ9fu39rw_OjQGsCifED7uN2Ln4C0qyX-lmsAFx\r\nX-Yahoo-Profile: gojomo\r\n\r\nJigar Patel wrote:\n&gt; Presently I am running two heritrix instances on the same machine on \n&gt; different port...\n\nIn general, you only want to use distributed crawling, with URIs \npartitioned across separate cooperating crawlers, to spread a crawl over \nmultiple independent machines. If using a single machine, a single \ncrawler instance will be more efficient.\n\n&gt; I am using decidingScope and inside it I apply SurtPrefixRule\n&gt; I added HashCrawlMapper at two places as you suggested\n&gt; I made same configuration setting and seed file at each place.\n&gt; \n&gt; But as I run my job it gives me following error in seed file and \n&gt; nothing was crawled.\n&gt; \n&gt; Heritrix(-5002)-Blocked by custom prefetch processor \n&gt; \n&gt; Please let me know why I am getting such error...\n&gt; \n&gt; Is anything missing ?\n\nThis is the expected crawl.log result for URIs that are considered by a \ncrawler, but mapped to be handled by one of the others in the group of \ncrawlers. With a proper configuration, some but not all lines in your \ncrawl.log will have this code.\n\nFor example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and \nthe other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; of &#39;2&#39;.\n\nEvery URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, but \nwas fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the \ncrawl.log as &#39;blocked by custom processor&#39;. It is then up to the \noperator if they want to cross-feed those URIs to the &#39;1&#39; crawler.\n\n- Gordon @ IA\n\n\n&gt; Regards,\n&gt; \n&gt; Jigar Patel\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; \n&gt; wrote:\n&gt;&gt; nt_bdr wrote:\n&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we \n&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600 \n&gt;&gt; million URLs visited), and know of work elsewhere over up to 8 \n&gt; machines \n&gt;&gt; (&gt;1 billion URLs fetched).\n&gt;&gt;\n&gt;&gt; For background see some previous threads including:\n&gt;&gt;\n&gt;&gt;    http://tech.groups.yahoo.com/group/archive-crawler/message/2909\n&gt;&gt;    http://tech.groups.yahoo.com/group/archive-crawler/message/3060\n&gt;&gt;\n&gt;&gt; Roughly how we do it:\n&gt;&gt;\n&gt;&gt;   - Use BloomFilterUriUniqFilter with its defaults -- which devotes \n&gt;&gt; about 500MB to this structure and keeps the false-positive \n&gt; (mistakenly \n&gt;&gt; believed to have been previously-scheduled) rate under 1-in-4-\n&gt; million up \n&gt;&gt; through 125 million URIs discovered.\n&gt;&gt;\n&gt;&gt;   - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+ \n&gt; heap\n&gt;&gt;   - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in \n&gt; queues \n&gt;&gt; named by the reversed-host (com,example,) rather than usual host \n&gt;&gt; (example.com)\n&gt;&gt;\n&gt;&gt;   - Insert HashCrawlMapper processors at 2 places in the processor \n&gt; chain:\n&gt;&gt;     * Once, immediately before the PreconditionEnforcer. This one \n&gt; has \n&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any \n&gt; scheduled \n&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)\n&gt;&gt;     * Again, immediately before the FrontierScheduler. This one has \n&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any \n&gt; discovered \n&gt;&gt; outlinks before they are scheduled.)\n&gt;&gt;\n&gt;&gt;     Both HashCrawlMappers should have the same &#39;local-name&#39; (a \n&gt; number 0 \n&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and \n&gt; all \n&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers, \n&gt; n).\n&gt;&gt;     HashCrawlMapper looks at the queue key of a URI -- here, the \n&gt; SURT \n&gt;&gt; authority part, because of the above choice -- and decides if a URI \n&gt; is \n&gt;&gt; handled by the current crawler or one of its siblings. If mapped to \n&gt; a \n&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally. \n&gt;&gt; Depending on the character of your crawl, you may want to feed \n&gt; these \n&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore \n&gt; them.\n&gt;&gt;     The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key \n&gt; before \n&gt;&gt; mapping -- used to ensure that all subdomains of example.com are \n&gt; treated \n&gt;&gt; the same as example.com for mapping purposes. The first match of \n&gt; this \n&gt;&gt; pattern, if present, is what is used for mapping purposes. A small \n&gt;&gt; example would be:\n&gt;&gt;\n&gt;&gt;     ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n&gt;&gt;\n&gt;&gt;     For 3-letter domains (com, org, net), this uses everything \n&gt; through \n&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it \n&gt; uses \n&gt;&gt; the first 9 characters. You could imagine more complicated patterns \n&gt; that \n&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs, \n&gt; like \n&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level \n&gt;&gt; domains.)\n&gt;&gt;\n&gt;&gt;    - All crawlers are launched with the same configuration, \n&gt; including \n&gt;&gt; the same seeds, but otherwise do not (themselves) communicate. \n&gt; Seeds \n&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early \n&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed \n&gt; are \n&gt;&gt; done so by an external process/scripts.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}