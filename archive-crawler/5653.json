{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"LN9oMi471megpVNeS8W_8IC1cf3FyVlmyuRJmA7VvZDi7KA9xoM2KCbsCNpb8PcPaWpvea4GwAtA-LWqL8jwBWzll4MiOK4","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] memory amount","postDate":"1233121007","msgId":5653,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ5N0ZFRUVGLjYwNDAxMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGM1MDczNjZmMDkwMTI3MDI0MGoyZGZjOWJieTM4ODY4ZjZmNGNlMWUyOWZAbWFpbC5nbWFpbC5jb20+","referencesHeader":"PGM1MDczNjZmMDkwMTI3MDI0MGoyZGZjOWJieTM4ODY4ZjZmNGNlMWUyOWZAbWFpbC5nbWFpbC5jb20+"},"prevInTopic":5652,"nextInTopic":5654,"prevInTime":5652,"nextInTime":5654,"topicId":5652,"numMessagesInTopic":4,"msgSnippet":"... The memory requirements depend on your crawl parameters, especially the number of ToeThreads configured. You should be safe with the default configuration","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 63849 invoked from network); 28 Jan 2009 05:36:47 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m54.grp.scd.yahoo.com with QMQP; 28 Jan 2009 05:36:47 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta16.grp.scd.yahoo.com with SMTP; 28 Jan 2009 05:36:47 -0000\r\nX-Received: (qmail 27926 invoked from network); 28 Jan 2009 05:36:46 -0000\r\nX-Received: from 70.137.133.158 (HELO ?10.0.13.7?) (70.137.133.158)\n  by relay01.pair.com with SMTP; 28 Jan 2009 05:36:46 -0000\r\nX-pair-Authenticated: 70.137.133.158\r\nMessage-ID: &lt;497FEEEF.6040106@...&gt;\r\nDate: Tue, 27 Jan 2009 21:36:47 -0800\r\nUser-Agent: Thunderbird 2.0.0.19 (Windows/20081209)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c507366f0901270240j2dfc9bby38868f6f4ce1e29f@...&gt;\r\nIn-Reply-To: &lt;c507366f0901270240j2dfc9bby38868f6f4ce1e29f@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] memory amount\r\nX-Yahoo-Group-Post: member; u=137285340; y=KvFBOhf8dCG6vAKesMmMMGSH0jfPOwVR5WiQDqiaM-y8\r\nX-Yahoo-Profile: gojomo\r\n\r\ntakeru sasaki wrote:\n&gt; Hello,\n&gt; \n&gt; I have a question.\n&gt; \n&gt; How many memory for one crawl job.\n&gt; My crawl is stoped with OutOfMemoryError, and Web UI is not work well.\n&gt; \n&gt; I created 4-5 jobs based on basic_seed_sites profile, and run these.\n&gt; \n&gt; I am using heritrix 2.0.2 with JAVA_OPTS=-Xmx1000m.\n&gt; Is it not enough?\n\nThe memory requirements depend on your crawl parameters, especially the \nnumber of ToeThreads configured. You should be safe with the default \nconfiguration to run a single crawl in that much memory.\n\nHowever, it sounds like you may have launched several crawls at the same \ntime. If so, the defaults that keep a single job within a reasonable, \nfixed amount of memory are no longer appropriate.\n\nMost significantly, each crawl opens its own BerkeleyDB database \nenvironment, and each such environment caps itself to a default of 60% \nof all heap space for its working-space cache. As soon as you run two \ncrawls at once, 60%+60%=120%, which nearly guarantees an \nOutOfMemoryError if the crawls are not small and quick.\n\nSo, if you are going to run more than one crawl at the same time, you \nshould definitely decrease the BDB cache-percents so they total a \nreasonable amount. You may also want to decrease the number of \nToeThreads per crawl -- they are the next largest determinant of \nsteady-state memory usage in a busy crawl.\n\nIf your OOMEs were caused by a single crawl, please supply more details \nabout your setup -- OS, JVM, number of seeds, changes from the default \nconfiguration, etc. -- and we&#39;ll try to figure what&#39;s gone wrong.\n\nHope this helps,\n\n- Gordon @ IA\n\n&gt; takeru\n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}