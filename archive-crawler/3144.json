{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"SZmEcDyo3B8mXXEflpK-lhBOq9VhosBumFcXfMtGUz8sg5UsdtoYFB3ItMXT6MvLKGUnAa1xcqrF9_zGzR0dkUAxUVN0npQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] small amount of crawled documents and -6 error in logs","postDate":"1154478354","msgId":3144,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0Q0ZGMTEyLjQwMTAwMDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGVhb2hmbituYWQ4QGVHcm91cHMuY29tPg==","referencesHeader":"PGVhb2hmbituYWQ4QGVHcm91cHMuY29tPg=="},"prevInTopic":3140,"nextInTopic":3152,"prevInTime":3143,"nextInTime":3145,"topicId":3140,"numMessagesInTopic":5,"msgSnippet":"... Normally, this would mean that the hostname DNS lookup for those URLs failed. With no successful DNS lookup, the URL cannot be fetched. Are these same URLs","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 90705 invoked from network); 2 Aug 2006 00:24:12 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m37.grp.scd.yahoo.com with QMQP; 2 Aug 2006 00:24:12 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta6.grp.scd.yahoo.com with SMTP; 2 Aug 2006 00:24:12 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id C044F140D8C53\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue,  1 Aug 2006 17:23:54 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 23155-01-36 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 1 Aug 2006 17:23:52 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-71-198-60-165.hsd1.ca.comcast.net [71.198.60.165])\n\tby mail.archive.org (Postfix) with ESMTP id 93E3114031746\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue,  1 Aug 2006 17:23:52 -0700 (PDT)\r\nMessage-ID: &lt;44CFF112.4010001@...&gt;\r\nDate: Tue, 01 Aug 2006 17:25:54 -0700\r\nUser-Agent: Mail/News 1.5 (X11/20060309)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;eaohfn+nad8@...&gt;\r\nIn-Reply-To: &lt;eaohfn+nad8@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] small amount of crawled documents and -6 error\n in logs\r\nX-Yahoo-Group-Post: member; u=137285340; y=NZYxvmT_b33lHyUnuAxRL7FeG4fIk1jIWGgCEXtOwQXT\r\nX-Yahoo-Profile: gojomo\r\n\r\ngoblin_cz wrote:\n&gt; Hi,\n&gt; I have been running heritrix for 3 days on a big pack of seeds (420\n&gt; 000). It has been ended normaly but downloaded only 99GB and only\n&gt; about 4 millions links.\n&gt; There are some things that I can not understand.\n&gt; What exactly mean URL fetch code -6 .. I found  it in manual as:\n&gt; Prerequisite domain-lookup failed, precluding fetch attempt, but which\n&gt; part of setting can cause that?\n&gt; I have got about 70000 seeds with this fetch code.\n\nNormally, this would mean that the hostname DNS lookup for those URLs \nfailed. With no successful DNS lookup, the URL cannot be fetched.\n\nAre these same URLs visitable in a browser?\n\nIf you crawl a handful of these URLs separately, with all other settings \nthe same, do they succeed? (Even if not, it may then be easier to \nexamine all the logs to see what happened -- there may be additional \nerrors in local-errors.log indicating what step went wrong.)\n\n&gt; Second, on which logs or setting i have to watch to track the problem\n&gt; with small amount of downloaded documents?\n&gt; \n&gt; I have used:\n&gt; SurtPrefixScope\n&gt; DomainSensitiveFrontier with 5000 max-docs\n&gt; whole order xml: http://harvester.webarchiv.cz/heritrix/order.xml\n\nI&#39;m not very familiar with the DomainSensitiveFrontier, so if that&#39;s \npart of the problem, I won&#39;t have much helpful input. But I do provide a \nnumber of comments on your order file below, which may point to some \npossible culprits.\n\n&gt; And last thing. When I set in SurtPrefixedScope property:\n&gt; surts-source-file to absolute path (/heritrix/zone.cz e.g.) where is\n&gt; one url - in http format - per line. The seeds file will be empty. And\n&gt; dump file too..\n\nWhich version of Heritrix are you using? Since 1.6 (and maybe earlier), \na SURTs source file may include either (1) actual URIs, which are \nconverted to implied SURTs for scoping purposes, or (2) lines that begin \n&#39;+&#39; with literal SURT prefixes (such as &quot;+http://(cz,&quot; -- which would be \na sufficient prefix to accept all discovered .CZ URLs as in-scope).\n\nComments looking over your order file:\n\n- We generally recommend using a DecidingScope with a series of rules \nrather than the classic scopes (such as SurtPrefixScope)\n\n- a &#39;max-trans-hops&#39; of 7 is very high; this is a mechanism for \nincluding chains of material that otherwise wouldn&#39;t be considered \nin-scope because of the manner in which it is discovered (such as an \nembedded frame, img, or JS ref). I would recommend 2-3 instead. \nOtherwise, certain chains of references could bring you far from your \nintended sites/domain.\n\n- the order you pointed at had *no* value for &#39;surts-source-file&#39;, and \nfurther had &#39;seeds-as-surt-prefixes&#39; as &#39;false&#39;. This means the \nSurtPrefixScope will have *no* list of acceptable &#39;prefixes&#39; to use for \naccepting new URLs as in-scope. This could easily cause the minimal \nprogress you&#39;re seeing -- even URLs on the same hosts as your seeds have \nnot been defined as in-scope.)\n\n- a &#39;min-delay-ms&#39; of 30 is very small; hardly different than 0.\n\n- &#39;hold-queues&#39; is recommended &#39;true&#39; for any large crawl; otherwise all \nfrontier queues are in an essential round-robin to provide the next URL, \nwhich prevents any memory/IO efficiencies which result from \nconcentrating on a smaller set of &#39;active&#39; queues at a time\n\n- You may not need to use DomainSensitiveFrontier; the default \nBdbFrontier with a UnitCostAssignmentPolicy and queue-total-budget of \n5000 will very closely approximate the DomainSensitiveFrontier in \n&#39;count-per-host&#39; mode, I think.\n\n- there&#39;s no need to change the &#39;snooze-deactivate-ms&#39; setting in normal \noperation; this very low setting could have unexpected results\n\n- the QuotaEnforcer in your processing is both disabled, and all of its \nindividual settings are set to no-quota (-1). So, it could be deleted. \nAlternatively, if enabled with real quotas, it would be another way to \naccomplish a very similar result to the DomainSensitiveFrontier, but \nusing the default BdbFrontier.\n\n&gt; Thank you for any advice and your time.\n&gt; \n\nYou&#39;re welcome! Hope this helps,\n\n- Gordon @ IA\n\n"}}