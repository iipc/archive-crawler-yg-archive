{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"david_pane1","from":"&quot;david_pane1&quot; &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"_wSwJajf4HMR62MSYlBqHAqZeMmbTK1NibIfO_MT1hGKgEoSnBVS6T6k1MPkDpU2SVaVEZQnBPxfJktzcI1a_B5-JlyX3nQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: H3 - distributed crawling and memory/cpu utilization","postDate":"1319501008","msgId":7372,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGo4NHVjZyt0bGY1QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRFOTkyNkY3LjgwMDAwMDlAYXJjaGl2ZS5vcmc+"},"prevInTopic":7355,"nextInTopic":7592,"prevInTime":7371,"nextInTime":7373,"topicId":7351,"numMessagesInTopic":7,"msgSnippet":"It is unclear to me how to reformat the diversion logs in order to make them action-directory ready ( .schedule file format).  Can you explain the expected","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 5153 invoked from network); 25 Oct 2011 00:03:32 -0000\r\nX-Received: from unknown (98.137.35.161)\n  by m4.grp.sp2.yahoo.com with QMQP; 25 Oct 2011 00:03:32 -0000\r\nX-Received: from unknown (HELO ng10-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.78)\n  by mta5.grp.sp2.yahoo.com with SMTP; 25 Oct 2011 00:03:32 -0000\r\nX-Received: from [98.139.164.126] by ng10.bullet.mail.bf1.yahoo.com with NNFMP; 25 Oct 2011 00:03:31 -0000\r\nX-Received: from [69.147.65.171] by tg7.bullet.mail.bf1.yahoo.com with NNFMP; 25 Oct 2011 00:03:30 -0000\r\nX-Received: from [98.137.34.51] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 25 Oct 2011 00:03:30 -0000\r\nDate: Tue, 25 Oct 2011 00:03:28 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;j84ucg+tlf5@...&gt;\r\nIn-Reply-To: &lt;4E9926F7.8000009@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;david_pane1&quot; &lt;dpane@...&gt;\r\nSubject: Re: H3 - distributed crawling and memory/cpu utilization\r\nX-Yahoo-Group-Post: member; u=500983475; y=PrMgIhg5pMP8XbLVNIf129GIrHISEEC_t84B3A957K7QzncqAw7KCQ\r\nX-Yahoo-Profile: david_pane1\r\n\r\nIt is unclear to me how to reformat the diversion logs in order to make the=\r\nm action-directory ready ( .schedule file format).  Can you explain the exp=\r\nected format or point me to where it is defined?\n\n\n--David\n\n\n\n--- In archiv=\r\ne-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; A few commen=\r\nts interspersed below:\n&gt; \n&gt; On 10/14/11 7:05 PM, Noah Levitt wrote:\n&gt; &gt;&gt; In=\r\ncreasing the maxToeThreads to a higher value than 1200 causes the\n&gt; &gt;&gt; java=\r\n application to fall to minimal to no cpu usage and the web\n&gt; &gt;&gt; interface =\r\nto be unresponsive.  Does anyone know why this is\n&gt; &gt;&gt; happening?\n&gt; &gt;\n&gt; &gt; D=\r\non&#39;t know without looking more closely at the logs and state of the\n&gt; &gt; jav=\r\na process when that happens. But 1200 threads for an 11000M heap\n&gt; &gt; seems =\r\nlike a lot, maybe too many depending on the rest of your\n&gt; &gt; config.\n&gt; \n&gt; Y=\r\nes, to really understand why a crawl might seem stuck, some key things \n&gt; t=\r\no check are:\n&gt; \n&gt; - the logs, especially the heritrix_out.log\n&gt; - thread st=\r\natus, via either the web UI threads report or Java tools like \n&gt; &#39;jstack&#39; (=\r\nor sending SIGQUIT JVM process)\n&gt; \n&gt; But, the rules-of-thumb for memory usa=\r\nge Noah lists are our best \n&gt; estimates of memory needs, and by those rules=\r\n of thumb (5-10MB per \n&gt; toethread, 60% default to BDB, some other space fo=\r\nr other structures) \n&gt; your crawl would tend to exceed 11000MB of heap, by =\r\nat least a little \n&gt; and possibly a lot.\n&gt; \n&gt; There shouldn&#39;t be anything i=\r\nnherently wrong with 1200 threads (given \n&gt; sufficient CPU and RAM), nor wi=\r\nth heaps 11GB+. But for comparison, in my \n&gt; recollection, the largest numb=\r\ner of threads used in crawls at the \n&gt; Internet Archive has been 400 (or ma=\r\nybe 600), and the largest assigned \n&gt; heap about 6.5-7GB. There may have be=\r\nen other crawls at IA or elsewhere \n&gt; I haven&#39;t heard of, but you are in so=\r\nme lesser-understood territory with \n&gt; these settings.\n&gt; \n&gt; Another concern=\r\n I hear from Java projects which regularly use &gt;4GB or \n&gt;  &gt;8GB heaps is oc=\r\ncasional multiple-minute-long global-GC cycles. Such \n&gt; pauses generally wo=\r\nn&#39;t harm a crawl -- perhaps a remote server will drop \n&gt; the connection, tr=\r\niggering an extra retry of that URL. My understanding \n&gt; is that the latest=\r\n or experimental/optional garbage-collector options \n&gt; may do better with g=\r\niant heaps -- it&#39;s something to keep in mind and \n&gt; perhaps experiment with=\r\n on such a large heap crawl.\n&gt; \n&gt; &gt;&gt; 2) One of the 2 crawlers stopped crawl=\r\ning due to a congestion ratio\n&gt; &gt;&gt; of infinity.  What are some ways to over=\r\ncome this? What can I do to\n&gt; &gt;&gt; avoid it happening in the future?\n&gt; &gt;\n&gt; &gt; =\r\nI don&#39;t know. Not clear what you mean by &quot;stopped&quot; exactly for one\n&gt; &gt; thin=\r\ng. If it happens again, maybe gather more information from\n&gt; &gt; heritrix_out=\r\n.log, toe thread report, frontier report, top, iostat,\n&gt; &gt; jstack, etc.\n&gt; \n=\r\n&gt; A congestion ration of &#39;infinity&#39; just means no threads are active; that =\r\n\n&gt; value won&#39;t cause any problems. As Noah notes, understanding what is \n&gt; =\r\nmeant by &#39;stopped&#39; requires more log/process/system analysis.\n&gt; \n&gt; &gt;&gt; 3) In=\r\n\n&gt; &gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/3846\n&gt; &gt;&gt; =\r\nGordon stated:\n&gt; &gt;&gt;\n&gt; &gt;&gt; &quot;... HashCrawlMapper looks at the queue key of a U=\r\nRI -- here, the\n&gt; &gt;&gt; SURT authority part, because of the above choice -- an=\r\nd decides if\n&gt; &gt;&gt; a URI is handled by the current crawler or one of its sib=\r\nlings. If\n&gt; &gt;&gt; mapped to a sibling, the URI is dumped to a log rather than =\r\ncrawled\n&gt; &gt;&gt; locally. Depending on the character of your crawl, you may wan=\r\nt to\n&gt; &gt;&gt; feed these logs to the other crawlers occasionally or it may be O=\r\nK\n&gt; &gt;&gt; to ignore them. ... &quot;\n&gt; &gt;&gt;\n&gt; &gt;&gt; How does one feed the diverted URIs/=\r\nlogs to a sibling crawler?\n&gt; &gt;\n&gt; &gt; A coworker kindly put up this page yeste=\r\nrday:\n&gt; &gt; https://webarchive.jira.com/wiki/display/Heritrix/Multiple+Machin=\r\ne+Crawling\n&gt; &gt;\n&gt; &gt;which says, &quot;Crawl operators must set up a process where =\r\nthe the URIs\n&gt; &gt; contained in .divert files are copied from each crawler to=\r\n their\n&gt; &gt; assigned crawlers and queued into the active crawl (putting the\n=\r\n&gt; &gt; .divert file in the actions directory as a .include should be\n&gt; &gt; suffi=\r\ncient).&quot;\n&gt; \n&gt; Actually, the &#39;.divert&#39; format and the (recovery-log-like) fo=\r\nrmats \n&gt; accepted by the action directory are not equivalent and interchang=\r\neable. \n&gt; (They probably should be.) So, the process outside the crawler wi=\r\nll need \n&gt; to massage the diversion logs a bit to make them action-director=\r\ny ready.\n&gt; \n&gt; Also, an &#39;.include&#39; URI list just tells a crawler to consider=\r\n those URIs \n&gt; &#39;already included&#39;. That is, it actually *prevents* those UR=\r\nIs from \n&gt; being enqueued, from that point onward. Instead, use the &#39;.sched=\r\nule&#39; \n&gt; suffix to indicate that the URIs in the file should be presented to=\r\n the \n&gt; frontier, and thus possibly enqueued (if not previously seen).\n&gt; \n&gt;=\r\n I&#39;ve corrected the wiki page to indicate a format-conversion and \n&gt; &#39;.sche=\r\ndule&#39; file is more appropriate.\n&gt; \n&gt; - Gordon\n&gt;\n\n\n\n"}}