{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"WHKc4xaUv4mJS9NdQDVjozz_KZrog2-IG0np5RYf8s1AHZO5N9UVYQYKZ7MDkI3_378XJbYM6uQt8EC7B1FjBnh5t0gqmio","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix - crawl job for single pages referenced by URL","postDate":"1266880872","msgId":6398,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRCODMxMTY4LjQwMDAyMDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGhsbTNpditzcGRvQGVHcm91cHMuY29tPg==","referencesHeader":"PGhsbTNpditzcGRvQGVHcm91cHMuY29tPg=="},"prevInTopic":6394,"nextInTopic":6416,"prevInTime":6397,"nextInTime":6399,"topicId":6394,"numMessagesInTopic":5,"msgSnippet":"... Roughy, this can be achieved by: - using the bundled default configuration - including all your URLs of interest as seeds - setting the maxHops value","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 88467 invoked from network); 22 Feb 2010 23:21:14 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m12.grp.re1.yahoo.com with QMQP; 22 Feb 2010 23:21:14 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta2.grp.re1.yahoo.com with SMTP; 22 Feb 2010 23:21:14 -0000\r\nX-Received: (qmail 96746 invoked from network); 22 Feb 2010 23:21:12 -0000\r\nX-Received: from 71.202.38.39 (HELO ?192.168.23.128?) (71.202.38.39)\n  by relay02.pair.com with SMTP; 22 Feb 2010 23:21:12 -0000\r\nX-pair-Authenticated: 71.202.38.39\r\nMessage-ID: &lt;4B831168.4000209@...&gt;\r\nDate: Mon, 22 Feb 2010 15:21:12 -0800\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;hlm3iv+spdo@...&gt;\r\nIn-Reply-To: &lt;hlm3iv+spdo@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix - crawl job for single pages referenced\n by URL\r\nX-Yahoo-Group-Post: member; u=137285340; y=uwimWNEEaGb651GbScVTNjnu6XBEclW5m26QMdXXxl6V\r\nX-Yahoo-Profile: gojomo\r\n\r\nswschilke wrote:\n&gt; Dear All,\n&gt; \n&gt; Heritrix: I was reading the manual and I have a little problem\n&gt; understanding how I can set up a crawl job. My  task would be to archive\n&gt; only certain pages in a crawl job, i.e., I want to give Heritrix a list of URLs referring to one page each and I want them to be collected (including all components of that page (e.g., PDF files, images, ...). Anybody here which could give me a hint / sample job definition?\n&gt; \n&gt; Thank you very much in advance\n\nRoughy, this can be achieved by:\n\n- using the bundled default configuration\n\n- including all your URLs of interest as &#39;seeds&#39;\n\n- setting the &#39;maxHops&#39; value of the TooManyHopsDecideRule to &#39;0&#39;, \nmeaning &quot;don&#39;t follow links more than 0 hops from seeds&quot;\n\nThe following TransclusionDecideRule will still rule-in items that are \nobviously required to inline-render the page (such as \nFRAME/IMG/STYLE/SCRIPT SRC links) or even possibly-required (in the case \nof URL-looking strings in certain form elements and scripts).\n\nSo, the overall effect is: get exactly the URLs provided as seeds, plus \nthose things definitely necessary to render them.\n\nNote: &quot;A HREF&quot; links, whether to resources on the same site or different \nsites, won&#39;t be ruled-in. (They&#39;ll all be considered &#39;1&#39; hop out, more \nthan &#39;0&#39; hops.) So if you have a page full of links to PDF files or \nphotos or whatever, those links won&#39;t be followed.\n\nSome options to consider if you want to get those:\n\n- increase the &#39;maxHops&#39; to 1 -- this will at least follow links into \nthe same sites/site-prefixes implied by the seeds\n\n- add an extra MatchesRegexPatternDecideRule, late in the scope, to \nACCEPT all URLs with certain desired patterns (such as ending &quot;.pdf&quot;)\n\nHope this helps,\n\n- Gordon @ IA\n\n\n"}}