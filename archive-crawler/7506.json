{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":90724651,"authorName":"John Lekashman","from":"John Lekashman &lt;lekash@...&gt;","profile":"lekash","replyTo":"LIST","senderId":"b4mdt4mRz_newS8KEPvUSmKS-22BSMEBUU6VmfrtjeKDYMQT0eIo1B8TZyXMDVOzWkq-wUGKFYPi2Q9CEPkFZpEeAkKNrEPg85o","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Question about H3 crawl management","postDate":"1326222152","msgId":7506,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMEM4QjQ4LjQwMjAwMDlAYmF5YXJlYS5uZXQ+","inReplyToHeader":"PDRGMEM3NjRFLjQwMDA3MDBAY3MuY211LmVkdT4=","referencesHeader":"PDRGMEM3NjRFLjQwMDA3MDBAY3MuY211LmVkdT4="},"prevInTopic":7505,"nextInTopic":7507,"prevInTime":7505,"nextInTime":7507,"topicId":7505,"numMessagesInTopic":7,"msgSnippet":"... Not really.  Its a static hash map at the beginning. You could in theory, save all the recover.gz files, stop the crawl, edit the config to increase the","rawEmail":"Return-Path: &lt;lekash@...&gt;\r\nX-Sender: lekash@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 32957 invoked from network); 10 Jan 2012 19:02:37 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m4.grp.sp2.yahoo.com with QMQP; 10 Jan 2012 19:02:37 -0000\r\nX-Received: from unknown (HELO mail.bayarea.net) (209.128.87.230)\n  by mta6.grp.sp2.yahoo.com with SMTP; 10 Jan 2012 19:02:37 -0000\r\nX-Received: from john-lekashmans-macbook-pro.local (173-164-186-205-SFBA.hfc.comcastbusiness.net [173.164.186.205])\n\t(authenticated bits=0)\n\tby mail.bayarea.net (8.13.8/8.13.8) with ESMTP id q0AJ2W5D020502;\n\tTue, 10 Jan 2012 11:02:33 -0800 (PST)\n\t(envelope-from lekash@...)\r\nMessage-ID: &lt;4F0C8B48.4020009@...&gt;\r\nDate: Tue, 10 Jan 2012 11:02:32 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com, dpane@...\r\nReferences: &lt;4F0C764E.4000700@...&gt;\r\nIn-Reply-To: &lt;4F0C764E.4000700@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------030607060001060503080608&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: John Lekashman &lt;lekash@...&gt;\r\nSubject: Re: [archive-crawler] Question about H3 crawl management\r\nX-Yahoo-Group-Post: member; u=90724651; y=whJYt4z0z6LV7yH1yTvWxdjAujukv5DSC5ZOwUfE0yt_\r\nX-Yahoo-Profile: lekash\r\n\r\n\r\n--------------030607060001060503080608\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nOn 1/10/12 9:33 AM, David Pane wrote:\n&gt;\n&gt;\n&gt; We have been running a 5 instance crawl for almost 3 weeks using\n&gt; Heritrix 3.1.1. I have a few questions that I was hoping someone could\n&gt; answer.\n&gt;\n&gt; 1) Is there a way to refactor/reconfigure a started crawl to have more\n&gt; instances. (i.e In the middle of the crawl increase the number of\n&gt; instances from 5 to some larger or smaller number)\n&gt;\n\nNot really.  Its a static hash map at the beginning.\nYou could in theory, save all the recover.gz files,\nstop the crawl, edit the config to increase the hashmap size,\nrestore all the recover.gz on all the crawlers to ensure everyone\nhas all the info.  It kind of works, I did that once.\n\nBut its a hassle.\n\n&gt;\n&gt; 2) Would moving the warc.gz files from the job&#39;s warc directory while\n&gt; the crawl is running to another directory cause any problems? Does\n&gt; Heritrix use these files? What about the files in the log directory -\n&gt; can they be moved? We want to prevent these directories from having a\n&gt; very large number of files in them where the OS would start to have\n&gt; problems.\n&gt;\nOnce closed, do whatever you want with warc.gz files.\nThe crawler is done.\n\nThe # of log files shouldn&#39;t grow, or are you doing something\ncomplex with checkpoints?  If so, any log file that isn&#39;t part of the\nactive stream, can move.\n\n&gt;\n&gt; 3) One big concern for us is invalid checkpoints. We would like to\n&gt; verify that our checkpoints are valid. Is it possible to copy a subset\n&gt; of a job directory to another location to use with another Heritrix job?\n&gt; The idea would be to start a separate instance of Heritrix on separate\n&gt; machine using this copied data and the last checkpoint to verify that\n&gt; the last checkpoint is a valid checkpoint (no plans to actually crawl\n&gt; from this test instance)? If possible this could also be useful for\n&gt; debugging while still crawling with the original instance.\n&gt;\nYeah, I hate checkpoints.  Yes, this is possible.\nThey fail at the most inopportune times.\n\n&gt; 4) What is the best way to inject additional URIs into a started crawl?\n&gt; We would like to inject a set of specific URIs daily and would like\n&gt; for this set of URIs to be crawled as soon as possible (if they have not\n&gt; already been crawled).\n&gt;\nDrop foo.seeds in the action directory.\nWhere foo.seeds is a file with the new uris.\n\n&gt; 5) Although I can capture the below statistics manually, can you suggest\n&gt; a way that I can automatically generate/collect the following statistics\n&gt; from the crawl. I would like to generate this data at least once every\n&gt; 24 hours and possibly as often as every hour.\n&gt;\nWell, you could write a script to do it.\n\n&gt; a) Total size of crawled data.\n&gt; b) total number of pages crawled (mime-type: text/html).\n&gt;\n&gt; Thanks,\n&gt;\n&gt; David\n&gt;\n&gt; \n\n\r\n--------------030607060001060503080608\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta content=&quot;text/html; charset=ISO-8859-1&quot;\n      http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#FFFFFF&quot; text=&quot;#000000&quot;&gt;\n    On 1/10/12 9:33 AM, David Pane wrote:\n    &lt;blockquote cite=&quot;mid:4F0C764E.4000700@...&quot; type=&quot;cite&quot;&gt;\n      &lt;span style=&quot;display:none&quot;&gt;&nbsp;&lt;/span&gt;\n      \n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;&lt;br&gt;\n              We have been running a 5 instance crawl for almost 3 weeks\n              using &lt;br&gt;\n              Heritrix 3.1.1. I have a few questions that I was hoping\n              someone could &lt;br&gt;\n              answer.&lt;br&gt;\n              &lt;br&gt;\n              1) Is there a way to refactor/reconfigure a started crawl\n              to have more &lt;br&gt;\n              instances. (i.e In the middle of the crawl increase the\n              number of &lt;br&gt;\n              instances from 5 to some larger or smaller number)&lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/blockquote&gt;\n    &lt;br&gt;\n    Not really.&nbsp; Its a static hash map at the beginning.&lt;br&gt;\n    You could in theory, save all the recover.gz files,&lt;br&gt;\n    stop the crawl, edit the config to increase the hashmap size,&lt;br&gt;\n    restore all the recover.gz on all the crawlers to ensure everyone&lt;br&gt;\n    has all the info.&nbsp; It kind of works, I did that once.&lt;br&gt;\n    &lt;br&gt;\n    But its a hassle.&lt;br&gt;\n    &lt;br&gt;\n    &lt;blockquote cite=&quot;mid:4F0C764E.4000700@...&quot; type=&quot;cite&quot;&gt;\n      &lt;div id=&quot;ygrp-mlmsg&quot; style=&quot;position: relative;&quot;&gt;\n        &lt;div id=&quot;ygrp-msg&quot; style=&quot;z-index: 1;&quot;&gt;\n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;\n              &lt;br&gt;\n              2) Would moving the warc.gz files from the job&#39;s warc\n              directory while &lt;br&gt;\n              the crawl is running to another directory cause any\n              problems? Does &lt;br&gt;\n              Heritrix use these files? What about the files in the log\n              directory - &lt;br&gt;\n              can they be moved? We want to prevent these directories\n              from having a &lt;br&gt;\n              very large number of files in them where the OS would\n              start to have &lt;br&gt;\n              problems.&lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/blockquote&gt;\n    Once closed, do whatever you want with warc.gz files.&lt;br&gt;\n    The crawler is done. &lt;br&gt;\n    &lt;br&gt;\n    The # of log files shouldn&#39;t grow, or are you doing something&lt;br&gt;\n    complex with checkpoints?&nbsp; If so, any log file that isn&#39;t part of\n    the&lt;br&gt;\n    active stream, can move.&lt;br&gt;\n    &lt;br&gt;\n    &lt;blockquote cite=&quot;mid:4F0C764E.4000700@...&quot; type=&quot;cite&quot;&gt;\n      &lt;div id=&quot;ygrp-mlmsg&quot; style=&quot;position: relative;&quot;&gt;\n        &lt;div id=&quot;ygrp-msg&quot; style=&quot;z-index: 1;&quot;&gt;\n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;\n              &lt;br&gt;\n              3) One big concern for us is invalid checkpoints. We would\n              like to &lt;br&gt;\n              verify that our checkpoints are valid. Is it possible to\n              copy a subset &lt;br&gt;\n              of a job directory to another location to use with another\n              Heritrix job? &lt;br&gt;\n              The idea would be to start a separate instance of Heritrix\n              on separate &lt;br&gt;\n              machine using this copied data and the last checkpoint to\n              verify that &lt;br&gt;\n              the last checkpoint is a valid checkpoint (no plans to\n              actually crawl &lt;br&gt;\n              from this test instance)? If possible this could also be\n              useful for &lt;br&gt;\n              debugging while still crawling with the original instance.&lt;br&gt;\n              &lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/blockquote&gt;\n    Yeah, I hate checkpoints.&nbsp; Yes, this is possible.&lt;br&gt;\n    They fail at the most inopportune times.&lt;br&gt;\n    &lt;br&gt;\n    &lt;blockquote cite=&quot;mid:4F0C764E.4000700@...&quot; type=&quot;cite&quot;&gt;\n      &lt;div id=&quot;ygrp-mlmsg&quot; style=&quot;position: relative;&quot;&gt;\n        &lt;div id=&quot;ygrp-msg&quot; style=&quot;z-index: 1;&quot;&gt;\n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;\n              4) What is the best way to inject additional URIs into a\n              started crawl? &lt;br&gt;\n              We would like to inject a set of specific URIs daily and\n              would like &lt;br&gt;\n              for this set of URIs to be crawled as soon as possible (if\n              they have not &lt;br&gt;\n              already been crawled).&lt;br&gt;\n              &lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/blockquote&gt;\n    Drop foo.seeds in the action directory.&lt;br&gt;\n    Where foo.seeds is a file with the new uris.&lt;br&gt;\n    &lt;br&gt;\n    &lt;blockquote cite=&quot;mid:4F0C764E.4000700@...&quot; type=&quot;cite&quot;&gt;\n      &lt;div id=&quot;ygrp-mlmsg&quot; style=&quot;position: relative;&quot;&gt;\n        &lt;div id=&quot;ygrp-msg&quot; style=&quot;z-index: 1;&quot;&gt;\n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;\n              5) Although I can capture the below statistics manually,\n              can you suggest &lt;br&gt;\n              a way that I can automatically generate/collect the\n              following statistics &lt;br&gt;\n              from the crawl. I would like to generate this data at\n              least once every &lt;br&gt;\n              24 hours and possibly as often as every hour.&lt;br&gt;\n              &lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/blockquote&gt;\n    Well, you could write a script to do it.&lt;br&gt;\n    &lt;br&gt;\n    &lt;blockquote cite=&quot;mid:4F0C764E.4000700@...&quot; type=&quot;cite&quot;&gt;\n      &lt;div id=&quot;ygrp-mlmsg&quot; style=&quot;position:relative;&quot;&gt;\n        &lt;div id=&quot;ygrp-msg&quot; style=&quot;z-index: 1;&quot;&gt;\n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;\n              a) Total size of crawled data.&lt;br&gt;\n              b) total number of pages crawled (mime-type: text/html).&lt;br&gt;\n              &lt;br&gt;\n              Thanks,&lt;br&gt;\n              &lt;br&gt;\n              David&lt;br&gt;\n              &lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- end group email --&gt;\n    &lt;/blockquote&gt;\n    &lt;br&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------030607060001060503080608--\r\n\n"}}