{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":308709787,"authorName":"John Kleven","from":"&quot;John Kleven&quot; &lt;johnkleven@...&gt;","replyTo":"LIST","senderId":"FpNC9zxGyXROBFvMEoROW1DD1LVYmz-3hG0L8odDbwUM6Hty2Y9f65lZI17mEfZ9UmUjHyGAnXgPk1TbwJDojM5Jwiu6F4QTdY4e","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Best practices for scripted multi-job crawling","postDate":"1177725606","msgId":4199,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDk0MjQzZDM3MDcwNDI3MTkwMG40NTA0NDFhYXY4NTFmMzQ1YWVmNTJkN2RmQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":4198,"nextInTime":4200,"topicId":4199,"numMessagesInTopic":1,"msgSnippet":"I m trying to figure out the best way to do multiple crawl jobs on the same computer using a python script.  I ve read the FAQ and this mailing list entry: ","rawEmail":"Return-Path: &lt;johnkleven@...&gt;\r\nX-Sender: johnkleven@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 70552 invoked from network); 28 Apr 2007 02:01:13 -0000\r\nReceived: from unknown (66.218.67.34)\n  by m45.grp.scd.yahoo.com with QMQP; 28 Apr 2007 02:01:13 -0000\r\nReceived: from unknown (HELO nz-out-0506.google.com) (64.233.162.224)\n  by mta8.grp.scd.yahoo.com with SMTP; 28 Apr 2007 02:01:09 -0000\r\nReceived: by nz-out-0506.google.com with SMTP id i11so170797nzi\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 27 Apr 2007 19:00:06 -0700 (PDT)\r\nDKIM-Signature: a=rsa-sha1; c=relaxed/relaxed;\n        d=gmail.com; s=beta;\n        h=domainkey-signature:received:received:message-id:date:from:to:subject:mime-version:content-type:content-transfer-encoding:content-disposition;\n        b=X5AHw/xOR3vPiI2UnTMch92M2G01GY6hcr/Y8/IQ/gG6z6E4OmKemB34wKXZjO4e9qsPTZ8OVVWJJoplWEHX8K/nwYL7UeMl1WYlGpHdbtOKMOVlz1r6bqNRhFvzM8tJzv+JakoPwumz4+kxk6Pw7SSiycjc1lJw6hvy/nEv+Gk=\r\nReceived: by 10.114.180.1 with SMTP id c1mr1200045waf.1177725606713;\n        Fri, 27 Apr 2007 19:00:06 -0700 (PDT)\r\nReceived: by 10.114.15.12 with HTTP; Fri, 27 Apr 2007 19:00:06 -0700 (PDT)\r\nMessage-ID: &lt;94243d370704271900n450441aav851f345aef52d7df@...&gt;\r\nDate: Fri, 27 Apr 2007 19:00:06 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: &quot;John Kleven&quot; &lt;johnkleven@...&gt;\r\nSubject: Best practices for scripted multi-job crawling\r\nX-Yahoo-Group-Post: member; u=308709787\r\n\r\nI&#39;m trying to figure out the best way to do multiple crawl jobs on the\nsame computer using a python script.  I&#39;ve read the FAQ and this\nmailing list entry:\nhttp://tech.groups.yahoo.com/group/archive-crawler/messages/1179?threaded=1&m=e&var=1&tidx=1\n\nSpawning completely seperate processes (as described in the link\nabove) seems like a waste of memory as each JVM takes a lot of memory.\n So, was looking into the other option, creating multiple &quot;instances&quot;\nof the app within 1 JVM via the WUI local-instances.jsp.  This sounds\npromsing but ..\n1) How do you spawn internal instances via the JMX client?\n\nAdditionally, I experimented with the local instances via the WUI.  I\ncreated 4 seperate instances within the JVM, and added 4 seperate jobs\nfor different hosts.  I then alternated between switching local\ninstances and starting the nodes.  I then received a bunch of errors\nlike ..\n\n&quot;...On crawl: lagreca1 Unable to setup crawl modules ()\nFatalConfigurationException: (JE 3.2.13) Lock expired. Locker\n-1_StartNextJob_BasicLocker: waited for lock on database=_jeNameMap\nnode=1791 type=WRITE ...&quot;\n\nIn general though, it did crawl the hosts.  But it almost seemed to\nnot be crawling in parallel, in other words, the local instances\nweren&#39;t crawling the way I envisioned they would.  What did I miss?\n\nFinally, I saw some Jira notes in regards to a &quot;Multimachine cluster\nconsole webapp&quot; -- are there any info pages on this?  Although I&#39;m\nhoping to get all my crawling done on one server, I may have to bring\nin others and maybe this cluster app would be the ultimate solution to\nmulti-job runs?\n\nThanks so much.  The crawler appears to be *excellent* and\nunbelievably configurable.\n\nJohn\n\n"}}