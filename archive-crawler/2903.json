{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256901594,"authorName":"jonathansiddharth","from":"&quot;jonathansiddharth&quot; &lt;jonathansiddharth@...&gt;","profile":"jonathansiddharth","replyTo":"LIST","senderId":"TloVIxWXuJ-JcszrevpUT4MUZA6dLiSC8LwI3BdE0q7U9Wf4C0x2IWPUni6whCz1a6bnavB1AKQIj1WSr86kFSm_M1HefNXOAtA5tC_1qVaKaapaH4juG2o5","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: failed to get char replay sequence","postDate":"1149218750","msgId":2903,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU1b2IzdSs3cnMwQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0N0Y3QzRCLjkwODAzMDRAYXJjaGl2ZS5vcmc+"},"prevInTopic":2902,"nextInTopic":2904,"prevInTime":2902,"nextInTime":2904,"topicId":2891,"numMessagesInTopic":8,"msgSnippet":"Thanks Gordon for your reply. I was using a bunch of decide rules and one of them was discarding hops greater than 2. That might explain a whole lot of URLs","rawEmail":"Return-Path: &lt;jonathansiddharth@...&gt;\r\nX-Sender: jonathansiddharth@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 24938 invoked from network); 2 Jun 2006 03:25:53 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m27.grp.scd.yahoo.com with QMQP; 2 Jun 2006 03:25:53 -0000\r\nReceived: from unknown (HELO n24.bullet.scd.yahoo.com) (66.94.237.53)\n  by mta4.grp.scd.yahoo.com with SMTP; 2 Jun 2006 03:25:53 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n24.bullet.scd.yahoo.com with NNFMP; 02 Jun 2006 03:25:52 -0000\r\nReceived: from [66.218.66.74] by t5.bullet.scd.yahoo.com with NNFMP; 02 Jun 2006 03:25:52 -0000\r\nDate: Fri, 02 Jun 2006 03:25:50 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;e5ob3u+7rs0@...&gt;\r\nIn-Reply-To: &lt;447F7C4B.9080304@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;jonathansiddharth&quot; &lt;jonathansiddharth@...&gt;\r\nSubject: Re: failed to get char replay sequence\r\nX-Yahoo-Group-Post: member; u=256901594; y=AJ3onOVIr0d8Jb6eNZKT8WmkFin20wX7eku4D-WNNcyy_9dpEItYvw8op5Y\r\nX-Yahoo-Profile: jonathansiddharth\r\n\r\nThanks Gordon for your reply.\n\nI was using a bunch of decide rules and one =\r\nof them was discarding\nhops greater than 2. That might explain a whole lot =\r\nof URLs being\ndiscarded very quickly towards the end of the crawl and hence=\r\n an\n&#39;abrupt&#39; end to the crawl..I guess. Do you think that&#39;s possible?\n\nJona=\r\nthan\n\n\n\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wr=\r\note:\n&gt;\n&gt; jonathansiddharth wrote:\n&gt; &gt; Yes that is all I&#39;m crawling(wikipedi=\r\na pages..and archiving just the\n&gt; &gt; history pages of featured articles. Im =\r\nstarting the crawl from the\n&gt; &gt; featured aticles home page).\n&gt; &gt; \n&gt; &gt; I don=\r\n&#39;t think the crawl hangs. Its just that the FRontier queue is so\n&gt; &gt; long t=\r\nhat progress in % appears slow. Looking at the crawl log in real\n&gt; &gt; time s=\r\nhows that it is crawling. \n&gt; &gt; \n&gt; &gt; But it seems to stop abruptly after cra=\r\nwling for say 7 to 8 hrs. The\n&gt; &gt; estimated time was 4 days. With only say =\r\n5% of the job done.\n&gt; &gt; The log says the job is &quot;finished&quot; ,no exception or=\r\n alert.\n&gt; &gt; \n&gt; &gt; I dont understand one thing. The log says the job is finis=\r\nhed. Yet\n&gt; &gt; each time I look at its crawl report it is updated. The URIs\n&gt;=\r\n &gt; discovered keeps increasing. Does it mean that when it says crawl job\n&gt; =\r\n&gt; done it means you are done crawling. But the % progress that is shown\n&gt; &gt;=\r\n tells you how many more URLs are left to explore? In other words, your\n&gt; &gt;=\r\n crawl can end when you&#39;re progress on the console page is just 5%. But\n&gt; &gt;=\r\n the URLs already in the queue keep getting downloaded?\n&gt; &gt; \n&gt; &gt; I&#39;m sorry =\r\nbut this really confused me.\n&gt; \n&gt; One thing to keep in mind is that the % c=\r\nompletion shown in the web UI \n&gt; is a very rough/flawed estimate -- almost =\r\ncertain to be a massive \n&gt; underestimate in usual situations. It would only=\r\n be accurate if no \n&gt; additional URIs are discovered, and future progress i=\r\ns at the same rate \n&gt; as previous progress. To the contrary, URIs are alway=\r\ns being\ndiscovered, \n&gt; and typically the end of crawls are dominated by lar=\r\nge/slow hosts where \n&gt; politeness dominates, making progress slower at the =\r\nend.\n&gt; \n&gt; (I thought there was an existing RFE for better estimates, but co=\r\nuldn&#39;t \n&gt; find it, so I made one: \n&gt; http://sourceforge.net/support/tracker=\r\n.php?aid=3D1499205 .)\n&gt; \n&gt; That said, the crawl should only show as &#39;finish=\r\ned&#39; if it has in fact \n&gt; stopped crawling -- so nothing new should be appea=\r\nring in the\ncrawl.log, \n&gt; nor changes occurring to the console/report total=\r\ns. In what log do you \n&gt; see &#39;finished&#39;?\n&gt; \n&gt; Are you sure that a followup =\r\njob hasn&#39;t started, after the first \n&gt; finishes? Have you set any URL/time =\r\nlimits on your crawl that could \n&gt; explain a finish while URLs remained in =\r\nqueues?\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; \n&gt; &gt; thanks,\n&gt; &gt; Jonathan\n&gt; &gt; \n&gt; &gt; --- In a=\r\nrchive-crawler@yahoogroups.com, Michael Stack &lt;stack@&gt; wrote:\n&gt; &gt;&gt; For sure=\r\n its hung?  You seem to be crawling wikipedia this time\nand in \n&gt; &gt;&gt; your l=\r\nast mail.  Is this all you are crawling?  If so, retries\nbecause \n&gt; &gt;&gt; of t=\r\nhe below connection timeouts with the Heritrix increasing\ninterval \n&gt; &gt;&gt; be=\r\ntween retries may look like the crawler is hung.  Check the\nfrontier \n&gt; &gt;&gt; =\r\nreport.  \n&gt; &gt;&gt;\n&gt; &gt;&gt; Is there nothing in heritrix_out.log?  You could send t=\r\nhe process a \n&gt; &gt;&gt; SIGQUIT signal -- $ kill -3 HERITRIX_PROCESS_ID -- and i=\r\nt&#39;ll send a \n&gt; &gt;&gt; thread dump to heritrix_out.log.  Study the output.  Wait=\r\n a while.  \n&gt; &gt;&gt; Redo. Compare the stack traces.  Any progress?  If none or=\r\n you can&#39;t \n&gt; &gt;&gt; make sense of it, send it on over. Maybe we can see someth=\r\ning in\nthread \n&gt; &gt;&gt; dump tea leaves.\n&gt; &gt;&gt;\n&gt; &gt;&gt; St.Ack\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; jonath=\r\nansiddharth wrote:\n&gt; &gt;&gt;&gt; Thanks for the reply St.Ack.\n&gt; &gt;&gt;&gt; I think you&#39;re =\r\nright. Although I got these alerts, I dont think the\n&gt; &gt;&gt;&gt; crawl terminated=\r\n because of that.\n&gt; &gt;&gt;&gt; Because on re-running the crawl it again mysterious=\r\nly stopped\nafter a\n&gt; &gt;&gt;&gt; (long) time even though I&#39;m sure the crawl was not=\r\n completed. It was\n&gt; &gt;&gt;&gt; only some 5% done. And this time there were no ale=\r\nrts.\n&gt; &gt;&gt;&gt; However I saw some errors in the local-errors.log of this sort\n&gt;=\r\n &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; 2006-06-01T04:42:01.909Z    -2          -\n&gt; &gt;&gt;&gt; http://en.wikipe=\r\ndia.org/wiki/J%C3%BCrgen_Habermas LL\n&gt; &gt;&gt;&gt; http://en.wikipedia.org/wiki/Ber=\r\nnard_Williams no-type #032 - - -\n&gt; &gt;&gt;&gt; le:SocketTimeoutException@HTTP\n&gt; &gt;&gt;&gt;=\r\n java.net.SocketTimeoutException: connect timed out: timeout set at\n&gt; &gt;&gt;&gt; 2=\r\n0000ms.\n&gt; &gt;&gt;&gt;       at\n&gt; &gt;&gt;&gt;\n&gt; &gt;\norg.archive.crawler.fetcher.HeritrixProtoc=\r\nolSocketFactory.createSocket(HeritrixProtocolSocketFactory.java:142)\n&gt; &gt;&gt;&gt; =\r\n      at\n&gt; &gt;&gt;&gt;\n&gt; &gt;\norg.apache.commons.httpclient.HttpConnection.open(HttpCo=\r\nnnection.java:707)\n&gt; &gt;&gt;&gt;       at\n&gt; &gt;&gt;&gt;\n&gt; &gt;\norg.apache.commons.httpclient.H=\r\nttpMethodDirector.executeWithRetry(HttpMethodDirector.java:382)\n&gt; &gt;&gt;&gt;      =\r\n at\n&gt; &gt;&gt;&gt;\n&gt; &gt;\norg.apache.commons.httpclient.HttpMethodDirector.executeMetho=\r\nd(HttpMethodDirector.java:168)\n&gt; &gt;&gt;&gt;       at\n&gt; &gt;&gt;&gt;\n&gt; &gt;\norg.apache.commons.=\r\nhttpclient.HttpClient.executeMethod(HttpClient.java:396)\n&gt; &gt;&gt;&gt;       at\n&gt; &gt;=\r\n&gt;&gt;\n&gt; &gt;\norg.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.ja=\r\nva:324)\n&gt; &gt;&gt;&gt;       at \n&gt; &gt;&gt;&gt;\norg.archive.crawler.fetcher.FetchHTTP.innerPr=\r\nocess(FetchHTTP.java:408)\n&gt; &gt;&gt;&gt;       at \n&gt; &gt;&gt;&gt; org.archive.crawler.framewo=\r\nrk.Processor.process(Processor.java:103)\n&gt; &gt;&gt;&gt;       at\n&gt; &gt;&gt;&gt;\n&gt; &gt;\norg.archi=\r\nve.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:306)\n&gt; &gt;&gt;&gt;   =\r\n    at\n&gt; &gt; org.archive.crawler.framework.ToeThread.run(ToeThread.java:153)\n=\r\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; There were 2 errors like this.\n&gt; &gt;&gt;&gt; I doubt if this caus=\r\ned the crawl to end though. Since there were two\n&gt; &gt;&gt;&gt; of these and if it w=\r\nas really terminal the first one should have\n&gt; &gt;&gt;&gt; brought the crawl to an =\r\nend.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; is there anyother condition for which the crawler gr=\r\nacefully\ngives up.\n&gt; &gt;&gt;&gt; Like if there are too many URLs in the frontier qu=\r\neue.\n&gt; &gt;&gt;&gt; I&#39;m assuming a heap overflow would have an explicit error messag=\r\ne\n&gt; &gt;&gt;&gt; indicating that.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; (My estimated crawl job time was 4 day=\r\ns)\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; thanks,\n&gt; &gt;&gt;&gt; Jonathan\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt;=\r\n &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, Michael Stack &lt;sta=\r\nck@&gt;\nwrote:\n&gt; &gt;&gt;&gt;&gt; jonathansiddharth wrote:\n&gt; &gt;&gt;&gt;&gt;&gt; Hi,\n&gt; &gt;&gt;&gt;&gt;&gt;         Thi=\r\ns alert/exception abruptly terminated my crawl job.\n&gt; &gt; Could\n&gt; &gt;&gt;&gt;&gt;&gt; someo=\r\nne tell me what it is and how it can be avoided?\n&gt; &gt;&gt;&gt;&gt; The below is an old=\r\n faithful.  See\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;\nhttp://sourceforge.net/tracker/index.php?func=3D=\r\ndetail&aid=3D1218961&group_id=3D73833&atid=3D539099.\n&gt; &gt; \n&gt; &gt;\n&lt;http://sourc=\r\neforge.net/tracker/index.php?func=3Ddetail&aid=3D1218961&group_id=3D73833&a=\r\ntid=3D539099.&gt;\n&gt; &gt;&gt;&gt;&gt; It rears its head from time to time but we&#39;ve not bee=\r\nn able to\n&gt; &gt; figure\n&gt; &gt;&gt;&gt;&gt; why it happens nor how to reliably reproduce.  =\r\nI&#39;m guessing if you\n&gt; &gt;&gt;&gt; crawl\n&gt; &gt;&gt;&gt;&gt; that same page again in wikipedia, y=\r\nou&#39;ll succeed (least it did\njust\n&gt; &gt;&gt;&gt; now\n&gt; &gt;&gt;&gt;&gt; for me when I tried it). =\r\n I&#39;m surprised it terminated your crawl. \n&gt; &gt;&gt;&gt;&gt; Usually we just fail extra=\r\nction on a particular page and just\n&gt; &gt; move to\n&gt; &gt;&gt;&gt;&gt; the next in the queu=\r\ne.\n&gt; &gt;&gt;&gt;&gt;\n&gt; &gt;&gt;&gt;&gt; St.Ack\n&gt; &gt;&gt;&gt;&gt; P.S. I&#39;ve just added more logging around thi=\r\ns exception.   Perhaps\n&gt; &gt;&gt;&gt; it&#39;ll\n&gt; &gt;&gt;&gt;&gt; turn up the needed clue.\n&gt; &gt;&gt;&gt;&gt;\n&gt;=\r\n &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt; SPONSORED LINKS\n&gt; &gt;&gt;&gt; Computer sec=\r\nurity \n&gt; &gt;&gt;&gt;\n&gt; &gt;\n&lt;http://groups.yahoo.com/gads?t=3Dms&k=3DComputer+security=\r\n&w1=3DComputer+security&w2=3DComputer+training&c=3D2&s=3D46&.sig=3DBHmcxBg5=\r\nsKfN9-gcWnJWDg&gt;\n&gt; &gt; \n&gt; &gt;&gt;&gt; \tComputer training \n&gt; &gt;&gt;&gt;\n&gt; &gt;\n&lt;http://groups.yah=\r\noo.com/gads?t=3Dms&k=3DComputer+training&w1=3DComputer+security&w2=3DComput=\r\ner+training&c=3D2&s=3D46&.sig=3Dv0JjJWA4s7mLnWQWdFxuTQ&gt;\n&gt; &gt; \n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; =\r\n&gt;&gt;&gt;\n&gt; &gt;\n-------------------------------------------------------------------=\r\n-----\n&gt; &gt;&gt;&gt; YAHOO! GROUPS LINKS\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;     *  Visit your group &quot;archiv=\r\ne-crawler\n&gt; &gt;&gt;&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on t=\r\nhe web.\n&gt; &gt;&gt;&gt;        \n&gt; &gt;&gt;&gt;     *  To unsubscribe from this group, send an =\r\nemail to:\n&gt; &gt;&gt;&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt; &gt;&gt;&gt;   =\r\n   \n&gt; &gt;\n&lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=3DUnsub=\r\nscribe&gt;\n&gt; &gt;&gt;&gt;        \n&gt; &gt;&gt;&gt;     *  Your use of Yahoo! Groups is subject to =\r\nthe Yahoo! Terms of\n&gt; &gt;&gt;&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;=\r\n.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;\n&gt; &gt;\n---------------------------------------------------=\r\n---------------------\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; Yah=\r\noo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n\n\n"}}