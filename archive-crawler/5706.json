{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":389114905,"authorName":"yanky young","from":"yanky young &lt;yanky.young@...&gt;","profile":"yanky_young","replyTo":"LIST","senderId":"43jWZRcCHnyKbFw0OWS6ZMhCG9kzJ5109Ju8x4GP7q-v0j8sQYEhwVJOQEzDrifHr_J0yAsjFPNOmySGphTWv4vesM32N079RDc","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] extract pdf links from web pages","postDate":"1235877192","msgId":5706,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQyOWVkYTZmMDkwMjI4MTkxM2o0NjVhYzA3N2c3ZjkzZDE3NjU2YzNlMTYxQG1haWwuZ21haWwuY29tPg==","inReplyToHeader":"PGdvY29raCtzbWs3QGVHcm91cHMuY29tPg==","referencesHeader":"PGdvY29raCtzbWs3QGVHcm91cHMuY29tPg=="},"prevInTopic":5705,"nextInTopic":0,"prevInTime":5705,"nextInTime":5707,"topicId":5705,"numMessagesInTopic":2,"msgSnippet":"Hi: I am actually a newbie on heritrix. But I think your problem is easy. You can check out the developer manual and read processor part( ","rawEmail":"Return-Path: &lt;yanky.young@...&gt;\r\nX-Sender: yanky.young@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 48952 invoked from network); 1 Mar 2009 03:13:13 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m43.grp.scd.yahoo.com with QMQP; 1 Mar 2009 03:13:13 -0000\r\nX-Received: from unknown (HELO wf-out-1314.google.com) (209.85.200.173)\n  by mta15.grp.scd.yahoo.com with SMTP; 1 Mar 2009 03:13:13 -0000\r\nX-Received: by wf-out-1314.google.com with SMTP id 25so3532901wfa.30\n        for &lt;archive-crawler@yahoogroups.com&gt;; Sat, 28 Feb 2009 19:13:13 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.142.50.6 with SMTP id x6mr2179270wfx.270.1235877192643; Sat, \n\t28 Feb 2009 19:13:12 -0800 (PST)\r\nIn-Reply-To: &lt;gocokh+smk7@...&gt;\r\nReferences: &lt;gocokh+smk7@...&gt;\r\nDate: Sun, 1 Mar 2009 11:13:12 +0800\r\nMessage-ID: &lt;429eda6f0902281913j465ac077g7f93d17656c3e161@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: multipart/alternative; boundary=000e0cd1a6760db45c0464061564\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: yanky young &lt;yanky.young@...&gt;\r\nSubject: Re: [archive-crawler] extract pdf links from web pages\r\nX-Yahoo-Group-Post: member; u=389114905; y=DgMafY3h6cP768yUIfDii7Rh9wVyr-tjU69YzpCPQObq9KBD4bU\r\nX-Yahoo-Profile: yanky_young\r\n\r\n\r\n--000e0cd1a6760db45c0464061564\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi:\n\nI am actually a newbie on heritrix. But I think your problem is easy.\n=\r\n\nYou can check out the developer manual and read processor part(\nhttp://cra=\r\nwler.archive.org/articles/developer_manual/processor.html)\n\nYou just write =\r\na processor. A processor is like a servlet filter. So there\nis a processor =\r\nchain. You can override its\n*process&lt;http://crawler.archive.org/apidocs/org=\r\n/archive/crawler/framework/Processor.html#process%28org.archive.crawler.dat=\r\namodel.CrawlURI%29&gt;\n*(CrawlURI&lt;http://crawler.archive.org/apidocs/org/archi=\r\nve/crawler/datamodel/CrawlURI.html&gt;\n curi) method and extract urls to be cr=\r\nawled and write to your own log file.\nAfter that, you should configure your=\r\n processor on your job by web ui.\n\ngood luck.\n\nyanky\n\n2009/3/1 Sanket Jain =\r\n&lt;sanket.d.jain@...&gt;\n\n&gt;   Hi\n&gt;\n&gt; I am working an application which req=\r\nuires to crawl a certain domain\n&gt; and extract a list of all pdf files on th=\r\ne domain. I have tried using\n&gt; filter rules and surt scope.\n&gt;\n&gt; But my diff=\r\niculty is how can I get the final pdf links in a single txt\n&gt; file. Current=\r\nly, I can see all types of links in the crawl.log file\n&gt; and the only solut=\r\nion I see is to parse this log file to get the pdf\n&gt; links.\n&gt;\n&gt; Please some=\r\none suggest a way to get only the pdf links in a file as\n&gt; the crawler outp=\r\nut. I need only the links, not the actual files.\n&gt;\n&gt; Thanks.\n&gt;\n&gt; -Sanket Ja=\r\nin\n&gt;\n&gt;  \n&gt;\n\r\n--000e0cd1a6760db45c0464061564\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi:&lt;br&gt;&lt;br&gt;I am actually a newbie on heritrix. But I think your problem is =\r\neasy.&lt;br&gt;&lt;br&gt;You can check out the developer manual and read processor part=\r\n(&lt;a href=3D&quot;http://crawler.archive.org/articles/developer_manual/processor.=\r\nhtml&quot;&gt;http://crawler.archive.org/articles/developer_manual/processor.html&lt;/=\r\na&gt;)&lt;br&gt;\n&lt;br&gt;You just write a processor. A processor is like a servlet filte=\r\nr. So there is a processor chain. You can override its &lt;code&gt;&lt;b&gt;&lt;a href=3D&quot;=\r\nhttp://crawler.archive.org/apidocs/org/archive/crawler/framework/Processor.=\r\nhtml#process%28org.archive.crawler.datamodel.CrawlURI%29&quot;&gt;process&lt;/a&gt;&lt;/b&gt;(&lt;=\r\na href=3D&quot;http://crawler.archive.org/apidocs/org/archive/crawler/datamodel/=\r\nCrawlURI.html&quot; title=3D&quot;class in org.archive.crawler.datamodel&quot;&gt;CrawlURI&lt;/a=\r\n&gt;=A0curi)&lt;/code&gt;\n\n method and extract urls to be crawled and write to your =\r\nown log file. After that, you should configure your processor on your job b=\r\ny web ui.&lt;br&gt;&lt;br&gt;good luck.&lt;br&gt;&lt;br&gt;yanky&lt;br&gt;=A0 &lt;br&gt;&lt;div class=3D&quot;gmail_quo=\r\nte&quot;&gt;2009/3/1 Sanket Jain &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:sanket.d.j=\r\nain@...&quot;&gt;sanket.d.jain@...&lt;/a&gt;&gt;&lt;/span&gt;&lt;br&gt;\n&lt;blockquote class=\r\n=3D&quot;gmail_quote&quot; style=3D&quot;border-left: 1px solid rgb(204, 204, 204); margin=\r\n: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;&quot;&gt;\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background=\r\n-color: rgb(255, 255, 255);&quot;&gt;\n\n\n&lt;div style=3D&quot;width: 655px;&quot;&gt;\n&lt;div style=3D=\r\n&quot;margin: 0pt; padding: 0pt 25px 0pt 0pt; width: 470px; float: left;&quot;&gt;\n\n\n   =\r\n &lt;div&gt;\n            &lt;p&gt;Hi&lt;br&gt;\n&lt;br&gt;\nI am working an application which require=\r\ns to crawl a certain domain&lt;br&gt;\nand extract a list of all pdf files on the =\r\ndomain. I have tried using&lt;br&gt;\nfilter rules and surt scope.&lt;br&gt;\n&lt;br&gt;\nBut my=\r\n difficulty is how can I get the final pdf links in a single txt&lt;br&gt;\nfile. =\r\nCurrently, I can see all types of links in the crawl.log file&lt;br&gt;\nand the o=\r\nnly solution I see is to parse this log file to get the pdf&lt;br&gt;\nlinks.&lt;br&gt;\n=\r\n&lt;br&gt;\nPlease someone suggest a way to get only the pdf links in a file as&lt;br=\r\n&gt;\nthe crawler output. I need only the links, not the actual files.&lt;br&gt;\n&lt;br&gt;=\r\n\nThanks.&lt;br&gt;\n&lt;br&gt;\n-Sanket Jain&lt;br&gt;\n&lt;br&gt;\n&lt;/p&gt;\n \n\n    &lt;/div&gt;  \n\n    \n    &lt;div=\r\n width=3D&quot;1&quot; style=3D&quot;color: white; clear: both;&quot;&gt;&lt;/div&gt;\n\t\n\t&lt;/div&gt;\n\t\n\t\n\n\n\t\n=\r\n\n\n\t\n\t\n\t\n\t\n\t\n\n&lt;/blockquote&gt;&lt;/div&gt;&lt;br&gt;\n\r\n--000e0cd1a6760db45c0464061564--\r\n\n"}}