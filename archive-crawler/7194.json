{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":404815145,"authorName":"cosmicpioneer2011","from":"&quot;cosmicpioneer2011&quot; &lt;vinamar@...&gt;","profile":"cosmicpioneer2011","replyTo":"LIST","senderId":"ePj2wSz9OolMehygM4oxrjZa4wcWP9_ZguL1QMOeH5yTnlU6_calFs6yhsPVtSkyr32eByULIMGOJTVDjWut9IUYqZou0WUyel9f4koa","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Heritrix performing crawls at very low rates","postDate":"1309381196","msgId":7194,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGl1ZzNvYys5cWkxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRFMEI3MkM2LjcwMjA3MDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":7193,"nextInTopic":7195,"prevInTime":7193,"nextInTime":7195,"topicId":7191,"numMessagesInTopic":4,"msgSnippet":"Thanks Gordon, What we want to ideally achieve is to have a focused crawler to crawl only specific portions of the web or a.k.a directed crawl which we think","rawEmail":"Return-Path: &lt;vinamar@...&gt;\r\nX-Sender: vinamar@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 1950 invoked from network); 29 Jun 2011 20:59:57 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m15.grp.re1.yahoo.com with QMQP; 29 Jun 2011 20:59:57 -0000\r\nX-Received: from unknown (HELO ng11-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.88)\n  by mta2.grp.re1.yahoo.com with SMTP; 29 Jun 2011 20:59:57 -0000\r\nX-Received: from [98.139.164.124] by ng11.bullet.mail.bf1.yahoo.com with NNFMP; 29 Jun 2011 20:59:57 -0000\r\nX-Received: from [69.147.65.151] by tg5.bullet.mail.bf1.yahoo.com with NNFMP; 29 Jun 2011 20:59:57 -0000\r\nX-Received: from [98.137.34.73] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 29 Jun 2011 20:59:57 -0000\r\nDate: Wed, 29 Jun 2011 20:59:56 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;iug3oc+9qi1@...&gt;\r\nIn-Reply-To: &lt;4E0B72C6.7020708@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;8-9659712141-2899467733=:0&quot;\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;cosmicpioneer2011&quot; &lt;vinamar@...&gt;\r\nSubject: Re: Heritrix performing crawls at very low rates\r\nX-Yahoo-Group-Post: member; u=404815145; y=dqfrWXu7VeFsd1EWYBWtRE9O9FRx4B3vAZRCtimP50wVItq9Ev0mRn112oznA-PIxvXTIQV0bt-HbCc\r\nX-Yahoo-Profile: cosmicpioneer2011\r\n\r\n\r\n--8-9659712141-2899467733=:0\r\nContent-Type: text/plain; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nThanks Gordon, What we want to ideally achieve is to have a focused\ncrawler=\r\n to crawl only specific portions of the web or a.k.a directed\ncrawl which w=\r\ne think can be achieved by writing a RegExp filter plugin.\n\nDo you think he=\r\nritrix can scale out well for our usecase where we have\nneeds to do a direc=\r\nted/focused crawl ?\n\nAlso attached is the xml settings file which we use fo=\r\nr directed crawls.\nCan you suggest any modifications to improve crawl rates=\r\n.\n\nhttp://f1.grp.yahoofs.com/v1/QIQLTjfsFvXQiibub_-pvI_POMQwbxbhRFAKbLg52-Z=\r\n&#92;\noIaUXowI9na8r9zW2M3VZ-9PlfW24TK62zHW1Lx_cmctQuoY7edQkxSJoySGPcg/CrawlOrd&#92;=\r\n\ner_xmlsettings.xml \n&lt;http://f1.grp.yahoofs.com/v1/QIQLTjfsFvXQiibub_-pvI_P=\r\nOMQwbxbhRFAKbLg52-&#92;\nZoIaUXowI9na8r9zW2M3VZ-9PlfW24TK62zHW1Lx_cmctQuoY7edQkx=\r\nSJoySGPcg/CrawlOr&#92;\nder_xmlsettings.xml%20&gt;\n\nthis is our system configuratio=\r\nn:\nWindows 2008 server [Intel(R) Xeon 2.40Ghz (dual core), Installed RAM\n(1=\r\n6g), 64-bit OS ]\n\nWe are using 200 threads @ 2g JVM heap.\n\n--- In archive-c=\r\nrawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; Regexes can be =\r\nwildly different in their performance characteristics\n&gt; with just a single =\r\ncharacter difference; you may want to profile and\n&gt; optimize your regexes.\n=\r\n&gt;\n&gt; Also, long lists of regexes to apply in sequence can be costly for\n&gt; ru=\r\nling things in or out; that&#39;s why we try to use simpler\n&gt; prefix-matching w=\r\nhenever possible.\n&gt;\n&gt; We&#39;ve had crawls on relatively modest machines (2 cor=\r\nes, ~4GB RAM, 4\n&gt; disks) that collect 150-200 URIs/sec at initial launch bu=\r\nt then settle\n&gt; to 30-60 URIs/sec after days or weeks of running.\n&gt;\n&gt; What =\r\nyou might get depends a lot on your system specs, and the general\n&gt; &#39;shape&#39;=\r\n/settings of your crawl. Tweaking the settings can make a big\n&gt; difference,=\r\n and there&#39;s also a lot more room for optimization in the\n&gt; code if any tea=\r\nm needed to really squeeze the most out of a small\n&gt; hardware budget. (We h=\r\nave often chosen &#39;more machines&#39; over &#39;more\ncoding\n&gt; optimizations&#39; when we=\r\n need greater throughput.)\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt; On 6/29/11 9:48 AM, cosmic=\r\npioneer2011 wrote:\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; Hi,\n&gt; &gt;\n&gt; &gt; We have been using heritrix for =\r\na while now and we like it so far\nwith\n&gt; &gt; all the pluggable modules and ea=\r\nse of extensibility. We see a big\n&gt; &gt; difference in the crawl rate. We ran =\r\ntwo parallel crawls one with a\nseed\n&gt; &gt; url and no RegExp Decide filters an=\r\nd another with RegExp Decide rule\n&gt; &gt; filters and we see that the crawler w=\r\nith filters performs crawl rate\nat\n&gt; &gt; 6 to 9 URIs per second compared to t=\r\nhe crawl without the decide\nfilter\n&gt; &gt; performs at 60 URIs/sec (on Avg). An=\r\nd we would like to scale to\n&gt; &gt; performing the crawl at a higher rate since=\r\n our needs are more.\n&gt; &gt;\n&gt; &gt; Our aim should be to get to &gt;500 URIs/sec whic=\r\nh translates to 1.8\n&gt; &gt; Million/Hr. What is the max crawl rate that can be =\r\nachived by\nheritrix\n&gt; &gt; crawler.\n&gt; &gt;\n&gt; &gt; Thanks, -Vinoth.\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;=\r\n\n\n\r\n--8-9659712141-2899467733=:0\r\nContent-Type: text/html; charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n\n\n\nThanks Gordon, What we want to ideally achieve is to have a focused craw=\r\nler to crawl only specific portions of the web or a.k.a directed crawl whic=\r\nh we think can be achieved by writing a RegExp filter plugin. &lt;br&gt;&lt;br&gt;Do yo=\r\nu think heritrix can scale out well for our usecase where we have needs to =\r\ndo a directed/focused crawl ?&lt;br&gt;&lt;br&gt;Also attached is the xml settings file=\r\n which we use for directed crawls. Can you suggest any modifications to imp=\r\nrove crawl rates.&lt;br&gt;&lt;br&gt;&lt;a href=3D&quot;http://f1.grp.yahoofs.com/v1/QIQLTjfsFv=\r\nXQiibub_-pvI_POMQwbxbhRFAKbLg52-ZoIaUXowI9na8r9zW2M3VZ-9PlfW24TK62zHW1Lx_cm=\r\nctQuoY7edQkxSJoySGPcg/CrawlOrder_xmlsettings.xml%20&quot;&gt;http://f1.grp.yahoofs.=\r\ncom/v1/QIQLTjfsFvXQiibub_-pvI_POMQwbxbhRFAKbLg52-ZoIaUXowI9na8r9zW2M3VZ-9Pl=\r\nfW24TK62zHW1Lx_cmctQuoY7edQkxSJoySGPcg/CrawlOrder_xmlsettings.xml &lt;/a&gt;&nbsp=\r\n;&lt;br&gt;&lt;br&gt;this is our system configuration:&lt;br&gt;Windows 2008 server [Intel(R)=\r\n Xeon 2.40Ghz (dual core), Installed RAM (16g), 64-bit OS ]&lt;br&gt;&lt;br&gt;We are u=\r\nsing 200 threads @ 2g JVM heap.&lt;br&gt;&lt;br&gt;--- In archive-crawler@yahoogroups.c=\r\nom, Gordon Mohr &lt;gojomo@...&gt; wrote:&lt;br&gt;&gt;&lt;br&gt;&gt; Regexes can be wi=\r\nldly different in their performance characteristics &lt;br&gt;&gt; with just a si=\r\nngle character difference; you may want to profile and &lt;br&gt;&gt; optimize yo=\r\nur regexes.&lt;br&gt;&gt; &lt;br&gt;&gt; Also, long lists of regexes to apply in sequen=\r\nce can be costly for &lt;br&gt;&gt; ruling things in or out; that&#39;s why we try to=\r\n use simpler &lt;br&gt;&gt; prefix-matching whenever possible.&lt;br&gt;&gt; &lt;br&gt;&gt; W=\r\ne&#39;ve had crawls on relatively modest machines (2 cores, ~4GB RAM, 4 &lt;br&gt;&gt=\r\n; disks) that collect 150-200 URIs/sec at initial launch but then settle &lt;b=\r\nr&gt;&gt; to 30-60 URIs/sec after days or weeks of running.&lt;br&gt;&gt; &lt;br&gt;&gt; W=\r\nhat you might get depends a lot on your system specs, and the general &lt;br&gt;&=\r\ngt; &#39;shape&#39;/settings of your crawl. Tweaking the settings can make a big &lt;b=\r\nr&gt;&gt; difference, and there&#39;s also a lot more room for optimization in the=\r\n &lt;br&gt;&gt; code if any team needed to really squeeze the most out of a small=\r\n &lt;br&gt;&gt; hardware budget. (We have often chosen &#39;more machines&#39; over &#39;more=\r\n coding &lt;br&gt;&gt; optimizations&#39; when we need greater throughput.)&lt;br&gt;&gt; &lt;=\r\nbr&gt;&gt; - Gordon @ IA&lt;br&gt;&gt; &lt;br&gt;&gt; On 6/29/11 9:48 AM, cosmicpioneer201=\r\n1 wrote:&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; Hi,&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &g=\r\nt; We have been using heritrix for a while now and we like it so far with&lt;b=\r\nr&gt;&gt; &gt; all the pluggable modules and ease of extensibility. We see a b=\r\nig&lt;br&gt;&gt; &gt; difference in the crawl rate. We ran two parallel crawls on=\r\ne with a seed&lt;br&gt;&gt; &gt; url and no RegExp Decide filters and another wit=\r\nh RegExp Decide rule&lt;br&gt;&gt; &gt; filters and we see that the crawler with =\r\nfilters performs crawl rate at&lt;br&gt;&gt; &gt; 6 to 9 URIs per second compared=\r\n to the crawl without the decide filter&lt;br&gt;&gt; &gt; performs at 60 URIs/se=\r\nc (on Avg). And we would like to scale to&lt;br&gt;&gt; &gt; performing the crawl=\r\n at a higher rate since our needs are more.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; Our a=\r\nim should be to get to &gt;500 URIs/sec which translates to 1.8&lt;br&gt;&gt; &gt=\r\n; Million/Hr. What is the max crawl rate that can be achived by heritrix&lt;br=\r\n&gt;&gt; &gt; crawler.&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt; Thanks, -Vinoth.&lt;br&gt;&gt; &gt;=\r\n&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt; &gt;&lt;br&gt;&gt;&lt;br&gt;\n\n\r\n--8-9659712141-2899467733=:0--\r\n\n"}}