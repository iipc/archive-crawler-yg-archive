{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":494554680,"authorName":"Travis Wellman","from":"Travis Wellman &lt;travis@...&gt;","replyTo":"LIST","senderId":"1jY3SC3CVqWQrIvIarpuwi_3OQvUMpk1PJ1q1rUPkk5YeUME6SMKbRc3bpMqt1GIs0wkvkmoVK3hZ6R4un6Vz6Esp_TYarl23Ow","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Crawl without URls?","postDate":"1359407270","msgId":7931,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMTMwMTI4MTMwNzUwLmE2NTNkNTIzNTdkNGY5NjFmMDY3ZTcyN0BhcmNoaXZlLm9yZz4=","inReplyToHeader":"PGtlNmtsdis1bDZwQGVHcm91cHMuY29tPg==","referencesHeader":"PDIwMTMwMTIyMTQzNDQ1LmM0Mjk2OTYxMTM4NjNkNjVlNmNhZmU5NEBhcmNoaXZlLm9yZz4JPGtlNmtsdis1bDZwQGVHcm91cHMuY29tPg=="},"prevInTopic":7930,"nextInTopic":0,"prevInTime":7930,"nextInTime":7932,"topicId":7925,"numMessagesInTopic":4,"msgSnippet":"Instead of using those keyword searches as seeds I would curl them and grep out the urls to seed your crawl with. I m not going to advise you to do so, but you","rawEmail":"Return-Path: &lt;travis@...&gt;\r\nX-Sender: travis@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 36197 invoked from network); 28 Jan 2013 21:07:54 -0000\r\nX-Received: from unknown (10.193.84.150)\n  by m2.grp.bf1.yahoo.com with QMQP; 28 Jan 2013 21:07:54 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.224.6)\n  by mta2.grp.bf1.yahoo.com with SMTP; 28 Jan 2013 21:07:54 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 3514F68401C1;\n\tMon, 28 Jan 2013 13:07:54 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id EIRmMrua4LCm; Mon, 28 Jan 2013 13:07:50 -0800 (PST)\r\nX-Received: from travis-ia-laptop (router300.sf.archive.org [208.70.27.190])\n\tby mail.archive.org (Postfix) with ESMTPSA id B0F7F6840156;\n\tMon, 28 Jan 2013 13:07:50 -0800 (PST)\r\nDate: Mon, 28 Jan 2013 13:07:50 -0800\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;David&quot; &lt;gtd_luna@...&gt;\r\nMessage-Id: &lt;20130128130750.a653d52357d4f961f067e727@...&gt;\r\nIn-Reply-To: &lt;ke6klv+5l6p@...&gt;\r\nReferences: &lt;20130122143445.c429696113863d65e6cafe94@...&gt;\n\t&lt;ke6klv+5l6p@...&gt;\r\nOrganization: Internet Archive\r\nX-Mailer: Sylpheed 3.2.0 (GTK+ 2.24.8; x86_64-redhat-linux-gnu)\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=US-ASCII\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Travis Wellman &lt;travis@...&gt;\r\nSubject: Re: [archive-crawler] Re: Crawl without URls?\r\nX-Yahoo-Group-Post: member; u=494554680\r\n\r\nInstead of using those keyword searches as seeds I would curl them and grep out the urls to seed your crawl with.\nI&#39;m not going to advise you to do so, but you could probably ignore robots for just the seed urls without getting anyone angry. Use a sheet overlay so as not to ignore robots for more than exactly what you need to hit.\n\nOn Mon, 28 Jan 2013 19:48:15 -0000\n&quot;David&quot; &lt;gtd_luna@...&gt; wrote:\n\n&gt; Hi Travis thank you for replying, \n&gt; \n&gt; Hmm how sad that nothing like that has come up yet\n\nIt&#39;s not that it hasn&#39;t come up, it&#39;s more that the web doesn&#39;t work in terms of keywords. You need to crawl to do search on keywords, and you need seeds to start a crawl, so I guess what I&#39;m saying is that a keyword is a chicken not an egg.\n\n&gt; I did what you suggested but heritrix is not crawling anything like that is  giving\n&gt; me a &quot;Heritrix(-9998)-Robots precluded / Failed to crawl&quot; is\n&gt; there a good way to do this, or should I just ignore robots.txt\n&gt; which seems to be a very bad idea... \n&gt; \n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Travis Wellman  wrote:\n&gt; &gt;\n&gt; &gt; To start with a keyword you&#39;d need to also start with an index that would translate that keyword into urls. There&#39;s no authoratative index, and no such mechanism (that I know of) has been created, but you could seed with a few urls like this:\n&gt; &gt; \n&gt; &gt; http://www.reddit.com/search.xml?q=keyword\n&gt; &gt; https://duckduckgo.com/html/?q=keyword\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; On Tue, 22 Jan 2013 20:40:06 -0000\n&gt; &gt; &quot;David&quot;  wrote:\n&gt; &gt; \n&gt; &gt; &gt; Hey I have been minning data with a few crawlers for over a year now, and this question may sound silly but is it possible to crawl for certain keywords, instead of url seedlist, someone told me that heritrix could do such thing, was he mistaken.. Is it possible.. How!? Thank you!\n&gt; &gt; &gt;\n&gt; &gt;\n&gt; \n\n"}}