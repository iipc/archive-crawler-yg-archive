{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"aaQ7hBCm_bKfLCtdrEAF64i0-dwwnVT4k8qI3MjymI7e2tDxIqCob52m68dJE6lE6H9hKUkbsWascQHOGONj8QNqrBIT4Jo","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Re: Web-Scale Frontier","postDate":"1255565922","msgId":6105,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBRDY2QTYyLjcwNTAzMDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGhiNHFrOSttanZkQGVHcm91cHMuY29tPg==","referencesHeader":"PGhiNHFrOSttanZkQGVHcm91cHMuY29tPg=="},"prevInTopic":6102,"nextInTopic":6107,"prevInTime":6104,"nextInTime":6106,"topicId":6099,"numMessagesInTopic":6,"msgSnippet":"... URI rates vary a lot but I would expect a higher rate than that, if there are enough candidate URIs to keep all threads busy. Things to check/consider: -","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 54286 invoked from network); 15 Oct 2009 00:18:49 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m8.grp.re1.yahoo.com with QMQP; 15 Oct 2009 00:18:49 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta3.grp.sp2.yahoo.com with SMTP; 15 Oct 2009 00:18:49 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 12C66483C0\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 14 Oct 2009 17:22:28 -0700 (PDT)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id oNYi27LEVDuK for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 14 Oct 2009 17:22:27 -0700 (PDT)\r\nX-Received: from [192.168.1.11] (cpe-70-112-233-153.austin.res.rr.com [70.112.233.153])\n\tby mail.archive.org (Postfix) with ESMTPSA id 1D73A4839C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 14 Oct 2009 17:22:27 -0700 (PDT)\r\nMessage-ID: &lt;4AD66A62.7050309@...&gt;\r\nDate: Wed, 14 Oct 2009 17:18:42 -0700\r\nUser-Agent: Thunderbird 2.0.0.23 (Windows/20090812)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;hb4qk9+mjvd@...&gt;\r\nIn-Reply-To: &lt;hb4qk9+mjvd@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Web-Scale Frontier\r\nX-Yahoo-Group-Post: member; u=137285340; y=HCB8H9LzMzmIfEyFlL7MAVaKoOKRoa8juV0K3f8-2T1d\r\nX-Yahoo-Profile: gojomo\r\n\r\nfarbgeist wrote:\n&gt; Thanks for the quick answer Gordon!\n&gt; \n&gt;&gt; Not true; the crawl should continue without problems as long as you&#39;ve \n&gt;&gt; still got free disk space on whatever volume holds your crawl&#39;s &#39;state&#39; \n&gt;&gt; directory. Frontier operations -- testing URIs for prior inclusion, and \n&gt;&gt; enqueuing new URIs -- will become slower as they require seeks/reads \n&gt;&gt; over ever-larger disk structures.\n&gt;&gt;\n&gt; \n&gt; Do you have some examples of typical URIs/s on different configurations (memory/# cpus) for crawls that already use the disk?\n&gt; A first test of a broad crawl (4 hops, 400 toe-threads on a dual core Athlon 3000+ with 2Gb ram  and 100Mb bandwith resulted in  ~50 URIs/s using Heritrix 2.0.2 after ~ 2 hours which did not change significantly after 10 hours..\n\nURI rates vary a lot but I would expect a higher rate than that, if \nthere are enough candidate URIs to keep all threads busy.\n\nThings to check/consider:\n\n- are there enough queues to keep all threads active? (This shouldn&#39;t be \nan issue with a true &#39;broad&#39; crawl that goes anywhere and keeps growing, \nbut if you limit it by hosts at all, or by hops such that only a subset \nof sites are still offering URIs after a while, the basic rules of \npolite crawling may limit the effective rate, no matter how many \nthreads/resources are available. Does it show all threads as active, and \na &#39;load factor&#39; much larger than 1?)\n\n- more threads aren&#39;t always better; some structures and IO are \ncontended so at some point more threads may be counterproductive. We&#39;d \nmore typically use 100-200 threads on a machine like you&#39;ve described, \nthough there&#39;s no rigorous throughput analysis that went into that \nrange, it&#39;s just what&#39;s seemed to work well for us.)\n\n- make sure you&#39;re not swapping. We would usually set the max-heap (-Xmx \nJVM parameter) for a 2GB machine to about 1500m-1700m -- the JVM uses a \nfew hundred MB of non-heap native memory. Using the most up-to-date \nJava6 JVM can avoid some memory/GC issues in earlier releases.\n\n- spread each of the main disk-IO destinations of the crawl onto \nindependent disk volumes, where possible. The &#39;state&#39; directory, \nespecially, should be on its own disk, but for our crawls we also make \nsure the &#39;scratch&#39;, &#39;logs&#39;, and &#39;arc&#39;/&#39;warc&#39; output directories are each \non independent disks\n\n- recent Heritrix3 builds have moved to much larger default in-memory \nbuffers for holding retrieved content before it&#39;s logged permanently, \nand that gave us a big boost in throughput. You could mimic the H3 \nchanges in H1/H2 by increasing a crawl&#39;s settings for \n&#39;recorder-in-buffer&#39; to 524288 and &#39;recorder-out-buffer&#39; to 16384.\n\n&gt;&gt; For crawls where a single machine is expected to visit &gt; 100 million \n&gt;&gt; URIs, to avoid the slowdown as the crawl grows, we usually swap the \n&gt;&gt; disk-based already-included class (BdbUriUniqFilter) for an in-memory\n&gt;&gt; implementation (BloomUriUniqFilter) that doesn&#39;t slow over time, but \n&gt;&gt; instead has a small false-positive rate (that then grows if the filter \n&gt;&gt; becomes oversaturated). (The defaults, which are adjustable if you have \n&gt;&gt; more RAM, use 512MB to acheive a 1-in-4-million false-positive rate up \n&gt;&gt; through 125 million discovered URIs.)\n&gt;&gt;\n&gt; \n&gt; Where is the upper limit? Do you need 512MB every 125 million URIs or\n&gt; is the false positive rate increasing drastically?\n\nThere are exact formulas for expected bloom filter error rates at chosen \nsizes you can find in online reference material about bloom filters in \ngeneral. The false positive rate creeps up over time -- no drastic \nincrease at any level, just moving further out of its target performance \nas it becomes oversaturated.\n\n\n&gt;&gt; IA has done crawls up to 2 billion URIs with Heritrix using multiple \n&gt;&gt; machines, and we know of outside teams who have done crawls of over 8 \n&gt;&gt; billion URIs using the same general techniques, which can be scaled \n&gt;&gt; further with more machines.\n&gt;&gt;\n&gt; Can you tell numbers of how many machines (RAM, cores, bandwith) you used in which timeframe for the 2 billion URIs? In that case you used the BdbUriUniqFilter, right?\n\nI *think* we used 8 dual-core 2GB machines over about 2 months for our \n2-billion-URL crawl, but I don&#39;t have the exact figures handy, so each \nof those figures could be off by up to a factor of 2.\n\n(CPU, disk IO, or project limits are always more restricting than our \ninbound pipes, so bandwidth is not really something we track/provision \nseparately. For truly broad crawls, I think a rough estimate of 20KB/URI \nremains safe for projecting bandwidth needs.)\n\n&gt;&gt; Separate but related: we&#39;ve long been interested in having a \n&gt;&gt; already-included structure matching that described in the Mercator \n&gt;&gt; papers (or as updated in the recent IRLbot paper), which would offer a \n&gt;&gt; disk-based structure that wouldn&#39;t slow as much with growth as our \n&gt;&gt; current implementation. \n&gt; (The manner in which candidate \n&gt;&gt; URIs are passed through a duplicate filter, allowing for batching and \n&gt;&gt; without the assumption of instant enqueuing, was designed to allow these \n&gt;&gt; techniques to be dropped-in when needed.)\n&gt;&gt;\n&gt; \n&gt; Do you believe HBase on a cluster could possibly act as a substitute to DRUM?\n\nI don&#39;t know enough about HBase&#39;s performance characteristics to know. \nIt&#39;s possible.\n\n&gt; My test-crawl resulted in less than 1/5 of bandwith usage (about 2000KB/s), while 1,600,000 pages where downloaded and ~ 5,000,000 queued. Why?\n\nThe ideas above suggest some of the reasons a crawl might not go as fast \nas expected or possible. If it continues to be a problem, a few lines of \nthe &quot;progress-statistics.log&quot; from the &#39;slow&#39; times might help narrow \ndown what the bottlenecks are. Also looking at a few all-threads stack \ndumps -- either via the web UI or the Java SIGQUIT or &#39;jstack&#39; \nmechanisms -- can highlight if a particularly processing step is \nbottlenecked.\n\n- Gordon @ IA\n\n"}}