{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"pfmhK2c1g3Drfjyz7I5ske5zVWjRaGyQQ8wd9F1szrSdtEwoCcTS7tRYH8YnbQ5ACWr59ChRSsR6Nm1gDbTfwfdtlL5Xb_M","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: settings to maximize number of unique domains crawled?","postDate":"1310603391","msgId":7209,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFMUUzODdGLjUwOTAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGl2bGJ2OCthdDBvQGVHcm91cHMuY29tPg==","referencesHeader":"PGl2bGJ2OCthdDBvQGVHcm91cHMuY29tPg=="},"prevInTopic":7208,"nextInTopic":7210,"prevInTime":7208,"nextInTime":7210,"topicId":7206,"numMessagesInTopic":5,"msgSnippet":"In H1, on the Settings tab of crawl job configuration, if you view expert settings , there should be a cost-policy setting on the frontier with drop-down","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 37350 invoked from network); 14 Jul 2011 00:29:52 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m14.grp.re1.yahoo.com with QMQP; 14 Jul 2011 00:29:52 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.re1.yahoo.com with SMTP; 14 Jul 2011 00:29:52 -0000\r\nX-Received: (qmail 15744 invoked by uid 0); 14 Jul 2011 00:29:51 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 14 Jul 2011 00:29:51 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4E1E387F.5090303@...&gt;\r\nDate: Wed, 13 Jul 2011 17:29:51 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.18) Gecko/20110616 Thunderbird/3.1.11\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: helloitsmaxine &lt;itsmaxine@...&gt;\r\nReferences: &lt;ivlbv8+at0o@...&gt;\r\nIn-Reply-To: &lt;ivlbv8+at0o@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: settings to maximize number of unique domains\n crawled?\r\nX-Yahoo-Group-Post: member; u=137285340; y=LT0ylDX_kqJB8awK7B41Y3x9HvYWChqKjj_ne2T7z3Bm\r\nX-Yahoo-Profile: gojomo\r\n\r\nIn H1, on the &#39;Settings&#39; tab of crawl job configuration, if you &#39;view\nexpert settings&#39;, there should be a &#39;cost-policy&#39; setting on the\nfrontier with drop-down options.\n\nIn H3, you&#39;d use the Spring XML syntax to specify a CostAssignmentPolicy\nbean of the desired class.\n\n- Gordon @ IA\n\nOn 7/13/11 5:07 PM, helloitsmaxine wrote:\n&gt; Hi Gordon,\n&gt;\n&gt; Thanks for your advice.\n&gt;\n&gt; I&#39;ve changed my scope now to Deciding with the AcceptRule on. As for\n&gt; the first two suggestions, they make sense but I can&#39;t seem to figure\n&gt; out how to implement those configurations through the web UI.\n&gt; Searching about the CostAssignmentPolicy has turned up something to\n&gt; do with a WorkQueueFrontier, which doesn&#39;t seem to come as an option?\n&gt; Am I just looking in the wrong place or are there certain features\n&gt; that need some additional work to set up?\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@...&gt;\n&gt; wrote:\n&gt;&gt;\n&gt;&gt; The best adjustment you could make to encourage wandering to a\n&gt;&gt; wide number of different hosts is to:\n&gt;&gt;\n&gt;&gt; (1) make sure the &#39;budgeting&#39; for allocating frontier effort among\n&gt;&gt; queues is in effect, by having a non-zero CostAssignmentPolicy.\n&gt;&gt; (UnitCostAssignmentPolicy, treating each URI as &#39;1&#39;, is fine.)\n&gt;&gt;\n&gt;&gt; (2) then, make the &#39;balance-replenish-amount&#39; very small. Each\n&gt;&gt; queue gets devoted frontier attention until this amount is &#39;spent&#39;,\n&gt;&gt; then that queue goes to the back of all other queues. (Usually, we\n&gt;&gt; want to intensely crawl a site for a while, ideally even finishing\n&gt;&gt; small sites, so that all the archived captures are close together\n&gt;&gt; in time... thus the 3000 default value here. But it could be 50, or\n&gt;&gt; 10, or even 1. Note that at &#39;1&#39; behavior between queues is\n&gt;&gt; completely round-robin, which spread attention across the widest\n&gt;&gt; number of sites but also generally keeps memory caches from getting\n&gt;&gt; any benefit from lots of crawling of the same site in rapid\n&gt;&gt; succession.)\n&gt;&gt;\n&gt;&gt; Lots of other refinements are possible, with advanced\n&gt;&gt; configuration tweaks (including some, like custom &#39;queue precedence\n&gt;&gt; policies&#39;, that theoretically should work but are so seldom used\n&gt;&gt; they&#39;re not battle-tested). But just that change biases things a\n&gt;&gt; lot more towards host diversity early, rather than eventually.\n&gt;&gt;\n&gt;&gt; Separately, the &#39;BroadScope&#39; and other ____Scope classes (other\n&gt;&gt; than DecidingScope) are discouraged; if you make a DecidingScope\n&gt;&gt; which starts with an AcceptDecideRule, then focus the other rules\n&gt;&gt; on knocking out things you don&#39;t want, you&#39;ll have a more modern\n&gt;&gt; (and H3-ready) broadly-scoped crawl.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; On 7/13/11 3:03 PM, helloitsmaxine wrote:\n&gt;&gt;&gt; I&#39;m doing a crawl and interested in maximum the number of unique\n&gt;&gt;&gt; domains crawled, ie. I&#39;d rather crawl one page from each of 5\n&gt;&gt;&gt; unique domains than have 10 pages from one domain. Right now it&#39;s\n&gt;&gt;&gt; weird because I have pretty standard/default settings, ie.\n&gt;&gt;&gt; BroadScope and high max hops and such, but out of ~50gb I&#39;ve\n&gt;&gt;&gt; crawled, less than 1000 unique domains have been produced. I&#39;m\n&gt;&gt;&gt; counting by counting the number of folders in the mirror folder\n&gt;&gt;&gt; (each of which seems to represent the content from one unique\n&gt;&gt;&gt; domain), as I&#39;m using the MirrorWriter.\n&gt;&gt;&gt;\n&gt;&gt;&gt; My question is if anyone knows what the problem could be and how\n&gt;&gt;&gt; to fix it? I was wondering if there were possible a maximum bytes\n&gt;&gt;&gt; per domain limit to set (I know there&#39;s an overall max bytes but\n&gt;&gt;&gt; that wouldn&#39;t seem to help) or some other settings that could\n&gt;&gt;&gt; help me get more domains per amount of space crawled?\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks for any help!\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}