{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264783887,"authorName":"pbaclace","from":"&quot;pbaclace&quot; &lt;pbaclace@...&gt;","profile":"pbaclace","replyTo":"LIST","senderId":"Epa2aL8-udykaJndlZ2BnlEoGoB1uujprsiSLXUo2g0QrRLH3eNCqxKqDE8vFpc7mo1Z8zm4_yWSfmBeFV0q_wYznemsKA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: lock contention in ServerCache.getServerFor()","postDate":"1234757097","msgId":5673,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGduYW9sOStpMnZnQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGduMmhhYSs2ZXVoQGVHcm91cHMuY29tPg=="},"prevInTopic":5665,"nextInTopic":5674,"prevInTime":5672,"nextInTime":5674,"topicId":5665,"numMessagesInTopic":8,"msgSnippet":"The un-knotting performance change worked.  I see a 2X speedup in heritrix v1.14.2: * 460KB/sec (from 230KB/sec) network usage * 100% cpu with load between","rawEmail":"Return-Path: &lt;pbaclace@...&gt;\r\nX-Sender: pbaclace@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 71114 invoked from network); 16 Feb 2009 04:05:00 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m1.grp.re1.yahoo.com with QMQP; 16 Feb 2009 04:05:00 -0000\r\nX-Received: from unknown (HELO n36b.bullet.mail.sp1.yahoo.com) (66.163.168.150)\n  by mta18.grp.scd.yahoo.com with SMTP; 16 Feb 2009 04:05:00 -0000\r\nX-Received: from [69.147.65.149] by n36.bullet.mail.sp1.yahoo.com with NNFMP; 16 Feb 2009 04:05:00 -0000\r\nX-Received: from [98.137.34.33] by t9.bullet.mail.sp1.yahoo.com with NNFMP; 16 Feb 2009 04:05:00 -0000\r\nDate: Mon, 16 Feb 2009 04:04:57 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gnaol9+i2vg@...&gt;\r\nIn-Reply-To: &lt;gn2haa+6euh@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;pbaclace&quot; &lt;pbaclace@...&gt;\r\nSubject: Re: lock contention in ServerCache.getServerFor()\r\nX-Yahoo-Group-Post: member; u=264783887; y=T0_zqIIF08-eNrCVsAGjygudTjQqK5cYzg4VjRssyLBcD9A\r\nX-Yahoo-Profile: pbaclace\r\n\r\nThe &quot;un-knotting&quot; performance change worked.  I see a 2X speedup in\nheritri=\r\nx v1.14.2:\n\n* 460KB/sec (from 230KB/sec) network usage\n* 100% cpu with load=\r\n between 15-19 (as reported by &quot;w&quot; in linux)\n* disk usage at 600KB/sec (fro=\r\nm 300KB/sec) \n* number of established HTTP sockets:  25 (an average from 13=\r\n netstats)\n\nBasically, the same run took half the time.  As long as logging=\r\n is not\nverbose, the TOE worker threads are blocked on writing the WARC fil=\r\nes.\n (Actually, this job writes out both WARC and ARC, so it could be\nimpro=\r\nved.) \n\nThe high load number might seem scary to some people, but it just\nm=\r\neans the cpu is fully utilized and more cores could help.\n\nIt requires edit=\r\ns to 3 files plus 3 other files need trivial changes.\n The un-knotting has =\r\nnot yet been tested against: multi-core,\nmulti-processor, checkpointing, an=\r\nd recovery.\n\n\nPaul\n\n\n--- In archive-crawler@yahoogroups.com, &quot;pbaclace&quot; &lt;pb=\r\naclace@...&gt; wrote:\n&gt;\n&gt; \n&gt; A test run of:\n&gt;   * Heritrix 1.14.2 on an AWS/EC=\r\n2, small instance, with 100 worker\n&gt; threads, 1.3M seeds, 900MB heap\n&gt; \n&gt; H=\r\nas the following resource utilization stats:\n&gt; \n&gt;   *  230KB/sec of the net=\r\nwork\n&gt;   * 100% cpu with load between 7 and 13\n&gt;   * disk starts out at 300=\r\nKB/sec, and 24 hours later is at 1MB/sec\n&gt;   * number of established HTTP s=\r\nockets:  ranges from 1 to 7,\n&gt; occasional spiking to 14\n&gt;   * Full GC every=\r\n 10 minutes\n&gt; \n&gt; The limiting resource is the cpu.  A one core machine shou=\r\nld\n&gt; theoretically be able to saturate either the network or the disk\n&gt; ban=\r\ndwidth before the cpu hits the wall, unless it has heavy lock\n&gt; contention.=\r\n \n&gt; \n&gt; See how many and where threads are waiting in some jstack thread dum=\r\nps::\n&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.9.threaddump  |sort |uniq -c\n=\r\n&gt;      25         - waiting to lock &lt;0x5c357d90&gt; (a\n&gt; org.archive.crawler.p=\r\nostprocessor.FrontierScheduler)\n&gt;      61         - waiting to lock &lt;0x5c38=\r\n2828&gt; (a\n&gt; org.archive.crawler.datamodel.ServerCache)\n&gt; # grep &#39;waiting to =\r\nlock&#39; /mnt/Heritrix.8.threaddump  |sort |uniq -c\n&gt;       7         - waitin=\r\ng to lock &lt;0x5c357d90&gt; (a\n&gt; org.archive.crawler.postprocessor.FrontierSched=\r\nuler)\n&gt;      56         - waiting to lock &lt;0x5c382828&gt; (a\n&gt; org.archive.cra=\r\nwler.datamodel.ServerCache)\n&gt; # grep &#39;waiting to lock&#39; /mnt/Heritrix.7.thre=\r\naddump  |sort |uniq -c\n&gt;      31         - waiting to lock &lt;0x5c357d90&gt; (a\n=\r\n&gt; org.archive.crawler.postprocessor.FrontierScheduler)\n&gt;      62         - =\r\nwaiting to lock &lt;0x5c382828&gt; (a\n&gt; org.archive.crawler.datamodel.ServerCache=\r\n)\n&gt; \n&gt; Examination of the FrontierScheduler lock shows that it is held in\n&gt;=\r\n threaddumps 7 and 9 by a thread waiting for ServerCache.\n&gt; \n&gt; Most threads=\r\n (about 90) are waiting for a lock on ServerCache in the\n&gt; method:\n&gt; \n&gt;   p=\r\nublic synchronized CrawlServer getServerFor(String serverKey)\n&gt; \n&gt; Presumab=\r\nly, a simple name to host/server would be fast, but one thread\n&gt; holds the =\r\nlock while doing a relatively long BDB read operation. \n&gt; Obviously, having=\r\n disk io block all cache lookups is not optimal,\n&gt; especially when BDB has =\r\na lock per file (FileManager).  In my test\n&gt; case, the bdb data is 3.6GB an=\r\nd there are about 360 *.jdb files in the\n&gt; job state directory.  If request=\r\ns to getServerFor(String) were not\n&gt; synchronized, then BDB should be able =\r\nto read from multiple *.jdb file\n&gt; at the same time and threads requesting =\r\nentries cached in memory by\n&gt; CachedBDBMap would not need to wait.  \n&gt; \n&gt; \n=\r\n&gt; I think the following high gain, small code footprint improvements\n&gt; woul=\r\nd help:\n&gt; \n&gt; * superficial thread-local caching (1 affected file)\n&gt; **  the=\r\n ServerCache lookups are done in many code locations, so it\n&gt; seems each th=\r\nread processing a uri might repeatedly do the same lookup\n&gt; and get stuck w=\r\naiting\n&gt; **  a ThreadLocal cache of one key-value pair could be checked bef=\r\nore\n&gt; the Maps in ServerCache.getServerFor(String) before synchronizing on\n=\r\n&gt; this instance of ServerCache.\n&gt; **  this must not interfere with soft ref=\r\nerence tracking, of course\n&gt; \n&gt; * deep un-knotting by lock-splitting and en=\r\nabling more concurrency in\n&gt; ServerCache, CachedBdbMap, and BDB.\n&gt; ** drop =\r\nsynchronization of ServerCache.getServerFor(String)\n&gt; ** drop synchronizati=\r\non of  CachedBdbMap.get(Object)\n&gt; ** use ConcurrentHashMap for CachedBdbMap=\r\n.memMap\n&gt; ** drop synchronization of CachedBdbMap.put(K,V) and expose\n&gt; put=\r\nIfAbsent(K,V) if needed.\n&gt; *** ServerCache.createServerFor(String) loses sy=\r\nnchronization when\n&gt; ServerCache.getServerFor(String) drops it.\n&gt; \n&gt; \n&gt; My =\r\nparticular crawl job exercises the ServerCache more than most jobs,\n&gt; but i=\r\nt is analogous to having a very wide, breadth-first crawl. \n&gt; Characteristi=\r\ncs of this performance case are shared by all jobs that\n&gt; crawl many thousa=\r\nnds of hosts.\n&gt; \n&gt; Since full GC was occurring about every 10 minutes, the =\r\nlock\n&gt; contention was not due to full GC frequency.  A heap histogram showe=\r\nd\n&gt; about 3700 CrawlServer instances at the end of the run.\n&gt; \n&gt; If this un=\r\n-knotting can work, there should be substantially better\n&gt; disk and network=\r\n utilization. \n&gt; \n&gt; \n&gt; \n&gt; Paul\n&gt; \n\n\n"}}