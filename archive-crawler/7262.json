{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"sDDfvK9NcLxgDsPCQwZEWJ0pzOXMbq4EgjBMzAJKBj9EyptWyJ_db0tZoyVHN5JRu9pn-j2u8lzqxO1tB4xNBDZgJ7HCriY","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] What is the best configuration for a crawl collecting domain names?","postDate":"1312840428","msgId":7262,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFNDA1QUVDLjYwMDAzMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGoxb29tYStwdHFsQGVHcm91cHMuY29tPg==","referencesHeader":"PGoxb29tYStwdHFsQGVHcm91cHMuY29tPg=="},"prevInTopic":7261,"nextInTopic":0,"prevInTime":7261,"nextInTime":7263,"topicId":7261,"numMessagesInTopic":2,"msgSnippet":"Depending on your needs, it might make sense to try to buy a domain list from a commercial source rather than collecting it yourself. Some issues I would","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 2675 invoked from network); 8 Aug 2011 21:53:51 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m15.grp.re1.yahoo.com with QMQP; 8 Aug 2011 21:53:51 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta2.grp.re1.yahoo.com with SMTP; 8 Aug 2011 21:53:51 -0000\r\nX-Received: (qmail 25789 invoked by uid 0); 8 Aug 2011 21:53:49 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 8 Aug 2011 21:53:49 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4E405AEC.6000300@...&gt;\r\nDate: Mon, 08 Aug 2011 14:53:48 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:5.0) Gecko/20110624 Thunderbird/5.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;j1ooma+ptql@...&gt;\r\nIn-Reply-To: &lt;j1ooma+ptql@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] What is the best configuration for a crawl\n collecting domain names?\r\nX-Yahoo-Group-Post: member; u=137285340; y=A7oRb1JTxbjxCKgO6SdyGwGcdTEte3PFP0x6B5vwMyjd\r\nX-Yahoo-Profile: gojomo\r\n\r\nDepending on your needs, it might make sense to try to buy a domain list \nfrom a commercial source rather than collecting it yourself. Some issues \nI would expect trying to build a domain list by crawling:\n\n- only find domains linked from webpages\n\n- might find a lot of defunct domains (unless you only treat them \n&#39;discovered&#39; when they reply\n\n- a crawl expected to hit a lot of defunct domains might need \nsignificant tuning to avoid spending most of its time waiting-out \nconnection failures\n\n- you might have to go arbitrarily deep into the link graph to find \nsmall/obscure domains -- so it&#39;s almost as hard in \ntime/bandwidth/queue-IO/set-IO as a worldwide crawl. (It&#39;s only easier \nin that you&#39;re not logging the retrieved content.) As you progress, \nyou&#39;ll be crawling many millions of pages for an ever-smaller number of \nnovel links/domains ï¿½ a lot of work for rapidly-decreasing marginal \nvalue. And you&#39;ll never be sure you have &#39;all domain names&#39; via this \nprocess.\n\nI believe &#39;netcraft.com&#39; sells active domain lists, which might meet \nyour need or provide a wider initial seed-set, but I have no idea of \ntheir prices.\n\nIf you have a crawl not going as fast as you&#39;d like or expect, first \ncheck out the things discussed in the FAQ:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/crawl+rate+considerations\n\nIn a rapidly-broadening crawl, you&#39;ll soon reach a point where most \nlinks/domains discovered are already known. If your data-structures \nrequire several disk-seeks for each already-known confirmation, that&#39;s \nusually the performance bottleneck.\n\nThis becomes an issue in large crawls with Heritrix&#39;s default \nBdbUriUniqFilter, and may be an issue with your MySQL code as well. (It \nis very rare for a large crawl to rely on SQL backends.) Some \nimprovement can be had in Heritrix by switching to the \nBloomUriUniqFilter, if some small rate of erroneous rejection of new \nmaterial as if it were already known is acceptable.\n\nOther than that, looking at the specific saturation of CPU/IO/bandwidth \non the machine, and where the active Java threads seem to be spending \ntheir time, is necessary for optimizing a particular crawl.\n\nGenerally, use as much RAM as you can, and use as many disks as you can \n(sending different output files to independent disks if they&#39;re not \nalready in some array to maximize throughput).\n\nSeparately, this is probably not a performance factor, but BroadScope \n(and all other legacy subclasses of CrawlScope other than DecidingScope) \nare now discouraged in favor of DecideRule-based scopes.\n\n- Gordon\n\nOn 8/8/11 6:35 AM, Sergey Alekseev wrote:\n&gt; Hello,\n&gt;\n&gt; I am running a Heritrix 1.14 crawl on a 8-core Xeon machine with 16\n&gt; GB RAM. My goal is to collect all domain names on the Internet\n&gt; possibly quickly and without doing anything else. What would be the\n&gt; best configuration to approach this?\n&gt;\n&gt; My current config is: BroadScope, no frontier changes, a single\n&gt; Extractor collecting every new domain name it sees to a MySQL DB,\n&gt; running with 100 threads and a couple thousand directory style seed\n&gt; sites. The problem I am running into is the following:\n&gt;\n&gt; - first, stuff goes well and domains are found at a rate of\n&gt; 100+/second - after about 30-40 minutes and about 50000 domains\n&gt; collected, the rate of new-found domains slows down very much to\n&gt; below 1/second and stays there. the crawl also slows down from\n&gt; 100+/second to 10 or less per second. no scope or seed changes seem\n&gt; to affect this. even if I take a completely new set of seeds, it only\n&gt; produces a couple hundred new domains at best and then similarly dies\n&gt; off into nothingness. - this seems strange behavior given that there\n&gt; are 200mil+ domains and an entirely unrelated set of seeds (I&#39;ve\n&gt; taken only .jp domains, for example), should produce at least a\n&gt; similar set of total new domains found. it doesn&#39;t, however. at the\n&gt; moment I&#39;m stuck at why this is the case.\n&gt;\n&gt; Any thoughts?\n&gt;\n&gt; Thanks, Sergey\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}