{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Y0qYdTTjmJvlsYQVawI0rYRaBnqIAUyvO7nPatyM6ggzC2IwmD4pMcHzO8twd6mFOU52b4ZNy3M5x5a1n4PbuG0Ww4w91oQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Newbie, Only download html pages in seeds","postDate":"1215808492","msgId":5355,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NzdDM0VDLjcwNTA5MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGc1N2ViZCt1MDU0QGVHcm91cHMuY29tPg==","referencesHeader":"PGc1N2ViZCt1MDU0QGVHcm91cHMuY29tPg=="},"prevInTopic":5354,"nextInTopic":5356,"prevInTime":5354,"nextInTime":5356,"topicId":5353,"numMessagesInTopic":6,"msgSnippet":"... If I understand correctly, you only want your seed URIs to be fetched, and nothing else -- no in-page resources (like images, scripts, or CSS), and no","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 56599 invoked from network); 11 Jul 2008 20:34:51 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m55.grp.scd.yahoo.com with QMQP; 11 Jul 2008 20:34:51 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta18.grp.scd.yahoo.com with SMTP; 11 Jul 2008 20:34:51 -0000\r\nX-Received: (qmail 31165 invoked from network); 11 Jul 2008 20:34:49 -0000\r\nX-Received: from unknown (HELO ?192.168.1.15?) (unknown)\n  by unknown with SMTP; 11 Jul 2008 20:34:49 -0000\r\nX-pair-Authenticated: 67.180.197.118\r\nMessage-ID: &lt;4877C3EC.7050907@...&gt;\r\nDate: Fri, 11 Jul 2008 13:34:52 -0700\r\nUser-Agent: Thunderbird 2.0.0.14 (Windows/20080421)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g57ebd+u054@...&gt;\r\nIn-Reply-To: &lt;g57ebd+u054@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Newbie, Only download html pages in seeds\r\nX-Yahoo-Group-Post: member; u=137285340; y=dQhyU7thFuOu7X6_83bGM1JkWiqrOhC6MjhrLYNKIkMu\r\nX-Yahoo-Profile: gojomo\r\n\r\nhaidong.pan wrote:\n&gt; Hi\n&gt; \n&gt; I&#39;m trying to fetch html pages only defined in seeds using\n&gt; heritrix2.0. and don&#39;t want other pages linked in html pages be download.\n\nIf I understand correctly, you only want your seed URIs to be fetched, \nand nothing else -- no in-page resources (like images, scripts, or CSS), \nand no linked HTML or other documents. Is that correct?\n\nThere are a couple ways to do this -- but the settings you&#39;ve changed \naren&#39;t directly relevant.\n\n&gt; This is my settings:\n&gt; global \n&gt; root:controller:processors:LinksScoper:seed-redirects-new-seeds \n&gt; boolean \tfalse \n\nWe call the set of rules for determining which discovered URIs should be \ncrawled the &#39;scope&#39; of the crawl. Some scopes are defined in terms of \nthe seed URI patterns. For example, if &#39;www.example.com&#39; is a seed, all \nURIs with a host of &#39;www.example.com&#39; are ruled-in.\n\nThis &#39;seed-redirects-new-seeds&#39; setting makes the target URIs of \nredirects (eg HTTP 301/302 responses) from seeds also be considered \nseeds, for the purpose of scope rules. So if your seed &#39;www.example.com&#39; \nredirects to &#39;www.other-site.com&#39;, &#39;www.other-site.com&#39; will be treated \njust as if you had entered it as a seed. Depending on your scope rules, \nthat might cause all &#39;www.other-site.com&#39; URIs to be ruled in-scope.\n\nSo, this is only tangentially related to your goal.\n\n&gt; global \troot:controller:processors:Scheduler:decide-rules:rules \n&gt; list \torg.archive.modules.deciderules.DecideRule\n&gt; global \troot:controller:processors:Scheduler:decide-rules:rules:0 \n&gt; object \torg.archive.modules.deciderules.SeedAcceptDecideRule\n&gt; global \troot:controller:processors:Scheduler:decide-rules:rules:1 \n&gt; object \torg.archive.modules.deciderules.RejectDecideRule \n\nHere, you are setting extra decide-rules on the &#39;Scheduler&#39; processor. \nThe rules that are set up per-processor only affect whether that \nprocessor runs on certain URIs -- they are a mechanism for certain steps \nto be skipped for some URIs.\n\nRules are applied in order with the last rule to apply a decision \n&#39;winning&#39;. (Later rules may PASS.)\n\nYour rules have no initial default decision in the first position (which \nis a good practice to have). So URIs start out with a neutral PASS \ndecision. Then your rule #0 takes any URI that is a seed and sets its \ndecision to ACCEPT. Then your rule #1 sets every URIs decision to \nREJECT. So the net effect is every URI is given a REJECT decision, and \nthe Scheduler is *never* run.\n\nThe Scheduler processor takes discovered URIs and inserts them into the \nFrontier for later crawling, so this guarantees no URIs discovered \nduring the crawl will be crawled. However, even crawling your seeds \nrequires some non-seed URIs to be scheduled -- the DNS and Robots.txt \nURIs that are prerequisites of your URIs.\n\nSo I would expect your changes here to prevent anything from being \ncrawled at all.\n\n&gt; But it&#39;s not works.\n&gt; \n&gt; Can i have any help?\n\nHere are 2 different ways you could achieve a crawl of only your seed URIs:\n\n(1) In the chain of processors, remove or disable all Extractor. These \nprocessors&#39; names all begin &quot;Extractor&quot;. If none are present or enabled, \nno URIs will be found in fetched results -- not in their response \nheaders (for redirects), HTML, or anywhere else.\n\n(2) Adjust the crawl scope so that even though other URIs are \ndiscovered, they do not pass scope-testing and are not scheduled.\n\nThe most simple way to do this is to take the decide-rule configuration \nfrom our default profile and, in the scope, change the \nTooManyHopsDecideRule &#39;max-hops&#39; setting to 0 (meaning anything more \nthan 1 normal hop from seeds will be REJECTed) and the \nTransclusionDecideRule &#39;max-trans-hops&#39; setting to 0 (meaning no \ntranscluded &#39;inline&#39; hops will be ACCEPTed).\n\nAnother way would be to create a custom scope that starts with a \nREJECT-all rule then only adds both the SeedAcceptDecideRule (so your \nseeds aren&#39;t REJECTed) and PrerequisiteAcceptDecideRule (so that the \nprerequisite DNS and robots URIs aren&#39;t REJECTed).\n\nHope this helps clear things up,\n\n- Gordon @ IA\n\n"}}