{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":160886731,"authorName":"Marco Baroni","from":"Marco Baroni &lt;baroni@...&gt;","profile":"kumaraja2000","replyTo":"LIST","senderId":"RuTuQjx57UEHu1KBQXlpmHoBz_z_kd3bHb5wvYtdHWOYnm7H6sH0nTDu9UDs9NXK58f9lvnZZKLZDNMX81IzMRqHjH7k_xO4cX7wvQ","spamInfo":{"isSpam":false,"reason":"4"},"subject":"downloading just a few pages from each site","postDate":"1122918161","msgId":2064,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFBpbmUuTE5YLjQuNTguMDUwODAxMTkzNTU3MC4yNjY0QGVpbnN0ZWluLnNzbG1pdC51bmliby5pdD4="},"prevInTopic":0,"nextInTopic":2065,"prevInTime":2063,"nextInTime":2065,"topicId":2064,"numMessagesInTopic":5,"msgSnippet":"Dear All, I ve been looking into the User Manual, but I was not able to find out how to do the following: I would like to do a crawl in which I do not download","rawEmail":"Return-Path: &lt;baroni@...&gt;\r\nX-Sender: baroni@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 9262 invoked from network); 1 Aug 2005 17:42:56 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m27.grp.scd.yahoo.com with QMQP; 1 Aug 2005 17:42:56 -0000\r\nReceived: from unknown (HELO einstein.sslmit.unibo.it) (137.204.200.1)\n  by mta2.grp.scd.yahoo.com with SMTP; 1 Aug 2005 17:42:56 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby einstein.sslmit.unibo.it (Postfix) with ESMTP id 2B65B1AA46\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  1 Aug 2005 19:42:45 +0200 (CEST)\r\nReceived: from einstein.sslmit.unibo.it ([127.0.0.1])\n by localhost (einstein.sslmit.unibo.it [127.0.0.1]) (amavisd-new, port 10024)\n with ESMTP id 01899-03 for &lt;archive-crawler@yahoogroups.com&gt;;\n Mon,  1 Aug 2005 19:42:42 +0200 (CEST)\r\nReceived: from localhost (localhost [127.0.0.1])\n\t(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))\n\t(No client certificate requested)\n\tby einstein.sslmit.unibo.it (Postfix) with ESMTP id D9C381A9E6\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  1 Aug 2005 19:42:41 +0200 (CEST)\r\nDate: Mon, 1 Aug 2005 19:42:41 +0200 (CEST)\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;Pine.LNX.4.58.0508011935570.2664@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: TEXT/PLAIN; charset=US-ASCII\r\nX-Virus-Scanned: by amavisd-new at sslmit.unibo.it\r\nX-eGroups-Msg-Info: 2:4:8\r\nFrom: Marco Baroni &lt;baroni@...&gt;\r\nSubject: downloading just a few pages from each site\r\nX-Yahoo-Group-Post: member; u=160886731; y=mdVSs6jMRsxEsHhCl-Oc5LE0IsSLS6sWOZkAXyxEpM-83Ogw3vXk\r\nX-Yahoo-Profile: kumaraja2000\r\n\r\nDear All,\n\nI&#39;ve been looking into the User Manual, but I was not able to find out how\nto do the following: I would like to do a crawl in which I do not download\nmore than N pages from each web-site.\n\nE.g., a SURT scope crawl of pages from the .de sites, where I do not\ndownload more than 10 pages from each (www)?&#92;.[^&#92;.]+&#92;.de domain (or, if\nthat is not possible, where I do not download more than 10 pages from the\nsame IP).\n\nIs this possible? Is it documented somewhere?\n\nThanks a lot.\n\nRegards,\n\nMarco\n\n-- \nMarco Baroni\nSSLMIT, University of Bologna\nhttp://sslmit.unibo.it/~baroni\n\n"}}