{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"HKKyAA0ZGTG0tUZ-VV_uWHw7qphu3DWVs35NV-FZNsPYuLCNSFtnklrumJZb0psGApv3cCL0XmdRurlX_VL4z4lG2kVALiI","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Re: Problem with robots.txt IGNORE policy","postDate":"1324582734","msgId":7475,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFRjM4NzRFLjYwMTA4MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDc5ODk2RDkxQTNFQjUzNEM5N0RDODhFOEU2OTQ3RDNGNDJGMkVEQEdBQk1CeDA0LmFkLnVudC5lZHU+","referencesHeader":"PDRDNjQ0NUQ5LjYwNzAxQGFyY2hpdmUub3JnPiw8amN2M3ExKzg5OGZAZUdyb3Vwcy5jb20+IDw3OTg5NkQ5MUEzRUI1MzRDOTdEQzg4RThFNjk0N0QzRjQyRjJFREBHQUJNQngwNC5hZC51bnQuZWR1Pg=="},"prevInTopic":7472,"nextInTopic":7484,"prevInTime":7474,"nextInTime":7476,"topicId":6671,"numMessagesInTopic":9,"msgSnippet":"Thanks for providing the clear example, Lauren! This is indeed the recommended strategy when you know that it s OK to ignore robots.txt for a certain target","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 30746 invoked from network); 22 Dec 2011 19:38:56 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m7.grp.sp2.yahoo.com with QMQP; 22 Dec 2011 19:38:56 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta3.grp.sp2.yahoo.com with SMTP; 22 Dec 2011 19:38:56 -0000\r\nX-Received: (qmail 82126 invoked by uid 0); 22 Dec 2011 19:38:54 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 22 Dec 2011 19:38:54 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4EF3874E.6010805@...&gt;\r\nDate: Thu, 22 Dec 2011 11:38:54 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;4C6445D9.60701@...&gt;,&lt;jcv3q1+898f@...&gt; &lt;79896D91A3EB534C97DC88E8E6947D3F42F2ED@...&gt;\r\nIn-Reply-To: &lt;79896D91A3EB534C97DC88E8E6947D3F42F2ED@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Problem with robots.txt IGNORE policy\r\nX-Yahoo-Group-Post: member; u=137285340; y=ChLgH5QieHwDl4-ElduvRdwZis357MJrc8zS4MLCAySi\r\nX-Yahoo-Profile: gojomo\r\n\r\nThanks for providing the clear example, Lauren!\n\nThis is indeed the recommended strategy when you know that it&#39;s OK to \nignore robots.txt for a certain target hostname/SURT-prefixed URIs (for \nexample by organizational affiliation or explicit legal permission).\n\nTwo added notes:\n\n(1) This works on the level of a hostname (or any other SURT-prefix \ndefining a range of URIs), *not* everything discovered from a certain \nseed. The hostname-centricity is usually what you want, because \nrobots.txt is set per hostname, and exceptions are usually by-hostname.\n\n(1) Instead of overlaying &#39;metadata.robotsPolicyName&#39; with a special \n&#39;ignore&#39; setting for some sites, you could also overlay \n&#39;preconditionEnforcer.calculateRobotsOnly&#39; for some sites.\n\nI mention this because you&#39;d already talked about that setting. Notably, \nthe &#39;calculateRobotsOnly&#39; option, by marking up the URI with a robots \ndecision but not enforcing it, can be helpful for enabling other \nfine-grained control. For example, you could insert a custom \nScriptedProcessor after PreconditionEnforcer and before FetchHTTP that \nperforms extra analysis on the in-process URI, and sometimes respects \nand sometimes ignores the preceding robots decision. (Perhaps you only \nwant to ignore robots for inline images/JS/CSS ï¿½ but still respect it \nfor other URIs. A custom processor at this stage has the necessary \ncontext to do this.)\n\n- Gordon\n\nOn 12/22/11 8:06 AM, Ko, Lauren wrote:\n&gt; For Heritrix 3.1:\n&gt; Put this in SETTINGS OVERLAY SHEETS section of crawler-beans.cxml:\n&gt;\n&gt; &lt;!-- ignoreRobots: any URI to which this sheet&#39;s settings are applied\n&gt;       will ignore robots.txt rules. For use when we are obeying robots.txt\n&gt;       but perhaps have permission to ignore for a domain. --&gt;\n&gt; &lt;bean id=&#39;ignoreRobots&#39; class=&#39;org.archive.spring.Sheet&#39;&gt;\n&gt;   &lt;property name=&#39;map&#39;&gt;\n&gt;    &lt;map&gt;\n&gt;     &lt;entry key=&#39;metadata.robotsPolicyName&#39; value=&#39;ignore&#39;/&gt;\n&gt;    &lt;/map&gt;\n&gt;   &lt;/property&gt;\n&gt; &lt;/bean&gt;\n&gt;\n&gt; Put the following (replace surtPrefixes values as needed) in SETTINGS OVERLAY SHEET-ASSOCIATION section of crawler-beans.cxml:\n&gt;\n&gt; &lt;bean class=&#39;org.archive.crawler.spring.SurtPrefixesSheetAssociation&#39;&gt;\n&gt;   &lt;property name=&#39;surtPrefixes&#39;&gt;\n&gt;    &lt;list&gt;\n&gt;     &lt;value&gt;http://(com,kntu,&lt;/value&gt;\n&gt;    &lt;/list&gt;\n&gt;   &lt;/property&gt;\n&gt;   &lt;property name=&#39;targetSheetNames&#39;&gt;\n&gt;    &lt;list&gt;\n&gt;     &lt;value&gt;ignoreRobots&lt;/value&gt;\n&gt;    &lt;/list&gt;\n&gt;   &lt;/property&gt;\n&gt; &lt;/bean&gt;\n&gt;\n&gt;\n&gt; Lauren Ko\n&gt; Web Archiving Programmer\n&gt; UNT Libraries\n&gt;\n&gt; ________________________________________\n&gt; From: archive-crawler@yahoogroups.com [archive-crawler@yahoogroups.com] on behalf of Mahmoud A. Mubarak [mahmoud.mubarak@...]\n&gt; Sent: Thursday, December 22, 2011 5:20 AM\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Subject: [archive-crawler] Re: Problem with robots.txt IGNORE policy\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com&lt;mailto:archive-crawler%40yahoogroups.com&gt;, Gordon Mohr&lt;gojomo@...&gt;  wrote:\n&gt;\n&gt;&gt; An option for simulating what you want without changing the current\n&gt;&gt; IGNORE or adding a new policy would be to run your crawl with a CLASSIC\n&gt;&gt; robots-respecting policy, but set the &#39;calculateRobotsOnly&#39; flag on\n&gt;&gt; PreconditionEnforcer. Rather than canceling the fetching of URIs that\n&gt;&gt; are robots-precluded, this setting merely marks them up with an annotation.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;\n&gt; I have set the &#39;calculateRobotsOnly&#39; flag on PreconditionEnforcer and It worked. But, how can I ignore robots.txt for one or more seeds, not all of them?\n&gt;\n&gt; Thanks in advance.\n&gt;\n&gt; Mahmoud A. Mubarak\n&gt; Bibliotheca Alexandrina\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}