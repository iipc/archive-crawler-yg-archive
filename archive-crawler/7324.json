{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":341518869,"authorName":"Sergey Alekseev","from":"&quot;Sergey Alekseev&quot; &lt;mumitroll863@...&gt;","profile":"mumitroll863","replyTo":"LIST","senderId":"RSamPTcaaRjCAMUPdLsnGeJL3Bx_wpjyJNMHfIDx-nT_Rx8r20N4ZyUoWG5NVGSaLoa1UcaW8iNrQtbr03elLlATH1tdxaSVPUOF2dF9pwyG","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Having trouble crawling a single domain","postDate":"1316623905","msgId":7324,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGo1ZDRuMitwNmtrQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRFNzY1NDZGLjIwMzA1MDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":7319,"nextInTopic":0,"prevInTime":7323,"nextInTime":7325,"topicId":7318,"numMessagesInTopic":3,"msgSnippet":"Hello Gordon, thanks - extremely helpful as usual. With your tips I was able to get it to do what I wanted to, my previous key mistake being simply not","rawEmail":"Return-Path: &lt;mumitroll863@...&gt;\r\nX-Sender: mumitroll863@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 14518 invoked from network); 21 Sep 2011 16:51:47 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m4.grp.sp2.yahoo.com with QMQP; 21 Sep 2011 16:51:47 -0000\r\nX-Received: from unknown (HELO n41b.bullet.mail.sp1.yahoo.com) (66.163.168.155)\n  by mta2.grp.sp2.yahoo.com with SMTP; 21 Sep 2011 16:51:47 -0000\r\nX-Received: from [69.147.65.151] by n41.bullet.mail.sp1.yahoo.com with NNFMP; 21 Sep 2011 16:51:46 -0000\r\nX-Received: from [98.137.34.35] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 21 Sep 2011 16:51:46 -0000\r\nDate: Wed, 21 Sep 2011 16:51:45 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;j5d4n2+p6kk@...&gt;\r\nIn-Reply-To: &lt;4E76546F.2030508@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Sergey Alekseev&quot; &lt;mumitroll863@...&gt;\r\nSubject: Re: Having trouble crawling a single domain\r\nX-Yahoo-Group-Post: member; u=341518869; y=TVrtU9cU1F8SjbTtRRuHIETzFu2DhnIN7YrOF8Cu3x5galj0-F_A\r\nX-Yahoo-Profile: mumitroll863\r\n\r\n\n\nHello Gordon,\n\nthanks - extremely helpful as usual. With your tips I was =\r\nable to get it to do what I wanted to, my previous key mistake being simply=\r\n not realizing the exact precedence mechanism of the accept/reject rules. A=\r\nfter some more playing around, to crawl everything on the domain in questio=\r\nn excluding the unwanted filetypes like images/CSS/etc, I used this configu=\r\nration:\n\n  &lt;newObject name=3D&quot;decide-rules&quot; class=3D&quot;org.archive.crawler.de=\r\nciderules.DecideRuleSequence&quot;&gt;\n        &lt;map name=3D&quot;rules&quot;&gt;\n          &lt;newO=\r\nbject name=3D&quot;acceptbyDefault&quot; class=3D&quot;org.archive.crawler.deciderules.Acc=\r\neptDecideRule&quot;&gt;\n          &lt;/newObject&gt;\n          &lt;newObject name=3D&quot;rejectT=\r\naxonomy&quot; class=3D&quot;org.archive.crawler.deciderules.MatchesFilePatternDecideR=\r\nule&quot;&gt;\n            &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n            &lt;st=\r\nring name=3D&quot;use-preset-pattern&quot;&gt;Custom&lt;/string&gt;\n            &lt;string name=\r\n=3D&quot;regexp&quot;&gt;.*(jpg|png|gif|css|rss|js|doc|pdf|ppt|swf)$&lt;/string&gt;\n          =\r\n&lt;/newObject&gt;\n          &lt;newObject name=3D&quot;rejectOffCDJapan&quot; class=3D&quot;org.ar=\r\nchive.crawler.deciderules.NotMatchesRegExpDecideRule&quot;&gt;\n            &lt;string =\r\nname=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n            &lt;string name=3D&quot;regexp&quot;&gt;.*(cd=\r\njapan.co.jp).*&lt;/string&gt;\n          &lt;/newObject&gt;\n          &lt;newObject name=3D=\r\n&quot;rejectSpecific&quot; class=3D&quot;org.archive.crawler.deciderules.MatchesRegExpDeci=\r\ndeRule&quot;&gt;\n            &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n            =\r\n&lt;string name=3D&quot;regexp&quot;&gt;.*(login|&#92;/register|&#92;/api|&#92;/special|&#92;/click|&#92;/rss).=\r\n*&lt;/string&gt;\n          &lt;/newObject&gt;\n        &lt;/map&gt;\n      &lt;/newObject&gt;\n\nThe wh=\r\nitespace thing was probably an artifact, it&#39;s not present in the original f=\r\nile. I am still not quite sure why the crawler went off the seeds&#39; domain w=\r\nith the prior config though - there were only the 3 above lines in the seed=\r\ns file, nothing else. \n\nRegards,\nSergey\n\n\n--- In archive-crawler@yahoogroup=\r\ns.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;\n&gt; Three key ideas to keep in mind =\r\nwhen customizing the decide-rules of a \n&gt; crawl scope:\n&gt; \n&gt; (1) The last ru=\r\nle to express a decision (like ACCEPT or REJECT rather \n&gt; than just PASS) w=\r\nins. So when you have a rule that should reject things \n&gt; other rules might=\r\n accept, place it later.\n&gt; \n&gt; (2) When first learning how rules interact, i=\r\nt&#39;s best to start with the \n&gt; default set, and make single small changes at=\r\n a time, testing each one \n&gt; to confirm it has the intended effect.\n&gt; \n&gt; Wi=\r\nth these in mind, and looking at your ruleset:\n&gt; \n&gt; =95 Your placement of t=\r\nhe &#39;rejectTaxonomy&#39; rule has no effect. At that \n&gt; step, all URIs are alrea=\r\ndy considered REJECTed, by the first rule. Only \n&gt; when they get to the nex=\r\nt rule will some be flipped to ACCEPT, when they \n&gt; match the SURT-URI-pref=\r\nix that was inferred from your seeds. But the \n&gt; &#39;rejectTaxonomy&#39; rule won&#39;=\r\nt be applied again. To have the desired \n&gt; effect, it must appear after any=\r\n ACCEPT rules it needs to reverse.\n&gt; \n&gt; =95 Also, it may be a paste artifac=\r\nt, but your custom regex looks like it \n&gt; starts with two whitespace charac=\r\nters. No URIs that reach scope-testing \n&gt; will still have unescaped whitesp=\r\nace, so even in the right position, \n&gt; this rule might have no effect until=\r\n the regex is tightened.\n&gt; \n&gt; =95 I don&#39;t see what, from this ruleset, what=\r\n would cause wandering off \n&gt; the initial seeds. Perhaps a stray line in th=\r\ne seeds file that rules-in \n&gt; more domains than you expected?\n&gt; \n&gt; =95 Gene=\r\nrally you won&#39;t need a NotOnHostsDecideRule if the (preferred) \n&gt; SurtPrefi=\r\nxedDecideRule is only ACCEPTing the right set of URLs. So I \n&gt; wouldn&#39;t try=\r\n throwing that into the mix unless I knew exactly what was \n&gt; failing with =\r\nthe prior configuration.\n&gt; \n&gt; If these tips don&#39;t help you resolve the issu=\r\ne, let us know what version \n&gt; you&#39;re using and any other changes you&#39;ve ma=\r\nde to the default config, \n&gt; and the exact contents of your seeds file (ie =\r\nseeds text-area in web UI).\n&gt; \n&gt; - Gordon\n&gt; \n&gt; On 9/18/11 2:17 AM, Sergey A=\r\nlekseev wrote:\n&gt; &gt; Hello,\n&gt; &gt;\n&gt; &gt; I may be overlooking something trivial, b=\r\nut I just can&#39;t get a simple crawl of a single domain working right with 1.=\r\n14.4 no matter what I try. Here is my current config:\n&gt; &gt;\n&gt; &gt;      &lt;newObje=\r\nct name=3D&quot;scope&quot; class=3D&quot;org.archive.crawler.deciderules.DecidingScope&quot;&gt;\n=\r\n&gt; &gt;        &lt;boolean name=3D&quot;enabled&quot;&gt;true&lt;/boolean&gt;\n&gt; &gt;        &lt;string name=\r\n=3D&quot;seedsfile&quot;&gt;seeds.txt&lt;/string&gt;\n&gt; &gt;        &lt;boolean name=3D&quot;reread-seeds-=\r\non-config&quot;&gt;true&lt;/boolean&gt;\n&gt; &gt;        &lt;newObject name=3D&quot;decide-rules&quot; class=\r\n=3D&quot;org.archive.crawler.deciderules.DecideRuleSequence&quot;&gt;\n&gt; &gt;          &lt;map =\r\nname=3D&quot;rules&quot;&gt;\n&gt; &gt;            &lt;newObject name=3D&quot;rejectByDefault&quot; class=3D=\r\n&quot;org.archive.crawler.deciderules.RejectDecideRule&quot;&gt;\n&gt; &gt;            &lt;/newObj=\r\nect&gt;\n&gt; &gt;            &lt;newObject name=3D&quot;rejectTaxonomy&quot; class=3D&quot;org.archive=\r\n.crawler.deciderules.MatchesFilePatternDecideRule&quot;&gt;\n&gt; &gt;              &lt;strin=\r\ng name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n&gt; &gt;              &lt;string name=3D&quot;use-pr=\r\neset-pattern&quot;&gt;Custom&lt;/string&gt;\n&gt; &gt;              &lt;string name=3D&quot;regexp&quot;&gt;  .*=\r\n(&#92;.(jpg|png|gif|css|rss|js|doc|pdf|ppt|swf))$&lt;/string&gt;\n&gt; &gt;            &lt;/new=\r\nObject&gt;\n&gt; &gt;            &lt;newObject name=3D&quot;acceptIfSurtPrefixed&quot; class=3D&quot;or=\r\ng.archive.crawler.deciderules.SurtPrefixedDecideRule&quot;&gt;\n&gt; &gt;              &lt;st=\r\nring name=3D&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n&gt; &gt;              &lt;string name=3D&quot;sur=\r\nts-source-file&quot;/&gt;\n&gt; &gt;              &lt;boolean name=3D&quot;seeds-as-surt-prefixes&quot;=\r\n&gt;true&lt;/boolean&gt;\n&gt; &gt;              &lt;string name=3D&quot;surts-dump-file&quot;/&gt;\n&gt; &gt;    =\r\n          &lt;boolean name=3D&quot;also-check-via&quot;&gt;false&lt;/boolean&gt;\n&gt; &gt;             =\r\n &lt;boolean name=3D&quot;rebuild-on-reconfig&quot;&gt;true&lt;/boolean&gt;\n&gt; &gt;            &lt;/newO=\r\nbject&gt;\n&gt; &gt;            &lt;newObject name=3D&quot;rejectIfTooManyHops&quot; class=3D&quot;org.=\r\narchive.crawler.deciderules.TooManyHopsDecideRule&quot;&gt;\n&gt; &gt;              &lt;integ=\r\ner name=3D&quot;max-hops&quot;&gt;20&lt;/integer&gt;\n&gt; &gt;            &lt;/newObject&gt;\n&gt; &gt;          =\r\n  &lt;newObject name=3D&quot;rejectIfPathological&quot; class=3D&quot;org.archive.crawler.dec=\r\niderules.PathologicalPathDecideRule&quot;&gt;\n&gt; &gt;              &lt;integer name=3D&quot;max=\r\n-repetitions&quot;&gt;2&lt;/integer&gt;\n&gt; &gt;            &lt;/newObject&gt;\n&gt; &gt;            &lt;newOb=\r\nject name=3D&quot;rejectIfTooManyPathSegs&quot; class=3D&quot;org.archive.crawler.decideru=\r\nles.TooManyPathSegmentsDecideRule&quot;&gt;\n&gt; &gt;              &lt;integer name=3D&quot;max-p=\r\nath-depth&quot;&gt;20&lt;/integer&gt;\n&gt; &gt;            &lt;/newObject&gt;\n&gt; &gt;\n&gt; &gt;          &lt;/map&gt;=\r\n\n&gt; &gt;        &lt;/newObject&gt;\n&gt; &gt;\n&gt; &gt; If I understand correctly, this should lim=\r\nit the crawl to the\n&gt; &gt; domains\n&gt; of the seeds, and not crawl images, pdfs,=\r\n etc, as specified above.\n&gt; &gt;\n&gt; &gt; Problem is: the crawler doesn&#39;t do either=\r\n. It both immediately goes\n&gt; off the seeds&#39; domains (even if it&#39;s just a si=\r\nngle TLD), ignoring the\n&gt; surt-prefix setting, and immediately crawls files=\r\n excluded in the\n&gt; rejectTaxonomy setting such as .css, .js, etc.\n&gt; &gt;\n&gt; &gt; W=\r\nhat could be wrong here?.... &gt;\n&gt; &gt; Now, if I add a notOnHost rule like this=\r\n:\n&gt; &gt;\n&gt; &gt;    &lt;newObject name=3D&quot;notonhostreject&quot; class=3D&quot;org.archive.crawl=\r\ner.deciderules.NotOnHostsDecideRule&quot;&gt;\n&gt; &gt;              &lt;string name=3D&quot;deci=\r\nsion&quot;&gt;REJECT&lt;/string&gt;\n&gt; &gt;              &lt;string name=3D&quot;surts-dump-file&quot;/&gt;\n&gt;=\r\n &gt;              &lt;boolean name=3D&quot;also-check-via&quot;&gt;false&lt;/boolean&gt;\n&gt; &gt;       =\r\n       &lt;boolean name=3D&quot;rebuild-on-reconfig&quot;&gt;true&lt;/boolean&gt;\n&gt; &gt;            =\r\n&lt;/newObject&gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; the crawler doesn&#39;t do *anything* at all and immed=\r\niately quits.\n&gt; &gt;\n&gt; &gt; My seeds are\n&gt; &gt;\n&gt; &gt; http://www.cdjapan.co.jp\n&gt; &gt; htt=\r\np://www.cdjapan.co.jp/anime/index.html\n&gt; &gt; http://www.cdjapan.co.jp/jpop/in=\r\ndex.html\n&gt; &gt;\n&gt; &gt; and the whole other setup is just defaults.\n&gt; &gt;\n&gt; &gt; Am I m=\r\nissing something?\n&gt; &gt;\n&gt; &gt; Regards\n&gt; &gt; Sergey\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; --=\r\n----------------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n=\r\n&gt;\n\n\n\n"}}