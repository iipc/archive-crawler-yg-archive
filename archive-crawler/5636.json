{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":299179219,"authorName":"mjjjhjemj","from":"&quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;","profile":"mjjjhjemj","replyTo":"LIST","senderId":"aMisvvlTa6nBAISS9I75jzIvn3ft7l4ydRy8tfEPaphthuZDvRjAaVsAYWKdGb2rFxpOSlg08pB7WNLiZF1Vui6OaFc-w_LWemI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Help with crawling sites without robots.txt and return the following responses","postDate":"1232125467","msgId":5636,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGdrcWVtcit1bTBkQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":5641,"prevInTime":5635,"nextInTime":5637,"topicId":5636,"numMessagesInTopic":4,"msgSnippet":"I would like to configure Heritrix to not even look for a robots.txt. I have permission for the sites I am crawling to ignore the robots.txt which I have done,","rawEmail":"Return-Path: &lt;bosoxchamps@...&gt;\r\nX-Sender: bosoxchamps@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 91425 invoked from network); 16 Jan 2009 17:04:29 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m50.grp.scd.yahoo.com with QMQP; 16 Jan 2009 17:04:29 -0000\r\nX-Received: from unknown (HELO n13d.bullet.sp1.yahoo.com) (69.147.64.236)\n  by mta18.grp.scd.yahoo.com with SMTP; 16 Jan 2009 17:04:28 -0000\r\nX-Received: from [69.147.65.151] by n13.bullet.sp1.yahoo.com with NNFMP; 16 Jan 2009 17:04:28 -0000\r\nX-Received: from [66.218.66.91] by t5.bullet.mail.sp1.yahoo.com with NNFMP; 16 Jan 2009 17:04:28 -0000\r\nDate: Fri, 16 Jan 2009 17:04:27 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;gkqemr+um0d@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;mjjjhjemj&quot; &lt;bosoxchamps@...&gt;\r\nSubject: Help with crawling sites without robots.txt and return the following responses\r\nX-Yahoo-Group-Post: member; u=299179219; y=UpVokVg2W6YNuujoteTOKygzdE4zZFeDa9zb4pIHVcETEiU8\r\nX-Yahoo-Profile: mjjjhjemj\r\n\r\nI would like to configure Heritrix to not even look for a robots.txt.\nI hav=\r\ne permission for the sites I am crawling to ignore the robots.txt\nwhich I h=\r\nave done, but the problem is even when the configuration is\nsetup to ignore=\r\n the robots.txt Heritrix still wants to download first\nand then apparently =\r\nignore if configured to do so. I have also tried\nthe &#39;custom&#39; robots.txt co=\r\nnfiguration with no success. If the\nrobots.txt is not present and the respo=\r\nnse falls under examples 2 or 3\nthen your are stuck.\n\nContinues to Crawl si=\r\nte example\n-------------------------------\n1. http://www.mysite1.com/robots=\r\n.txt\nresponse: 404 Not Found\nNot Found\nThe requested URL /robots.txt was no=\r\nt found on this server.\n* Heritrix continues to crawl www.mysite1.com\n\nFail=\r\ns to continue to crawl examples\n-----------------------------------\n2. http=\r\n://www.mysite2.com/robots.txt\nresponse: 301 - moved permanently\nand then th=\r\ne server issues a 301 and serves back\n&#39;http://www.mysite2.com/ShortUrlError=\r\nPage.jsp&#39;\nInvalid Short URL pattern Please redefine your Short URL pattern!=\r\n\n\n3. http://www.mysite3/robots.txt\nresponse: Connection Interrupted\nThe con=\r\nnection to the server was reset while the page was loading.\nThe network lin=\r\nk was interrupted while negotiating a connection.\nPlease try again.\n\n\n"}}