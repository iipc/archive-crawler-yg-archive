{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":496150545,"authorName":"Markus Mirsberger","from":"Markus Mirsberger &lt;markus.mirsberger@...&gt;","profile":"mirschi74","replyTo":"LIST","senderId":"JkjRBYJZIWStC1Ddf3gGlyW1u5xeji97iGsxKOJ0_eQmKNGhdfzmADzK6mCG0OGV8o9YRqNdNxFun-ev489yuMVQ5y3asVDIKgg69vTlWNjJ4vo","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Limit the crawls to e.g. 100 URLs/Host","postDate":"1348031284","msgId":7773,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwNTk1MzM0LjYwNDA2MDhAZ214LmRlPg==","inReplyToHeader":"PDUwNTI2N0FELjYwMjA4MDZAYXJjaGl2ZS5vcmc+","referencesHeader":"PGsxcXFyOSthZm9pQGVHcm91cHMuY29tPiA8NTA1MjY3QUQuNjAyMDgwNkBhcmNoaXZlLm9yZz4="},"prevInTopic":7758,"nextInTopic":7774,"prevInTime":7772,"nextInTime":7774,"topicId":7755,"numMessagesInTopic":8,"msgSnippet":"Hello Noah, thanks for your reply. This works well when I only use one thread for one host. Unfortunately I am crawling most hosts with parallel queues and","rawEmail":"Return-Path: &lt;markus.mirsberger@...&gt;\r\nX-Sender: markus.mirsberger@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 85578 invoked from network); 19 Sep 2012 05:08:10 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m7.grp.sp2.yahoo.com with QMQP; 19 Sep 2012 05:08:10 -0000\r\nX-Received: from unknown (HELO mailout-de.gmx.net) (213.165.64.22)\n  by mta3.grp.sp2.yahoo.com with SMTP; 19 Sep 2012 05:08:10 -0000\r\nX-Received: (qmail invoked by alias); 19 Sep 2012 05:08:08 -0000\r\nX-Received: from mx-ll-14.207.114-14.dynamic.3bb.co.th (EHLO [192.168.1.11]) [14.207.114.14]\n  by mail.gmx.net (mp070) with SMTP; 19 Sep 2012 07:08:08 +0200\r\nX-Authenticated: #10074639\r\nX-Provags-ID: V01U2FsdGVkX1+h+Tcrdx6hsW6o67sayXg0hNpnIN+RMv7i3AHyoY\n\tqx4iEs5geX6VZH\r\nMessage-ID: &lt;50595334.6040608@...&gt;\r\nDate: Wed, 19 Sep 2012 12:08:04 +0700\r\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:15.0) Gecko/20120827 Thunderbird/15.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;k1qqr9+afoi@...&gt; &lt;505267AD.6020806@...&gt;\r\nIn-Reply-To: &lt;505267AD.6020806@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------090206010905030406000004&quot;\r\nX-Y-GMX-Trusted: 0\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Markus Mirsberger &lt;markus.mirsberger@...&gt;\r\nSubject: Re: [archive-crawler] Limit the crawls to e.g. 100 URLs/Host\r\nX-Yahoo-Group-Post: member; u=496150545; y=GAcR9BbQsT1znt1HtcNwULaP9rR7tp0XqrI6z9tjECI9gW_XIVPQ_XRxZKKPCg8rNssXLy7ocEgQ0ro\r\nX-Yahoo-Profile: mirschi74\r\n\r\n\r\n--------------090206010905030406000004\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nHello Noah,\n\nthanks for your reply. This works well when I only use one thread for \none host.\nUnfortunately I am crawling most hosts with parallel queues and this \nsetting affects every queue.\nI thought first .... ok no problem..just part the amount of sites to the \nqueues .. e.g. if I like to crawl 10.000 URLs with 10 parallel queues so \nI set queueTotalBudget to 1000.\n\nThis worked in tests with small hosts but now I tried it with a bigger \nhost and the result is completely different from what I expected.\nWith 30 parallelQueues I tried to get a maximum of 250.000 URIs (out of \n2.000.000) from one host. So I set the queueTotalBudgt to 8334 but the \nresult are only about 55.000 crawled URIs.\n\nDid I use this in a wrong way or do I have to use another setting when I \nuse parallel queues?\n\n\nThanks and regarads,\nMarkus\n\n\nOn 09/14/2012 06:09 AM, Noah Levitt wrote:\n&gt;\n&gt; Hello Markus,\n&gt;\n&gt; You can set the value of queueTotalBudget to 100 on your frontier. Since\n&gt; by default each queue corresponds to one host, the effect is what you\n&gt; describe.\n&gt;\n&gt; Noah\n&gt;\n&gt; On 08/31/2012 10:04 AM, mirschi74 wrote:\n&gt; &gt; Hi,\n&gt; &gt;\n&gt; &gt; I have a seed file filled with hosts, but want only crawl e.g. 100 \n&gt; URLs from each host.\n&gt; &gt; Can you please give me a hint where I can configure that?\n&gt; &gt; I think it should be somewhere in the BDBFrontier, but I cant find \n&gt; any documentation about that.\n&gt; &gt; There is another setting that limits the maxdocuments. But this is a \n&gt; global setting and limits the crawls for a jobrun and not meant to \n&gt; limit crawls by host.\n&gt; &gt;\n&gt; &gt; Thanks in advance,\n&gt; &gt; Markus\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt; &gt; ------------------------------------\n&gt; &gt;\n&gt; &gt; Yahoo! Groups Links\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n&gt; \n\n\r\n--------------090206010905030406000004\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;meta content=&quot;text/html; charset=ISO-8859-1&quot;\n      http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;/head&gt;\n  &lt;body bgcolor=&quot;#FFFFFF&quot; text=&quot;#000000&quot;&gt;\n    Hello Noah,&lt;br&gt;\n    &lt;br&gt;\n    thanks for your reply. This works well when I only use one thread\n    for one host.&lt;br&gt;\n    Unfortunately I am crawling most hosts with parallel queues and this\n    setting affects every queue.&lt;br&gt;\n    I thought first .... ok no problem..just part the amount of sites to\n    the queues .. e.g. if I like to crawl 10.000 URLs with 10 parallel\n    queues so I set queueTotalBudget to 1000.&lt;br&gt;\n    &lt;br&gt;\n    This worked in tests with small hosts but now I tried it with a\n    bigger host and the result is completely different from what I\n    expected. &lt;br&gt;\n    With 30 parallelQueues I tried to get a maximum of 250.000 URIs (out\n    of 2.000.000) from one host. So I set the queueTotalBudgt to 8334\n    but the result are only about 55.000 crawled URIs.&lt;br&gt;\n    &lt;br&gt;\n    Did I use this in a wrong way or do I have to use another setting\n    when I use parallel queues?&lt;br&gt;\n    &lt;br&gt;\n    &lt;br&gt;\n    Thanks and regarads,&lt;br&gt;\n    Markus&lt;br&gt;\n    &lt;br&gt;\n    &lt;br&gt;\n    &lt;div class=&quot;moz-cite-prefix&quot;&gt;On 09/14/2012 06:09 AM, Noah Levitt\n      wrote:&lt;br&gt;\n    &lt;/div&gt;\n    &lt;blockquote cite=&quot;mid:505267AD.6020806@...&quot; type=&quot;cite&quot;&gt;\n      &lt;span style=&quot;display:none&quot;&gt;&nbsp;&lt;/span&gt;\n      \n          &lt;div id=&quot;ygrp-text&quot;&gt;\n            &lt;p&gt;Hello Markus,&lt;br&gt;\n              &lt;br&gt;\n              You can set the value of queueTotalBudget to 100 on your\n              frontier. Since &lt;br&gt;\n              by default each queue corresponds to one host, the effect\n              is what you &lt;br&gt;\n              describe.&lt;br&gt;\n              &lt;br&gt;\n              Noah&lt;br&gt;\n              &lt;br&gt;\n              On 08/31/2012 10:04 AM, mirschi74 wrote:&lt;br&gt;\n              &gt; Hi,&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt; I have a seed file filled with hosts, but want only\n              crawl e.g. 100 URLs from each host.&lt;br&gt;\n              &gt; Can you please give me a hint where I can configure\n              that?&lt;br&gt;\n              &gt; I think it should be somewhere in the BDBFrontier,\n              but I cant find any documentation about that.&lt;br&gt;\n              &gt; There is another setting that limits the\n              maxdocuments. But this is a global setting and limits the\n              crawls for a jobrun and not meant to limit crawls by host.&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt; Thanks in advance,&lt;br&gt;\n              &gt; Markus&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt; ------------------------------------&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt; Yahoo! Groups Links&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt;&lt;br&gt;\n              &gt;&lt;br&gt;\n              &lt;br&gt;\n            &lt;/p&gt;\n          &lt;/div&gt;\n          \n      \n      &lt;!-- end group email --&gt;\n    &lt;/blockquote&gt;\n    &lt;br&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------090206010905030406000004--\r\n\n"}}