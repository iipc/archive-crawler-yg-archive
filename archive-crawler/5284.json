{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"zJZOoBc_G9aEmxJ1xsB6c36hS0NpANXPaDIkoSJHROk47oCUxVBOxIk8EYYleSzmiV771wjqUkZ6u0qgsNgCoUi-DRRl7Vo","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Content type specific crawling?","postDate":"1212643176","msgId":5284,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NDc3NzY4LjEwMTAwMDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGcyNmY3ayttN2ZhQGVHcm91cHMuY29tPg==","referencesHeader":"PGcyNmY3ayttN2ZhQGVHcm91cHMuY29tPg=="},"prevInTopic":5282,"nextInTopic":5289,"prevInTime":5283,"nextInTime":5285,"topicId":5248,"numMessagesInTopic":9,"msgSnippet":"... Looks OK. ... Not sure where exactly this Match segment appears -- the mid-fetch rules? -- but I think I see the problem below. ... OK, if these are the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 31474 invoked from network); 5 Jun 2008 05:19:33 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m48.grp.scd.yahoo.com with QMQP; 5 Jun 2008 05:19:33 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta18.grp.scd.yahoo.com with SMTP; 5 Jun 2008 05:19:33 -0000\r\nX-Received: (qmail 34108 invoked from network); 5 Jun 2008 05:19:29 -0000\r\nX-Received: from unknown (HELO ?10.0.10.7?) (unknown)\n  by unknown with SMTP; 5 Jun 2008 05:19:29 -0000\r\nX-pair-Authenticated: 70.137.178.185\r\nMessage-ID: &lt;48477768.1010004@...&gt;\r\nDate: Wed, 04 Jun 2008 22:19:36 -0700\r\nUser-Agent: Thunderbird 2.0.0.14 (Windows/20080421)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g26f7k+m7fa@...&gt;\r\nIn-Reply-To: &lt;g26f7k+m7fa@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Content type specific crawling?\r\nX-Yahoo-Group-Post: member; u=137285340; y=aA-ZGrOyyojBDZiD7l71Knr8TwDwYcDDII0RTvp3HjKi\r\nX-Yahoo-Profile: gojomo\r\n\r\n\n\nlpeterus wrote:\n&gt; Yes, the setup is like you described. I&#39;m using the standard arc writer\n&gt; and I checked the arc files to see the type. Besides application/xml\n&gt; content, the other ones are all text/html. Here are the settings for\n&gt; parts where I do have filters and such.\n&gt; \n&gt; 1) On the scope:  I put the huge regex on separate lines for easier\n&gt; read.\n&gt; rejectByDefault\n&gt; acceptIfSurtPrefixed\n&gt; rejectIfTooManyHops\n&gt; rejectIfPathological\n&gt; rejectIfTooManyPathSegs\n&gt; acceptIfPrerequisite\n\nLooks OK.\n\n&gt; &lt;newObject name=&quot;Match&quot;\n&gt; class=&quot;org.archive.crawler.deciderules.MatchesRegExpDecideRule&quot;&gt;\n&gt; &lt;string name=&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n&gt; &lt;string name=&quot;regexp&quot;&gt;\n&gt; .*(?i)&#92;.(a|ai|aif|aifc|aiff|asc|asp|avi|bat|bcpio|bin|bmp|bz2|c|cdf|cfm|&#92;\n&gt; cgi|cgm|\n&gt; class|cpio|cpp|cpt|csh|css|cxx|dcr|dif|dir|djv|djvu|dll|dmg|dms|doc|dtd|&#92;\n&gt; dv|dvi|\n&gt; dxr|eps|etx|exe|ez|gram|grxml|gtar|gz|gif|h|html|htm|hdf|hqx|ice|ico|ics&#92;\n&gt; |ief|ifb|\n&gt; iges|igs|iso|jnlp|jp2|jpe|jpeg|jpg|js|jsp|kar|latex|lha|lzh|m3u|mac|man|&#92;\n&gt; mathml|me|\n&gt; mm|mesh|mid|midi|mif|mov|movie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|\n&gt; nc|o|oda|odt|old|ogg|pbm|pct|pdb|pdf|pgm|pgn|php|pic|pict|pl|png|pjpeg|p&#92;\n&gt; nm|pnt|pntg|\n&gt; ppm|ppt|ps|py|qt|qti|qtif|ra|ram|ras|rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|s&#92;\n&gt; gml|sh|shar|shtml|\n&gt; silo|sit|skd|skm|skp|skt|smi|smil|snd|so|spl|sql|src|srpm|sv4cpio|sv4crc&#92;\n&gt; |svg|swf|sxd|sxw|t|\n&gt; tar|tcl|tex|texi|tpl|texinfo|tgz|tif|tiff|tr|tsv|ustar|vcd|vrml|vxml|wav&#92;\n&gt; |wbmp|wbxml|wml|wmv\n&gt; |wmlc|wmls|wmlsc|wrl|xbm|xht|xhtml|xls|xpm|xslt|xwd|xyz|z|zip)$\n\nNot sure where exactly this &#39;Match&#39; segment appears -- the mid-fetch \nrules? -- but I think I see the problem below.\n\n&gt; 2) On the midfetch-decide-rulesand Archiver#decide-rules I have the same\n&gt; thing.\n&gt; &lt;newObject name=&quot;Archiver#decide-rules&quot;\n&gt; class=&quot;org.archive.crawler.deciderules.DecideRuleSequence&quot;&gt;\n&gt; &lt;map name=&quot;rules&quot;&gt;\n&gt; &lt;newObject name=&quot;rejectdefault&quot;\n&gt; class=&quot;org.archive.crawler.deciderules.RejectDecideRule&quot;&gt;\n&gt;              &lt;/newObject&gt;\n&gt; &lt;newObject name=&quot;fetch&quot;\n&gt; class=&quot;org.archive.crawler.deciderules.FetchStatusDecideRule&quot;&gt;\n&gt; &lt;string name=&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n&gt; &lt;integer name=&quot;target-status&quot;&gt;200&lt;/integer&gt;\n&gt; &lt;/newObject&gt;\n&gt; &lt;newObject name=&quot;content&quot;\n&gt; class=&quot;org.archive.crawler.deciderules.ContentTypeMatchesRegExpDecideRul&#92;\n&gt; e&quot;&gt;\n&gt; &lt;string name=&quot;decision&quot;&gt;ACCEPT&lt;/string&gt;\n&gt; &lt;string name=&quot;regexp&quot;&gt;(?i)application/xml.*&lt;/string&gt;\n&gt; &lt;/newObject&gt;\n&gt; &lt;/map&gt;\n&gt; &lt;/newObject&gt;\n\nOK, if these are the exact decide-rules you have the archiver, and they \nare applied in order:\n  - every URI is marked REJECT (so far so good)\n  - every status 200 URI is marked ACCEPT (no matter its type -- bad)\n  - every type application/xml is redundantly marked ACCEPT (though in \nfact, an optimization causes this step to be skipped: if a decide-rule \ncan only make something ACCEPT and it&#39;s already ACCEPT, that condition \nisn&#39;t even tested)\n\nI suspect the FetchStatusDecideRule can be just dropped; only successful \nfetches will have an application/xml type. Or, if there are some 30X/40X \nresponse codes that also have XML (unlikely but possible), rule them out \ninstead of ruling all 200s in.\n\nHTH,\n\n- Gordon @ IA\n\n&gt; Thanks!\n&gt; -Shawn\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt; So if I may summarize:\n&gt;&gt;\n&gt;&gt; - You have a set of decide rules set up on the writer processor\n&gt;&gt; - these rules begin with a REJECT-all rule, then have a\n&gt;&gt; ContentTypeMatchesRegexpDecideRule set to only ACCEPT XML content\n&gt;&gt; - you still see HTML being written by that writer processor\n&gt;&gt;\n&gt;&gt; Is that correct?\n&gt;&gt;\n&gt;&gt; Some ideas:\n&gt;&gt; - are you using a non-standard writer that isn&#39;t consulting\n&gt;&gt; shouldProcess correctly?\n&gt;&gt; - do you have multiple writer processors active, and another processor\n&gt;&gt; is writing?\n&gt;&gt; - the content is actually coming back as XHTML, and has &#39;xml&#39; in its\n&gt;&gt; content-type?\n&gt;&gt; - the writer-processor&#39;s decide-rules are somehow not-enabled?\n&gt;&gt; - an error in the regex is accepting everything?\n&gt;&gt;\n&gt;&gt; If it&#39;s none of these, feel free to send an excerpt of your\n&gt;&gt; configuration for the writer processor (from your heritrix1 order.xml\n&gt; or\n&gt;&gt; heritrix2.0 global.sheet) for analysis.\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; lpeterus wrote:\n&gt;&gt;&gt; I had a default REJECT in the scope to start with but not in the\n&gt;&gt;&gt; mid-fetch or writer processor phase. I also tried adding those in as\n&gt;&gt;&gt; the first rule for them but with no success. The\n&gt;&gt;&gt; ContentTypeMatchesRegexpDecideRule is configured as accept with the\n&gt;&gt;&gt; regex for xml content type. I got the idea from a previous post but\n&gt;&gt;&gt; configured to just accept html content.\n&gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2824\n&gt;&gt;&gt;\n&gt;&gt;&gt; -Shawn\n&gt;&gt;&gt;\n&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr gojomo@ wrote:\n&gt;&gt;&gt;&gt; Do you have a REJECT rule first that applies to everything, then\n&gt; the\n&gt;&gt;&gt;&gt; ContentTypeMatchesRegExpDecideRule to ACCEPT the right kind of\n&gt; content?\n&gt;&gt;&gt;&gt; Otherwise, the decision of the decide-rules will be ambiguous\n&gt; (PASS),\n&gt;&gt;&gt;&gt; and in the case of Processor decide-rules, anything but a REJECT\n&gt; allows\n&gt;&gt;&gt;&gt; the Processor to run. (See Processor.process().)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - Gordon @ IA\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; lpeterus wrote:\n&gt;&gt;&gt;&gt;&gt; Hi!\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Thanks for the reply. I did have a\n&gt; ContentTypeMatchesRegExpDecideRule\n&gt;&gt;&gt;&gt;&gt; under the writer processor section with the following regex\n&gt;&gt;&gt;&gt;&gt; (?i)application/xml.*\n&gt;&gt;&gt;&gt;&gt; But it still seems to be writing the text/html pages from dynamic\n&gt;&gt;&gt;&gt;&gt; URLs. Did I use the wrong type of expression? Thanks.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, &quot;ermhes82&quot; &lt;ermhes@&gt;\n&gt; wrote:\n&gt;&gt;&gt;&gt;&gt;&gt; You can prevent this if you include in decide-rules a\n&gt;&gt;&gt;&gt;&gt;&gt; ContentTypeMatchesRegExpDecideRule o\n&gt;&gt;&gt;&gt;&gt;&gt; ContentTypeNotMatchesRegExpDecideRule.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Mario.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, &quot;lpeterus&quot; &lt;lpeterus@&gt;\n&gt; wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi all!\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; I have a question about how to exclude dynamic URLs. I&#39;m using\n&gt;&gt;&gt; regular\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; expression rules based on suggestions from previous posting. I\n&gt; would\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; like to filter out everything except for text/xml and\n&gt;&gt;&gt; application/xml\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; files. The filters are applied on the scope, midfetch, and the\n&gt; arc\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; writer processor.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; So far everything works ok, except it still dowloads some\n&gt; text/html\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; files even though html is part of the rejection regexp in the\n&gt; scope.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Turns out all of the text/html downloads are dynamic URLs like\n&gt;&gt;&gt; this,\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; http://somewebpage/subdir/?C=D;O=A\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; My question then is how do I prevent these text/html from being\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; written into the arc file. Thanks!\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Shawn\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt; \n&gt; \n\n"}}