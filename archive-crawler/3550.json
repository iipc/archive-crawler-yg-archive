{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"NQX8HlvaCZqPN5bFsnfMwliqTgAnkZwGkQLBTk3aOLQWh1YhohC3U8SvvvCQPvHaGNfbXjbNsi7IaR_PtKKRrag70TdViNf7","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Feature suggestion for the frontier","postDate":"1164130124","msgId":3550,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1NjMzNzRDLjYwOTAzMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGVqdGZycSs4cnRwQGVHcm91cHMuY29tPg==","referencesHeader":"PGVqdGZycSs4cnRwQGVHcm91cHMuY29tPg=="},"prevInTopic":3547,"nextInTopic":3551,"prevInTime":3549,"nextInTime":3551,"topicId":3547,"numMessagesInTopic":6,"msgSnippet":"Have you tried disabling the FrontierScheduler once the crawler goes past 120M fetched?  You might even be able to set the disabled flag from JMX.  If so,","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 95080 invoked from network); 21 Nov 2006 17:30:14 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m34.grp.scd.yahoo.com with QMQP; 21 Nov 2006 17:30:14 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.118)\n  by mta10.grp.scd.yahoo.com with SMTP; 21 Nov 2006 17:30:14 -0000\r\nReceived: by dns.duboce.net (Postfix, from userid 1008)\n\tid 06B9EC51D; Tue, 21 Nov 2006 08:10:14 -0800 (PST)\r\nX-Spam-Checker-Version: SpamAssassin 3.1.4 (2006-07-26) on dns.duboce.net\r\nX-Spam-Level: \r\nX-Spam-Status: No, score=-4.4 required=5.0 tests=ALL_TRUSTED,AWL,BAYES_00 \n\tautolearn=ham version=3.1.4\r\nReceived: from [192.168.1.10] (debord.duboce.net [192.168.1.10])\n\tby dns.duboce.net (Postfix) with ESMTP id B9802C519;\n\tTue, 21 Nov 2006 08:10:09 -0800 (PST)\r\nMessage-ID: &lt;4563374C.6090303@...&gt;\r\nDate: Tue, 21 Nov 2006 09:28:44 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.7) Gecko/20060910 SeaMonkey/1.0.5\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ejtfrq+8rtp@...&gt;\r\nIn-Reply-To: &lt;ejtfrq+8rtp@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Feature suggestion for the frontier\r\nX-Yahoo-Group-Post: member; u=168599281; y=S2ALLhnhm8gFsra6YdRd2wmllIrU0qS93ScEMuKdJOYg2fByE3TmKOmP\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nHave you tried disabling the FrontierScheduler once the crawler goes \npast 120M fetched?  You might even be able to set the disabled flag from \nJMX.  If so, since you can get counts of crawled and queued via JMX, you \ncould automate switching off the processor.\n\nOtherwise, do you have a patch for the below for us to apply Joe?\n\nSt.Ack\n\n\njoehung302 wrote:\n&gt;\n&gt; Hi,\n&gt;\n&gt; We&#39;ve been using heritrix with crawl map for crawling &gt; 1B urls. We\n&gt; current have 8 crawl machines at one time.\n&gt;\n&gt; Under this configuration, the crawl can only be successful if the\n&gt; crawlers don&#39;t fail. Hence we&#39;re really focused on how to keep the\n&gt; crawler running as long as possible.\n&gt;\n&gt; We thought it might be a good idea to *limit* bdb growth after\n&gt; certain size. And we also know that bdb size has a lot to do with\n&gt; the to-be-crawled URL list. Hence the question: Does it make sense\n&gt; to set a upper limit on the Frontier so, when the to-be-cralwed URL\n&gt; reached a certain number (say, 120M urls), stop collecting to-be-\n&gt; crawled and throw any new links away? The idea is, in order to\n&gt; download 120M URLs, we probably would have a to-be-crawled list of\n&gt; 400M, and bdb probably won&#39;t survive under that size.\n&gt;\n&gt; Does it make sense?\n&gt;\n&gt; cheers,\n&gt; -Joe\n&gt;\n&gt;  \n\n\n"}}