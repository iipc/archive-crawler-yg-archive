{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":274489225,"authorName":"Anmol Bhasin","from":"&quot;Anmol Bhasin&quot; &lt;anmol.bhasin@...&gt;","profile":"molzbh","replyTo":"LIST","senderId":"2qNUokRkoBAMLAYNuNCYQ6CwRonBvFZ7cr18axabsqoFlFo0Su6IafdErlI2LaPjTWFyEP-9OJIw5vbiZgHvDyUxlN5rZUMprOd46ybK","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Crawling 100 million pages","postDate":"1212790282","msgId":5300,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGI3NmUxZGYwODA2MDYxNTExajQ4ZDM3NjM0d2EzNzc4M2QwMjk2MjJmOTJAbWFpbC5nbWFpbC5jb20+","inReplyToHeader":"PDQ4NDlhYzk0LjExYmQ3MjBhLjQ2ZDcuNzI0OFNNVFBJTl9BRERFREBteC5nb29nbGUuY29tPg==","referencesHeader":"PDQ4NDk5QzMxLjkwNTA3MDJAYmF5YXJlYS5uZXQ+CSA8NDg0OWFjOTQuMTFiZDcyMGEuNDZkNy43MjQ4U01UUElOX0FEREVEQG14Lmdvb2dsZS5jb20+"},"prevInTopic":5299,"nextInTopic":5301,"prevInTime":5299,"nextInTime":5301,"topicId":5292,"numMessagesInTopic":11,"msgSnippet":"Hi John, Did you go for a focussed crawl or a broad-scope crawl? I believe Heritrix-slows down considerably in focussed crawling with a sequence of","rawEmail":"Return-Path: &lt;anmol.bhasin@...&gt;\r\nX-Sender: anmol.bhasin@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 92869 invoked from network); 6 Jun 2008 22:11:22 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m50.grp.scd.yahoo.com with QMQP; 6 Jun 2008 22:11:22 -0000\r\nX-Received: from unknown (HELO wf-out-1314.google.com) (209.85.200.173)\n  by mta15.grp.scd.yahoo.com with SMTP; 6 Jun 2008 22:11:22 -0000\r\nX-Received: by wf-out-1314.google.com with SMTP id 24so1652687wfg.23\n        for &lt;archive-crawler@yahoogroups.com&gt;; Fri, 06 Jun 2008 15:11:22 -0700 (PDT)\r\nX-Received: by 10.142.211.10 with SMTP id j10mr213184wfg.197.1212790282397;\n        Fri, 06 Jun 2008 15:11:22 -0700 (PDT)\r\nX-Received: by 10.143.123.3 with HTTP; Fri, 6 Jun 2008 15:11:22 -0700 (PDT)\r\nMessage-ID: &lt;b76e1df0806061511j48d37634wa37783d029622f92@...&gt;\r\nDate: Fri, 6 Jun 2008 15:11:22 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;4849ac94.11bd720a.46d7.7248SMTPIN_ADDED@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nContent-Disposition: inline\r\nReferences: &lt;48499C31.9050702@...&gt;\n\t &lt;4849ac94.11bd720a.46d7.7248SMTPIN_ADDED@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: &quot;Anmol Bhasin&quot; &lt;anmol.bhasin@...&gt;\r\nSubject: Re: [archive-crawler] Crawling 100 million pages\r\nX-Yahoo-Group-Post: member; u=274489225; y=qPnx9T4I4zvHiKnvL1e3kjOPXRxOrTy2-U5glz49uLST\r\nX-Yahoo-Profile: molzbh\r\n\r\nHi John,\n\nDid you go for a focussed crawl or a broad-scope crawl? I believe\nHeritrix-slows down considerably in focussed crawling with a sequence\nof decision-filters applied to queued URLs also, if we used SURT based\nPrefixes then the politeness considerations slow down crawling\nfurther.\n\nAnmol\n\nOn Fri, Jun 6, 2008 at 2:30 PM, Leo Dagum &lt;leo_dagum@...&gt; wrote:\n&gt; Hi John,\n&gt;\n&gt;\n&gt;\n&gt; What hardware does a crawler instance run on and what is your Internet b/w?\n&gt; 385M/3m from a single crawler instance is very impressive.\n&gt;\n&gt;\n&gt;\n&gt; - leo\n&gt;\n&gt;\n&gt;\n&gt; ________________________________\n&gt;\n&gt; From: archive-crawler@yahoogroups.com\n&gt; [mailto:archive-crawler@yahoogroups.com] On Behalf Of lekash\n&gt; Sent: Friday, June 06, 2008 1:21 PM\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Subject: Re: [archive-crawler] Crawling 100 million pages\n&gt;\n&gt;\n&gt;\n&gt; Hi there,\n&gt;\n&gt; I&#39;m like to very much encourage being careful about &#39;speeding up heritrix&#39;.\n&gt; Politeness is really important on the net. I keep doubling my inter\n&gt; crawl delay every\n&gt; year, and still people have problems.\n&gt;\n&gt; Its not a heretrix limit you will be looking at. Its a hardware\n&gt; capacity limit.\n&gt; (Though the most I&#39;ve gotten a single crawler to do is\n&gt; 385M over a three month period.)\n&gt; A crawler group, I&#39;ve gotten to 6B /year.\n&gt; So, 100 M /month is well within the operating range.\n&gt;\n&gt; The 1.x crawler seems to have a limit around 680M for a single crawler,\n&gt; like the queued numbers stop going up then. Never got there, so don&#39;t\n&gt; know what would happen. I haven&#39;t run a multi-billion one on the 2.x line,\n&gt; maybe next year.\n&gt;\n&gt; The things that run out of space are memory for java heap space, and\n&gt; something funny with the database. Re-writing scratch files in that\n&gt; seems to be what slows it down later, despite a still wide frontier.\n&gt;\n&gt; And of course, urls are not a consistent measure, as size varies widely.\n&gt; e.g. .gov sites are 10 times as dense in text/pdf as .com sites,\n&gt; and .com sites have all the visuals and movies. Depends on what\n&gt; you are crawling.\n&gt;\n&gt; John\n&gt;\n&gt; hijbul_bd wrote:\n&gt;\n&gt;&gt; Dear All\n&gt;&gt;\n&gt;&gt; I would like to crawl 100 million pages with in a month for crawling\n&gt;&gt; research. As far i know some research crawler(IRLbot(6 billion pages in\n&gt;&gt; 41 days), polybot(120 millions pages in 19 days)) can download huge\n&gt;&gt; pages in short amount of time wich is not open source. In 2005\n&gt;&gt; according to some blog site Heritrix can download about 20 miilion\n&gt;&gt; pages in a month. What is the speed of current Heritrix version and How\n&gt;&gt; can I speed up heritrix to download 100 million or at least 50 million\n&gt;&gt; pages with in a month. Are there any ohter open source crawler which\n&gt;&gt; can do this?\n&gt;&gt;\n&gt;&gt; Thanks in Advance\n&gt;&gt; Hijbul Alam\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt; \n\n\n\n-- \nCourage doesn&#39;t always roar. Sometimes courage is the quiet voice at\nthe end of the day saying, &quot;I will try again tomorrow&quot;\n\n\nAnmol Bhasin\nSSE Data Platform\nwww.linkedin.com\n\n"}}