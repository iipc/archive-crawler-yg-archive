{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":188703988,"authorName":"mark","from":"mark &lt;Mark.Williamson@...&gt;","replyTo":"LIST","senderId":"YSyvq7s1KcoxSngond81dz3fNmhyn79va-gapCGn7Lo8rNIyI3wBmIquMnzBBID9gLxqMao7Xa-fXSl-hsjLnM2Pvg","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Large experimental crawl - Too many open files","postDate":"1096304977","msgId":1038,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxNTg0OTUxLjQwOTA1MDFAYmwudWs+","inReplyToHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBEMTZAbG9raS5ib2suaGkuaXM+","referencesHeader":"PEQ5NTgxMTBCMjczQ0Q1MTFBQ0MxMDBCMEQwNzlBQTRFMDE5NjBEMTZAbG9raS5ib2suaGkuaXM+"},"prevInTopic":1027,"nextInTopic":0,"prevInTime":1037,"nextInTime":1039,"topicId":1005,"numMessagesInTopic":6,"msgSnippet":"Hi,. I m just doing a really broad crawl - broadscope, seed www.yahoo.com , path depth of 3 - everything else is default I think. Heritrix has lots of memory","rawEmail":"Return-Path: &lt;mark.williamson@...&gt;\r\nX-Sender: mark.williamson@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 18907 invoked from network); 27 Sep 2004 16:11:59 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m13.grp.scd.yahoo.com with QMQP; 27 Sep 2004 16:11:59 -0000\r\nReceived: from unknown (HELO nt-lonex2.bl.uk) (193.60.210.61)\n  by mta6.grp.scd.yahoo.com with SMTP; 27 Sep 2004 16:11:59 -0000\r\nReceived: from [194.66.228.182] (194.66.228.182 [194.66.228.182]) by nt-lonex2.bl.uk with SMTP (Microsoft Exchange Internet Mail Service Version 5.5.2657.72)\n\tid SYK57V6M; Mon, 27 Sep 2004 17:11:39 +0100\r\nMessage-ID: &lt;41584951.4090501@...&gt;\r\nDate: Mon, 27 Sep 2004 17:09:37 +0000\r\nUser-Agent: Mozilla Thunderbird 0.7.3 (X11/20040909)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;D958110B273CD511ACC100B0D079AA4E01960D16@...&gt;\r\nIn-Reply-To: &lt;D958110B273CD511ACC100B0D079AA4E01960D16@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Remote-IP: 193.60.210.61\r\nFrom: mark &lt;Mark.Williamson@...&gt;\r\nSubject: Re: [archive-crawler] Large experimental crawl - Too many open files\r\nX-Yahoo-Group-Post: member; u=188703988\r\n\r\nHi,.\n\nI&#39;m just doing a really broad crawl - broadscope, seed &quot;www.yahoo.com&quot;, \npath depth of 3 -\neverything else is default I think. Heritrix has lots of memory and its \nthe only thing on the machine\nat the moment.\nI&#39;m getting a lot of these errors:\n\n20040927170044568 -2 . #38 http://www.sun.com/partners/ . . . \nLRLLLLLLLLEXLLLLLLRRLLL http://www.sun.com/ java.net.SocketException: \nToo many open files at java.net.Socket.createImpl(Socket.java:331) at \njava.net.Socket.(Socket.java:304) at java.net.Socket.(Socket.java:124) \nat \norg.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:118) \nat \norg.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:686) \nat \norg.archive.httpclient.PatchedHttpClient.executeMethod(PatchedHttpClient.java:279) \nat \norg.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:529) \nat \norg.archive.crawler.fetcher.FetchHTTP.innerProcess(FetchHTTP.java:213) \nat org.archive.crawler.framework.Processor.process(Processor.java:106) \nat \norg.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:255) \nat org.archive.crawler.framework.ToeThread.run(ToeThread.java:138)\n\nHaven&#39;t looked into it carefully - just noticed that it wasn&#39;t as busy \nas it should be. Is there\nsomething I&#39;ve overlooked somewhere?\n\ncheers\n\nmark\n\n\nKristinn Sigurdsson wrote:\n\n&gt; Hi Sebastian,\n&gt;\n&gt; The main problem with running broad crawls (over a larger number of \n&gt; hosts) lies in the memory overhead associated with each active host.\n&gt;\n&gt; That is, each host that is being crawled requires memory, both for \n&gt; host specific information and the �top� of the per host queue of \n&gt; waiting URIs.\n&gt;\n&gt; One of the best way to combat this is to enable site first, limiting \n&gt; the number of active hosts to the bare minimum needed to fully occupy \n&gt; the crawler. Judging from your post I�m unsure if that is acceptiable \n&gt; to you since this will favor certain hosts until they are exhausted \n&gt; before moving on.\n&gt;\n&gt; Another thing that can be done to mitigate the memory use is to limit \n&gt; the number of URIs kept in memory for each host. This is done by \n&gt; setting the �host-queues-memory-capacity� setting to a smaller value. \n&gt; The default is 200, with a true broad crawl this is too much. I�d \n&gt; suggest something in the range of 20-50. A low value will incur more \n&gt; disk access but save memory.\n&gt;\n&gt; Other then the site-first and host-queues-memory-capacity there isn�t \n&gt; all that much you can to limit memory use by configuring Heritrix. I \n&gt; know the guys at the Archive are looking at some ideas for limiting \n&gt; memory use further but that is still a long way away.\n&gt;\n&gt; As for hardware and heap size. Well the latter is easy. Assign as \n&gt; large a heap as the hardware allows. I�ve been running on a macine \n&gt; with 1.5GB RAM and I usually assign 1.25GB to the java heap.\n&gt;\n&gt; As for hardware you�ll want plenty of memory (duh) and a fast \n&gt; processor also helps if you are concerned about the speed the crawl \n&gt; runs at. The crawl tends to be limited by the processing power \n&gt; availible. If you choose to limit memory use by lowering the \n&gt; host-queues-memory-capacity significantly a fast HD will be very \n&gt; useful. I�d suggest having different HD for state/scratch/log files \n&gt; and ARC files.\n&gt;\n&gt; - Kris\n&gt;\n&gt; -----Original Message-----\n&gt; *From:* Sebastian de Castelberg [mailto:sdecaste@...]\n&gt; *Sent:* 23. september 2004 09:32\n&gt; *To:* archive-crawler@yahoogroups.com\n&gt; *Subject:* [archive-crawler] Large experimental crawl\n&gt;\n&gt; Hi,\n&gt;\n&gt; for a research-project we have to implement two different random-walk\n&gt; algorithms for uniform page sampling. This needs us to gather about\n&gt; 2-4Mio. URL&#39;s.\n&gt; So we have to chose an adapted Broad Crawl, which chooses the URL&#39;s,\n&gt; which are fed back into the frontier, randomly. So the fetch queue\n&gt; wouldn&#39;t grow exponential. We also do not need to write the whole\n&gt; content to disk.\n&gt; Heritrix seemed to work quite well as crawler for antother project\n&gt; (thanks for the good development work at this place). But we got often\n&gt; problems, based on memory limitations.\n&gt;\n&gt; On the known limitations page, there&#39;s written that it is possible to\n&gt; crawl about 6Mio URL&#39;s and about 10000 hosts with default settings.\n&gt; My question is: What&#39;s the best setup to reach this number of URL&#39;s\n&gt; (HW/Java heap size/Heritrix config)?\n&gt; Do we need special hardware, or can it be done with a common pc (p4\n&gt; 2.6GHz 512-1024 MB ram)?\n&gt;\n&gt; We planned to use Debian GNU/Linux as os. Maybe there&#39;s someone who has\n&gt; already experiences with large-scaled crawls and can give me some hints.\n&gt;\n&gt; thanks\n&gt; sebastian de castelberg\n&gt;\n&gt;\n&gt;\n&gt; *Yahoo! Groups Sponsor*\n&gt; ADVERTISEMENT\n&gt; click here \n&gt; &lt;http://us.ard.yahoo.com/SIG=129gc3s8a/M=298184.5285298.6392945.3001176/D=groups/S=1705004924:HM/EXP=1096020304/A=2319498/R=0/SIG=11thfntfp/*http://www.netflix.com/Default?mqso=60185352&partid=5285298&gt; \n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\n"}}