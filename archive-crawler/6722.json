{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"W31JAZPs6-WdgLzTCgbsVNWENL-mhK-AxnYr_ygfyWwfRKMUtoQe8j3VEC97QdTKh1yP3Kc5pnMYi9ZHGE9YSffQ7-Ha1So","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Ideas to make the distributed crawling.","postDate":"1284491192","msgId":6722,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDOEZDN0I4LjIwNzA1MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGk2bzUwaytlaGI3QGVHcm91cHMuY29tPg==","referencesHeader":"PGk2bzUwaytlaGI3QGVHcm91cHMuY29tPg=="},"prevInTopic":6721,"nextInTopic":6725,"prevInTime":6721,"nextInTime":6723,"topicId":6719,"numMessagesInTopic":5,"msgSnippet":"It is the intent of the Heritrix design that any components, including the frontier implementation, should be swappable. So creating new frontier","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 99219 invoked from network); 14 Sep 2010 19:06:34 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m13.grp.re1.yahoo.com with QMQP; 14 Sep 2010 19:06:34 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.re1.yahoo.com with SMTP; 14 Sep 2010 19:06:34 -0000\r\nX-Received: (qmail 93807 invoked from network); 14 Sep 2010 19:06:33 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 14 Sep 2010 19:06:33 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4C8FC7B8.2070504@...&gt;\r\nDate: Tue, 14 Sep 2010 12:06:32 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.9) Gecko/20100825 Thunderbird/3.1.3\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Mackram Raydan &lt;mackram@...&gt;\r\nReferences: &lt;i6o50k+ehb7@...&gt;\r\nIn-Reply-To: &lt;i6o50k+ehb7@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Ideas to make the distributed crawling.\r\nX-Yahoo-Group-Post: member; u=137285340; y=TTufJbaZFId4TpC6gvWryRWZDaOVxizDaxEiqaB3QNBR\r\nX-Yahoo-Profile: gojomo\r\n\r\nIt is the intent of the Heritrix design that any components, including \nthe frontier implementation, should be swappable. So creating new \nfrontier implementations is a plausible approach.\n\nHowever, the frontier is the most complicated component, and many other \nfeatures are dependent on the behavior of current implementations. So, \noffering new variants (by subclassing or offering the Frontier \ninterface) should be done carefully.\n\nBacking up a bit: what is the primary goal that makes you interested in \na distributed approach? Why is one crawler not enough?\n\n(I have my own ideas about this but would like to hear yours.)\n\nHave you thoroughly understood (and run/benchmarked) the ad-hoc \ndistribution technique, making use of &#39;CrawlMapper&#39; subclass instances, \nthat&#39;s been described in past list traffic? What is good or bad about \nthat approach for your purposes?\n\n- Gordon @ IA\n\nOn 9/14/10 8:40 AM, Mackram Raydan wrote:\n&gt; So I have spent all day looking at the Heritrix code and the info on its architecture. The more I read the more I realized that doing a change in the code directly might not be the smartest thing to do as it would require a huge set of changes, so I had an idea that I wanted to run by the group if possible.\n&gt;\n&gt; Since Heritrix so easily supports modules I am thinking that I would develop 2 new Frontiers that will do the distributed crawling.\n&gt;\n&gt; The master Frontier (the distributed crawling I am envisioning has a central master) will pretty much do the same code done with the following exceptions:\n&gt; 1- A socket listener which will be able to dispatch new URIs to schedule, to mark URIs as finished and to send out new URIs to be crawled when requested\n&gt;\n&gt; The slave Frontier (for each instance of Heritrix) will be the simplest form of a Frontier which will do the following:\n&gt; 1- When requested for a next URI it will get it send a request for the master for a new URI\n&gt; 2- When a new URI is discovered and should be scheduled it is sent to the master\n&gt; 3- When a URI is finished the slave will inform the master of that state.\n&gt;\n&gt; My idea of the above is to keep the basic code of Heritrix unchanged and just allow the addition of distributed code through a new module. I would love input on the above especially if you think it would not work for some reason or the other.\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, Mackram Raydan&lt;mackram@...&gt;  wrote:\n&gt;&gt;\n&gt;&gt; Hey everyone,\n&gt;&gt;\n&gt;&gt; I have recently been looking into whether or not we can have heritrix run in\n&gt;&gt; a distributed manner and had opened a minor issue for it. Gordon was kind\n&gt;&gt; enough to point out the list here might be helpful. Also Noah was kind\n&gt;&gt; enough to point to the hcc project which as I understand is in the alpha\n&gt;&gt; stage. I was considering porting heritrixs to hadoop and would like the\n&gt;&gt; opinions of the group. My line of thought was to change the worker threads\n&gt;&gt; into map functions and then use reduce to manage the URLs. Arguably one does\n&gt;&gt; not really need map/reduce for this and could go more with the approach of\n&gt;&gt; the hcc project but honestly I am still going through heritrix code so, l\n&gt;&gt; would love the input from people on this.\n&gt;&gt;\n&gt;&gt; Thanks and best regards\n&gt;&gt;\n&gt;&gt; Mackram Raydan\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}