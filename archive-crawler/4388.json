{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"lP1fuOz1XYFbrIpAwPZk1etGTmL-qRQVImmo2DL8pKyIQnSwNqHsoGI_Lh3GvQInz88XAh7o-EiKENa33acqlwr3ONgxptnW","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Questions running multiple crawler&#39;s","postDate":"1183043778","msgId":4388,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2ODNEMEMyLjYwNTAwMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGY1dWZtbisyMWQ2QGVHcm91cHMuY29tPg==","referencesHeader":"PGY1dWZtbisyMWQ2QGVHcm91cHMuY29tPg=="},"prevInTopic":4376,"nextInTopic":0,"prevInTime":4387,"nextInTime":4389,"topicId":4376,"numMessagesInTopic":2,"msgSnippet":"Hello Badswalu, I can answer some of the questions. See bellow. ... Yes, you can have separate job directories. For example you can have: /instance1/jobs/job1/","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 69067 invoked from network); 28 Jun 2007 15:16:28 -0000\r\nReceived: from unknown (66.218.67.35)\n  by m51.grp.scd.yahoo.com with QMQP; 28 Jun 2007 15:16:28 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta9.grp.scd.yahoo.com with SMTP; 28 Jun 2007 15:16:28 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 221C81415FDD7\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 28 Jun 2007 08:16:24 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 21160-07-89 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tThu, 28 Jun 2007 08:16:23 -0700 (PDT)\r\nReceived: from [127.0.0.1] (nor75-24-88-170-99-175.fbx.proxad.net [88.170.99.175])\n\tby mail.archive.org (Postfix) with ESMTP id 2958F14156C7B\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 28 Jun 2007 08:16:22 -0700 (PDT)\r\nMessage-ID: &lt;4683D0C2.6050008@...&gt;\r\nDate: Thu, 28 Jun 2007 08:16:18 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (Windows/20070509)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f5ufmn+21d6@...&gt;\r\nIn-Reply-To: &lt;f5ufmn+21d6@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Questions running multiple crawler&#39;s\r\nX-Yahoo-Group-Post: member; u=137477665; y=1cOkcNNO0yLvh4RF1HZ9pTWNQw1BaafRhKJYyZeWGBrusW6ZMg\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHello Badswalu,\n\nI can answer some of the questions. See bellow.\n\n\n&gt; I want to start several heritrix crawlers, each in their own JVM\n&gt; running on different JMX ports and web GUI ports. I have noticed that\n&gt; if there are any uncrawled jobs in the jobs folder, that each Heritrix\n&gt; crawler I start will have those jobs pending (i am starting each\n&gt; crawler from the same source location).\n&gt;     Is there a way to run several Heritrix crawlers (each in their own\n&gt; JVM) on the same machine, without them sharing jobs?  i.e., Could each\n&gt; crawler have it&#39;s own &#39;jobs&#39; folder? How could that be set up? Would I\n&gt; have to create several different copies of the Heritrix executible?\n\nYes, you can have separate job directories. For example you can have:\n\n/instance1/jobs/job1/\n/instance2/jobs/job1/\n\nIn this case you would place order.xml files within these directories \nand start jobs from the commandline as in\n$: heritrix /instance1/jobs/instance1-job1/order.xml\n\nIf you don&#39;t want instances to share heritrix_out.log you can set \nHERITRIX_OUT env variable for each crawl. To follow the example from \nabove you can start a job as:\n$: HERITRIX_OUT=/instance1/jobs/job1/heritrix_out.log heritrix \n/instance1/jobs/instance1-job1/order.xml\n\nBy default all disk paths in order.xml are relative to the order file \nitself.\n\nOne heritrix executable should be OK.\n\n&gt; I am also trying to monitor or &quot;watch&quot; a crawler, and would like to do\n&gt; so from a java application. I am most interested in knowing when the\n&gt; crawler being watched is done crawling (isCrawling=false) so I can\n&gt; schedule another job with it. I would be monitoring from the same\n&gt; machine the crawler is on, and would like to monitor more than one\n&gt; crawler, each in it&#39;s own JVM. \n&gt;     Is there a good way to do this? Is there a way my application\n&gt; could receive a notification from the crawler when it is done, or do I\n&gt; have to manually pole the crawler waiting for isCrawling=false?\n\nYou might want to look at HCC (The Heritrix Cluster Controller). I have \nnot been using it myself so I cannot help much. There is a recent \nposting that might be a good starting point. See\nhttp://tech.groups.yahoo.com/group/archive-crawler/message/4379\n\n&gt; My last question has to do with the number of URL&#39;s submitted as\n&gt; Seeds.  How many seeds are too many for one job? How many seeds\n&gt; can/will heritrix try to crawl simultainously from one job?\n\nIn theory it could be very large. I have used seed lists up to 2 million \nwithout any problems. There is also an option to import uris to heritrix \n(as seeds or not) once is running. I have done this with batches of tens \nof millions of non-seed URIs.\n\nBy default, Heritrix is setup to run with 200 simultaneous threads. So, \nin the lamest terms it would crawl 200 different hosts simultaneously. \nHaving more than 200 threads is rarely useful but that depends on \nhardware you have. More about threads (known as ToeThreads) can be found \nin the user manual (crawler.archive.org). User manual&#39;s Glossary is \nprobably a good start.\n\nI hope this helps.\n\nTake care,\ni.\n\n\n\n&gt; \n&gt; Thank you,\n&gt; -badswalu\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}