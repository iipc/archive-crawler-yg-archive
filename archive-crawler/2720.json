{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":256346715,"authorName":"Adam Fisk","from":"&quot;Adam Fisk&quot; &lt;adamfisk@...&gt;","profile":"afisk3","replyTo":"LIST","senderId":"SlRohMiKSHb_GWhBpFKemb83OAY9rcsqeFm-l7KQ42pVqO7YDBnvU7hU7KlITvSV1723sfm7N6VgwoNR_dMKJ0JBb5rR-vI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: OutOfMemoryError on small crawl","postDate":"1141160221","msgId":2720,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGR1MmRldCtsY2p0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0MDRBOUJCLjYwNjA0MDVAYXJjaGl2ZS5vcmc+"},"prevInTopic":2719,"nextInTopic":0,"prevInTime":2719,"nextInTime":2721,"topicId":2709,"numMessagesInTopic":7,"msgSnippet":"OK great -- the Berkeley DB inclusion certainly makes a lot of sense. I m using Heritrix 1.6.0 and the JVM is build 1.5.0_05-b05.  The machine is a 64-bit","rawEmail":"Return-Path: &lt;adamfisk@...&gt;\r\nX-Sender: adamfisk@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 57202 invoked from network); 28 Feb 2006 20:57:42 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m28.grp.scd.yahoo.com with QMQP; 28 Feb 2006 20:57:42 -0000\r\nReceived: from unknown (HELO n9a.bullet.scd.yahoo.com) (66.94.237.43)\n  by mta7.grp.scd.yahoo.com with SMTP; 28 Feb 2006 20:57:42 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.2] by n9.bullet.scd.yahoo.com with NNFMP; 28 Feb 2006 20:57:03 -0000\r\nReceived: from [66.218.66.78] by t2.bullet.scd.yahoo.com with NNFMP; 28 Feb 2006 20:57:03 -0000\r\nDate: Tue, 28 Feb 2006 20:57:01 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;du2det+lcjt@...&gt;\r\nIn-Reply-To: &lt;4404A9BB.6060405@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Adam Fisk&quot; &lt;adamfisk@...&gt;\r\nSubject: Re: OutOfMemoryError on small crawl\r\nX-Yahoo-Group-Post: member; u=256346715; y=yFvdTasm4SNtpAvgp7mjy7gzl8jyNtEJqeKEO6Q5XLVr\r\nX-Yahoo-Profile: afisk3\r\n\r\nOK great -- the Berkeley DB inclusion certainly makes a lot of sense.  \nI&#39;m=\r\n using Heritrix 1.6.0 and the JVM is build 1.5.0_05-b05.  The \nmachine is a=\r\n 64-bit Xeon, but it&#39;s running a 32-bit kernel and JVM.\n\nInteresting on the=\r\n point about when you just run out of patience.  Any \nidea when folks like =\r\ngoogle run out of patience?  When do you guys run \nout of patience in your =\r\nown crawls?  I guess both you and Google must \njust eventually stop without=\r\n getting everything, is that right?  \n\nWe&#39;d ideally like to crawl every pag=\r\ne on these sites, but we need to \ncrawl several hundred sites all within 1 =\r\nmonth windows.  This could be \na dubious proposition given that a 3 day cra=\r\nwl did not complete for \nthe 20 test sites I&#39;m running.  The way it looks t=\r\no me, it would be \nimpossible to even crawl every page on nytimes.com withi=\r\nn a month with \nthe default politeness rules.  Does that sound accurate to =\r\nyou?\n\nThat said, I&#39;ve played with the politeness settings, putting them now=\r\n \nat:\n\n&lt;float name=3D&quot;delay-factor&quot;&gt;1.0&lt;/float&gt;\n&lt;integer name=3D&quot;max-delay-=\r\nms&quot;&gt;10000&lt;/integer&gt;\n&lt;integer name=3D&quot;min-delay-ms&quot;&gt;200&lt;/integer&gt;\n\nDoes that=\r\n sound overly aggressive?  Could I be more aggressive? \n\nFinally, I&#39;m tryin=\r\ng to estimate the size of these sites using simple \nGoogle searches like &quot;s=\r\nite: nytimes.com&quot;.  Do you think the numbers \nGoogle returns here are accur=\r\nate?  I&#39;ve also tried things like &quot;site: \nnytimes.com (filetype: html | fil=\r\netype: htm)&quot;, but that restricts \nresults only to urls ending in those file=\r\n types, not apparently taking \nContent-Type into account, so it misses most=\r\n html pages on most sites.\n\nDo you guys have any other tricks for estimatin=\r\ng the number of pages \non specific sites, in my case particularly for html?=\r\n  Possibly some \nAlexa magic?  I&#39;m basically trying to get a handle on the =\r\nscope of \nthis problem and am having devil of a time doing it.\n\nThanks so m=\r\nuch.\n\n-Adam\n\n\n\n--- In archive-crawler@yahoogroups.com, &quot;Gordon Mohr (archiv=\r\ne.org)&quot; \n&lt;gojomo@...&gt; wrote:\n&gt;\n&gt; Adam,\n&gt; \n&gt; FYI, I never received your seco=\r\nnd message on this thread (the one \nyou \n&gt; quote yourself, &quot;Yes, our heap s=\r\nize is definitely capped...&quot;) -- not \n&gt; even caught by junk filters. It is =\r\nin the list archives, though.\n&gt; \n&gt; The memory footprint in long runs should=\r\n not generally be a function \n&gt; of how many URIs have been crawled or are p=\r\nending. Such records are \n&gt; all kept in BerkeleyDB-JE structures, and those=\r\n are capped at using \n&gt; the assigned amount of JE cache memory (default 60%=\r\n of heap). So no \n&gt; matter how many URIs are crawled, that shouldn&#39;t trigge=\r\nr an OOME.\n&gt; \n&gt; ToeThreads each have a constant amount of memory overhead, =\r\nand then \n&gt; more as each URI is processed, but their consumption should not=\r\n grow \n&gt; over time.\n&gt; \n&gt; Memory usage will grow with respect to the number =\r\nof queues ever \n&gt; encountered, so broad crawls (or domain crawls where ther=\r\ne are many, \n&gt; many subdomains) tend to use more memory. I believe some of =\r\nthe \n&gt; statistics structures will grow with the number of categories \ntrack=\r\ned \n&gt; (hosts encountered / MIME types encountered / etc.) so some crawls \no=\r\nr \n&gt; pathological servers could cause problems there. But, I don&#39;t think \n&gt;=\r\n any of those would apply to your crawl, from how you&#39;ve described \nit.\n&gt; \n=\r\n&gt; What version of Heritrix are you using, and what JVM? Those aren&#39;t \n&gt; 64-=\r\nbit Xeons by any chance, are they? (If so, see the release note \nat \n&gt; &lt;htt=\r\np://crawler.archive.org/articles/releasenotes.html#bdb_64bit&gt;).\n&gt; \n&gt; It&#39;s h=\r\nard to say what&#39;s typical for website size -- &quot;most&quot; sites (by \n&gt; hostname)=\r\n are tiny, but plenty of sites (especially media and \n&gt; government sites) h=\r\nave many hundreds of thousands of legitimate \nURLs. \n&gt; Also, given both dyn=\r\namic content and traps/chaff, the &#39;size&#39; of a \nsite \n&gt; is often simply when=\r\n you run out of patience/interest in crawling \nany \n&gt; deeper. I don&#39;t know =\r\nwhat kinds of sites you&#39;re crawling, but your \n&gt; numbers don&#39;t seem atypica=\r\nl.\n&gt; \n&gt; - Gordon @ IA\n&gt; \n&gt; \n&gt; Adam Fisk wrote:\n&gt; &gt; This is running well now=\r\n, just for anyone else searching through \nthe \n&gt; &gt; forum.  I first increase=\r\nd the mx to 256MB and ran out of memory \nagain. \n&gt; &gt;  I then bumped it up t=\r\no 600MB, and the crawl has been running for \n&gt; &gt; about 3 days with about 80=\r\n0,000 html pages downloaded.\n&gt; &gt; \n&gt; &gt; Does my volume sound pretty typical t=\r\no other folks out there?  I&#39;m \n&gt; &gt; only crawling 20 sites, but they include=\r\n sites like nytimes.com.  \n&gt; &gt; That&#39;s about 40,000 html pages per site, and=\r\n the crawl&#39;s still not \n&gt; &gt; done.  Is that a pretty typical size for larger=\r\n sites out there?\n&gt; &gt; \n&gt; &gt; Thanks very much.\n&gt; &gt; \n&gt; &gt; -Adam\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; -=\r\n-- In archive-crawler@yahoogroups.com, &quot;Adam Fisk&quot; &lt;adamfisk@&gt; \n&gt; &gt; wrote:\n=\r\n&gt; &gt; \n&gt; &gt;&gt;Thanks Gordon-\n&gt; &gt;&gt;\n&gt; &gt;&gt;Yes -- our heap size is definitely capped =\r\nat the Java default.  I\n&gt; &gt;&gt;can&#39;t go much higher than 256 on this machine, =\r\nbut I&#39;ll see what I \n&gt; &gt; \n&gt; &gt; get\n&gt; &gt; \n&gt; &gt;&gt;with that.  I just didn&#39;t want t=\r\no mask another problem by going \n&gt; &gt; \n&gt; &gt; ahead\n&gt; &gt; \n&gt; &gt;&gt;and cranking the m=\r\nemory, especially for such a small crawl.  Those \n&gt; &gt; \n&gt; &gt; 20\n&gt; &gt; \n&gt; &gt;&gt;were=\r\n bringing in upwards of 40,000 raw text/html pages, though, so \nI\n&gt; &gt;&gt;guess=\r\n memory issues are to be expected.\n&gt; &gt;&gt;\n&gt; &gt;&gt;Where does most of the memory g=\r\no?  I assume the ToeThreads \n&gt; &gt; \n&gt; &gt; accumulate\n&gt; &gt; \n&gt; &gt;&gt;a lot of data ove=\r\nr time?  We&#39;re going to be doing much larger \ncrawls\n&gt; &gt;&gt;soon (hundreds of =\r\nsites), and we&#39;d prefer not to dedicate a full\n&gt; &gt;&gt;machine to this task, bu=\r\nt it looks like we might have to.\n&gt; &gt;&gt;\n&gt; &gt;&gt;Thanks again.\n&gt; &gt;&gt;\n&gt; &gt;&gt;-Adam\n&gt; &gt;=\r\n&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt;--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@&gt; \nw=\r\nrote:\n&gt; &gt;&gt;\n&gt; &gt;&gt;&gt;It looks like your heap is capped at a maximum size of 64MB=\r\n -- \n&gt; &gt;&gt;&gt;that&#39;s the Java default in the absence of any -Xmx setting in \nJa=\r\nva \n&gt; &gt;&gt;&gt;1.4 and previous -- so your 2GB of RAM isn&#39;t doing the crawler \nan=\r\ny \n&gt; &gt;&gt;&gt;good.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;Use of -Xmx is definitely indicated; if the machin=\r\ne is dedicated \n&gt; &gt;&gt;&gt;to crawling, and you want the crawler to be able to us=\r\ne all the \n&gt; &gt;&gt;&gt;RAM, -Xmx1500m would be justified.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;(It&#39;s probabl=\r\ny possible to crawl 20 hosts in 64MB, if there isn&#39;t \n&gt; &gt;&gt;&gt;an explosion of =\r\nsubdomains, and you use only a small number of \n&gt; &gt;&gt;&gt;threads -- but I doubt=\r\n that&#39;s a real constraint you want to try \nto \n&gt; &gt;&gt;&gt;live within.)\n&gt; &gt;&gt;&gt;\n&gt; &gt;=\r\n&gt;&gt;The &#39;max-depth&#39; and &#39;average-depth&#39; readings on that status line \n&gt; &gt;&gt;&gt;(a=\r\nnd in the crawler console) refer to the size of queues: \n&gt; &gt;&gt;&gt;&#39;max-depth&#39; i=\r\ns the longest queue in the frontier, &#39;average-depth&#39; \n&gt; &gt;&gt;&gt;is the average o=\r\nf all queue sizes.\n&gt; &gt;&gt;&gt;\n&gt; &gt;&gt;&gt;- Gordon @ IA\n&gt; &gt;&gt;\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt;=\r\n &gt; \n&gt; &gt;  \n&gt; &gt; Yahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n\n=\r\n\n"}}