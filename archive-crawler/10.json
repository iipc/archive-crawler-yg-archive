{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"&quot;Gordon Mohr&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"q3pXXRPGQoLiiEjfkrUVhH0qMFTGH_2BfL1toxtDAqFNTxe_KZ92v6JMvao4oStFeYxDWPLd0y-c3iVpEIPtuQq6LK7kyJFiKw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Crawler engineering review kickoff - 10am Friday","postDate":"1045844261","msgId":10,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAwNDMwMWMyZDljNCRjNWI0NzJiMCQ2NTBhMDAwYUBnb2xkZW4+","referencesHeader":"PDUuMi4wLjkuMC4yMDAzMDIyMDIwNTAyMi4wMGI3ZmMyOEBtYWlsLmFyY2hpdmUub3JnPiA8NS4yLjAuOS4wLjIwMDMwMjIxMDc1MjAxLjAyMjQ1NzM4QG1haWwuYXJjaGl2ZS5vcmc+"},"prevInTopic":9,"nextInTopic":11,"prevInTime":9,"nextInTime":11,"topicId":8,"numMessagesInTopic":4,"msgSnippet":"I don t think we can build the best mega-scale crawler until after we ve built a really good, modular, efficient small-scale crawler. That s how the existing","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (EGP: mail-8_2_3_4); 21 Feb 2003 16:17:45 -0000\r\nReceived: (qmail 67029 invoked from network); 21 Feb 2003 16:17:45 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m13.grp.scd.yahoo.com with QMQP; 21 Feb 2003 16:17:45 -0000\r\nReceived: from unknown (HELO mail.archive.org) (209.237.232.3)\n  by mta2.grp.scd.yahoo.com with SMTP; 21 Feb 2003 16:17:45 -0000\r\nReceived: from golden (adsl-67-119-25-219.dsl.snfc21.pacbell.net [67.119.25.219])\n\tby mail.archive.org (8.10.2/8.10.2) with SMTP id h1LFeNm23469;\n\tFri, 21 Feb 2003 07:40:23 -0800\r\nMessage-ID: &lt;004301c2d9c4$c5b472b0$650a000a@golden&gt;\r\nTo: &quot;Brewster Kahle&quot; &lt;brewster@...&gt;\r\nCc: &lt;archive-crawler@yahoogroups.com&gt;\r\nReferences: &lt;5.2.0.9.0.20030220205022.00b7fc28@...&gt; &lt;5.2.0.9.0.20030221075201.02245738@...&gt;\r\nSubject: Re: Crawler engineering review kickoff - 10am Friday\r\nDate: Fri, 21 Feb 2003 08:17:41 -0800\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2800.1106\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1106\r\nFrom: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nI don&#39;t think we can build the best mega-scale crawler until after\nwe&#39;ve built a really good, modular, efficient small-scale crawler.\nThat&#39;s how the existing big crawlers reached their current state,\nand while we can leverage their wisdom a lot, without large swaths\nof their source code we&#39;ve got to take an incremental path.\n\nIf you look over the requirements, they&#39;re very ambitious -- with\nthe eventual software surpassing anything currently available to\nus (or the public), and probably exceeding in many respects even\nthe capabilities of the state-of-the-art private, proprietary\ncrawlers.\n\n- Gordon\n\n----- Original Message -----\nFrom: &quot;Brewster Kahle&quot; &lt;brewster@...&gt;\nTo: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\nCc: &lt;archive-crawler@yahoogroups.com&gt;\nSent: Friday, February 21, 2003 7:53 AM\nSubject: Re: Crawler engineering review kickoff - 10am Friday\n\n\n&gt;\n&gt;\n&gt; Maybe I am missing something.    what is the point of writing something that wont help us do something big?      we have the\nmercator crawler for doing little things.\n&gt;\n&gt; is it a way to practice coding java?\n&gt;\n&gt; -brewster\n&gt;\n&gt;\n&gt;\n&gt; At 11:15 PM 2/20/2003 -0800, Gordon Mohr wrote:\n&gt; &gt;[cc&#39;d to the archive-crawler@yahoogroups.com discussion list]\n&gt; &gt;\n&gt; &gt;These are all important matters to address -- and for most of these issues,\n&gt; &gt;I think there will be three overlapping answers:\n&gt; &gt;\n&gt; &gt;  (1) what range of choices are enabled by the design;\n&gt; &gt;  (2) what choice we make in a first, simple, proof-of-design\n&gt; &gt;      crawler;\n&gt; &gt;...and ultimately...\n&gt; &gt;  (3) what choice we make in the eventual high-performance,\n&gt; &gt;      multi-machine, whole-web, max-any-downlink crawler\n&gt; &gt;\n&gt; &gt;Considering a few of the specifics:\n&gt; &gt;&gt;    * how to prioritize the urls to crawl.   maybe we could list the different ways current crawlers work as a guideline for\nsome\n&gt; &gt;of the options.\n&gt; &gt;&gt;         we know xyleme and alexa and mercator.\n&gt; &gt;&gt;       gordon, maybe you could write up a short summary as a way to understand the options.\n&gt; &gt;\n&gt; &gt;Common choices have included:\n&gt; &gt;\n&gt; &gt;  - a breadth-first traversal of the seed URI set and discovered URIs\n&gt; &gt;  - a depth-first traversal (within politeness and site-saturation constraints)\n&gt; &gt;  - descending estimated value of unvisited URIs, where estimated value is...\n&gt; &gt;    - number of inlinks\n&gt; &gt;    - classic Google PageRank (where inlinks from high-inlink pages are worth more)\n&gt; &gt;    - apparent popularity by user visits (eg Alexa traffic data)\n&gt; &gt;    - weighted by URI contents or content-analysis of the inlink sources\n&gt; &gt;\n&gt; &gt;Into these can be mixed other measures of a page&#39;s importance or\n&gt; &gt;implied volativility -- to influence how often something is recrawled.\n&gt; &gt;\n&gt; &gt;Because of the wide range of anticipated users, the crawler design must\n&gt; &gt;be capable of having any of these policies -- or weighted combinations of\n&gt; &gt;these policies -- plugged in, either by configuration file or code\n&gt; &gt;modules. (&quot;Answer #1&quot;)\n&gt; &gt;\n&gt; &gt;For our first instantiation, simpleminded swappable breadth-first and\n&gt; &gt;depth-first policies will suffice for verifying the design and initial\n&gt; &gt;testing. (&quot;Answer #2&quot;)\n&gt; &gt;\n&gt; &gt;For the eventual high-performance, mega-scale crawler, a more\n&gt; &gt;sophisticated ranking for ensuring &quot;more important&quot; pages are\n&gt; &gt;fetched before (or more often than) other pages will eventually\n&gt; &gt;be devised. Some research indicates breadth-first is pretty good\n&gt; &gt;at approximating the (much harder to compute) inlink and PageRank\n&gt; &gt;approaches. We&#39;ll have to tinker over time. (&quot;Answer #3&quot;)\n&gt; &gt;\n&gt; &gt;&gt;    * how tasks will be broken up on different machines.  for instance:\n&gt; &gt;&gt;        1.  different machines for different stages of processing, or\n&gt; &gt;&gt;        2.  a host does all stages, but urls are broken up onto different machines some way\n&gt; &gt;\n&gt; &gt;The second approach -- distribution to relatively full-fledged hosts\n&gt; &gt;by URI ranges -- seems to dominate at first (it&#39;s straightforward), but\n&gt; &gt;then at  really large scales there may be opportunities for specialized\n&gt; &gt;machines to take over tough subproblems.\n&gt; &gt;\n&gt; &gt;A nice part of the stages-and-queues model is that every stage\n&gt; &gt;transition, through a queue, can potentially travel to another machine\n&gt; &gt;in a clean fashion. So breaking up the process &quot;horizontally&quot; (a URI\n&gt; &gt;lifecycle per machine) or &quot;vertically&quot; (a URI lifecycle spans many\n&gt; &gt;machines) or a mixture of the two are all possible.\n&gt; &gt;\n&gt; &gt;&gt;    *  will we use an RAM based system or disk based (for the url list)\n&gt; &gt;&gt;        (xyleme and mercator used RAM,    (xyleme said this was a big limitation on their system)\n&gt; &gt;&gt;          alexa does not)\n&gt; &gt;&gt;        I dont know if you can buy cost-effective large ram machines.\n&gt; &gt;&gt;        talking with Jad at alexa, he said 2GB is cheap otherwise very expensive (this would be\n&gt; &gt;&gt;        based on the HP line).\n&gt; &gt;\n&gt; &gt;Again, swappable strategies will be enabled, and I think our choice will\n&gt; &gt;change over time, starting with a simple RAM-based approach to get\n&gt; &gt;the crawler testable for small crawls, transitioning to a disk-based\n&gt; &gt;system when we get to web-wide crawls.\n&gt; &gt;\n&gt; &gt;&gt;    *  how can remote coordination be done between crawlers that are geographically far apart\n&gt; &gt;\n&gt; &gt;This is an interesting question, but right now seems pretty far down\n&gt; &gt;the priority scale. I recall Paula at Alexa saying they were capable of\n&gt; &gt;doing this -- but didn&#39;t really use that capability much. This also\n&gt; &gt;may not be much different from the multiple-machines-in-same-location\n&gt; &gt;situation, if the method by which proximate machines coordinate their\n&gt; &gt;actions isn&#39;t especially bandwidth and low-latency intensive.\n&gt; &gt;\n&gt; &gt;&gt;    * what the thread/process model should be for processing the urls.   separate processes or threads\n&gt; &gt;&gt;       in a single process.\n&gt; &gt;\n&gt; &gt;Unsure exactly what you mean; but with the assumption that we&#39;re using Java,\n&gt; &gt;it will make sense to keep all related activities in the same running\n&gt; &gt;JVM, and split tasks among Java threads.\n&gt; &gt;\n&gt; &gt;Even with a relatively simple/traditional structure -- one thread works\n&gt; &gt;its way through the whole process of handling one URI, blocking socket\n&gt; &gt;I/O -- using a lot of Java threads can achieve pretty good throughput.\n&gt; &gt;We&#39;re likely to make use of the new nonblocking socket support, and\n&gt; &gt;decompose the process into bite-sized nonblocking stages, so that a\n&gt; &gt;smaller number of threads can scream through the workload with minimal\n&gt; &gt;scheduling overhead.\n&gt; &gt;\n&gt; &gt;&gt;    * duplicate site detection techniques\n&gt; &gt;\n&gt; &gt;I want to enable hash-based content-body duplication detection to the\n&gt; &gt;greatest extent possible. Detecting when whole sites are exactly or\n&gt; &gt;nearly alike is a challenge for further down the road; at least with\n&gt; &gt;content-body duplicate detection, the storage cost of revisiting entire\n&gt; &gt;exact-mirror sites remains very small.\n&gt; &gt;\n&gt; &gt;&gt;    * session-id detection and handling\n&gt; &gt;\n&gt; &gt;This is an area where there seems to be a lot of folk wisdom and\n&gt; &gt;hand-tuning (eg the Alexa URL purification regexp rules) but not\n&gt; &gt;much documentation.\n&gt; &gt;\n&gt; &gt;I&#39;ve started a tracking database at our Sourceforge project for\n&gt; &gt;capturing exactly these kind of real-web tricky/nonobvious challenges\n&gt; &gt;for crawlers and retrieval tools -- so that issues and potential\n&gt; &gt;fixes don&#39;t just get hidden in the code (and mind) of the first\n&gt; &gt;person to encounter them.\n&gt; &gt;\n&gt; &gt;&gt;    * what user-agent we use.\n&gt; &gt;\n&gt; &gt;I think we need to pick a name for the software package better than\n&gt; &gt;&quot;archive-crawler&quot; -- and that name should be featured in the User-Agent\n&gt; &gt;string during official Archive crawls.\n&gt; &gt;\n&gt; &gt;Some crawl applications/customers will need to change User-Agent\n&gt; &gt;arbitrarily, so that will also be supported.\n&gt; &gt;\n&gt; &gt;- Gordon\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;----- Original Message -----\n&gt; &gt;From: &quot;Brewster Kahle&quot; &lt;brewster@...&gt;\n&gt; &gt;To: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;; &quot;Brad Tofel&quot; &lt;brad@...&gt;; &quot;Igor Ranitovic&quot; &lt;igor@...&gt;;\n&lt;michele@...&gt;\n&gt; &gt;Cc: &quot;Raymie Stata&quot; &lt;raymie@...&gt;; &lt;gojomo@...&gt;\n&gt; &gt;Sent: Thursday, February 20, 2003 9:08 PM\n&gt; &gt;Subject: Re: Crawler engineering review kickoff - 10am Friday\n&gt; &gt;\n&gt; &gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; thank you for pulling this together.\n&gt; &gt;&gt;\n&gt; &gt;&gt; A few other design decisions that seem to be important at this stage:\n&gt; &gt;&gt;\n&gt; &gt;&gt;    * how to prioritize the urls to crawl.   maybe we could list the different ways current crawlers work as a guideline for\nsome\n&gt; &gt;of the options.\n&gt; &gt;&gt;         we know xyleme and alexa and mercator.\n&gt; &gt;&gt;       gordon, maybe you could write up a short summary as a way to understand the options.\n&gt; &gt;&gt;\n&gt; &gt;&gt;    * how tasks will be broken up on different machines.  for instance:\n&gt; &gt;&gt;        1.  different machines for different stages of processing, or\n&gt; &gt;&gt;        2.  a host does all stages, but urls are broken up onto different machines some way\n&gt; &gt;&gt;\n&gt; &gt;&gt;    *  will we use an RAM based system or disk based (for the url list)\n&gt; &gt;&gt;        (xyleme and mercator used RAM,    (xyleme said this was a big limitation on their system)\n&gt; &gt;&gt;          alexa does not)\n&gt; &gt;&gt;        I dont know if you can buy cost-effective large ram machines.\n&gt; &gt;&gt;        talking with Jad at alexa, he said 2GB is cheap otherwise very expensive (this would be\n&gt; &gt;&gt;        based on the HP line).\n&gt; &gt;&gt;\n&gt; &gt;&gt;    *  how can remote coordination be done between crawlers that are geographically far apart\n&gt; &gt;&gt;\n&gt; &gt;&gt;    * what the thread/process model should be for processing the urls.   separate processes or threads\n&gt; &gt;&gt;       in a single process.\n&gt; &gt;&gt;\n&gt; &gt;&gt;    * duplicate site detection techniques\n&gt; &gt;&gt;\n&gt; &gt;&gt;    * session-id detection and handling\n&gt; &gt;&gt;\n&gt; &gt;&gt;    * what user-agent we use.\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; -brewster\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt; &gt;&gt; At 06:13 PM 2/20/2003 -0800, Gordon Mohr wrote:\n&gt; &gt;&gt; &gt;Brad, Brewster, Igor:\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;Key points to cover in our crawler engineering review meeting\n&gt; &gt;&gt; &gt;(tomorrow morning at 10am) are listed below.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;Most important will be going over the general requirements,\n&gt; &gt;&gt; &gt;so if you have a chance to look over the first document referenced\n&gt; &gt;&gt; &gt;below before the meeting, that&#39;d be ideal.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;We lose Brewster to another meeting at 11am -- but it&#39;s early in\n&gt; &gt;&gt; &gt;the project and we should try to keep discussion to broad issues\n&gt; &gt;&gt; &gt;rather than details, so I&#39;m hopeful we&#39;ll get through what we\n&gt; &gt;&gt; &gt;need to in an hour.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;Agenda:\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;(1) General crawler requirements\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  This document is a starting point:\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  http://homeserver.archive.org/webprojects/crawler-requirements.htm\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Does this breakdown cover everything it should? What are\n&gt; &gt;&gt; &gt;  the relative priorities of all the requirements -- in\n&gt; &gt;&gt; &gt;  both urgency and long-term importance?\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;(2) Key early design decisions\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Using Java\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Crawler modeled as loosely-linked program stages connected\n&gt; &gt;&gt; &gt;  by queues.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Top-level stages:\n&gt; &gt;&gt; &gt;   -&gt; URIChoosing    (what URI next?)\n&gt; &gt;&gt; &gt;   &#92;- Preprocessing  (robots/politeness/etc.)\n&gt; &gt;&gt; &gt;   &#92;_ Fetching       (HTTP)\n&gt; &gt;&gt; &gt;   &#92;_ Postprocessing (analysis/archival)\n&gt; &gt;&gt; &gt;   &#92;_ URIStoring     (return URI(s) to pool for revisiting later)\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  First priority: dumb, bad single-machine crawler -- then\n&gt; &gt;&gt; &gt;  incremental extension to better, multi-machine crawler.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;(3) Chosen tools and processes\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  &quot;archive-crawler&quot; Yahoo Group for team discussion and file sharing:\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;    http://groups.yahoo.com/group/archive-crawler/\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  &quot;archive-crawler&quot; Sourceforge project for source code/CVS hosting\n&gt; &gt;&gt; &gt;  and engineering-issue tracking:\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;    http://sourceforge.net/projects/archive-crawler\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Overview design documents, plus Javadoc extracted from skeleton/\n&gt; &gt;&gt; &gt;  actual code files.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Engineering review meetings (like this one) every few weeks to\n&gt; &gt;&gt; &gt;  get feedback and report progress.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Others needed or recommended?\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;(4) Test plans\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Unit/stage conformance tests alongside coding\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Acceptable output tests using existing ARC processing tools\n&gt; &gt;&gt; &gt;  and compared on &quot;test crawls&quot; against historic data or data\n&gt; &gt;&gt; &gt;  obtained by other crawlers.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;  Coordination with acceptance rules/processes established\n&gt; &gt;&gt; &gt;  by the national libraries and project agreement.\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;(5) Everything else (?)\n&gt; &gt;&gt; &gt;\n&gt; &gt;&gt; &gt;- Gordon\n&gt; &gt;&gt;\n&gt; &gt;&gt;\n&gt;\n&gt;\n\n\n"}}