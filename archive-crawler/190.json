{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163922992,"authorName":"John Erik Halse","from":"John Erik Halse &lt;johnh@...&gt;","profile":"johnerikhalse","replyTo":"LIST","senderId":"RAm7eqA7zCGof1V2Rp7v5q9R3_oZbeAZCFaQHJoBRhtiQc7uTU_FfUj66PjDVNwKvDHaseLl2bCWXUtjSlblKEfR7VBr6GHfXnQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"[Fwd: Re: First draft of per host settings document]","postDate":"1070318635","msgId":190,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEwNzAzMTg2MzUuMjU4NS4xLmNhbWVsQGIxMTYtZHluLTM3LmFyY2hpdmUub3JnPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":189,"nextInTime":191,"topicId":190,"numMessagesInTopic":1,"msgSnippet":"I m forwarding this response from Gordon to the group. John ... From: Gordon Mohr  To: John Erik Halse  Cc: Igor","rawEmail":"Return-Path: &lt;johnh@...&gt;\r\nX-Sender: johnh@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 20235 invoked from network); 1 Dec 2003 22:47:01 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m5.grp.scd.yahoo.com with QMQP; 1 Dec 2003 22:47:01 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta3.grp.scd.yahoo.com with SMTP; 1 Dec 2003 22:47:00 -0000\r\nReceived: (qmail 12259 invoked by uid 100); 1 Dec 2003 22:46:39 -0000\r\nReceived: from b116-dyn-37.archive.org (johnh@...@209.237.240.37)\n  by mail-dev.archive.org with RC4-MD5 encrypted SMTP; 1 Dec 2003 22:46:39 -0000\r\nSubject: [Fwd: Re: First draft of per host settings document]\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nContent-Type: text/plain; charset=\r\nMessage-Id: &lt;1070318635.2585.1.camel@...&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Ximian Evolution 1.4.5 \r\nDate: Mon, 01 Dec 2003 14:43:55 -0800\r\nContent-Transfer-Encoding: 8bit\r\nX-Spam-Status: No, hits=-8.3 required=6.0\n\ttests=AWL,BAYES_01,EMAIL_ATTRIBUTION,QUOTED_EMAIL_TEXT,\n\t      USER_AGENT_XIMIAN\n\tautolearn=ham version=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: John Erik Halse &lt;johnh@...&gt;\r\nX-Yahoo-Group-Post: member; u=163922992\r\nX-Yahoo-Profile: johnerikhalse\r\n\r\nI&#39;m forwarding this response from Gordon to the group.\n\nJohn\n\n-----Forwarded Message-----\nFrom: Gordon Mohr &lt;gojomo@...&gt;\nTo: John Erik Halse &lt;johnh@...&gt;\nCc: Igor Ranitovic &lt;igor@...&gt;, Kristinn Sigur√∞sson &lt;kris@...&gt;\nSubject: Re: First draft of per host settings document\nDate: Tue, 25 Nov 2003 09:31:19 -0800\n\nInitial reactions:\n\nGood proposal! Your approach makes sense.\n\nWe should make a list of common tasks a crawl operator would want to\nachieve with such a capability, and test the proposal against those\ntasks.\n\nMore comments interspersed below...\n\nJohn Erik Halse wrote:\n&gt; This is my first draft on the per host settings document. I would like\n&gt; to get some comments on this before I post it on the yahoogroups.\n&gt; \n&gt; John\n&gt; \n&gt; ------------------------------------------------------------------------\n&gt; \n&gt; SUGGESTIONS FOR PER HOST SETTINGS\n&gt; \n&gt; This document aims to describe a solution for the per host settings as\n&gt; required by the &quot;Memorandum of understanding&quot; between the Internet\n&gt; Archive and the Nordic National Libraries. The requirement is to be\n&gt; able to specify settings for each site, domain defaults and global\n&gt; default. This document does not address the possibility to set\n&gt; configuration settings on a per document basis.\n&gt; \n&gt; \n&gt; HIERARCHY OF SETTINGS\n&gt; \n&gt; Settings should be settable in a hierarchy with the default or global\n&gt; settings on top. Then there should be possible to set settings on a\n&gt; per domain basis and on a per host basis. The latter should always has\n&gt; precedence over the former.\n&gt; \n&gt; In addition to this we might want to have two sets of hierarchies so\n&gt; that a set of settings could be shared among different crawls. And\n&gt; another that would eventually override the first set. The use for this\n&gt; could be that there is a set of configurations shared by different\n&gt; institutions doing similar crawls for example different national\n&gt; libraries doing almost the same crawl but for different domains. The\n&gt; settings hierarchy will then look something like this for a host A in\n&gt; the domain B, we call the first configuration hierarchy for common and\n&gt; the second for local:\n&gt; \n&gt; 1. read the order file which contains the default settings for the\n&gt;    crawl and also points to the common and local configurations.\n&gt; 2. override if there are global settings in common\n&gt; 3. override if there are global settings in local\n&gt; 4. override if there are domain B settings in common\n&gt; 5. override if there are domain B settings in local\n&gt; 6. override if there are host A settings in common\n&gt; 7. override if there are host A settings in local\n&gt; \n&gt; All settings except for the initial order file should only contain\n&gt; those fields which are to be overridden.\n\nThe idea of nested hierarchies makes sense to me; however the\ncommon&lt;-&gt;local override dimension seems to add a lot of complexity\nfor minimal benefit.\n\nI can&#39;t think of a specific need, and making the overrides &quot;zig-zag&quot;\nbetween two hierarchies rather than &quot;telescope&quot; up a single hierarchy\nadds implementation and comprehension difficulties. I would defer\nsuch a capability until the more basic override capabilities are\nworking, and a need is expressed.\n\nDo you expect that top-level-domains like &quot;.com&quot; can also have\nsettings?\n\n&gt; \n&gt; SETTINGS THAT SHOULD BE MODIFIABLE PER HOST\n&gt; \n&gt; Not all settings should be settable on a per domain or per host\n&gt; basis. Especially this is the case with certain settings as the\n&gt; number of threads and which directories is to be used for log and arc\n&gt; files. This list shows some of the settings that should be\n&gt; overrideable by domain or host settings.\n&gt; \n&gt; * Scope settings\n&gt;   - max link hops\n&gt;   - max trans hops\n\nHadn&#39;t previously thought of these as per-host changeable -- need\nto consider further (or see example usage).\n\n&gt; * Behavior settings\n&gt;   - robots honoring policy\n&gt;   - user-agent and from settings\n&gt; * Politeness settings\n&gt;   - delay factor\n&gt;   - min delay\n&gt;   - max delay\n\nThese, together with a generic &quot;don&#39;t fetch anyting with this site/prefix/pattern&quot;,\nseem to be the typical cases motivating this functionality.\n\nWhen our scheduling capabilites are more advanced we might also add &quot;only\nfetch this site during these specific times&quot;.\n\nAlso, site-specific rules for reinterpreting away superficial URI info\n(like session-IDs) may be a common use of this facility.\n\n&gt; * Processor settings\n&gt;   - max file size to fetch\n&gt;   - which filters for inclusion and exclusion to apply\n&gt; \n&gt; In addition we might want to change which processors should handle URIs\n&gt; from a specific host.\n\nSuppressing or enabling specific processors sounds interesting but\npotentially dangerous; operators might expect that they have a\nclear view of what runs from their global setup alone, and lingering\nobscure per-site settings could lead to confusion. Seems like we should\nbe careful here. Perhaps in this case (and others?) the global settings\nshould indicate whether local overrides are allowed?\n\nThe per-site configuration facility should support whatever arbitrary\nsettings future processor modules need.\n\n&gt; \n&gt; CONFIGURATION FILES\n&gt; \n&gt; Probably the best way of extending how the configuration is to be read\n&gt; is by adding a two new fields in the order file which points to\n&gt; directories containing the common and the local configuration\n&gt; hierarchies. These directories has three different kind of files which\n&gt; is all optional. One global file with the global settings. One or more\n&gt; domain files with settings for one domain each. One or more host files\n&gt; with settings for one host each.\n\nThis seems like it could map well to a directory-hierarchy matching\nthe domain levels (eg: /org/archive/movies) and possibly even subpaths\n(eg /com/yahoo/geocities/TelevisionCity/studio/9999/) -- provided we do\nsensible things to limit both excessive depth when there are just a small\nnumber of configuration items, and excessive fanout in other cases.\n\nA key factor is how well the storage scales when there are potentially\nmillions of custom overrides, each potentially including lengthy\ndirectives, and the crawler can only keep some subset in memory at a\ntime.\n\n&gt; \n&gt; IMPLEMENTATION\n&gt; \n&gt; * FRONTIER\n&gt; \n&gt; To achieve these goals we need a place where the frontier should get the\n&gt; configuration settings for each host.\n&gt; \n&gt; In the current implementation there is a CrawlServer class which keeps\n&gt; track of the robots.txt for a host and also makes sure that we only\n&gt; have one fetch in progress from a certain host. I think this class\n&gt; might be a good candidate for keeping the per host configuration as\n&gt; well. The CrawlServer should at instantiation ask the CrawlController\n&gt; for its configuration by issuing its host name. Then it is up to the\n&gt; controller to figure out if it should stick with the default settings\n&gt; or if some or all settings should be overridden for this host.\n&gt; \n&gt; The frontier should be altered to associate a CrawlServer with the URI\n&gt; as soon as it is fetched from the pending queue and turned into a\n&gt; CrawlURI. This way the URI could be asked for the configuration\n&gt; settings it should use.\n&gt; \n&gt; The big implication this has on the design is that the different\n&gt; modules that currently extends the XMLConfig class for getting\n&gt; its own configuration settings should instead ask other components for\n&gt; its settings so that they are more dynamic. This way the module will\n&gt; not be as dependent of the structure of the order file as it is\n&gt; today. The configuration will be a data structure on its own and most\n&gt; modules should not extend the XMLConfig class.\n\nAsking the CrawlURI, for its CrawlServer, which supplies the\n&quot;current&quot; settings is a promising technique.\n\nFor the common case where there are no overrides, though, it seems\nbeneficial to retain the idea that modules have their own\nconfiguration -- sort of the global default. Then the CrawlURI&#39;s\ninfo can be seen as overrides when present.\n\n&gt; * CONTROLLER\n&gt; \n&gt; The second issue to be considered is how the controller keeps track of\n&gt; the configurations for each host. One way of doing this is to compile\n&gt; the hierarchy of settings for a host into one order object which is then\n&gt; delivered to the CrawlServer class upon request. Another approach is to\n&gt; keep the hierarchy of settings in memory and let the CrawlServer get\n&gt; the configuration object with the fines granularity for a certain\n&gt; host. When a setting is requested the request will be thrown up the\n&gt; hierarchy until it finds something to respond. Both solutions can be\n&gt; implemented with a late-initialization approach so that the actual\n&gt; configuration is not read into memory before it is needed.\n&gt; \n&gt; The main problem is that with a large broad crawl there might be a lot\n&gt; of CrawlServers active and that will demand a lot of memory to be\n&gt; allocated. Several things could be done to reduce that problem.\n&gt; \n&gt; The frontier could be constrained to only work with a limited set of\n&gt; hosts at a time. When it thinks it has finished a host, either by\n&gt; constraints given to it or there is no more pending URIs, it should\n&gt; throw the CrawlServer away and start working on new ones. If the host\n&gt; is to be revisited, that is that other hosts pointing to links inside\n&gt; this host which haven&#39;t been visited, then the configuration has to be\n&gt; reconstructed.\n&gt; \n&gt; A variant of the above is to keep track of when a CrawlServer object\n&gt; was last accessed and dispose the object after a certain time.\n\nYes, I think the approaches in order of increasing sophistication would\nbe:\n   (1) Keep all settings in memory (fast but not scalable)\n   (2) Keep all settings on disk, load every time it&#39;s needed, discard upon\n       each completed CrawlURI (scales up, but very inefficient)\n   (3) Keep it all on disk, use in-memory cache so that many settings\n       loads are served from cache (scales up, may tend towards (2)\n       for certain patterns of use)\n   (4) Keep it all on disk, but constrain the crawler to work on URIs in\n       related batches which tend to either improve the efficiency of\n       a (3)-style cache, or which provide clear/natural signals as\n       to when different settings-groups should be paged in or out of\n       fast memory.\n\nIt seems there may be another role-module here -- a &quot;ConfigurationProvider&quot;\nor some such -- which would initially use the configuration-files-and-\ndirectories approach above but could be swapped with a relational-DB-based\nimplementation someday if someone prefers that.\n\n&gt; Another optimization would be to not use the XML DOM as the underlying\n&gt; data structure for configuration settings. The DOM creates a lot of\n&gt; unnecessary objects. By building a custom data structure created by a\n&gt; SAX parser the burden both in terms of memory and processing, could be\n&gt; reduced.\n\nAgree, we&#39;ll have to limit our use of full DOM/XPath operations if extending\nthis mechanism down to fine-grained per-CrawlURI control.\n\nA possibility that&#39;s similar to something Kris and I discussed in the\ncontext of the Admin UI would be a &quot;pseudo-DOM&quot; -- where we use a small\nfixed subset of the XPath strings that would be used to access a rich DOM\nas keys into a flat HashMap instead.\n\nThen the global settings might still come from a hierarchical DOM, but\noverrides would be specified as:\n\n   XPath-&gt;alternate value\n   XPath-&gt;alternate value\n   etc.\n\n- Gordon\n\n\n\n\n\n"}}