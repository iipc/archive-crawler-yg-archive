{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"4Ivr6gktIu09Apqs3NWVN2UGT2wGyl3zYpfRjMFd-Mmnyp-nnffl99U9jCCS42iHRQeq0xRtZKjCE140XbKcjGoQuozPoFA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Configuration for open-ended crawl","postDate":"1291329690","msgId":6848,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRDRjgyMDlBLjUwODAwMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PEFBTkxrVGltZmlHODVxSjVOWHlBUGZ3RFA1dFZ4UWFXTF95cXQ2cEFrRlptU0BtYWlsLmdtYWlsLmNvbT4=","referencesHeader":"PEFBTkxrVGltZmlHODVxSjVOWHlBUGZ3RFA1dFZ4UWFXTF95cXQ2cEFrRlptU0BtYWlsLmdtYWlsLmNvbT4="},"prevInTopic":6843,"nextInTopic":0,"prevInTime":6847,"nextInTime":6849,"topicId":6843,"numMessagesInTopic":2,"msgSnippet":"... That s a good way to do it. Some variants to consider (functionally equivalent but perhaps convenient): - SURT prefixes to initialize the","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 9511 invoked from network); 2 Dec 2010 22:41:31 -0000\r\nX-Received: from unknown (66.196.94.107)\n  by m17.grp.re1.yahoo.com with QMQP; 2 Dec 2010 22:41:31 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta3.grp.re1.yahoo.com with SMTP; 2 Dec 2010 22:41:31 -0000\r\nX-Received: (qmail 26133 invoked by uid 0); 2 Dec 2010 22:41:30 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay00.pair.com with SMTP; 2 Dec 2010 22:41:30 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4CF8209A.5080000@...&gt;\r\nDate: Thu, 02 Dec 2010 14:41:30 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.12) Gecko/20101027 Thunderbird/3.1.6\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: Zach Bailey &lt;zach.bailey@...&gt;\r\nReferences: &lt;AANLkTimfiG85qJ5NXyAPfwDP5tVxQaWL_yqt6pAkFZmS@...&gt;\r\nIn-Reply-To: &lt;AANLkTimfiG85qJ5NXyAPfwDP5tVxQaWL_yqt6pAkFZmS@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Configuration for open-ended crawl\r\nX-Yahoo-Group-Post: member; u=137285340; y=rVJmQF7hzc3b1SlMq1CxYlXr7eHvnvAurQ4csjMdljTO\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 12/1/10 12:11 PM, Zach Bailey wrote:\n&gt;\n&gt;\n&gt; I would like to modify my Heritrix 3 configuration to perform an\n&gt; open-ended crawl. From reading through the documentation I know this is\n&gt; possible, but wanted to run the configuration by the experts to see if\n&gt; there were any other things I need to think about here.\n&gt;\n&gt; Here are the crawl rules in plain english:\n&gt;\n&gt; 1.) Candidate sites must end in .com, .net, .org, .edu, .gov\n&gt; 2.) Only crawl 12 items per domain (dns + robots.txt + 10 HTML pages)\n&gt; 3.) Ignore css, javascript, image, etc (really only interested in HTML\n&gt; as much as possible)\n&gt; 4.) When a new domain is discovered, start the crawl for that site at\n&gt; the root. For instance if I see a link in a page to\n&gt; http://www.somedomain.com/a/b/c.html I want a new seed added for\n&gt; www.somedomain.com &lt;http://www.somedomain.com&gt;\n&gt;\n&gt; 1 can be accomplished by pointing the SurtPrefixedDecideRule to a file\n&gt; with the SURTs for the desired TLDs with an ACCEPT (in conjunction with\n&gt; the default behavior of having the scope chain start with a REJECT all\n&gt; rule).\n\nThat&#39;s a good way to do it. Some variants to consider (functionally \nequivalent but perhaps convenient):\n\n- SURT prefixes to initialize the SurtPrefixedDecidecan also be \nexpressed mixed with the seeds, as lines beginning &quot;+&quot;. And, such &#39;plus&#39; \ndirectives assume &quot;http://&quot; if not present, and change plain \ndomains/URLs into an implied-SURT if there&#39;s not already a \nSURT-suggestive &#39;(&#39; (open-paren) present. So a directive like &quot;+com&quot; is \nenough to rule-in all .com domains.\n\n- just about anything that can be in an external file can also be \nspecified inline via a ConfigString, if you prefer to keep as much as \npossible in one file.\n\n&gt; 2 is accomplished by wiring a QuotaEnforcer into the fetch chain\n\nThat&#39;s one good way. Another is to choose a &#39;cost policy&#39; and &#39;budget&#39; \nvalues (especially &#39;totalBudget&#39;) that causes a queue to use up its \nbudget after the right number of URIs have been fetched. (This might be \nrequire more tweaking if *exactly* 10 is your target, as opposed to \n*approximately* 10, because budgets can be used up on other tries/errors \ntoo.)\n\n&gt; 3 is accomplished by a MatchesListRegexDecideRule (js, css, ico)\n&gt; and MatchesFilePatternDecideRule (REJECT with preset ALL) in the scope\n&gt; chain.\n\nYes, since most of what you don&#39;t want will have distinctive \nfile-extensions, that should do the job. You could make do with just the \nMatchesFilePatternDecideRule, with a suitably broad pattern. Also note \nthat both of these match against the full URI, not just the \npath-component, so sites using query-string tricks to force cache \nrefreshes of JS/CSS (eg &quot;/style.css?v=107120101&quot;) may slip through the \nstandard patterns unless you update them further.\n\n&gt; 4 I&#39;m a little stumped on, and not sure if that&#39;s possible. Or maybe\n&gt; it&#39;s not necessary. I&#39;ll do some experimenting but would appreciate any\n&gt; input on how new seeds are handled during an &quot;open-ended&quot; crawl.\n\nH3 trunk has a new feature on the ExtractorHTTP, &#39;inferRootPage&#39;, which \nmay be of some interest, though it&#39;s not exactly what you&#39;re trying. It \nconsiders every HTTP URL that&#39;s fetched to have an implicit link to the \nsite&#39;s root page, ensuring it&#39;s discovered. (All but the first redundant \nrediscovery just gets ignored by later steps.)\n\nYou could best achieve your exact need with some custom code, probably \njust a scripted Processor between the Extractors and the \nCandidatesProcessor, which reviews all cross-domain outlinks, and then \nadds the root page link according to your desired heuristic. (You could \neven discard the deep-link, if you only want to crawl each site starting \nfrom its root.)\n\n&gt; Finally, I have noticed some performance issues with BDB as the crawl\n&gt; frontier grows large. Are there any suggestions for performance tuning\n&gt; BDB or possibly using an alternative database solution on a separate\n&gt; machine for better scalability?\n\nAs the crawl grows, the disk seeks across the BDB-based structures \n(queues, the already-seen set) can become the limiting factor. A few \nthings we do to help are:\n\n- always put the &#39;state&#39; directory (BDB environment) on a disk separate \nfrom other IO\n\n- for crawls expected to grow larger than 20-30 million URIs, and where \na tiny error rate occasionally misclassifying a URI as seen when it \nhasn&#39;t been is acceptable, swap out the BdbUriUniqFilter with the \nBloomUriUniqFilter. (See BloomUriUniqFilter class comment and past list \ntraffic for more details; it keeps the entire already-seen in RAM -- \n~500MB with the default settings which have a 1-in-4-million error rate \nup through 125 million inserts.)\n\n- ensure the machine has plenty of RAM for either the in-heap BDB cache, \nor out-of-process automatic OS disk caching\n\nImproving the efficiency at large scale continues to be an on-again, \noff-again goal for our ongoing projects, so keep an eye for continuing \nchanges, and let us know the bottlenecks or ideas for improvement that \ncome up in your typical configurations.\n\n- Gordon @ IA\n\n"}}