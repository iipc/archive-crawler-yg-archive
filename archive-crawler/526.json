{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack@archive.org","from":"stack@...","replyTo":"LIST","senderId":"raU3Ah7qNzMCeTb-FIw3K9a3Yl-2uSdodfLOCBMgiLlTRRwe8s4hxaiGjO8sRPI2FPQCJNOQ6eQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Inserting information to MYSQL during crawl","postDate":"1086771071","msgId":526,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDEwODY3NzEwNzEuNDBjNmNmN2YxN2VlYkBtYWlsLWRldi5hcmNoaXZlLm9yZz4=","inReplyToHeader":"PDEwODY2ODA3MjUuNDBjNTZlOTU5ZTM1OEBtYWlsLWRldi5hcmNoaXZlLm9yZz4=","referencesHeader":"PGNhMXVlaSs2dXY4QGVHcm91cHMuY29tPiA8MTA4NjY4MDcyNS40MGM1NmU5NTllMzU4QG1haWwtZGV2LmFyY2hpdmUub3JnPg=="},"prevInTopic":518,"nextInTopic":544,"prevInTime":525,"nextInTime":527,"topicId":507,"numMessagesInTopic":19,"msgSnippet":"... On further consideration, look at ARCWriterPool.  In particular, the inner class ARCWriterFactory.  See how it is responsible for the manufacture of the ","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 38632 invoked from network); 9 Jun 2004 08:59:47 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m22.grp.scd.yahoo.com with QMQP; 9 Jun 2004 08:59:47 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta6.grp.scd.yahoo.com with SMTP; 9 Jun 2004 08:59:47 -0000\r\nReceived: (qmail 26187 invoked by uid 48); 9 Jun 2004 08:51:11 -0000\r\nReceived: from kb224104.kb.dk (kb224104.kb.dk [130.226.224.104]) \n\tby mail-dev.archive.org (IMP) with HTTP \n\tfor &lt;stack@...@localhost&gt;; Wed,  9 Jun 2004 01:51:11 -0700\r\nMessage-ID: &lt;1086771071.40c6cf7f17eeb@...&gt;\r\nDate: Wed,  9 Jun 2004 01:51:11 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ca1uei+6uv8@...&gt; &lt;1086680725.40c56e959e358@...&gt;\r\nIn-Reply-To: &lt;1086680725.40c56e959e358@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 8bit\r\nUser-Agent: Internet Messaging Program (IMP) 3.2.1\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: **\r\nX-Spam-Status: No, hits=2.3 required=6.0 tests=NO_REAL_NAME autolearn=no \n\tversion=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: stack@...\r\nSubject: Re: [archive-crawler] Re: Inserting information to MYSQL during crawl\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\nQuoting stack@...:\n\n&gt; Are you clear on where to start making your changes?  That you would put in \n&gt; place an alterate ARCWriterProcessor, one that did effectively what the\n&gt; current \n&gt; one does but rather than it talk to ARCWriterPool, instead it would keep up \n&gt; pool of database connections.\n\nOn further consideration, look at ARCWriterPool.  In particular, the inner class\n ARCWriterFactory.  See how it is responsible for the manufacture of the\n&#39;writer&#39;.  An override here, though awkward, might be better because then you&#39;d\nget the pooling benefits (We should make the insertion of custom writers more\namenable by changing the ARCWriterFactory to read a configuration describing\nwriter class to load).\nSt.Ack \n\n\n&gt; \n&gt; Would you want to do the document filtering before you did the db insert\n&gt; (i.e. \n&gt; the stripping of markup from html and the totexting of word, pdf, etc.)?\n&gt; \n&gt; Keep asking questions.\n&gt; St.Ack\n&gt; \n&gt; Quoting Ahnu Nahki &lt;ahnunahki@...&gt;:\n&gt; \n&gt; &gt; --- In archive-crawler@yahoogroups.com, Andy Boyko &lt;aboy@l...&gt; wrote:\n&gt; &gt; &gt; Ahnu Nahki wrote:\n&gt; &gt; &gt; &gt; Instead of writing to an arc file, Id like to create a method that\n&gt; &gt; &gt; &gt; takes the URI info, Content, headers, ect into a MYSQL database. Does\n&gt; &gt; &gt; &gt; anyone have any suggestion on how to do this , where I should look to\n&gt; &gt; &gt; &gt; place my methods?\n&gt; &gt; &gt; \n&gt; &gt; &gt; Are you interested in that specifically to get away from ARC, or more \n&gt; &gt; &gt; simply because you&#39;re interested in being able to issue queries on the \n&gt; &gt; &gt; crawl results in interesting/relational ways?  I ask because we&#39;re \n&gt; &gt; &gt; looking into a slightly different approach - rather than building the \n&gt; &gt; &gt; database logic into Heritrix, treating the DB import as a \n&gt; &gt; &gt; post-processing step on the crawl output (ARC files & logs) once the \n&gt; &gt; &gt; crawl is complete.  I believe Tom Emerson has also talked about \n&gt; &gt; &gt; populating a DB from ARC files in future versions of his libarc library.\n&gt; &gt; &gt; \n&gt; &gt; &gt; By keeping the content in ARCs, you get the ability to leverage the \n&gt; &gt; &gt; growing number of tools for dealing with the format coming from this \n&gt; &gt; &gt; community, and if the post-processing approach can work for you, code \n&gt; &gt; &gt; for populating a DB may be forthcoming from a couple of sources in the \n&gt; &gt; &gt; near future.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Can you describe in more detail what you&#39;re envisioning with your \n&gt; &gt; &gt; planned DB crawl storage?\n&gt; &gt; &gt; \n&gt; &gt; &gt; Regards,\n&gt; &gt; &gt; Andy Boyko    aboy@l...     Library of Congress\n&gt; &gt; \n&gt; &gt; We had considered that aswell initially as a quick way of importing\n&gt; &gt; the data into the db. Going the arc route after a crawl. But we have a\n&gt; &gt; search engine we run that is powered by\n&gt; &gt; lucene(http://jakarta.apache.org/lucene/docs/index.html). We have our\n&gt; &gt; own crawler which has done all it can and is not nearly as powerful as\n&gt; &gt; heritrix. So we want to incorporate heritrix into out environment\n&gt; &gt; quickly. We make all our indexes off of content stored in the db, and\n&gt; &gt; it makes alot of sense for us to just have the crawler populate the db\n&gt; &gt; at runtime than have some post crawl method were we do it. \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; Yahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt; \n&gt; &gt;  \n&gt; &gt; \n&gt; &gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n\n\n\n\n"}}