{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":361563137,"authorName":"Adam Broke≈°","from":"=?UTF-8?B?QWRhbSBCcm9rZcWh?= &lt;adam.brokes@...&gt;","profile":"goblin_cz","replyTo":"LIST","senderId":"87OEo8iYXf_3joL-3XQvbEgR5l27z6SNeF_FzoHoSCkZPGYQlndo6sOjxVwx1_CyIS-vscwWKecRtML4rLOAgcsvD1nkPxLbNc66RED-5EbY91_YlNPWsdMEOA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Few questions about Heritrix 1","postDate":"1303806013","msgId":7129,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEJBTkxrVGluYk5kellKQmc1elFwY2lOX29hPXBnakJwZHFRQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":0,"nextInTopic":7130,"prevInTime":7128,"nextInTime":7130,"topicId":7129,"numMessagesInTopic":3,"msgSnippet":"Hi all, we have found few strange issues when we have been building our new crawl strategy. Basically what we want to do is having three separate ","rawEmail":"Return-Path: &lt;adam.brokes@...&gt;\r\nX-Sender: adam.brokes@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 82666 invoked from network); 26 Apr 2011 08:27:25 -0000\r\nX-Received: from unknown (66.196.94.105)\n  by m14.grp.re1.yahoo.com with QMQP; 26 Apr 2011 08:27:25 -0000\r\nX-Received: from unknown (HELO mail-vx0-f180.google.com) (209.85.220.180)\n  by mta1.grp.re1.yahoo.com with SMTP; 26 Apr 2011 08:27:24 -0000\r\nX-Received: by vxk12 with SMTP id 12so346920vxk.25\n        for &lt;archive-crawler@yahoogroups.com&gt;; Tue, 26 Apr 2011 01:27:24 -0700 (PDT)\r\nX-Received: by 10.52.91.7 with SMTP id ca7mr666548vdb.256.1303806444002; Tue, 26\n Apr 2011 01:27:24 -0700 (PDT)\r\nMIME-Version: 1.0\r\nX-Received: by 10.52.163.201 with HTTP; Tue, 26 Apr 2011 01:20:13 -0700 (PDT)\r\nDate: Tue, 26 Apr 2011 10:20:13 +0200\r\nX-Google-Sender-Auth: _MQqplqd2QUyrEHY-jdihIyAVGw\r\nMessage-ID: &lt;BANLkTinbNdzYJBg5zQpciN_oa=pgjBpdqQ@...&gt;\r\nTo: archive-crawler &lt;archive-crawler@yahoogroups.com&gt;\r\nContent-Type: multipart/mixed; boundary=20cf307f3400f35f1604a1ce179f\r\nX-eGroups-From: =?UTF-8?B?QWRhbSBCcm9rZcWh?= &lt;adam@...&gt;\r\nFrom: =?UTF-8?B?QWRhbSBCcm9rZcWh?= &lt;adam.brokes@...&gt;\r\nSubject: Few questions about Heritrix 1\r\nX-Yahoo-Group-Post: member; u=361563137; y=KYiQkQzhY2JhstbPx4n2kP1qexPIj7iQ0BhrcEEYqIKfrvuz\r\nX-Yahoo-Profile: goblin_cz\r\n\r\n\r\n--20cf307f3400f35f1604a1ce179f\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi all,\n\nwe have found few strange issues when we have been building our ne=\r\nw\ncrawl strategy. Basically what we want to do is having three separate\nMat=\r\nchesListRegExpDecideRules, all with different purpose. One is for\nglobal ex=\r\ncludes which are set by publishers (e.g. eshop on their\nwebsite), second is=\r\n for global traps (like calendars, cms generated\nforms) and local excludes =\r\nwhich are set only for particular website\n(if the trap is on one domain but=\r\n could not be global because it could\nprevent from downloading meaningful s=\r\ntuff).\n\nThe process is as follow:\n1] we export global excludes from our dat=\r\nabase and insert into the\nlast job based on desired profile (if we would do=\r\n that on profile we\nhave to copy every change in settings to that profile)\n=\r\n2] we export global traps from document and insert them into order.xml\nlike=\r\n the step above.\n3] for every domain which have local trap we set override =\r\nand set the\ntraps in localTrapsRegexpList (this list is on the top level em=\r\npty)\n\nSo far I think it is not something complex and should work fine, but\n=\r\nwe encounter few problems:\n\n1] when we have job with some expressions in lo=\r\ncalTrapsRegexpList and\nwe build new job based on it, when I go to override =\r\nwhich should have\nregexp, it is there, but the checkbox next to the listbox=\r\n is unchecked\nand thus if I do not check it, the list is set as empty (oper=\r\nators\noften forgot about it, but I think it is not the desired behavior).\n2=\r\n] when the job is running and we change for example total budget\nlimit (it =\r\nis the value which is always changed during the crawl) and\nrise it from 10k=\r\n to 15k, some of overrides get lost\n3] sometimes the overrides get lost wit=\r\nhout any obvious reason, we are\nstill figuring it out and narrowing the pos=\r\nsible reasons for it.\n4] our budgeting policy is set on\nhttps://webarchive.=\r\njira.com/wiki/display/Heritrix/Frontier+queue+budgets\n- when we do the midc=\r\nrawl adjustments in some cases the change is not\nexteriorized to the fronti=\r\ner and the retired queue which has increased\nbudget is still retired and th=\r\ne old number of total budget is shown in\nfrontier report.\n\nI have few more =\r\ngeneral question.\n1] about the budgeting policy - redirects and 404 are cou=\r\nnted to the\nexpenditure with UnitCost policy? From browsing the source code=\r\n, I\nassume it is counted. If it is, it is good for discovering traps in\near=\r\nly stage of the crawl, but not exactly the best approach to set\nlimits for =\r\nobjects on one domain. Could I combine this rule with\nQuotaEnforcer (I thin=\r\nk this one counts only successful downloads)\n2] sometimes when the crawl is=\r\n near the finish (the budget on almost\nevery queue is spent), but there are=\r\n few queues (up to ten, mainly\nlarge sites with big files) where the snooze=\r\n time gets really long:\n\nQueue cz,czhops,\n  148 items\n   wakes in: 9h17m33s=\r\n296ms\n    last enqueued:\nhttp://czhops.cz/index.php/component/content/artic=\r\nle/37-pravni-predpisy/78-zakon-971996-sb\n      last peeked:\nhttp://czhops.c=\r\nz/index.php/cs/hop-growing?tmpl=3Dcomponent&print=3D1&page=3D\n   total expe=\r\nnded: 1115 (total budget: 15000)\n   active balance: 2630\n   last(avg) cost:=\r\n 1(1)\n\nEven though the max delay is set to short time. How is exactly snooz=\r\ne\ntime computed and could I affect it?\n3] could be some of the issues menti=\r\noned above solved by using\nHeritrix 3. If so, is there any automated or sem=\r\niautomated way to\nconvert profiles from Heritrix 1?\n\nI am sorry, for such l=\r\nong email full of questions, but I get a little\nbit confused by Heritrix be=\r\nhaviour and I would like to make sure if\nthere is problem on our side or if=\r\n there could be bug in Heritrix.\n\nThe order.xml is in attachment.\n\nCheers,\n=\r\n\nAdam\n--\nAdam Broke=C5=A1\nhttp://www.brokes.net\nadam@...\n\r\n--20cf307f3400f35f1604a1ce179f\r\nContent-Type: text/xml; charset=US-ASCII; name=&quot;order.xml&quot;\r\nContent-Disposition: attachment; filename=&quot;order.xml&quot;\r\nX-Attachment-Id: f_gmykejvx0\r\n\r\n[ Attachment content not displayed ]\r\n--20cf307f3400f35f1604a1ce179f--\r\n\n"}}