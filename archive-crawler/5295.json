{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":335447337,"authorName":"pilotboy_84","from":"&quot;pilotboy_84&quot; &lt;car.pilotboy@...&gt;","profile":"pilotboy_84","replyTo":"LIST","senderId":"o-th11l14r1UmjfnupYw4YzqdyvS2wtBJhEmFHCKOwbS9nSeLPUw9ljxxnXeDQZ-u_ETbPRc8y-bsqrbTZ37ewppQDshCDg7cUVocDk","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: Crawling 100 million pages","postDate":"1212782693","msgId":5295,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGcyYzU5NStydGNoQGVHcm91cHMuY29tPg=="},"prevInTopic":5294,"nextInTopic":5296,"prevInTime":5294,"nextInTime":5296,"topicId":5292,"numMessagesInTopic":11,"msgSnippet":"Hy, i would like to ask you something? Do you have more information about that crawler of 120 million pages. I would like to know the resources that I need to","rawEmail":"Return-Path: &lt;car.pilotboy@...&gt;\r\nX-Sender: car.pilotboy@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 83230 invoked from network); 6 Jun 2008 20:04:55 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m55.grp.scd.yahoo.com with QMQP; 6 Jun 2008 20:04:55 -0000\r\nX-Received: from unknown (HELO n36b.bullet.mail.sp1.yahoo.com) (66.163.168.150)\n  by mta17.grp.scd.yahoo.com with SMTP; 6 Jun 2008 20:04:55 -0000\r\nX-Received: from [216.252.122.218] by n36.bullet.mail.sp1.yahoo.com with NNFMP; 06 Jun 2008 20:04:55 -0000\r\nX-Received: from [209.73.164.83] by t3.bullet.sp1.yahoo.com with NNFMP; 06 Jun 2008 20:04:55 -0000\r\nX-Received: from [66.218.66.78] by t7.bullet.scd.yahoo.com with NNFMP; 06 Jun 2008 20:04:55 -0000\r\nDate: Fri, 06 Jun 2008 20:04:53 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g2c595+rtch@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;pilotboy_84&quot; &lt;car.pilotboy@...&gt;\r\nSubject: Re: Crawling 100 million pages\r\nX-Yahoo-Group-Post: member; u=335447337; y=1QIUUDxTcK3ThXGsDntb5QOLAAcSoSJHJYdbqjNi8KqhF1vpq18\r\nX-Yahoo-Profile: pilotboy_84\r\n\r\nHy, i would like to ask you something?\nDo you have more information about t=\r\nhat crawler of 120 million pages.\nI would like to know the resources that I=\r\n need to match your results.\nIf you had a document or report that could be =\r\nshared would be very\nhelpful to me.\nThanks\n\n--- In archive-crawler@yahoogro=\r\nups.com, &quot;Leo Dagum&quot; &lt;leo_dagum@...&gt; wrote:\n&gt;\n&gt; We did 120million pages in =\r\nmarch.  We&#39;ve somewhat modified heritrix\nfor our\n&gt; purposes but I&#39;m sure yo=\r\nu could do it with the release version. \nBasically\n&gt; all you need is enough=\r\n Internet bandwidth, good DNS and multiple crawler\n&gt; instances.  We have 10=\r\nMbit/s and with our set up we saturated that\nwith 4\n&gt; crawler instances run=\r\nning on single cpu dual core Xeon boxes, each\nwith 8GB\n&gt; memory, used 5GB f=\r\nor java heap.  Each machine needs a boatload of disk\n&gt; space, not just for =\r\nthe content but for checkpoints (which we actually\n&gt; turned off since we&#39;ve=\r\n been unable to restart a big, multi-million page\n&gt; crawl from checkpoint, =\r\nalthough it works fine for smaller crawls)\nand for\n&gt; the various BDB instan=\r\nces used by the crawler for queueing uri&#39;s.  \n&gt; \n&gt;  \n&gt; \n&gt; The crawl rate di=\r\nd slow down for us substantially towards the end of the\n&gt; month.  I don&#39;t k=\r\nnow why, and it&#39;s definitely something we&#39;ll have to\n&gt; investigate when we =\r\nnext do a big crawl.\n&gt; \n&gt;  \n&gt; \n&gt; Another possibility for you would be the n=\r\nutch crawler.  Depends on what\n&gt; exactly you want from the crawler.  Heritr=\r\nix is very nice as a stand\nalone\n&gt; crawler used for harvesting pages from t=\r\nhe Internet.  From the\nlittle I&#39;ve\n&gt; seen, nutch is pretty tightly integrat=\r\ned with the nutch indexing/search\n&gt; capability, and designed more for conti=\r\nnuous crawling.  \n&gt; \n&gt;  \n&gt; \n&gt; - leo\n&gt; \n&gt;  \n&gt; \n&gt;   _____  \n&gt; \n&gt; From: archiv=\r\ne-crawler@yahoogroups.com\n&gt; [mailto:archive-crawler@yahoogroups.com] On Beh=\r\nalf Of hijbul_bd\n&gt; Sent: Friday, June 06, 2008 8:27 AM\n&gt; To: archive-crawle=\r\nr@yahoogroups.com\n&gt; Subject: [archive-crawler] Crawling 100 million pages\n&gt;=\r\n \n&gt;  \n&gt; \n&gt; Dear All\n&gt; \n&gt; I would like to crawl 100 million pages with in a =\r\nmonth for crawling \n&gt; research. As far i know some research crawler(IRLbot(=\r\n6 billion pages in \n&gt; 41 days), polybot(120 millions pages in 19 days)) can=\r\n download huge \n&gt; pages in short amount of time wich is not open source. In=\r\n 2005 \n&gt; according to some blog site Heritrix can download about 20 miilion=\r\n \n&gt; pages in a month. What is the speed of current Heritrix version and How=\r\n \n&gt; can I speed up heritrix to download 100 million or at least 50 million =\r\n\n&gt; pages with in a month. Are there any ohter open source crawler which \n&gt; =\r\ncan do this?\n&gt; \n&gt; Thanks in Advance\n&gt; Hijbul Alam\n&gt;\n\n\n\n"}}