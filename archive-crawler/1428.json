{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"aKKTdo9TO4QOzUbuB6uqQ3dEMmeoEhz0bwMUU6S7x-wbqKuqi4yMsPrKu0mU2RoEGdBlIw6_AHWpkoXfjT8h9HaltAy1jfJq","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] user agent!","postDate":"1106698992","msgId":1428,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxRjZFMkYwLjkwMzAwMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQxRjZEMjY2LjQwNjAzMDhAYXJjaGl2ZS5vcmc+","referencesHeader":"PDIwMDUwMTI1MTQyOTQxLjY0OTI4LnFtYWlsQHdlYjQyMjAyLm1haWwueWFob28uY29tPiA8NDFGNkQyNjYuNDA2MDMwOEBhcmNoaXZlLm9yZz4="},"prevInTopic":1426,"nextInTopic":1432,"prevInTime":1427,"nextInTime":1429,"topicId":1422,"numMessagesInTopic":4,"msgSnippet":"Hi Christoph, I will just like to add on to the Stack s note. These days webmasters pay close attention to Web crawlers. If you pose to be googlebot it is most","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 49864 invoked from network); 26 Jan 2005 00:25:17 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m15.grp.scd.yahoo.com with QMQP; 26 Jan 2005 00:25:17 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta5.grp.scd.yahoo.com with SMTP; 26 Jan 2005 00:25:17 -0000\r\nReceived: (qmail 14643 invoked by uid 100); 26 Jan 2005 00:09:49 -0000\r\nReceived: from pauk.archive.org (HELO ?207.241.238.153?) (igor@...@207.241.238.153)\n  by mail-dev.archive.org with SMTP; 26 Jan 2005 00:09:49 -0000\r\nMessage-ID: &lt;41F6E2F0.9030008@...&gt;\r\nDate: Tue, 25 Jan 2005 16:23:12 -0800\r\nUser-Agent: Mozilla Thunderbird 0.7.3 (Windows/20040803)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;20050125142941.64928.qmail@...&gt; &lt;41F6D266.4060308@...&gt;\r\nIn-Reply-To: &lt;41F6D266.4060308@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: *\r\nX-Spam-Status: No, hits=1.4 required=6.5 tests=AWL,HTTP_ESCAPED_HOST \n\tautolearn=no version=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] user agent!\r\nX-Yahoo-Group-Post: member; u=137477665\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Christoph,\n\nI will just like to add on to the Stack&#39;s note.\nThese days webmasters pay close attention to Web crawlers. If you pose to be googlebot it is most \nlikely that IP addresses or entire IP range that you are using for crawling will be blocked from \nmany sites. This will indefinitely prevent you from crawling these sites in future.\nOn the other hand, there are many sites out there (a.k.a cloakers) that are trying to trick Google \nand other search-engine crawlers to get better ranking. In this case you will be crawling a lot \n&quot;junk&quot; that these cloakers server to popular crawlers...All in all, I strongly recommend not to pose \nas other popular crawlers since it will cause you more trouble than good. As Stack suggested you \nshould work on creating good relationships with webmasters. Keep in mind that in crawling business \nit is easy to get bad reputation.\n\nTake care,\ni.\n\n&gt;&gt;Hi everybody!\n&gt;&gt;\n&gt;&gt;I got two questions for you:\n&gt;&gt;\n&gt;&gt;First let&#39;s assume you want to crawl a web-page and\n&gt;&gt;you want heritrix to pretend to be one of the &quot;big&quot;\n&gt;&gt;webspiders (GoogleBot e.g.). I know i can change the\n&gt;&gt;user-agent to something like &lt;user-agent&gt;(+&lt;url&gt;) but\n&gt;&gt;is the usage of &quot;GoogleBot (+http://x.net) &lt;http://x.net%29&gt; enough to\n&gt;&gt;pretend that this is Googlebot??\n&gt; \n&gt; \n&gt; Effectively, yes.  This &#39;user-agent&#39; string is usually how servers would \n&gt; distingush agents whether its adding special handling/defenses or when \n&gt; designating access rules in robots.txt.\n&gt; \n&gt; But note, we&#39;d strongly counsel against you posing as someone else&#39;s \n&gt; agent.  Because of misconfigurations or because you&#39;re not privvy to \n&gt; arrangements made between the target site and agent, there is the danger \n&gt; that you could blacken the  name of the robot you&#39;re posing as.  If the \n&gt; site only allows crawling by GoogleBot or some other agent, rather than \n&gt; pose as GoogleBot, try contacting the webmaster and ask that your \n&gt; crawler be allowed access.\n&gt; \n&gt; \n&gt;&gt;Second if you take a look at the crawlorder-file you\n&gt;&gt;can see something like that:\n&gt;&gt;\n&gt;&gt;&lt;newObject name=&quot;robots-honoring-policy&quot;\n&gt;&gt;class=&quot;org.archive.crawler.datamodel.RobotsHonoringPolicy&quot;&gt;\n&gt;&gt;      &lt;string name=&quot;type&quot;&gt;classic&lt;/string&gt;\n&gt;&gt;      &lt;boolean name=&quot;masquerade&quot;&gt;false&lt;/boolean&gt;\n&gt;&gt;      &lt;text name=&quot;custom-robots&quot;&gt;&lt;/text&gt;\n&gt;&gt;      &lt;stringList name=&quot;user-agents&quot;&gt;\n&gt;&gt;      &lt;/stringList&gt;\n&gt;&gt;    &lt;/newObject&gt;\n&gt;&gt;\n&gt;&gt;Can anybody explain me what the idea of this thing\n&gt;&gt;is??\n&gt; \n&gt; \n&gt; Take a look at &#39;6.5.1.4. Robots honoring policy&#39; in the user manual: \n&gt; http://crawler.archive.org/articles/user_manual.html#settings.  It makes \n&gt; a cut at explaining these fields.\n&gt; \n&gt; Good luck,\n&gt; St.Ack\n&gt; \n&gt; \n&gt;&gt;Thanks,\n&gt;&gt;\n&gt;&gt;Christoph\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;           \n&gt;&gt;__________________________________\n&gt;&gt;Do you Yahoo!?\n&gt;&gt;Yahoo! Mail - 250MB free storage. Do more. Manage less.\n&gt;&gt;http://info.mail.yahoo.com/mail_250\n&gt;&gt;\n&gt;&gt;------------------------------------------------------------------------\n&gt;&gt;*Yahoo! Groups Links*\n&gt;&gt;\n&gt;&gt;    * To visit your group on the web, go to:\n&gt;&gt;      http://groups.yahoo.com/group/archive-crawler/\n&gt;&gt;       \n&gt;&gt;    * To unsubscribe from this group, send an email to:\n&gt;&gt;      archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;      &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;&gt;       \n&gt;&gt;    * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;&gt;      Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;&gt;\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}