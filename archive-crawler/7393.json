{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"B9vIKXtRrMAP8bj3d7ixwmsa4uhZTMNfLmVIO7JvsQRisZofDKxmAO_iL-yae51-A8cMbAZGtvnjEUO-C533Fh3H5SwZPDU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix crawling strategy","postDate":"1320720787","msgId":7393,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFQjg5OTkzLjgwNDAyMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRFQjg0MzhGLjMwNDA2MDZAY3MuY211LmVkdT4=","referencesHeader":"PGo4bXRkcisybnI1QGVHcm91cHMuY29tPiA8NEVCODEyRjQuNDA3MDYwOEBhcmNoaXZlLm9yZz4gPDRFQjgyQ0E3LjYwMDA3MDRAY3MuY211LmVkdT4gPDRFQjgzNTA5LjUwMzAyMDlAYXJjaGl2ZS5vcmc+IDw0RUI4NDM4Ri4zMDQwNjA2QGNzLmNtdS5lZHU+"},"prevInTopic":7392,"nextInTopic":7394,"prevInTime":7392,"nextInTime":7394,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"... If the end goal is a best billion dataset that is then reused for a long time, you still might want to do that in indirect/incremental steps or recrawls:","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 73420 invoked from network); 8 Nov 2011 02:53:09 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m3.grp.sp2.yahoo.com with QMQP; 8 Nov 2011 02:53:09 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta2.grp.sp2.yahoo.com with SMTP; 8 Nov 2011 02:53:09 -0000\r\nX-Received: (qmail 78089 invoked by uid 0); 8 Nov 2011 02:53:07 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay03.pair.com with SMTP; 8 Nov 2011 02:53:07 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4EB89993.8040208@...&gt;\r\nDate: Mon, 07 Nov 2011 18:53:07 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:7.0.1) Gecko/20110929 Thunderbird/7.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: David Pane &lt;dpane@...&gt;, Noah Levitt &lt;nlevitt@...&gt;\r\nReferences: &lt;j8mtdr+2nr5@...&gt; &lt;4EB812F4.4070608@...&gt; &lt;4EB82CA7.6000704@...&gt; &lt;4EB83509.5030209@...&gt; &lt;4EB8438F.3040606@...&gt;\r\nIn-Reply-To: &lt;4EB8438F.3040606@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix crawling strategy\r\nX-Yahoo-Group-Post: member; u=137285340; y=sYqVXykPldk0SDHmqIXqZpC86abqYmFRx80Bs417x8vk\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 11/7/11 12:46 PM, David Pane wrote:\n&gt; Thank you Gordon.\n&gt;\n&gt; We do not plan to do repeated crawls, but capture a 1 billion page\n&gt; dataset to be used for information retrieval research purposes. We would\n&gt; like to capture as many high quality pages as possible.\n\nIf the end goal is a &quot;best billion&quot; dataset that is then reused for a \nlong time, you still might want to do that in indirect/incremental steps \nor recrawls: for example, crawl 2 billion, then pick the best billion. \nOr crawl 500 million with a naive approach, do an offline PR \ncalculation, then do the &#39;production&#39; 1B crawl biased by the results of \nthe 500 million. Etc...\n\n&gt; We would also like to have a blacklist of unwanted sites. This blacklist\n&gt; would possibly contain millions hosts. Do you have any experience in the\n&gt; resources needed for this? Would this slow down the speed of the crawl?\n\nI&#39;ve not done anything with a blacklist that big. You certainly wouldn&#39;t \nwant to use a list of regexes!\n\nThe SURT-based DecideRules use a sorted list of prefixes, all in memory, \nand have roughly O(log n) lookup. Perhaps that would work for your \npurposes, if you have a big RAM machine; you should do some tests and \nback-of-the-envelope calculations to check for sure. If the blacklist \nentries are always hosts, some other hash-based structure might work \nwith even less RAM overhead and O(1) lookup.\n\n&gt; In our small crawls that we have been running, we found that although we\n&gt; have setup for a breadth first crawl, days into the crawl and 10&#39;s of\n&gt; million of pages crawled, the crawler has still only crawled a small\n&gt; percentage of seeds (under 10% of a 2 million host seed list - 1200\n&gt; threads). It appears that most of the seeds are in a separate queue so\n&gt; would cycling through the queues (balance-replenish-amount to a lower\n&gt; amount maybe 100) help cover all of the queues?\n\nYes, lowering the balance-replenish-amount will cycle through the queues \nmore quickly, since each will stay in active rotation a shorter amount \nof time until other queues are given a chance.\n\nAnother factor which has been previously noted as slowing the progress \nthrough a diversity of hosts, in crawls with giant seed lists, is if \nmany of the hosts turn out to be unresponsive.\n\nIn some cases, being unresponsive means a long 20-second socket-timeout, \nwhich occupies the worker thread that whole time. Then, the queue goes \ninto a &#39;long snooze&#39; (default 15 minutes) before retrying. Only after a \nbunch of retries (default 30) will the URI completely fail out as having \nfailed for too many times over too long. Imagining a worst-case \nscenario: if your first 54000 seeds were all unresponsive like that, \nyour 1200 threads could be doing nothing but these timeouts for over 7 \nhours.\n\nSome possible ways to minimize this effect:\n\n� shorten the soTimeoutMs (socket timeout) in FetchHTTP below the \ndefault 20 seconds (less time holding a thread)\n\n� lengthen the retryDelaySeconds above the default 900 seconds (more \ntime between held sessions\n\n� lessen the maxRetries below the default 30 (give up entirely on a URI \nsooner)\n\n� prescreen the URIs outside Heritrix to eliminate those not \nresolving/running-any-webserver\n\nThough, changing those default parameters does make the process less \nrobust against the sort of temporary network/server outages that are \ncommon and which you normally wouldn&#39;t want to completely exclude a site \nfrom your crawl.\n\nLooking at the source now, it also seems like a option in Heritrix 1.0 \nthat could push &#39;long snoozes&#39; to the back of the list of waiting queues \n(preventing a logjam of unresponsive hosts among the active queues) does \nnot appear to have been translated properly into H3, so is now inactive. \n(The WorkQueueFrontier property snoozeLongMs, a threshold above which \nsnoozes are supposed to result in deactivation to the end of the list of \nwaiting queues -- and thus potentially a much longer snooze -- to give \nother queues a chance.) Fixing this could improve this behavior, in \ncrawls reaching many unresponsive hosts, as well.\n\n- Gordon\n\n&gt; --David\n&gt;\n&gt; On 11/7/2011 2:44 PM, Gordon Mohr wrote:\n&gt;&gt; Heritrix is not very accommodating for an OPIC-ordered crawl in the way\n&gt;&gt; that it is not easy to reorder URIs inside a single queue, once they are\n&gt;&gt; queued. (You&#39;d have to peel them all off, sort, and re-queue... which\n&gt;&gt; might offset some or all of whatever ordering benefit you were hoping to\n&gt;&gt; achieve.)\n&gt;&gt;\n&gt;&gt; You can, however, re-prioritize entire queues via the queue-precedence\n&gt;&gt; mechanism. So you could bias the crawler to spend more early effort on\n&gt;&gt; sites that are large, or have a larger number of total inlinks, or other\n&gt;&gt; measures.\n&gt;&gt;\n&gt;&gt; If you are performing repeated crawls, you could also use PR\n&gt;&gt; calculations from a completed earlier crawl to bias the ordering of a\n&gt;&gt; later crawl (because you would have PR values usable at the moment of\n&gt;&gt; queueing).\n&gt;&gt;\n&gt;&gt; From a quick glance at the paper you reference, it doesn&#39;t seem like\n&gt;&gt; the magnitude of benefit for incremental-PR-estimation is very large. As\n&gt;&gt; they attribute to their footnote [11], and other broad-crawl studies\n&gt;&gt; have also suggested, &quot;even a random strategy can perform well on the\n&gt;&gt; Web, in the sense that a random walk on the Web is biased towards pages\n&gt;&gt; with high Pagerank&quot;. For this reason, I wouldn&#39;t be too concerned about\n&gt;&gt; crawling in PR-order unless you know you often have to cut your crawl\n&gt;&gt; short and thus strongly suspect more important pages are not being\n&gt;&gt; crawled... and even in that case, in a repeated crawl series you could\n&gt;&gt; adjust future crawls to offset this problem.\n&gt;&gt;\n&gt;&gt; Also important to note: a strict &#39;breadth-first&#39; crawl isn&#39;t a realistic\n&gt;&gt; description of any large politeness-limited crawl. Almost immediately in\n&gt;&gt; any crawl, politeness-to-servers means you may only be a few hops deep\n&gt;&gt; into big sites, while being arbitrarily deep on other paths that are\n&gt;&gt; spread over many non-politeness-bottlenecked servers. This certainly\n&gt;&gt; helps in discovering (and crawling to completeness) new and\n&gt;&gt; small-URI-count independent servers... though it leaves the risk that\n&gt;&gt; potentially &#39;important&#39; (by PR) pages, &#39;deep&#39; within large servers, may\n&gt;&gt; not be reached within some capped time/storage budget.\n&gt;&gt;\n&gt;&gt; - Gordon\n&gt;&gt;\n&gt;&gt; On 11/7/11 11:08 AM, David Pane wrote:\n&gt;&gt;&gt; Hi Noah,\n&gt;&gt;&gt;\n&gt;&gt;&gt; OPIC stands for Online Page Importance Computation.\n&gt;&gt;&gt;\n&gt;&gt;&gt; This scoring is in Nutch as a plugin\n&gt;&gt;&gt;\n&gt;&gt;&gt; Here is a paper that shows that OPIC scoring is better than a breadth\n&gt;&gt;&gt; first crawl.\n&gt;&gt;&gt;\n&gt;&gt;&gt; &quot;Crawling a country: better strategies than breadth-first for web page\n&gt;&gt;&gt; ordering&quot;\n&gt;&gt;&gt; http://dl.acm.org/citation.cfm?id=1062768\n&gt;&gt;&gt;\n&gt;&gt;&gt; --David\n&gt;&gt;&gt; On 11/7/2011 12:18 PM, Noah Levitt wrote:\n&gt;&gt;&gt;&gt; Hello David,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; What is OPIC?\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Noah\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; On 2011-10-31 12:37 , david_pane1 wrote:\n&gt;&gt;&gt;&gt;&gt; Is it possible to apply a crawling strategy based OPIC algorithm to a\n&gt;&gt;&gt;&gt;&gt; Heritrix 3.x crawl?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; We initially though that we could use the URI precedence, but it\n&gt;&gt;&gt;&gt;&gt; appears that there isn&#39;t any provision for a URI&#39;s precedence to\n&gt;&gt;&gt;&gt;&gt; change after initial assignment and as stated here\n&gt;&gt;&gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/6022 by\n&gt;&gt;&gt;&gt;&gt; Gordon, depending on how they are queued, they could be crawled out of\n&gt;&gt;&gt;&gt;&gt; order.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Can anyone share some insight on this?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; --David\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n\n"}}