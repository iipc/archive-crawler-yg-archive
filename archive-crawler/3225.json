{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":139911393,"authorName":"thiru_sundaram","from":"&quot;thiru_sundaram&quot; &lt;thiru_sundaram@...&gt;","profile":"thiru_sundaram","replyTo":"LIST","senderId":"37OJIsk3IeJRSwLCYUacdDl68e5QP161P6RYGuN2NGsGqXT46-OLnCkHRKG-GNqcYCvmb14kTThEc7r6tu9X9bYAcGwIyR-Ih9PxGTSPZwh6GQ","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: crawling unique urls","postDate":"1157218829","msgId":3225,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVkY2ZtZCtoN21vQGVHcm91cHMuY29tPg==","inReplyToHeader":"PGVkNzgxNStsNm5wQGVHcm91cHMuY29tPg=="},"prevInTopic":3223,"nextInTopic":3240,"prevInTime":3224,"nextInTime":3226,"topicId":3223,"numMessagesInTopic":3,"msgSnippet":"Well, Its simple. The RegEx rule i added [ when the job was crawling] did not apply on already queued URIs . The canonicalization rules applied only before","rawEmail":"Return-Path: &lt;thiru_sundaram@...&gt;\r\nX-Sender: thiru_sundaram@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 10012 invoked from network); 2 Sep 2006 17:40:48 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m41.grp.scd.yahoo.com with QMQP; 2 Sep 2006 17:40:48 -0000\r\nReceived: from unknown (HELO n16a.bullet.scd.yahoo.com) (66.94.237.45)\n  by mta7.grp.scd.yahoo.com with SMTP; 2 Sep 2006 17:40:48 -0000\r\nReceived: from [66.218.69.5] by n16.bullet.scd.yahoo.com with NNFMP; 02 Sep 2006 17:40:31 -0000\r\nReceived: from [66.218.66.72] by t5.bullet.scd.yahoo.com with NNFMP; 02 Sep 2006 17:40:31 -0000\r\nDate: Sat, 02 Sep 2006 17:40:29 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;edcfmd+h7mo@...&gt;\r\nIn-Reply-To: &lt;ed7815+l6np@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0\r\nFrom: &quot;thiru_sundaram&quot; &lt;thiru_sundaram@...&gt;\r\nSubject: Re: crawling unique urls\r\nX-Yahoo-Group-Post: member; u=139911393; y=c8n3zlQjpmoYXrUtOhF1vNEzuu_wSJaqfJ0SINE1FRGlUDg9LWU4_po\r\nX-Yahoo-Profile: thiru_sundaram\r\n\r\nWell, Its simple. The RegEx rule i added [ when the job was crawling]\ndid n=\r\not apply on already queued URIs . The canonicalization rules\napplied only b=\r\nefore queuing the URIs So i had to start my job afresh \nonce again. \n\n--- I=\r\nn archive-crawler@yahoogroups.com, &quot;thiru_sundaram&quot;\n&lt;thiru_sundaram@...&gt; wr=\r\note:\n&gt;\n&gt; Hi,\n&gt; Heritrix crawls a url only once. But the website I try to cr=\r\nawl poses\n&gt; a problem. There are different url patterns for the same page. =\r\nThe url\n&gt; for the same page changes based on the path i take to reach the p=\r\nage. \n&gt; \n&gt; \n&gt; Say x.com is the domain i wish to crawl\n&gt; \n&gt; http://www.x.com=\r\n/page?id=3D1234&path1=3Dlevel1&path2=3Dlevel2\n&gt; http://www.x.com/page?id=3D=\r\n1234\n&gt; http://www.x.com/page?id=3D1234&path1=3Dsearch\n&gt; \n&gt; Can I make the u=\r\nrl compartor in Heritrix to coniser the url are same\n&gt; if thier &#39;id&#39; field =\r\nmatches? I want Heritrix to consider all the above\n&gt; said  urls as same . \n=\r\n&gt; \n&gt; I tried using the RegexRule [under URL Canonicalization]. But it can\n&gt;=\r\n trim down the current uri under consideration & not the list of\n&gt; crawled =\r\nurls .\n&gt; \n&gt; Thanks in advance for the help\n&gt;\n\n\n\n\n\n"}}