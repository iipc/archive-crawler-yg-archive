{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":267551907,"authorName":"anand_akela","from":"&quot;anand_akela&quot; &lt;anand.akela@...&gt;","profile":"anand_akela","replyTo":"LIST","senderId":"uc4s6QKwJa4GGGRGwpHGahh04VZoPKMD_H6vNj2CCp_egKMe1i3b1VrhU0vm2RNcsnfbg_4Cfa6o9ykkcjErD5-0cNNw5yZpM3MuKpg_0Fnt","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Has anyone tried Libarc library on Suse Linux?","postDate":"1149285126","msgId":2907,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGU1cWJ1NitocHNxQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDE2ODM4LjUyNTQ3LjUzNzA4OS40NDgwNDZAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4="},"prevInTopic":1295,"nextInTopic":2908,"prevInTime":2906,"nextInTime":2908,"topicId":1286,"numMessagesInTopic":8,"msgSnippet":"Has anyone tried Libarc library on Suse Linux? I am getting errors when I am trying to make the library. I am interested in arcdump utility and wondering if","rawEmail":"Return-Path: &lt;anand.akela@...&gt;\r\nX-Sender: anand.akela@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 15859 invoked from network); 2 Jun 2006 21:52:47 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m34.grp.scd.yahoo.com with QMQP; 2 Jun 2006 21:52:47 -0000\r\nReceived: from unknown (HELO n28b.bullet.scd.yahoo.com) (209.73.160.85)\n  by mta7.grp.scd.yahoo.com with SMTP; 2 Jun 2006 21:52:47 -0000\r\nComment: DomainKeys? See http://antispam.yahoo.com/domainkeys\r\nReceived: from [66.218.69.5] by n28.bullet.scd.yahoo.com with NNFMP; 02 Jun 2006 21:52:06 -0000\r\nReceived: from [66.218.66.82] by t5.bullet.scd.yahoo.com with NNFMP; 02 Jun 2006 21:52:06 -0000\r\nDate: Fri, 02 Jun 2006 21:52:06 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;e5qbu6+hpsq@...&gt;\r\nIn-Reply-To: &lt;16838.52547.537089.448046@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;anand_akela&quot; &lt;anand.akela@...&gt;\r\nSubject: Has anyone tried Libarc library on Suse Linux?\r\nX-Yahoo-Group-Post: member; u=267551907; y=agDnNABYHbjrrGIP49cmI3xquHwsa2fkfela2KbCTjSSXBEf9Ks\r\nX-Yahoo-Profile: anand_akela\r\n\r\nHas anyone tried Libarc library on Suse Linux? I am getting errors\nwhen I a=\r\nm trying to make the library. I am interested in arcdump\nutility and wonder=\r\ning if anyone has built and tried on Suse Linux\nrecently.\n\nThanks\n\n--- In a=\r\nrchive-crawler@yahoogroups.com, Tom Emerson &lt;Tree@...&gt; wrote:\n&gt;\n&gt; Marco Bar=\r\noni writes:\n&gt; &gt; As a linguist, I have been interested in downloading large =\r\namounts of \n&gt; &gt; text from the web for various forms of statistical analyses=\r\n.\n&gt; \n&gt; Indeed: we&#39;re using Heritrix for exactly this purpose.\n&gt; \n&gt; &gt; Now, a=\r\ns I should get some funds to buy a few dedicated servers, I \n&gt; &gt; would like=\r\n to get started with some more ambitious crawl, and\nheritrix \n&gt; &gt; looks lik=\r\ne the ideal choice.\n&gt; \n&gt; It is, in the latest releases. And you can do not =\r\nnecessarily need\n&gt; significant hardware to run your crawls, though post-pro=\r\ncessing will\n&gt; be dependent on local processing power.\n&gt; \n&gt; &gt; 1) I understa=\r\nnd that there is a way to stop fetching documents\nthat do \n&gt; &gt; not match ce=\r\nrtain content-types with a filter. Is there a way to\ninsert \n&gt; &gt; such a fil=\r\nter via the standard WUI?\n&gt; \n&gt; Yes, you can specify filters at various stag=\r\nes of the process from\n&gt; within the WUI. In my data crawls I define three d=\r\nifferent filters in\n&gt; the &quot;Filters&quot; page:\n&gt; \n&gt; 1. crawl-order:scope:exclude=\r\n-filter:filters\n&gt;       URIRegExpFilter\n&gt; \n&gt; 2. fetch-processors:HTTP:midfe=\r\ntch-filters\n&gt;       ContentTypeRegExpFilter\n&gt; \n&gt; 3. write-processors:Archiv=\r\ner:filters\n&gt;       ContentTypeRegExpFilter\n&gt; \n&gt; The first is configured to =\r\nrestrict the set of file extensions that\n&gt; are treated as being in-scope:\n&gt;=\r\n \n&gt;\n.*(?i)&#92;.(a|ai|aif|aifc|aiff|asc|au|avi|bcpio|bin|bmp|bz2|c|cdf|cgi|cgm|=\r\nclass|cpio|cpp?|cpt|csh|css|cxx|dcr|dif|dir|djv|djvu|dll|dmg|dms|doc|dtd|dv=\r\n|dvi|dxr|eps|etx|exe|ez|gif|gram|grxml|gtar|h|hdf|hqx|ice|ico|ics|ief|ifb|i=\r\nges|igs|iso|jnlp|jp2|jpe|jpeg|jpg|js|kar|latex|lha|lzh|m3u|mac|man|mathml|m=\r\ne|mesh|mid|midi|mif|mov|movie|mp2|mp3|mp4|mpe|mpeg|mpg|mpga|ms|msh|mxu|nc|o=\r\n|oda|ogg|pbm|pct|pdb|pdf|pgm|pgn|pic|pict|pl|png|pnm|pnt|pntg|ppm|ppt|ps|py=\r\n|qt|qti|qtif|ra|ram|ras|rdf|rgb|rm|roff|rpm|rtf|rtx|s|sgm|sgml|sh|shar|silo=\r\n|sit|skd|skm|skp|skt|smi|smil|snd|so|spl|src|srpm|sv4cpio|sv4crc|svg|swf|t|=\r\ntar|tcl|tex|texi|texinfo|tgz|tif|tiff|tr|tsv|ustar|vcd|vrml|vxml|wav|wbmp|w=\r\nbxml|wml|wmlc|wmls|wmlsc|wrl|xbm|xht|xhtml|xls|xml|xpm|xsl|xslt|xwd|xyz|z|z=\r\nip)$\n&gt; \n&gt; Note that this excludes &quot;.au&quot;, which can be problematic if you ar=\r\ne\n&gt; fetching documents from domains in Australia, since the DNS request\n&gt; g=\r\nets filtered and this is a required prerequisite, so the entire site\n&gt; gets=\r\n filtered.\n&gt; \n&gt; The second and third have the same RegExp, and restrict the=\r\n content to\n&gt; text/html:\n&gt; \n&gt; (?i)text/html.*\n&gt; \n&gt; If you don&#39;t include the=\r\n write-processor filter then you will end up\n&gt; with an ARC file containing =\r\npartial content interrupted by the\n&gt; mid-fetch filter.\n&gt; \n&gt; If you want to =\r\ndownload and save other content types then you will\n&gt; obviously need to mod=\r\nify these regular expressions appropriately.\n&gt; \n&gt; &gt; 2) Similarly, is it pos=\r\nsible to restrict the documents to be\nwritten to \n&gt; &gt; the arc files to a ce=\r\nrtain maximum and minimum size?\n&gt; \n&gt; No, I don&#39;t believe so. It would be pr=\r\netty easy to write a new type of\n&gt; filter and add it to the write-processor=\r\n. You would not want to do\n&gt; this as a mid-fetch filter though, because not=\r\n all servers include the\n&gt; Content-Length: header.\n&gt; \n&gt; I filter on this du=\r\nring post-processing: at that point you know both\n&gt; the size of the returne=\r\nd markup and, after removing the markup, the\n&gt; size of the remaining data. =\r\nI limit my saved data to the extracted\n&gt; size, not the size including marku=\r\np since I&#39;ve found that in the\n&gt; contemporary web the markup absolutely dwa=\r\nrfs the content in many\n&gt; cases.\n&gt; \n&gt; &gt; 3) In order to extract pure text fr=\r\nom the documents, I plan to write \n&gt; &gt; scripts that invoke arcdump to get f=\r\nile type and contents out of the \n&gt; &gt; arcs, and then call appropriate tools=\r\n such as pdftotex. Actually, for \n&gt; &gt; now I would be quite happy just extra=\r\ncting text from the html files. \n&gt; &gt; Before I re-invent the wheel, is there=\r\n already some tool/module to do \n&gt; &gt; this?\n&gt; \n&gt; I&#39;ve written this a few tim=\r\nes, using my libarc C++ library. Currently\n&gt; I strip markup with extreme pr=\r\nejudice, run a language/content\n&gt; identifier on the remainder, transcode th=\r\ne original to UTF-8 from the\n&gt; detected encoding, then dump the HTML (*not*=\r\n the extracted text) to\n&gt; disk that is within my size constraints. At the e=\r\nnd of this phase I\n&gt; have hundreds of thousands to millions of HTML files t=\r\nhat fit my size\n&gt; and language constraints. For my work I need to preserve =\r\nas much\n&gt; logical structure in the original texts as possible, so simple HT=\r\nML\n&gt; stripping does not work for me. Instead I use an open source tool\n&gt; ca=\r\nlled Vilistextum to extract the text from the HTML in all of the\n&gt; extracte=\r\nd files.\n&gt; \n&gt; Once that is done we have lots of plain text to use for vario=\r\nus\n&gt; linguistics work. By way of example I&#39;ve used this technique to\n&gt; coll=\r\nect a corpus of over 2 GB of plain (i.e., post-processed)\n&gt; Traditional Chi=\r\nnese text, and the crawl is still going.\n&gt; \n&gt; We&#39;ve done the same for about=\r\n 200-300 MB of post-processed French,\n&gt; Italian, Vietnamese, and Tagalog (w=\r\nell, an order or magnitude less\n&gt; Tagalog).\n&gt; \n&gt; &gt; 4) Am I right in thinkin=\r\ng that heritrix will not download the same\nurl \n&gt; &gt; twice (within the same =\r\ncrawling job)?\n&gt; \n&gt; Yes.\n&gt; \n&gt; This scratches the surface. For the Tagalog a=\r\nnd Vietnamese crawls we\n&gt; seeded using techniques described in your BootCat=\r\n papers, and we&#39;ve\n&gt; thought about how to integrate BootCat type processes =\r\ninto Heritrix\n&gt; proper, but for now we do it separately using some Java app=\r\ns utilizing\n&gt; the Google API and our language identifier technology.\n&gt; \n&gt;  =\r\n   -tree\n&gt; \n&gt; -- \n&gt; Tom Emerson                                          Ba=\r\nsis\nTechnology Corp.\n&gt; Software Architect                                \nh=\r\nttp://www.basistech.com\n&gt;   &quot;Beware the lollipop of mediocrity: lick it onc=\r\ne and you suck forever&quot;\n&gt;\n\n\n\n\n\n"}}