{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"xod-X7Y-DFEDM2iKNN1roOPCSgzIwWtErK9X4Cc7SZXL8K0aHCHlfUt3DRMZ65KxvQBBlqcMCD4bKggXiLpjtpRaM6JhsvU","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix Frontier","postDate":"1297366367","msgId":7014,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENTQzRDVGLjkwNjA0MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRENTQyQjA5LjkwMjA0MDRAZ21haWwuY29tPg==","referencesHeader":"PEM5Nzc0OTQwLjIxMUUlbXVyYWxpa3BAYW1hem9uLmNvbT4gPDRENTFEODI4LjkwNzA5MDJAYXJjaGl2ZS5vcmc+IDw0RDU0MkIwOS45MDIwNDA0QGdtYWlsLmNvbT4="},"prevInTopic":7013,"nextInTopic":7023,"prevInTime":7013,"nextInTime":7015,"topicId":7005,"numMessagesInTopic":11,"msgSnippet":"Kenji s prototype falls somewhere between the first and second options in the range of possibilities I d listed for a remote/distributed frontier, if I","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 17487 invoked from network); 10 Feb 2011 19:32:54 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.sp2.yahoo.com with QMQP; 10 Feb 2011 19:32:54 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta1.grp.sp2.yahoo.com with SMTP; 10 Feb 2011 19:32:54 -0000\r\nX-Received: (qmail 40276 invoked by uid 0); 10 Feb 2011 19:32:47 -0000\r\nX-Received: from 67.188.34.83 (HELO silverbook.local) (67.188.34.83)\n  by relay02.pair.com with SMTP; 10 Feb 2011 19:32:47 -0000\r\nX-pair-Authenticated: 67.188.34.83\r\nMessage-ID: &lt;4D543D5F.9060404@...&gt;\r\nDate: Thu, 10 Feb 2011 11:32:47 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;C9774940.211E%muralikp@...&gt; &lt;4D51D828.9070902@...&gt; &lt;4D542B09.9020404@...&gt;\r\nIn-Reply-To: &lt;4D542B09.9020404@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix Frontier\r\nX-Yahoo-Group-Post: member; u=137285340; y=aF3tQvKUPwBLbM_FzTxS4u8tJIhIdHJxYsTTldEPqSnD\r\nX-Yahoo-Profile: gojomo\r\n\r\nKenji&#39;s prototype falls somewhere between the first and second options \nin the &#39;range of possibilities&#39; I&#39;d listed for a &#39;remote/distributed&#39; \nfrontier, if I understand correctly. I haven&#39;t yet seen the code.\n\nBecause the frontier (and especially the already-seen test currently \nperformed by the UriUniqFilter) is filled with operations requiring the \nmost disk seeks, and benefits the most from extra main memory, I have \nconcerns that any design which introduces a central control server for \nall frontier operations could limit multi-machine performance. But, I&#39;m \ninterested to see the results of real tests.\n\n- Gordon @ IA\n\nOn 2/10/11 10:14 AM, Kenji Nagahashi wrote:\n&gt; Hi Krishna,\n&gt;\n&gt; I&#39;m an engineer working on web-wide crawl at Internet Archive.\n&gt; As Gordon said, We&#39;re strongly interested in the architecture you\n&gt; described, and in fact developing a Heritrix-3 based prototype right\n&gt; now. For your information, current approach is:\n&gt;\n&gt; - a simple central control server, backed by distributed storage, that\n&gt; does &quot;seen&quot; check, de-deplication and URL scheduling\n&gt; - Heritrix 3 configured with custom UniqUriFilter and a disposition\n&gt; processor talking with the central server over HTTP\n&gt;\n&gt; currently it requires small modification to WorkQueueFrontier that\n&gt; allows it to efficiently &quot;pull&quot; more URIs from the central server when\n&gt; it run out of &quot;ready&quot; queues. So it&#39;s unlikely that it will be\n&gt; compatible with the next release of Heritrix 3, although I&#39;d be happy to\n&gt; make it a part of future releases.\n&gt;\n&gt; I really welcome discussions on this topic.\n&gt;\n&gt; Thanks,\n&gt; Kenji Nagahashi\n&gt;\n&gt; (2/8/11 3:56 PM), Gordon Mohr wrote:\n&gt;&gt; On 2/8/11 5:55 AM, Krishna, Murali wrote:\n&gt;&gt;   &gt;  Thanks Gordon for the detailed response.\n&gt;&gt;   &gt;  We don&#39;t have all the urls upfront, it is a continuous stream of urls\n&gt;&gt;   &gt;  and we don&#39;t want to wait for the previous heritrix jobs to finish.\n&gt;&gt;   &gt;  Essentially, we want to schedule them as and when is possible (honoring\n&gt;&gt;   &gt;  politeness). Also some of the urls have higher priority for crawling.\n&gt;&gt;\n&gt;&gt; If you want your prioritization to also take effect within the standard\n&gt;&gt; Heritrix frontier queues, there&#39;s a (seldom-used) pair of features,\n&gt;&gt; queue and URI &#39;precedence&#39; values, which affect (1) how inactive queues\n&gt;&gt; are sorted while waiting for their chance to be active; (2) where URIs\n&gt;&gt; are inserted into individual queues (if all the other factors which\n&gt;&gt; affect whether they are pushed-near-the-top or queued-to-the-back are\n&gt;&gt; equal).\n&gt;&gt;\n&gt;&gt;   &gt;  The problem with BDB frontier is that it is tied to the box and is a\n&gt;&gt;   &gt;  reliability concern if the machine goes down. We are thinking of\n&gt;&gt; having the\n&gt;&gt;   &gt;  urls in reliable queue service in a different cluster and make the\n&gt;&gt; heritrix\n&gt;&gt;   &gt;  read from that queue. This makes heritrix instance stateless (the crawled\n&gt;&gt;   &gt;  content goes to another cluster) and easy to replace with another box. Of\n&gt;&gt;   &gt;  course this calls for a new checkpointing mechanism outside the box.\n&gt;&gt;\n&gt;&gt; We&#39;ve considered possibilities for this as well, and some sort of\n&gt;&gt; remote/distributed frontier is likely in the future for Heritrix, though\n&gt;&gt; nothing is definitively scheduled/prioritized for an upcoming release.\n&gt;&gt;\n&gt;&gt; Some of the range of possibilities could include:\n&gt;&gt;\n&gt;&gt; - Replacing the current BDB-JE binary key-value store in the standard\n&gt;&gt; frontier with a remote/distributed alternative. This might be most\n&gt;&gt; straightforward, though a few issues that don&#39;t come up with the local\n&gt;&gt; store would have to be addressed, including: (1) new latencies/overhead;\n&gt;&gt; (2) multiple crawl processes sharing the same store, (3) what recovery\n&gt;&gt; from arbitrary crashes might be possible.\n&gt;&gt;\n&gt;&gt; - Still using the traditional default frontier locally, but pull URIs in\n&gt;&gt; batches from the shared/remote store. (This might not require any\n&gt;&gt; rewriting of the local frontier; just some other module that reports\n&gt;&gt; results up, and pulls the right batches of new URIs down.) The shared\n&gt;&gt; URI-queues/history info could be very different in its representations,\n&gt;&gt; checkpointing, etc.\n&gt;&gt;\n&gt;&gt; - Replacing the usual local frontier with an alternative which operates\n&gt;&gt; in a whole new way with the remote queues/store, just implementing the\n&gt;&gt; minimal expected frontier behavior. We want the code to support this\n&gt;&gt; possibility, but there are almost certainly hidden assumptions in other\n&gt;&gt; parts of the code that the frontier behaves like our standard one, which\n&gt;&gt; would need some clean up when tested by this new approach. And, much of\n&gt;&gt; the existing timing/politeness/ordering behavior would have to be\n&gt;&gt; reimplementing in broadly-similar ways... though perhaps much of the\n&gt;&gt; existing design could be mirrored albeit with remote queues/sets.\n&gt;&gt;\n&gt;&gt;   &gt;  I understand it is an overkill to use heritrix, but In future, we might\n&gt;&gt;   &gt;  need depth crawl which we can easily implement by scheduling the newly\n&gt;&gt;   &gt;  detected urls back into the reliable queue service. We are just trying to\n&gt;&gt;   &gt;  leverage the politeness, threading and pluggable processor frameworks of\n&gt;&gt;   &gt;  heritrix.\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;  Thoughts?\n&gt;&gt;\n&gt;&gt; I better understand your motivations and they seem reasonable. I&#39;ll be\n&gt;&gt; very interested to hear any architectural directions you take, and can\n&gt;&gt; discuss. If either the code, or simply the changes that make Heritrix\n&gt;&gt; better able to rely on remote frontier functionality, can be contributed\n&gt;&gt; back, it will likely be of interest to the IA and other Heritrix users\n&gt;&gt; as well!\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt;   &gt;  Thanks,\n&gt;&gt;   &gt;  Murali\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;  On 2/8/11 1:56 PM, &quot;Gordon Mohr&quot;&lt;gojomo@...\n&gt;&gt; &lt;mailto:gojomo%40archive.org&gt;&gt;  wrote:\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;&gt;  On 2/7/11 3:17 AM, Krishna, Murali wrote:\n&gt;&gt;   &gt;&gt;&gt;\n&gt;&gt;   &gt;&gt;&gt;\n&gt;&gt;   &gt;&gt;&gt;  Hi all,\n&gt;&gt;   &gt;&gt;&gt;  We have a list of urls to be crawled, essentially just a fetch and some\n&gt;&gt;   &gt;&gt;&gt;  processing. Assume that the list can be huge and run into billions. So,\n&gt;&gt;   &gt;&gt;&gt;  we are thinking of writing a new Frontier which will accomplish this.\n&gt;&gt;   &gt;&gt;&gt;  Will have multiple heritrix worker boxes, each of the worker�s frontier\n&gt;&gt;   &gt;&gt;&gt;  will get one portion of the centralized url repository (distributed\n&gt;&gt;   &gt;&gt;&gt;  storage) and schedule them for crawling.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  You probably won&#39;t need a new Frontier for this; the default frontier\n&gt;&gt;   &gt;&gt;  (BdbFrontier) should work for tens to even hundreds of millions of\n&gt;&gt;   &gt;&gt;  queued URIs per node.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  I have more confidence in H3 for loading millions of seed URIs at\n&gt;&gt;   &gt;&gt;  startup (which should be even more efficient in H3 SVN TRUNK and the\n&gt;&gt;   &gt;&gt;  next H3 release), though you could also feed them in smaller batches via\n&gt;&gt;   &gt;&gt;  the H1 JMX interface, or in batches via the &#39;action&#39; directory mechanism\n&gt;&gt;   &gt;&gt;  in H3.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  If you simply have a large static list of URIs to crawl -- and don&#39;t\n&gt;&gt;   &gt;&gt;  need link-extraction and any other running analysis/reporting --\n&gt;&gt;   &gt;&gt;  Heritrix may be overkill for your purposes.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;&gt;  1. Can we achieve this by extending the WorkQueueFrontier ? I couldn�t\n&gt;&gt;   &gt;&gt;&gt;  find much documentation on how WQF handles politeness. I am thinking of\n&gt;&gt;   &gt;&gt;&gt;  grouping the urls into workqueue based on politeness requirement, will\n&gt;&gt;   &gt;&gt;&gt;  it automatically take care of politeness if I group correctly? Can I\n&gt;&gt;   &gt;&gt;&gt;  configure crawl-delay per WorkQueue?\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  You can control what is crawled -- whether outlinks from your starting\n&gt;&gt;   &gt;&gt;  URIs are followed, for example, to get inline resources or other linked\n&gt;&gt;   &gt;&gt;  pages -- by customizing the scoping rules. If grouping URIs by hostname\n&gt;&gt;   &gt;&gt;  into queues is insufficient, you can implement a new\n&gt;&gt;   &gt;&gt;  QueueAssignmentPolicy. In H3, politeness delays per URI -- affecting the\n&gt;&gt;   &gt;&gt;  queue from which the URI came -- are configured outside the frontier, in\n&gt;&gt;   &gt;&gt;  the DispositionProcessor. So lots of behavioral customization doesn&#39;t\n&gt;&gt;   &gt;&gt;  require reimplementing or specializing the frontier.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  The queues are the units of politeness: by default, only one URI from a\n&gt;&gt;   &gt;&gt;  queue will be in-process at a time. When a URI finishes, a configurable\n&gt;&gt;   &gt;&gt;  pause (see the minDelay, maxDelay, delayFactor, and\n&gt;&gt;   &gt;&gt;  respectCrawlDelayUpToSeconds settings on DispositionProcessor in H3) is\n&gt;&gt;   &gt;&gt;  applied to that queue before any other URIs are tried. Note that URI\n&gt;&gt;   &gt;&gt;  domain-lookup/connectivity failures cause the same URI to be pushed back\n&gt;&gt;   &gt;&gt;  atop the queue, a longer (frontier retryDelaySeconds) pause to be taken,\n&gt;&gt;   &gt;&gt;  and multiple (frontier maxRetries) attempts to be made, before the next\n&gt;&gt;   &gt;&gt;  URI is tried. This means you usually do not want URIs on different\n&gt;&gt;   &gt;&gt;  hosts, where one host might be unreachable, mixed in the same queue --\n&gt;&gt;   &gt;&gt;  one failure will delay them all -- unless you also knock the\n&gt;&gt;   &gt;&gt;  retries/retryDelay way down.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  You can use the settings &#39;sheet overlay&#39; (aka &#39;overrides&#39; in H1) to set\n&gt;&gt;   &gt;&gt;  different politeness values for different URIs by host or other\n&gt;&gt;   &gt;&gt;  patterns; the queue then takes on the delay of the URI that was just\n&gt;&gt;   &gt;&gt;  offered/completed.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  You should also look at previous list traffic about HashCrawlMapper for\n&gt;&gt;   &gt;&gt;  ideas on splitting the URI space, and the BloomUriUniqFilter as an\n&gt;&gt;   &gt;&gt;  option for an all in-memory URI-already-seen filter that may be\n&gt;&gt;   &gt;&gt;  appropriate for larger crawls.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;&gt;  2. What are inactive queues, retired queues and getURIList here?\n&gt;&gt; (sorry,\n&gt;&gt;   &gt;&gt;&gt;  couldn�t find doc)\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  &#39;inactive&#39; queues are those that are not yet being considered to keep a\n&gt;&gt;   &gt;&gt;  thread busy. All those queues that are &#39;active&#39; round-robin to provide\n&gt;&gt;   &gt;&gt;  URIs to available threads, until the queue exhausts its &#39;session&#39;\n&gt;&gt;   &gt;&gt;  budget; then it goes to the back of all &#39;inactive&#39; queues. If a thread\n&gt;&gt;   &gt;&gt;  can&#39;t be kept busy with an available &#39;active&#39; queue, then the top\n&gt;&gt;   &gt;&gt;  &#39;inactive&#39; queue is activated. The intent is for the crawler to\n&gt;&gt;   &gt;&gt;  intensely focus on some queues for a while -- hoping to finish them, or\n&gt;&gt;   &gt;&gt;  at least get a big batch of URIs with as little time-skew as possible ��\n&gt;&gt;   &gt;&gt;  but then rotate others into activity eventually. (The &#39;budgeting&#39; and\n&gt;&gt;   &gt;&gt;  URI &#39;cost&#39; parameters affect this cycle.)\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  &#39;retired&#39; queues have already offered up URIs whose total &#39;cost&#39; exceeds\n&gt;&gt;   &gt;&gt;  their &#39;totalBudget&#39;, and so they continue to collect newly-discovered\n&gt;&gt;   &gt;&gt;  URIs, but will never b considered for &#39;active&#39; rotation (unless you\n&gt;&gt;   &gt;&gt;  raise their &#39;totalBudget&#39;). You probably don&#39;t need this feature, and\n&gt;&gt;   &gt;&gt;  won&#39;t notice any &#39;retired&#39; queues unless you set a &#39;totalBudget&#39; and\n&gt;&gt;   &gt;&gt;  nonzero URI cost policy.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  I don&#39;t know what you mean by &quot;getURIList&quot;.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;&gt;  3. How does checkpointing work, I want to restart from the last crawled\n&gt;&gt;   &gt;&gt;&gt;  state. Is there a callback to do the frequent checkpointing for\n&gt;&gt;   &gt;&gt;&gt;  WorkQueueFrontier�s implementations.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  Checkpointing tries to save the whole crawl state at requested points.\n&gt;&gt;   &gt;&gt;  You can set it to automatically checkpoint at a certain interval. In H1,\n&gt;&gt;   &gt;&gt;  it&#39;s via a system property; see Checkpointer.initialize(). In H3, it&#39;s\n&gt;&gt;   &gt;&gt;  CheckpointService&#39;s checkpointIntervalMinutes property. In H1, the crawl\n&gt;&gt;   &gt;&gt;  must reach a full pause for a checkpoint to occur; with long downloads\n&gt;&gt;   &gt;&gt;  and connection timeouts, this can mean a slowdown in the tens of\n&gt;&gt;   &gt;&gt;  minutes. In H3, a checkpoint requires a much narrower lock, so may only\n&gt;&gt;   &gt;&gt;  take a few seconds or a minute or two, and long network fetches may\n&gt;&gt;   &gt;&gt;  continue during the checkpoint.\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  In both cases, you need to retain the &#39;state&#39; directory files\n&gt;&gt;   &gt;&gt;  (.JDB/.DEL) corresponding to the checkpoints from which you might want\n&gt;&gt;   &gt;&gt;  to resume. (If not needed for a checkpoint, you can freely delete the\n&gt;&gt;   &gt;&gt;  .DELs.) Resuming from an earlier checkpoint may foul any other\n&gt;&gt;   &gt;&gt;  subsequent-but-unused checkpoints (though future checkpoints should be\n&gt;&gt;   &gt;&gt;  fine).\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  The checkpointing system has always been a bit rough but I have more\n&gt;&gt;   &gt;&gt;  confidence in its flexibility and speed in H3. I would not yet count on\n&gt;&gt;   &gt;&gt;  it for perfect resumability in a large crawl without more experience\n&gt;&gt;   &gt;&gt;  using it. If not using checkpoints, or checkpoints fail for some reason,\n&gt;&gt;   &gt;&gt;  an approximation of a frontier&#39;s state at the time of a crash can be\n&gt;&gt;   &gt;&gt;  recreated from the &#39;frontier recovery log&#39; also kept by the crawler.\n&gt;&gt;   &gt;&gt;  (Not all running stats/state can be reconstructed, but essentially all\n&gt;&gt;   &gt;&gt;  the same pending URIs will be reenqueued.)\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  I believe there&#39;s a JMX call in H1 to request a checkpoint, and in H3\n&gt;&gt;   &gt;&gt;  it&#39;s just one of the web UI/web service calls easy to trigger by a\n&gt;&gt; web hit:\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;\n&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Herit\n&gt;&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Heritrix+3.0+API+Guide#Herit&gt;\n&gt;&gt;   &gt;&gt;  rix3.0APIGuide-CheckpointJob\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  Hope this helps!\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  - Gordon @ IA\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  ------------------------------------\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;  Yahoo! Groups Links\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;&gt;\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;  ------------------------------------\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;  Yahoo! Groups Links\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;\n&gt;&gt;   &gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}