{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":91078969,"authorName":"Jigar Patel","from":"Jigar Patel &lt;jigar_bca@...&gt;","profile":"jigar_bca","replyTo":"LIST","senderId":"zZf1qtNhE-hYW0aqcuyuFvd60xel_SETBr6OL_v1NmvAUvLZMnAsrFN34iEQCT-a9JnbzdeVItPN4Gm52XCYQXkA5XcQMk9U","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Distributed Crawling","postDate":"1183010025","msgId":4383,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3MzM2OC4yMDg3Ny5xbUB3ZWI1MDMwNS5tYWlsLnJlMi55YWhvby5jb20+","inReplyToHeader":"PDQ2ODMwMjEyLjMwNjAzMDlAZ3JhZW1lcy5jb20+"},"prevInTopic":4379,"nextInTopic":4385,"prevInTime":4382,"nextInTime":4384,"topicId":3834,"numMessagesInTopic":26,"msgSnippet":"Hi, Thanks a lot for you replies... When you follow http://crawler. sourceforge. net/hcc link it gives you HTTP 404 error. By the way where can I get it and","rawEmail":"Return-Path: &lt;jigar_bca@...&gt;\r\nX-Sender: jigar_bca@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 88043 invoked from network); 28 Jun 2007 05:53:54 -0000\r\nReceived: from unknown (66.218.66.68)\n  by m45.grp.scd.yahoo.com with QMQP; 28 Jun 2007 05:53:54 -0000\r\nReceived: from unknown (HELO web50305.mail.re2.yahoo.com) (206.190.38.59)\n  by mta11.grp.scd.yahoo.com with SMTP; 28 Jun 2007 05:53:52 -0000\r\nReceived: (qmail 21241 invoked by uid 60001); 28 Jun 2007 05:53:45 -0000\r\nX-YMail-OSG: aafUGFwVM1lrI1xnbuegDCGRw1d5xZMEGB48TVoB\r\nReceived: from [203.199.114.33] by web50305.mail.re2.yahoo.com via HTTP; Wed, 27 Jun 2007 22:53:45 PDT\r\nDate: Wed, 27 Jun 2007 22:53:45 -0700 (PDT)\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;46830212.3060309@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-26056162-1183010025=:20877&quot;\r\nContent-Transfer-Encoding: 8bit\r\nMessage-ID: &lt;473368.20877.qm@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Jigar Patel &lt;jigar_bca@...&gt;\r\nSubject: Re: [archive-crawler] Re: Distributed Crawling\r\nX-Yahoo-Group-Post: member; u=91078969; y=A8T-YMAaBcTQrIWd0TJY8medimYKy0QVZ9GMMeE1IqzqWGWu\r\nX-Yahoo-Profile: jigar_bca\r\n\r\n\r\n--0-26056162-1183010025=:20877\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nHi,\n\nThanks a lot for you replies...\n\nWhen you follow http://crawler. sourceforge. net/hcc link it gives you HTTP 404 error.\n\nBy the way where can I get it and how can I download HCC ?\n\nThanks and Regards,\n\nJigar Patel\n\n&quot;lists@...&quot; &lt;lists@...&gt; wrote:                                      HCC seems to offer a number of answers to this kind of configuration but getting hold of documentation regarding it (try http://crawler.sourceforge.net/hcc ) is a slightly challenging.  Is HCC still in active use? If so, can someone point me to a tad more doco.?\n \n As an aside - How many toes should a Heritrix on a quad core/cpu system be able to mange if it is assumed that memory/io/bandwidth are removed from the equation?\n \n Regards,\n Graeme\n \n Gordon Mohr wrote: \n       Jigar Patel wrote:\n &gt; Thanks a lot Gordon,\n &gt; \n &gt; You solved my problem.\n &gt; \n &gt; One more thing I want to know that, \n &gt; Can I use same settings for two different independent machines for distributed crawling ?\n   \n Yes; that is the usual case for distributed crawling: each of the \n crawlers has the exact same initial configuration, *except* for the \n HashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to \n decline to process URIs mapped to the others.\n   \n There is no automatic facility for sharing the settings; you must \n manually copy the order.xml (and possibly other files) between crawlers. \n Also, there is no automatic facility for cross-feeding the URIs each \n crawler rejects. You must watch the crawl.log (or diversions logs from \n the mapper) and decide if the URIs should be fed to the sibling crawlers.\n   \n - Gordon @ IA\n   \n &gt; Please tell me how does it work and coordinate with different machine ?\n &gt; \n &gt; Thanks\n &gt; \n &gt; Jigar\n &gt; \n &gt; Gordon Mohr &lt;gojomo@...&gt; wrote:\n &gt; Jigar Patel wrote:\n &gt;&gt; Presently I am running two heritrix instances on the same machine on \n &gt;&gt; different port...\n &gt; \n &gt; In general, you only want to use distributed crawling, with URIs \n &gt; partitioned across separate cooperating crawlers, to spread a crawl over \n &gt; multiple independent machines. If using a single machine, a single   \n &gt; crawler instance will be more efficient.\n &gt; \n &gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule\n &gt;&gt; I added HashCrawlMapper at two places as you suggested\n &gt;&gt; I made same configuration setting and seed file at each place.\n &gt;&gt;\n &gt;&gt; But as I run my job it gives me following error in seed file and \n &gt;&gt; nothing was crawled.\n &gt;&gt;\n &gt;&gt; Heritrix(-5002)-Blocked by custom prefetch processor \n &gt;&gt;\n &gt;&gt; Please let me know why I am getting such error...\n &gt;&gt;\n &gt;&gt; Is anything missing ?\n &gt; \n &gt; This is the expected crawl.log result for URIs that are considered by a \n &gt; crawler, but mapped to be handled by one of the others in the group of \n &gt; crawlers. With a proper configuration, some but not all lines in your \n &gt; crawl.log will have this code.\n &gt; \n &gt; For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and \n &gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; of &#39;2&#39;.\n &gt; \n &gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, but \n &gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will appear in the \n &gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the \n &gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.\n &gt; \n &gt; - Gordon @ IA\n &gt; \n &gt;&gt; Regards,\n &gt;&gt;\n &gt;&gt; Jigar Patel\n &gt;&gt;\n &gt;&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; \n &gt;&gt; wrote:\n &gt;&gt;&gt; nt_bdr wrote:\n &gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?\n &gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we \n &gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600 \n &gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8 \n &gt;&gt; machines \n &gt;&gt;&gt; (&gt;1 billion URLs fetched).\n &gt;&gt;&gt;\n &gt;&gt;&gt; For background see some previous threads including:\n &gt;&gt;&gt;\n &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2909\n &gt;&gt;&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/3060\n &gt;&gt;&gt;\n &gt;&gt;&gt; Roughly how we do it:\n &gt;&gt;&gt;\n &gt;&gt;&gt; - Use BloomFilterUriUniqFilter with its defaults -- which devotes \n &gt;&gt;&gt; about 500MB to this structure and keeps the false-positive   \n &gt;&gt; (mistakenly \n &gt;&gt;&gt; believed to have been previously-scheduled) rate under 1-in-4-\n &gt;&gt; million up \n &gt;&gt;&gt; through 125 million URIs discovered.\n &gt;&gt;&gt;\n &gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with ~1.8GB+ \n &gt;&gt; heap\n &gt;&gt;&gt; - Use SurtAuthorityAssignmentPolicy, so URIs are grouped in \n &gt;&gt; queues \n &gt;&gt;&gt; named by the reversed-host (com,example,) rather than usual host \n &gt;&gt;&gt; (example.com)\n &gt;&gt;&gt;\n &gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor \n &gt;&gt; chain:\n &gt;&gt;&gt; * Once, immediately before the PreconditionEnforcer. This one \n &gt;&gt; has \n &gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any \n &gt;&gt; scheduled \n &gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)\n &gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has \n &gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any \n &gt;&gt; discovered \n &gt;&gt;&gt; outlinks before they are scheduled.)\n &gt;&gt;&gt;\n &gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39; (a   \n &gt;&gt; number 0 \n &gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and \n &gt;&gt; all \n &gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers, \n &gt;&gt; n).\n &gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the \n &gt;&gt; SURT \n &gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI \n &gt;&gt; is \n &gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to \n &gt;&gt; a \n &gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally. \n &gt;&gt;&gt; Depending on the character of your crawl, you may want to feed \n &gt;&gt; these \n &gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore \n &gt;&gt; them.\n &gt;&gt;&gt; The &#39;reduce-prefix-pattern&#39; may be used to trim the queue key \n &gt;&gt; before \n &gt;&gt;&gt; mapping -- used to ensure that all subdomains of example.com are \n &gt;&gt; treated \n &gt;&gt;&gt; the same as example.com for mapping purposes. The first match of \n &gt;&gt; this \n &gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small \n &gt;&gt;&gt; example would be:\n &gt;&gt;&gt;\n &gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*)|[&#92;w,]{9})\n &gt;&gt;&gt;\n &gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything   \n &gt;&gt; through \n &gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it \n &gt;&gt; uses \n &gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns \n &gt;&gt; that \n &gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs, \n &gt;&gt; like \n &gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level \n &gt;&gt;&gt; domains.)\n &gt;&gt;&gt;\n &gt;&gt;&gt; - All crawlers are launched with the same configuration, \n &gt;&gt; including \n &gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate. \n &gt;&gt; Seeds \n &gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early \n &gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed \n &gt;&gt; are \n &gt;&gt;&gt; done so by an external process/scripts.\n &gt;&gt;&gt;\n &gt;&gt;&gt; - Gordon @ IA\n &gt;&gt;&gt;\n &gt;&gt;\n &gt;&gt;\n &gt;&gt;\n &gt;&gt;\n &gt;&gt; Yahoo! Groups Links\n &gt;&gt;\n &gt;&gt;\n &gt;&gt;\n &gt; \n &gt; \n &gt; \n &gt; \n &gt; \n &gt; \n &gt; ---------------------------------\n &gt; Food fight? Enjoy some healthy debate\n &gt; in the Yahoo! Answers Food & Drink Q&A.\n   \n   \n   \n     \n     \n                       \n\n \n---------------------------------\n Get your own web address.\n Have a HUGE year through Yahoo! Small Business.\r\n--0-26056162-1183010025=:20877\r\nContent-Type: text/html; charset=iso-8859-1\r\nContent-Transfer-Encoding: 8bit\r\n\r\nHi,&lt;br&gt;&lt;br&gt;Thanks a lot for you replies...&lt;br&gt;&lt;br&gt;When you follow &lt;a rel=&quot;nofollow&quot; class=&quot;moz-txt-link-freetext&quot; target=&quot;_blank&quot; href=&quot;http://crawler.sourceforge.net/hcc&quot;&gt;&lt;span style=&quot;background: transparent none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;&quot; id=&quot;lw_1183009790_0&quot;&gt;http://crawler. sourceforge. net/hcc&lt;/span&gt;&lt;/a&gt; link it gives you HTTP 404 error.&lt;br&gt;&lt;br&gt;By the way where can I get it and how can I download HCC ?&lt;br&gt;&lt;br&gt;Thanks and Regards,&lt;br&gt;&lt;br&gt;Jigar Patel&lt;br&gt;&lt;a rel=&quot;nofollow&quot; class=&quot;moz-txt-link-freetext&quot; target=&quot;_blank&quot; href=&quot;http://crawler.sourceforge.net/hcc&quot;&gt;&lt;span style=&quot;background: transparent none repeat scroll 0% 50%; -moz-background-clip: -moz-initial; -moz-background-origin: -moz-initial; -moz-background-inline-policy: -moz-initial;&quot; id=&quot;lw_1183009790_0&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;b&gt;&lt;i&gt;&quot;lists@...&quot; &lt;lists@...&gt;&lt;/i&gt;&lt;/b&gt; wrote:&lt;blockquote\n class=&quot;replbq&quot; style=&quot;border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;&quot;&gt;     &lt;!-- Network content --&gt;           &lt;div id=&quot;ygrp-text&quot;&gt;             &lt;div&gt;    HCC seems to offer a number of answers to this kind of configuration but getting hold of documentation regarding it (try &lt;a class=&quot;moz-txt-link-freetext&quot; href=&quot;http://crawler.sourceforge.net/hcc&quot;&gt;http://crawler.&lt;wbr&gt;sourceforge.&lt;wbr&gt;net/hcc&lt;/a&gt; ) is a slightly challenging.&nbsp; Is HCC still in active use? If so, can someone point me to a tad more doco.?&lt;br&gt; &lt;br&gt; As an aside - How many toes should a Heritrix on a quad core/cpu system be able to mange if it is assumed that memory/io/bandwidth are removed from the equation?&lt;br&gt; &lt;br&gt; Regards,&lt;br&gt; Graeme&lt;br&gt; &lt;br&gt; Gordon\n Mohr wrote: &lt;/div&gt;&lt;blockquote cite=&quot;mid:4682A47C.8050805@...&quot; type=&quot;cite&quot;&gt;&lt;!-- Network content --&gt;    &lt;div id=&quot;ygrp-text&quot;&gt;   &lt;div&gt;Jigar Patel wrote:&lt;br&gt; &gt; Thanks a lot Gordon,&lt;br&gt; &gt; &lt;br&gt; &gt; You solved my problem.&lt;br&gt; &gt; &lt;br&gt; &gt; One more thing I want to know that, &lt;br&gt; &gt; Can I use same settings for two different independent machines for distributed crawling ?&lt;br&gt;   &lt;br&gt; Yes; that is the usual case for distributed crawling: each of the &lt;br&gt; crawlers has the exact same initial configuration, *except* for the &lt;br&gt; HashCrawlMapper &#39;local-name&#39; parameter, which causes each crawler to &lt;br&gt; decline to process URIs mapped to the others.&lt;br&gt;   &lt;br&gt; There is no automatic facility for sharing the settings; you must &lt;br&gt; manually copy the order.xml (and possibly other files) between crawlers. &lt;br&gt; Also, there is no automatic facility for cross-feeding the URIs each &lt;br&gt; crawler rejects. You must watch the crawl.log (or diversions logs from &lt;br&gt; the mapper) and\n decide if the URIs should be fed to the sibling crawlers.&lt;br&gt;   &lt;br&gt; - Gordon @ IA&lt;br&gt;   &lt;br&gt; &gt; Please tell me how does it work and coordinate with different machine ?&lt;br&gt; &gt; &lt;br&gt; &gt; Thanks&lt;br&gt; &gt; &lt;br&gt; &gt; Jigar&lt;br&gt; &gt; &lt;br&gt; &gt; Gordon Mohr &lt;&lt;a moz=&quot;true&quot; href=&quot;mailto:gojomo%40archive.org&quot;&gt;gojomo@archive.&lt;wbr&gt;org&lt;/a&gt;&gt; wrote:&lt;br&gt; &gt; Jigar Patel wrote:&lt;br&gt; &gt;&gt; Presently I am running two heritrix instances on the same machine on &lt;br&gt; &gt;&gt; different port...&lt;br&gt; &gt; &lt;br&gt; &gt; In general, you only want to use distributed crawling, with URIs &lt;br&gt; &gt; partitioned across separate cooperating crawlers, to spread a crawl over &lt;br&gt; &gt; multiple independent machines. If using a single machine, a single   &lt;br&gt; &gt; crawler instance will be more efficient.&lt;br&gt; &gt; &lt;br&gt; &gt;&gt; I am using decidingScope and inside it I apply SurtPrefixRule&lt;br&gt; &gt;&gt; I added HashCrawlMapper at two places as you suggested&lt;br&gt; &gt;&gt; I made same configuration setting and\n seed file at each place.&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; But as I run my job it gives me following error in seed file and &lt;br&gt; &gt;&gt; nothing was crawled.&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; Heritrix(-5002)&lt;wbr&gt;-Blocked by custom prefetch processor &lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; Please let me know why I am getting such error...&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; Is anything missing ?&lt;br&gt; &gt; &lt;br&gt; &gt; This is the expected crawl.log result for URIs that are considered by a &lt;br&gt; &gt; crawler, but mapped to be handled by one of the others in the group of &lt;br&gt; &gt; crawlers. With a proper configuration, some but not all lines in your &lt;br&gt; &gt; crawl.log will have this code.&lt;br&gt; &gt; &lt;br&gt; &gt; For example, for two crawlers, one should have the &#39;local-name&#39; &#39;0&#39; and &lt;br&gt; &gt; the other the &#39;local-name&#39; &#39;1&#39;. Both should have a &#39;crawler-count&#39; of &#39;2&#39;.&lt;br&gt; &gt; &lt;br&gt; &gt; Every URI is mapped to either &#39;0&#39; or &#39;1&#39;. If a URI is mapped to &#39;1&#39;, but &lt;br&gt; &gt; was fed (as a seed or discovered URI) on &#39;0&#39;, it will\n appear in the &lt;br&gt; &gt; crawl.log as &#39;blocked by custom processor&#39;. It is then up to the &lt;br&gt; &gt; operator if they want to cross-feed those URIs to the &#39;1&#39; crawler.&lt;br&gt; &gt; &lt;br&gt; &gt; - Gordon @ IA&lt;br&gt; &gt; &lt;br&gt; &gt;&gt; Regards,&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; Jigar Patel&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; --- In &lt;a moz=&quot;true&quot; href=&quot;mailto:archive-crawler%40yahoogroups.com&quot;&gt;archive-crawler@&lt;wbr&gt;yahoogroups.&lt;wbr&gt;com&lt;/a&gt;, Gordon Mohr &lt;a class=&quot;moz-txt-link-rfc2396E&quot; href=&quot;mailto:gojomo@...&quot;&gt;&lt;gojomo@...&gt;&lt;/a&gt; &lt;br&gt; &gt;&gt; wrote:&lt;br&gt; &gt;&gt;&gt; nt_bdr wrote:&lt;br&gt; &gt;&gt;&gt;&gt; Can Heretrix 1.10.2 be used as a distributed crawler?&lt;br&gt; &gt;&gt;&gt; In a crude fashion, yes. It is more manual and less dynamic than we &lt;br&gt; &gt;&gt;&gt; would like, but at IA we&#39;ve run crawls over up to 6 machines (&gt;600 &lt;br&gt; &gt;&gt;&gt; million URLs visited), and know of work elsewhere over up to 8 &lt;br&gt; &gt;&gt; machines &lt;br&gt; &gt;&gt;&gt; (&gt;1 billion URLs fetched).&lt;br&gt; &gt;&gt;&gt;&lt;br&gt;\n &gt;&gt;&gt; For background see some previous threads including:&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; &lt;a moz=&quot;true&quot; href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/2909&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/2909&lt;/a&gt;&lt;br&gt; &gt;&gt;&gt; &lt;a moz=&quot;true&quot; href=&quot;http://tech.groups.yahoo.com/group/archive-crawler/message/3060&quot;&gt;http://tech.&lt;wbr&gt;groups.yahoo.&lt;wbr&gt;com/group/&lt;wbr&gt;archive-crawler/&lt;wbr&gt;message/3060&lt;/a&gt;&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; Roughly how we do it:&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; - Use BloomFilterUriUniqF&lt;wbr&gt;ilter with its defaults -- which devotes &lt;br&gt; &gt;&gt;&gt; about 500MB to this structure and keeps the false-positive   &lt;br&gt; &gt;&gt; (mistakenly &lt;br&gt; &gt;&gt;&gt; believed to have been previously-schedule&lt;wbr&gt;d) rate under 1-in-4-&lt;br&gt; &gt;&gt; million up &lt;br&gt; &gt;&gt;&gt; through 125 million URIs discovered.&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; - Use 3-6 crawlers (constant number per crawl), each with\n ~1.8GB+ &lt;br&gt; &gt;&gt; heap&lt;br&gt; &gt;&gt;&gt; - Use SurtAuthorityAssign&lt;wbr&gt;mentPolicy, so URIs are grouped in &lt;br&gt; &gt;&gt; queues &lt;br&gt; &gt;&gt;&gt; named by the reversed-host (com,example,&lt;wbr&gt;) rather than usual host &lt;br&gt; &gt;&gt;&gt; (example.com)&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; - Insert HashCrawlMapper processors at 2 places in the processor &lt;br&gt; &gt;&gt; chain:&lt;br&gt; &gt;&gt;&gt; * Once, immediately before the PreconditionEnforce&lt;wbr&gt;r. This one &lt;br&gt; &gt;&gt; has &lt;br&gt; &gt;&gt;&gt; &#39;check-uri&#39; true but &#39;check-outlinks&#39; false. (It diverts any &lt;br&gt; &gt;&gt; scheduled &lt;br&gt; &gt;&gt;&gt; URIs that should be handled by other crawlers -- chiefly seeds.)&lt;br&gt; &gt;&gt;&gt; * Again, immediately before the FrontierScheduler. This one has &lt;br&gt; &gt;&gt;&gt; &#39;check-uri&#39; false and &#39;check-outlinks&#39; true. (It diverts any &lt;br&gt; &gt;&gt; discovered &lt;br&gt; &gt;&gt;&gt; outlinks before they are scheduled.)&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; Both HashCrawlMappers should have the same &#39;local-name&#39;\n (a   &lt;br&gt; &gt;&gt; number 0 &lt;br&gt; &gt;&gt;&gt; to n-1, where n is the nubmer of crawlers in use) per machine, and &lt;br&gt; &gt;&gt; all &lt;br&gt; &gt;&gt;&gt; machines should have the same &#39;crawler-count&#39; (number of crawlers, &lt;br&gt; &gt;&gt; n).&lt;br&gt; &gt;&gt;&gt; HashCrawlMapper looks at the queue key of a URI -- here, the &lt;br&gt; &gt;&gt; SURT &lt;br&gt; &gt;&gt;&gt; authority part, because of the above choice -- and decides if a URI &lt;br&gt; &gt;&gt; is &lt;br&gt; &gt;&gt;&gt; handled by the current crawler or one of its siblings. If mapped to &lt;br&gt; &gt;&gt; a &lt;br&gt; &gt;&gt;&gt; sibling, the URI is dumped to a log rather than crawled locally. &lt;br&gt; &gt;&gt;&gt; Depending on the character of your crawl, you may want to feed &lt;br&gt; &gt;&gt; these &lt;br&gt; &gt;&gt;&gt; logs to the other crawlers occasionally or it may be OK to ignore &lt;br&gt; &gt;&gt; them.&lt;br&gt; &gt;&gt;&gt; The &#39;reduce-prefix-&lt;wbr&gt;pattern&#39; may be used to trim the queue key &lt;br&gt; &gt;&gt; before &lt;br&gt; &gt;&gt;&gt; mapping -- used to ensure that all\n subdomains of example.com are &lt;br&gt; &gt;&gt; treated &lt;br&gt; &gt;&gt;&gt; the same as example.com for mapping purposes. The first match of &lt;br&gt; &gt;&gt; this &lt;br&gt; &gt;&gt;&gt; pattern, if present, is what is used for mapping purposes. A small &lt;br&gt; &gt;&gt;&gt; example would be:&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; ^((&#92;w&#92;w&#92;w,&#92;w*&lt;wbr&gt;)|[&#92;w,]{9}&lt;wbr&gt;)&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; For 3-letter domains (com, org, net), this uses everything   &lt;br&gt; &gt;&gt; through &lt;br&gt; &gt;&gt;&gt; the 2nd-level domain for mapping purposes. For everything else, it &lt;br&gt; &gt;&gt; uses &lt;br&gt; &gt;&gt;&gt; the first 9 characters. You could imagine more complicated patterns &lt;br&gt; &gt;&gt; that &lt;br&gt; &gt;&gt;&gt; take into account other TLDs. (For example, some 2-letter TLDs, &lt;br&gt; &gt;&gt; like &lt;br&gt; &gt;&gt;&gt; &#39;fr&#39;, assign 2nd-level domains; others, like &#39;uk&#39;, assign 3rd-level &lt;br&gt; &gt;&gt;&gt; domains.)&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; - All crawlers are launched with the same configuration, &lt;br&gt;\n &gt;&gt; including &lt;br&gt; &gt;&gt;&gt; the same seeds, but otherwise do not (themselves) communicate. &lt;br&gt; &gt;&gt; Seeds &lt;br&gt; &gt;&gt;&gt; that don&#39;t belong on any one crawler are dropped out by the early &lt;br&gt; &gt;&gt;&gt; HashCrawlMapper. Discovered outlinks logs that need to be cross-fed &lt;br&gt; &gt;&gt; are &lt;br&gt; &gt;&gt;&gt; done so by an external process/scripts.&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&gt; - Gordon @ IA&lt;br&gt; &gt;&gt;&gt;&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt; Yahoo! Groups Links&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt;&lt;br&gt; &gt;&gt;&lt;br&gt; &gt; &lt;br&gt; &gt; &lt;br&gt; &gt; &lt;br&gt; &gt; &lt;br&gt; &gt; &lt;br&gt; &gt; &lt;br&gt; &gt; ------------&lt;wbr&gt;---------&lt;wbr&gt;---------&lt;wbr&gt;---&lt;br&gt; &gt; Food fight? Enjoy some healthy debate&lt;br&gt; &gt; in the Yahoo! Answers Food &amp; Drink Q&amp;A.&lt;br&gt;   &lt;br&gt;   &lt;/div&gt;   &lt;/div&gt;  &lt;!--End group email --&gt;&lt;/blockquote&gt;   &lt;div&gt;&lt;/div&gt;     &lt;/div&gt;          &lt;!--End group email --&gt;  &lt;/blockquote&gt;&lt;br&gt;&lt;p&gt;&#32;\n\n&lt;hr size=1&gt;\n&lt;a href=&quot;http://us.rd.yahoo.com/evt=49678/*http://smallbusiness.yahoo.com/domains/?p=BESTDEAL&quot;&gt; Get your own web address.&lt;/a&gt;&lt;br&gt; Have a HUGE year through &lt;a href=&quot;\nhttp://us.rd.yahoo.com/evt=49678/*http://smallbusiness.yahoo.com/domains/?p=BESTDEAL&quot;&gt;Yahoo! Small Business.&lt;/a&gt;\r\n--0-26056162-1183010025=:20877--\r\n\n"}}