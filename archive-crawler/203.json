{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","replyTo":"LIST","senderId":"MuR0bGGa08zv-Rqjp4K0ulq7Ygw4I227sILpQPX1Wo2uVNCcHorMCRslRrPIoL_mP8kKmanBbnU2XZRpUoa4DUibLjC4a652","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Checkpointing","postDate":"1071181983","msgId":203,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNGRDhGMDlGLjYwMjAwMDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDEwNzExMDEwMDMuMTA5MDQuOTEuY2FtZWxAYjExNi1keW4tMzcuYXJjaGl2ZS5vcmc+","referencesHeader":"PDNGQkJGNjUzLjkwNjAxMDdAYXJjaGl2ZS5vcmc+IDwxMDcxMTAxMDAzLjEwOTA0LjkxLmNhbWVsQGIxMTYtZHluLTM3LmFyY2hpdmUub3JnPg=="},"prevInTopic":202,"nextInTopic":206,"prevInTime":202,"nextInTime":204,"topicId":178,"numMessagesInTopic":7,"msgSnippet":"Ignore my last request for definition of checkpoint.  I see it below in Gordon s original mail. Comments inline below. ... As you say above, sounds like","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 48862 invoked from network); 11 Dec 2003 22:38:45 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m9.grp.scd.yahoo.com with QMQP; 11 Dec 2003 22:38:45 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta3.grp.scd.yahoo.com with SMTP; 11 Dec 2003 22:38:46 -0000\r\nReceived: (qmail 18331 invoked by uid 100); 11 Dec 2003 22:37:59 -0000\r\nReceived: from b116-dyn-60.archive.org (HELO archive.org) (stack@...@209.237.240.60)\n  by mail-dev.archive.org with SMTP; 11 Dec 2003 22:37:59 -0000\r\nMessage-ID: &lt;3FD8F09F.6020003@...&gt;\r\nDate: Thu, 11 Dec 2003 14:33:03 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.5) Gecko/20031007\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nSubject: Re: [archive-crawler] Checkpointing\r\nReferences: &lt;3FBBF653.9060107@...&gt; &lt;1071101003.10904.91.camel@...&gt;\r\nIn-Reply-To: &lt;1071101003.10904.91.camel@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-Status: No, hits=-4.8 required=6.0\n\ttests=AWL,BAYES_10,EMAIL_ATTRIBUTION,IN_REP_TO,QUOTED_EMAIL_TEXT,\n\t      REFERENCES,REPLY_WITH_QUOTES,TONER,USER_AGENT_MOZILLA_UA\n\tversion=2.55\r\nX-Spam-Level: \r\nX-Spam-Checker-Version: SpamAssassin 2.55 (1.174.2.19-2003-05-19-exp)\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nX-Yahoo-Group-Post: member; u=168599281\r\n\r\nIgnore my last request for definition of checkpoint.  I see it below in \nGordon&#39;s original mail.\n\nComments inline below.\n\nJohn Erik Halse wrote:\n\n&gt;Since Gordon wrote this mail, he has added a recover log which could be\n&gt;used to &quot;replay&quot; the crawl. The actual replaying functionality isn&#39;t\n&gt;added though.\n&gt;\n&gt;The replaying approach is simple and by adding a little more information\n&gt;to the recover log (timestamps) it should be possible to reconstruct the\n&gt;statistics as well. For focused crawls this should work very well. But\n&gt;with very broad crawls, the recover log grows to a size that is not easy\n&gt;to handle. And if the crawler sometime in the future will support\n&gt;infinite/incremental crawls, then the recover log will not work.\n&gt;  \n&gt;\n&gt;When thinking of different approaches for doing checkpoints I came up\n&gt;with a some questions that should be answered before we try to design\n&gt;it.\n&gt;\n&gt;* How often are we supposed to do a checkpoint (aka how costly is a\n&gt;checkpoint allowed to be).\n&gt;If the checkpoints are very expensive, we could do a combination of\n&gt;checkpoint and recovery log. The recovery log should then be reset at\n&gt;every checkpoint.\n&gt;  \n&gt;\nAs you say above, sounds like crawler has to do a &#39;total crawler state&#39; \ncheckpoint either on a period or on the crossing of a threshold -- size \nof replay log, size of data crawled so far, etc. -- just so it can \nsafely throw away the recover log.\n\n&gt;* Is it ok to pause the crawler for a checkpoint? It might take some\n&gt;time to wait for all the threads to finish. Is this acceptable?\n&gt;* Is checkpointing just for recovering from crashes?\n&gt;If not:\n&gt;  - Should it be possible to manipulate queues in a suspended state? For\n&gt;example adding or removing URIs in the pending queue.\n&gt;  \n&gt;\nShouldn&#39;t we be able to do this anyways?  While the crawler is running?\n\n&gt;  - Should it be possible to change implementation of modules between\n&gt;suspend and resume? For example fixing bugs.\n&gt;  - Should it be possible to alter the configuration in suspended state\n&gt;* Is it ok to insert a checkpoint mark in the working files or should\n&gt;everything be copied to a safe location to make sure that a crash would\n&gt;not corrupt files?\n&gt;\n&gt;If we add the possibility to run a multiple machine crawl; Should the\n&gt;checkpoint span all the crawler instances or should the checkpoint be\n&gt;local to a single instance?\n&gt;\n&gt;  \n&gt;\nCan we list out what state needs to be saved on a &#39;total crawler state&#39; \ncheckpoint?\n\n&gt;Comments/answers to these questions? Other questions that should be\n&gt;asked?\n&gt;\n&gt;John\n&gt;\n&gt;On Wed, 2003-11-19 at 15:01, Gordon Mohr wrote:\n&gt;  \n&gt;\n&gt;&gt;The top frustration during our recent evaluation crawl was\n&gt;&gt;that we don&#39;t yet have a working system for resuming a crawl\n&gt;&gt;in progress from disk-based state, aka &quot;checkpointing&quot;.\n&gt;&gt;\n&gt;&gt;There are many ways we could remedy this shortcoming,\n&gt;&gt;some incremental, some comprehensive.\n&gt;&gt;\n&gt;&gt;Two extremes of checkpoint functionality would be &quot;just enough\n&gt;&gt;to ensure coverage&quot; and &quot;total crawler state&quot;.\n&gt;&gt;\n&gt;&gt;In &quot;just enough to ensure coverage&quot;, a resumption might not\n&gt;&gt;have internal state and running totals that closely match\n&gt;&gt;those that the crawler would have had, if not interrupted.\n&gt;&gt;However, it would be trusted to still visit every URI that\n&gt;&gt;the original crawler would have. (In some ways, this could be\n&gt;&gt;considered a &quot;checkpoint Frontier only&quot; or &quot;checkpoint\n&gt;&gt;simplified view of Frontier&quot;.)\n&gt;&gt;\n&gt;&gt;In &quot;total crawler state&quot;, a resumption would reliably reload\n&gt;&gt;the state of any component which accumulates information --\n&gt;&gt;such as the statistics tracker or a hypothetical postprocessor\n&gt;&gt;which tallies relative proportions of document features -- such\n&gt;&gt;that a resumption makes the crawler work exactly like it had\n&gt;&gt;never stopped.\n&gt;&gt;    \n&gt;&gt;\n\nWe might send out a checkpoint event.  Anyone interested -- the above \nhypothetical postprocessor -- would have a listener out to receive the \nevent.\n\n&gt;&gt;Several different approaches fall along a continuum of\n&gt;&gt;increasing sophistication at crawl/checkpoint time:\n&gt;&gt;\n&gt;&gt;A &quot;forensic&quot; approach would require almost no crawl-time\n&gt;&gt;support. A resume would simply look at the output generated\n&gt;&gt;by the crawler, essentially &quot;replaying&quot; the crawl at an\n&gt;&gt;accelerated rate (perhaps even repeating the extraction\n&gt;&gt;steps against ARCed resources), eventually winding up at\n&gt;&gt;a state closely mimicking that of the previously-suspended\n&gt;&gt;crawler. This requires little to no support at crawl/checkpoint\n&gt;&gt;time, but a sophisticated resumption routing.\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\nA &#39;journaling&#39; mechanism such as that described would be best but would \nprobably be hard to develop?\n\n&gt;&gt;A &quot;transaction log&quot; approach would generate extra logs\n&gt;&gt;at crawl-time to assist in rapid resumption -- for example,\n&gt;&gt;all inserts to the froniter would be logged, to save the\n&gt;&gt;resumption from having to re-scan source material. This\n&gt;&gt;remains very straightforward, if we&#39;re only considering\n&gt;&gt;the matter of including/excluding URIs for visitation, and\n&gt;&gt;could have other debugging benefits.\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n\nHow does this differ from the &#39;forensic&#39; approach?  Are you suggesting \nhere that we wouldn&#39;t replay the &#39;transaction&#39; log, but that we&#39;d just \njump to the last complete transaction in the log and resume there?\n\n&gt;&gt;A &quot;snapshot&quot; approach would, at certain intervals or at\n&gt;&gt;operator request, dump some or all of the crawler&#39;s state\n&gt;&gt;to files. A resumption could occur quickly, and mimic the\n&gt;&gt;dumped state as accurately as we care to enable.\n&gt;&gt;    \n&gt;&gt;\n\nWe should do this too.   In the UI, somehow you could force a &#39;total \ncrawler state&#39; checkpoint.\n\nSt.Ack\n\n&gt;&gt;==\n&gt;&gt;There are merits and tradeoffs to each approach. My understanding\n&gt;&gt;is that Mercator implements a &quot;total crawler state&quot;/&quot;snapshot&quot;\n&gt;&gt;approach, giving every module a chance to persist itself to\n&gt;&gt;disk at checkpoint time.\n&gt;&gt;\n&gt;&gt;I think our initial implementation should be a &quot;just enough for\n&gt;&gt;coverage&quot;/&quot;transaction log&quot; approach, as this requires minimal\n&gt;&gt;invasiveness and effectively tackles the largest issue: unbroken\n&gt;&gt;coverage across crawler hiccups. We can re-synthesize any\n&gt;&gt;running statistics post-crawl.\n&gt;&gt;\n&gt;&gt;Later, we should enable the Mercator-style &quot;total state&quot;/&quot;snapshot&quot;\n&gt;&gt;approach.\n&gt;&gt;\n&gt;&gt;Agree or disagree? Comments? Other ideas?\n&gt;&gt;\n&gt;&gt;- Gordon\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;To unsubscribe from this group, send an email to:\n&gt;&gt;archive-crawler-unsubscribe@yahoogroups.com\n&gt;&gt;\n&gt;&gt; \n&gt;&gt;\n&gt;&gt;Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;    \n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;To unsubscribe from this group, send an email to:\n&gt;archive-crawler-unsubscribe@yahoogroups.com\n&gt;\n&gt; \n&gt;\n&gt;Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \n&gt;\n&gt;\n&gt;  \n&gt;\n\n\n\n"}}