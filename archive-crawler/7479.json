{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"David Pane","from":"David Pane &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"cW4df-WJIs6plbJzZLJ8OobQLKl-s-6DmvEstxqKrBlZ55_z0PsmutgNe5O5UzTJrBmaylxwC-YCgpJg3109P-Y15Vk","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] robots.txt problem","postDate":"1324595133","msgId":7479,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFRjNCN0JELjYwODA1MDZAY3MuY211LmVkdT4=","inReplyToHeader":"PDRFRjNBRTJCLjUwMTA5MDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PGpjNThodCt1Z3VwQGVHcm91cHMuY29tPiA8NEVFODA5RDcuNzAxMDEwMEBhcmNoaXZlLm9yZz4gPDRFRjM4RDk5LjEwODA0MDZAY3MuY211LmVkdT4gPDRFRjNBRTJCLjUwMTA5MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":7478,"nextInTopic":7480,"prevInTime":7478,"nextInTime":7480,"topicId":7379,"numMessagesInTopic":23,"msgSnippet":"Thank you Gordon. ... I chose to add the additional functionality so that if the SURT is found through the initial check, it does not check for the host.  I","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 25792 invoked from network); 22 Dec 2011 23:05:39 -0000\r\nX-Received: from unknown (98.137.35.160)\n  by m1.grp.sp2.yahoo.com with QMQP; 22 Dec 2011 23:05:39 -0000\r\nX-Received: from unknown (HELO smtp.andrew.cmu.edu) (128.2.11.61)\n  by mta4.grp.sp2.yahoo.com with SMTP; 22 Dec 2011 23:05:39 -0000\r\nX-Received: from scsvpn-43.net.cs.cmu.edu ([128.2.161.43])\n\t(user=dpane mech=PLAIN (0 bits))\n\tby smtp.andrew.cmu.edu (8.14.4/8.14.4) with ESMTP id pBMN5Ykd028254\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NOT);\n\tThu, 22 Dec 2011 18:05:34 -0500\r\nMessage-ID: &lt;4EF3B7BD.6080506@...&gt;\r\nDate: Thu, 22 Dec 2011 18:05:33 -0500\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:8.0) Gecko/20111105 Thunderbird/8.0\r\nMIME-Version: 1.0\r\nTo: Gordon Mohr &lt;gojomo@...&gt;\r\nCc: archive-crawler@yahoogroups.com\r\nReferences: &lt;jc58ht+ugup@...&gt; &lt;4EE809D7.7010100@...&gt; &lt;4EF38D99.1080406@...&gt; &lt;4EF3AE2B.5010907@...&gt;\r\nIn-Reply-To: &lt;4EF3AE2B.5010907@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-PMX-Version: 5.5.9.388399, Antispam-Engine: 2.7.2.376379, Antispam-Data: 2011.3.18.170322\r\nX-SMTP-Spam-Clean: 8% (\n BODY_SIZE_6000_6999 0, BODY_SIZE_7000_LESS 0, RDNS_NXDOMAIN 0, RDNS_SUSP 0, RDNS_SUSP_GENERIC 0, __BOUNCE_CHALLENGE_SUBJ 0, __BOUNCE_NDR_SUBJ_EXEMPT 0, __CP_URI_IN_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __HAS_MSGID 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __MOZILLA_MSGID 0, __SANE_MSGID 0, __STOCK_PHRASE_7 0, __TO_MALFORMED_2 0, __URI_NO_MAILTO 0, __URI_NO_WWW 0, __USER_AGENT 0)\r\nX-SMTP-Spam-Score: 8%\r\nX-Scanned-By: MIMEDefang 2.60 on 128.2.11.61\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: David Pane &lt;dpane@...&gt;\r\nSubject: Re: [archive-crawler] robots.txt problem\r\nX-Yahoo-Group-Post: member; u=500983475; y=AAWArx8RXD4b4kT4eVamebrm-753jESmgBnixPus1Bm8DR5sgZVRrA\r\nX-Yahoo-Profile: david_pane1\r\n\r\nThank you Gordon.\n\nOn 12/22/11 5:24 PM, Gordon Mohr wrote:\n&gt; Robots-enforcement is completely separate from scoping/DecideRules, so \n&gt; changes to SurtPrefixedDecideRule couldn&#39;t be responsible for robots \n&gt; issues.\n&gt;\n&gt; (Though, if your changes are a significant new capability, such as \n&gt; triggering DNS lookups or even consulting the existing cached \n&gt; CrawlHost lookups during scoping, you might want to package them as \n&gt; another rule that could be applied in serial, rather than an extension \n&gt; of SurtPrefixedDecideRule.)\n\nI chose to add the additional functionality so that if the SURT is found \nthrough the initial check, it does not check for the host.  I may indeed \nadd this functionality as a separate deciderule in the futre.\n&gt;\n&gt; From the description of the URLs and the site&#39;s robots.txt, it seems \n&gt; they should be prohibited.\n&gt;\n&gt; Heritrix robots.txt-parsing does not yet support the Google-originated \n&gt; convention of internal &#39;*&#39; wildcards in declared paths, but we \n&gt; tolerate &#39;*&#39; at the end (as being equivalent to not being there at \n&gt; all). And, we should simply interpret internal &#39;*&#39; literally (and \n&gt; continue respecting all other rules, such as the ones that are \n&gt; relevant here). So this is probably not a factor, even though some \n&gt; internal wildcards are used in this site&#39;s robots.txt.\n&gt;\n&gt; Can you confirm from your own crawl.log that you visited the relevant \n&gt; robots.txt, received it (as a 200 of appropriate size), and then \n&gt; fetched the other URIs that should have been precluded?\n&gt;\ncrawl.log.cp00004-20111222182012:2011-12-22T18:05:54.416Z   200      \n20813 http://tmbw.net/wiki/index.php?title=1996&action=edit E \nhttp://tmbw.net/wiki/1996 text/html #107 20111222180548843+556 \nsha1:FJGZ5SRP3NAYOS4KAU4VKF4RKEHJZNFQ - -\n\n&gt; Are there any other changes to your code or configuration related to \n&gt; robots or the PreconditionEnforcer processor?\n&gt;\nNo. The follow is taken from the crawler-beans.cxml\n\n&lt;bean id=&quot;preconditions&quot; \nclass=&quot;org.archive.crawler.prefetch.PreconditionEnforcer&quot;&gt;\n&lt;!-- &lt;property name=&quot;ipValidityDurationSeconds&quot; value=&quot;21600&quot; /&gt; --&gt;\n&lt;!-- &lt;property name=&quot;robotsValidityDurationSeconds&quot; value=&quot;86400&quot; /&gt; --&gt;\n&lt;!-- &lt;property name=&quot;calculateRobotsOnly&quot; value=&quot;false&quot; /&gt; --&gt;\n&lt;/bean&gt;\n\nAlthough, I have changed the recheckScope in the Preselector to true.\n&gt; Can you reproduce the same behavior when trying to crawl a single one \n&gt; of the problem URIs, such as for example....\n&gt;\n&gt; http:/tmbw.net/wiki/index.php?title=1996&action=edit\n&gt;\n&gt; ...with either the default configuration or your usual configuration \n&gt; with just this one seed?\nI would rather not do this as I am a little concerne.  The webmaster was \nnot too happy that this was happening.. He has also blocked access to us.\n&gt;\n&gt; FYI, whether a mid-crawl change has taken effect the way you&#39;d like \n&gt; depends on how you added it. The rules that take their starting \n&gt; configuration from lists of URIs/patterns (in either CXML or from \n&gt; source files on disk) are *not* constantly monitoring those files for \n&gt; changes... so simply editing those files on disk *won&#39;t* take \n&gt; immediate effect.\n&gt;\n&gt; However, if you drop a &quot;updates.seeds&quot; file into the &#39;action&#39; \n&gt; directory, seeds and directive lines (starting with &#39;+&#39; or &#39;-&#39;) do get \n&gt; announced to the crawl in a way that (in the usual configuration) will \n&gt; be noticed by accept/reject SurtPrefixedDecideRules, in which case \n&gt; they do live-update their in-memory rule-in/rule-out lists.\n&gt;\nTo be clear, I can create a &quot;updates.seeds&quot; file and drop it in the \naction directory in the format as this so that they are included in the \nblack list.\n\n+http://(net,tmbw,\n\nWill an edited blacklist surts file used that is configured in the \nSurtPrefixedDecideRules eventually get applied in a live crawl?\n\n--David\n\n&gt; - Gordon\n&gt;\n&gt; On 12/22/11 12:05 PM, David Pane wrote:\n&gt;&gt; I am running a crawl using heritrix.3.1.1 from the git repository with\n&gt;&gt; some minor changes that I do not believe is the cause of this problem.\n&gt;&gt; (I made changes in SurtPrefixedDecideRule.java to accommodate the\n&gt;&gt; checking of ip addresses as well as surts in my black list surt file. )\n&gt;&gt;\n&gt;&gt; I had a webmaster complain that the crawler was not obeying his\n&gt;&gt; robots.txt file. He reported that the crawler was requesting pages that\n&gt;&gt; should be dissallowed by directives for &quot;*&quot; user agent.\n&gt;&gt;\n&gt;&gt; Here are some sample pages that it is requesting:\n&gt;&gt; /wiki/index.php?title=They_Might_Be_Giants&action=edit\n&gt;&gt; Http Code: 200 Date: Dec 22 13:04:15 Http Version: HTTP/1.0 Size in\n&gt;&gt; Bytes: 54449\n&gt;&gt; Referer: http://tmbw.net/wiki/They_Might_Be_Giants\n&gt;&gt;\n&gt;&gt; /wiki/index.php?title=Category:Sung_By_John_Flansburgh&action=edit\n&gt;&gt; Http Code: 200 Date: Dec 22 13:04:54 Http Version: HTTP/1.0 Size in\n&gt;&gt; Bytes: 21164\n&gt;&gt; Referer: http://tmbw.net/wiki/Category:Sung_By_John_Flansburgh\n&gt;&gt;\n&gt;&gt; /wiki/index.php?title=Help_talk:Contents&action=edit\n&gt;&gt; Http Code: 200 Date: Dec 22 13:05:23 Http Version: HTTP/1.0 Size in\n&gt;&gt; Bytes: 22155\n&gt;&gt; Referer: http://tmbw.net/wiki/Help_talk:Contents\n&gt;&gt;\n&gt;&gt; /wiki/index.php?title=1996&action=edit\n&gt;&gt; Http Code: 200 Date: Dec 22 13:05:48 Http Version: HTTP/1.0 Size in\n&gt;&gt; Bytes: 20813\n&gt;&gt; Referer: http://tmbw.net/wiki/1996\n&gt;&gt;\n&gt;&gt; /wiki/index.php?title=Here_Comes_Science/Charts&action=render&chartOnly=Billboard%20Kids%20Albums \n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Http Code: 200 Date: Dec 22 13:06:18 Http Version: HTTP/1.0 Size in\n&gt;&gt; Bytes: 1069\n&gt;&gt; Referer: http://tmbw.net/wiki/Here_Comes_Science\n&gt;&gt;\n&gt;&gt; /wiki/index.php?title=Dial-A-Song&action=edit\n&gt;&gt; Http Code: 200 Date: Dec 22 13:06:46 Http Version: HTTP/1.0 Size in\n&gt;&gt; Bytes: 29906\n&gt;&gt; Referer: http://tmbw.net/wiki/Dial-A-Song\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; His robots.txt file is here: http://tmbw.net/robots.txt\n&gt;&gt;\n&gt;&gt; I added his domain to my surt black list file while the crawler is\n&gt;&gt; running. I am assuming that this update will be applied. Please let me\n&gt;&gt; know if I am correct in this assumption.\n&gt;&gt;\n&gt;&gt; Thanks,\n&gt;&gt;\n&gt;&gt; David\n&gt;\n\n"}}