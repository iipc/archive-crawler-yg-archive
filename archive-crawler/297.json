{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigurðsson","from":"=?ISO-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"Dgd1g0sgU75RB77z4eLcxxPJ6gZcXdCE_Qou762lm8xJu-50CWkc_8fF5ZnWthLN9tr10P6BTEy4gij0kOaibtd2rBJU6H_QWXsEX3hXDjg4bUP5Yw1lmvLOCYOT2xrX","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Max Size Related Configuration","postDate":"1080664830","msgId":297,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQwNjlBMkZFLjUwNDA0MDNAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGM0YzNyNStjdDVwQGVHcm91cHMuY29tPg==","referencesHeader":"PGM0YzNyNStjdDVwQGVHcm91cHMuY29tPg=="},"prevInTopic":295,"nextInTopic":299,"prevInTime":296,"nextInTime":298,"topicId":289,"numMessagesInTopic":9,"msgSnippet":"See below ... We are also aware of the fact that you can t overload websites and that is why the crawler is very polite. If you look at the settings under","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 87441 invoked from network); 30 Mar 2004 16:42:55 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m7.grp.scd.yahoo.com with QMQP; 30 Mar 2004 16:42:55 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (209.237.232.202)\n  by mta4.grp.scd.yahoo.com with SMTP; 30 Mar 2004 16:42:54 -0000\r\nReceived: (qmail 19566 invoked by uid 100); 30 Mar 2004 16:36:06 -0000\r\nReceived: from b116-dyn-55.archive.org (HELO archive.org) (kris@...@209.237.240.55)\n  by mail-dev.archive.org with SMTP; 30 Mar 2004 16:36:06 -0000\r\nMessage-ID: &lt;4069A2FE.5040403@...&gt;\r\nDate: Tue, 30 Mar 2004 08:40:30 -0800\r\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.6b) Gecko/20031205 Thunderbird/0.4\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;c4c3r5+ct5p@...&gt;\r\nIn-Reply-To: &lt;c4c3r5+ct5p@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------090502000804060305060207&quot;\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.2 required=6.0 tests=AWL,HTML_MESSAGE,\n\tHTML_TITLE_EMPTY autolearn=ham version=2.63\r\nX-eGroups-Remote-IP: 209.237.232.202\r\nFrom: =?ISO-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;\r\nSubject: Re: [archive-crawler] Re: Max Size Related Configuration\r\nX-Yahoo-Group-Post: member; u=163406187\r\nX-Yahoo-Profile: kristsi25\r\n\r\n\r\n--------------090502000804060305060207\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\n\r\nSee below\n\nsebastiandelachica wrote:\n\n&gt; Hiya Kris,\n&gt;\n&gt; Thanks for the hint. That is pretty much where I ended up last nite.\n&gt;\n&gt; To clarify, my original intent was to manage xtiple sites from a\n&gt; single crawl order, but to get the size limit per site, I ended\n&gt; up &quot;simplifying&quot; my approach and limiting each order to a single\n&gt; seed. I like the idea of controlling limits on # of downloaded  bytes\n&gt; on a per domain/host/etc. basis for xtiple seeds from a single order.\n&gt; I hope that concept becomes part of heritrix at some point in the\n&gt; future.\n&gt;\n&gt; To put things into perspective, my research work is related to the\n&gt; Digital Library for Earth Sciences (DLESE) http://www.dlese.org. I am\n&gt; trying to automatically tag online educational resources with\n&gt; National Science Educational Standards using natural language\n&gt; processing techniques. The reason for the size limit per site is b/c\n&gt; I need to avoid upsetting the community that provides the content.\n\nWe are also aware of the fact that you can&#39;t overload websites and that \nis why the crawler is very\npolite. If you look at the settings under &#39;frontier&#39; you will find the \nfollowing:\n\ndelay-factor:\nmax-delay-ms:\nmin-delay-ms:\nmin-interval-ms:\n\nThese settings insure that Heritrix does not hammer a site needlessly. \nAt any given time only\none document is being fetched from the same host. The delay-factor says \nthat for every X\nmillisec you spend downloading a document you should wait factor of X \nbefore starting the\nnext document download from that same host.  max and min delay allow you \nto set upper\nand lower bounds on this factor.\n\nExample: Last document took 200 msec and your delay factor is 3. That \nmeans that 600 msec\nwill have to elapse before Heritrix starts another download.  If \nmin-delay is set to 800 that will\noverride and Heritrix will wait 800 msec.\n\nEven at it&#39;s most aggressive Heritrix will never fetch more then one \ndoc. at a time and with the\npoliteness settings found in the Simple profile you definately don&#39;t \nneed to worry about\nupsetting anyone.\n\n- Kris\n\n&gt; Thanks for all the good advice and the Most Excellent Work on\n&gt; Heritrix!\n&gt;\n&gt; Seb\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, Kristinn Sigur�sson\n&gt; &lt;kris@a...&gt; wrote:\n&gt; &gt; Hei Seb.\n&gt; &gt;\n&gt; &gt; See below\n&gt; &gt;\n&gt; &gt; sebastiandelachica wrote:\n&gt; &gt;\n&gt; &gt; &gt; Michael,\n&gt; &gt; &gt;\n&gt; &gt; &gt; Thanks for the prompt reply. Indeed I upgraded to 0.6.0 over the\n&gt; &gt; &gt; weekend: amazing how close a 9 looks to a 6 given enough lack of\n&gt; sleep\n&gt; &gt; &gt;\n&gt; &gt; &gt; In English (to the best of my ability), what I am trying to do is\n&gt; use\n&gt; &gt; &gt; a separate order for each site I need to crawl. Each order hence\n&gt; has\n&gt; &gt; &gt; a single seed. The purpose is to place an upper bound on the\n&gt; number\n&gt; &gt; &gt; of &quot;useful&quot; bytes downloaded for each crawl order (about 100K or\n&gt; 1MB\n&gt; &gt; &gt; say). In other words, stop crawling the site once we have\n&gt; downloaded\n&gt; &gt; &gt; some number of usable bytes.\n&gt; &gt;\n&gt; &gt; If you are only crawling one site at a time (using DomainScope) the\n&gt; &gt; max-bytes-download\n&gt; &gt; just the thing for you.  It limits the total amount of data\n&gt; downloaded\n&gt; &gt; in one CrawlJob. It is\n&gt; &gt; only if you are crawling multiple domains in the same job (as is\n&gt; usual)\n&gt; &gt; that you can&#39;t use it\n&gt; &gt; for this purpose as it would only cut you off once the total from\n&gt; all\n&gt; &gt; domain hit the limit.\n&gt; &gt;\n&gt; &gt; Once the limit is hit the current job will end.  At some point we\n&gt; may\n&gt; &gt; allow overrides on this\n&gt; &gt; setting enabling cutoffs on specific domains but that is well into\n&gt; the\n&gt; &gt; future.\n&gt; &gt;\n&gt; &gt; I hope this is of some use to you.\n&gt; &gt;\n&gt; &gt; - Kris\n&gt; &gt;\n&gt; &gt; &gt; I thought setting a max size limit on\n&gt; &gt; &gt; the ARC file would stop logging past that point, but I see now\n&gt; that\n&gt; &gt; &gt; it means a slightly different thing. I tried the number of files\n&gt; &gt; &gt; limit and while it works, I am not sure it matches my intent as\n&gt; some\n&gt; &gt; &gt; files may be noticably shorter than others.\n&gt; &gt; &gt;\n&gt; &gt; &gt; I am using a DomainScope crawl per the settings in the Profile\n&gt; used\n&gt; &gt; &gt; to create the Job.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Based on the information you sent me, I need to think about what\n&gt; &gt; &gt; might serve my purpose. Thanks for pointing me at the right code\n&gt; in\n&gt; &gt; &gt; ARCWriter.\n&gt; &gt; &gt;\n&gt; &gt; &gt; Seb\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In archive-crawler@yahoogroups.com, Michael Stack &lt;stack@a...&gt;\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt; &gt; Thanks for trying Heritrix Seb.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; See below.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; sebastiandelachica wrote:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;I have been playing around with heritrix for a few weeks now\n&gt; and I\n&gt; &gt; &gt; am\n&gt; &gt; &gt; &gt; &gt;in the process of turning it loose on a controlled environment\n&gt; for\n&gt; &gt; &gt; &gt; &gt;one of my research strands. I am currently using version\n&gt; 0.9.0. My\n&gt; &gt; &gt; &gt; &gt;objective is to limit the amount of data scooped from a site\n&gt; onto\n&gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt;ARC file. I tried using the HTTP Processor max-length-bytes and\n&gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt;Archiver max-size-bytes.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Do you mean 0.4.0. You say 0.9.0 above.  We just released 0.6.0\n&gt; on\n&gt; &gt; &gt; &gt; friday.  Try it if you haven&#39;t already.  Lots of fixes and\n&gt; &gt; &gt; improvements.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; If you&#39;re doing a broad crawl, you have the following options\n&gt; &gt; &gt; available\n&gt; &gt; &gt; &gt; to you:\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; max-bytes-download\n&gt; &gt; &gt; &gt; max-document-download\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; These options are not available in a domain scoped crawl which\n&gt; &gt; &gt; seems to\n&gt; &gt; &gt; &gt; be what it is you&#39;d like to do.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Tell us more about what it is that you&#39;d like.\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; The max-length-bytes options limits size of a particular\n&gt; download\n&gt; &gt; &gt; only.\n&gt; &gt; &gt; &gt; The max-size-bytes  is upper-bound on the size of ARC files\n&gt; written\n&gt; &gt; &gt; (See\n&gt; &gt; &gt; &gt; the code here http://crawler.archive.org/xref/index.html). \n&gt; &lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt; &gt; &gt; &lt;http://crawler.archive.org/xref/index.html%29.&gt;\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; Yours,\n&gt; &gt; &gt; &gt; St.Ack\n&gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;The Processor max-length-bytes seems to work at some level as\n&gt; the\n&gt; &gt; &gt; &gt; &gt;requests are reported as length-truncated in the logs, but the\n&gt; &gt; &gt; actual\n&gt; &gt; &gt; &gt; &gt;HTML files still make it into the archive.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;The Archive max-size-bytes appears to be ignored. Looking at\n&gt; the\n&gt; &gt; &gt; &gt; &gt;code, it does not seem to be used by the ARCWriterProcessor\n&gt; class\n&gt; &gt; &gt; or\n&gt; &gt; &gt; &gt; &gt;any other class for that matter...I just starting digging\n&gt; through\n&gt; &gt; &gt; the\n&gt; &gt; &gt; &gt; &gt;code earlier today.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;I continue my experimentation and code reading, but figured,\n&gt; I&#39;d\n&gt; &gt; &gt; &gt; &gt;check in to see if I am missing something very obvious or if\n&gt; &gt; &gt; anyone\n&gt; &gt; &gt; &gt; &gt;had experienced similar behavior.\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;Thanks in advance for your time,\n&gt; &gt; &gt; &gt; &gt;Seb\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;Yahoo! Groups Links\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; ------------------------------------------------------------------\n&gt; ------\n&gt; &gt; &gt; *Yahoo! Groups Links*\n&gt; &gt; &gt;\n&gt; &gt; &gt;     * To visit your group on the web, go to:\n&gt; &gt; &gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt; &gt; &gt;       \n&gt; &gt; &gt;     * To unsubscribe from this group, send an email to:\n&gt; &gt; &gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt; &gt; &gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?\n&gt; subject=Unsubscribe&gt;\n&gt; &gt; &gt;       \n&gt; &gt; &gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt; &gt; &gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; *Yahoo! Groups Links*\n&gt;\n&gt;     * To visit your group on the web, go to:\n&gt;       http://groups.yahoo.com/group/archive-crawler/\n&gt;        \n&gt;     * To unsubscribe from this group, send an email to:\n&gt;       archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     * Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n\n\r\n--------------090502000804060305060207\r\nContent-Type: text/html; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta content=&quot;text/html;charset=ISO-8859-1&quot; http-equiv=&quot;Content-Type&quot;&gt;\n  &lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\nSee below&lt;br&gt;\n&lt;br&gt;\nsebastiandelachica wrote:&lt;br&gt;\n&lt;blockquote cite=&quot;midc4c3r5+ct5p@...&quot; type=&quot;cite&quot;&gt;&lt;tt&gt;\nHiya Kris,&lt;br&gt;\n  &lt;br&gt;\nThanks for the hint. That is pretty much where I ended up last nite.&lt;br&gt;\n  &lt;br&gt;\nTo clarify, my original intent was to manage xtiple sites from a &lt;br&gt;\nsingle crawl order, but to get the size limit per site, I ended &lt;br&gt;\nup &quot;simplifying&quot; my approach and limiting each order to a single &lt;br&gt;\nseed. I like the idea of controlling limits on # of downloaded&nbsp; bytes &lt;br&gt;\non a per domain/host/etc. basis for xtiple seeds from a single order. &lt;br&gt;\nI hope that concept becomes part of heritrix at some point in the &lt;br&gt;\nfuture.&lt;br&gt;\n  &lt;br&gt;\nTo put things into perspective, my research work is related to the &lt;br&gt;\nDigital Library for Earth Sciences (DLESE) &lt;a\n href=&quot;http://www.dlese.org.&quot;&gt;http://www.dlese.org.&lt;/a&gt; I am &lt;br&gt;\ntrying to automatically tag online educational resources with &lt;br&gt;\nNational Science Educational Standards using natural language &lt;br&gt;\nprocessing techniques. The reason for the size limit per site is b/c &lt;br&gt;\nI need to avoid upsetting the community that provides the content.&lt;br&gt;\n  &lt;/tt&gt;&lt;/blockquote&gt;\nWe are also aware of the fact that you can&#39;t overload websites and that\nis why the crawler is very &lt;br&gt;\npolite. If you look at the settings under &#39;frontier&#39; you will find the\nfollowing:&lt;br&gt;\n&lt;br&gt;\ndelay-factor:&lt;br&gt;\nmax-delay-ms:&lt;br&gt;\nmin-delay-ms:&lt;br&gt;\nmin-interval-ms:&lt;br&gt;\n&lt;br&gt;\nThese settings insure that Heritrix does not hammer a site needlessly.\nAt any given time only&lt;br&gt;\none document is being fetched from the same host. The delay-factor says\nthat for every X&lt;br&gt;\nmillisec you spend downloading a document you should wait factor of X\nbefore starting the&lt;br&gt;\nnext document download from that same host.&nbsp; max and min delay allow\nyou to set upper&lt;br&gt;\nand lower bounds on this factor.&lt;br&gt;\n&lt;br&gt;\nExample: Last document took 200 msec and your delay factor is 3. That\nmeans that 600 msec&lt;br&gt;\nwill have to elapse before Heritrix starts another download.&nbsp; If\nmin-delay is set to 800 that will&lt;br&gt;\noverride and Heritrix will wait 800 msec.&lt;br&gt;\n&lt;br&gt;\nEven at it&#39;s most aggressive Heritrix will never fetch more then one\ndoc. at a time and with the&lt;br&gt;\npoliteness settings found in the Simple profile you definately don&#39;t\nneed to worry about &lt;br&gt;\nupsetting anyone.&lt;br&gt;\n&lt;br&gt;\n- Kris&lt;br&gt;\n&lt;br&gt;\n&lt;blockquote cite=&quot;midc4c3r5+ct5p@...&quot; type=&quot;cite&quot;&gt;&lt;tt&gt;Thanks\nfor all the good advice and the Most Excellent Work on &lt;br&gt;\nHeritrix!&lt;br&gt;\n  &lt;br&gt;\nSeb&lt;br&gt;\n  &lt;br&gt;\n--- In &lt;a class=&quot;moz-txt-link-abbreviated&quot; href=&quot;mailto:archive-crawler@yahoogroups.com&quot;&gt;archive-crawler@yahoogroups.com&lt;/a&gt;, Kristinn Sigur&eth;sson &lt;br&gt;\n&lt;a class=&quot;moz-txt-link-rfc2396E&quot; href=&quot;mailto:kris@a...&quot;&gt;&lt;kris@a...&gt;&lt;/a&gt; wrote:&lt;br&gt;\n&gt; Hei Seb.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; See below&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; sebastiandelachica wrote:&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &gt; Michael,&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; Thanks for the prompt reply. Indeed I upgraded to 0.6.0 over\nthe&lt;br&gt;\n&gt; &gt; weekend: amazing how close a 9 looks to a 6 given enough lack\nof &lt;br&gt;\nsleep&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; In English (to the best of my ability), what I am trying to\ndo is &lt;br&gt;\nuse&lt;br&gt;\n&gt; &gt; a separate order for each site I need to crawl. Each order\nhence &lt;br&gt;\nhas&lt;br&gt;\n&gt; &gt; a single seed. The purpose is to place an upper bound on the &lt;br&gt;\nnumber&lt;br&gt;\n&gt; &gt; of &quot;useful&quot; bytes downloaded for each crawl order (about 100K\nor &lt;br&gt;\n1MB&lt;br&gt;\n&gt; &gt; say). In other words, stop crawling the site once we have &lt;br&gt;\ndownloaded&lt;br&gt;\n&gt; &gt; some number of usable bytes. &lt;br&gt;\n&gt; &lt;br&gt;\n&gt; If you are only crawling one site at a time (using DomainScope)\nthe &lt;br&gt;\n&gt; max-bytes-download&lt;br&gt;\n&gt; just the thing for you.&nbsp; It limits the total amount of data &lt;br&gt;\ndownloaded &lt;br&gt;\n&gt; in one CrawlJob. It is&lt;br&gt;\n&gt; only if you are crawling multiple domains in the same job (as is &lt;br&gt;\nusual) &lt;br&gt;\n&gt; that you can&#39;t use it&lt;br&gt;\n&gt; for this purpose as it would only cut you off once the total from &lt;br&gt;\nall &lt;br&gt;\n&gt; domain hit the limit.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; Once the limit is hit the current job will end.&nbsp; At some point we &lt;br&gt;\nmay &lt;br&gt;\n&gt; allow overrides on this&lt;br&gt;\n&gt; setting enabling cutoffs on specific domains but that is well into\n  &lt;br&gt;\nthe &lt;br&gt;\n&gt; future.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; I hope this is of some use to you.&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; - Kris&lt;br&gt;\n&gt; &lt;br&gt;\n&gt; &gt; I thought setting a max size limit on&lt;br&gt;\n&gt; &gt; the ARC file would stop logging past that point, but I see\nnow &lt;br&gt;\nthat&lt;br&gt;\n&gt; &gt; it means a slightly different thing. I tried the number of\nfiles&lt;br&gt;\n&gt; &gt; limit and while it works, I am not sure it matches my intent\nas &lt;br&gt;\nsome&lt;br&gt;\n&gt; &gt; files may be noticably shorter than others.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; I am using a DomainScope crawl per the settings in the\nProfile &lt;br&gt;\nused&lt;br&gt;\n&gt; &gt; to create the Job.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; Based on the information you sent me, I need to think about\nwhat&lt;br&gt;\n&gt; &gt; might serve my purpose. Thanks for pointing me at the right\ncode &lt;br&gt;\nin&lt;br&gt;\n&gt; &gt; ARCWriter.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; Seb&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt; --- In &lt;a class=&quot;moz-txt-link-abbreviated&quot; href=&quot;mailto:archive-crawler@yahoogroups.com&quot;&gt;archive-crawler@yahoogroups.com&lt;/a&gt;, Michael Stack\n&lt;a class=&quot;moz-txt-link-rfc2396E&quot; href=&quot;mailto:stack@a...&quot;&gt;&lt;stack@a...&gt;&lt;/a&gt;&lt;br&gt;\n&gt; &gt; wrote:&lt;br&gt;\n&gt; &gt; &gt; Thanks for trying Heritrix Seb.&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; See below.&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; sebastiandelachica wrote:&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;I have been playing around with heritrix for a few\nweeks now &lt;br&gt;\nand I&lt;br&gt;\n&gt; &gt; am&lt;br&gt;\n&gt; &gt; &gt; &gt;in the process of turning it loose on a controlled\nenvironment &lt;br&gt;\nfor&lt;br&gt;\n&gt; &gt; &gt; &gt;one of my research strands. I am currently using\nversion &lt;br&gt;\n0.9.0. My&lt;br&gt;\n&gt; &gt; &gt; &gt;objective is to limit the amount of data scooped\nfrom a site &lt;br&gt;\nonto&lt;br&gt;\n&gt; &gt; the&lt;br&gt;\n&gt; &gt; &gt; &gt;ARC file. I tried using the HTTP Processor\nmax-length-bytes and&lt;br&gt;\n&gt; &gt; the&lt;br&gt;\n&gt; &gt; &gt; &gt;Archiver max-size-bytes.&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt; &lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; Do you mean 0.4.0. You say 0.9.0 above.&nbsp; We just\nreleased 0.6.0 &lt;br&gt;\non&lt;br&gt;\n&gt; &gt; &gt; friday.&nbsp; Try it if you haven&#39;t already.&nbsp; Lots of fixes\nand&lt;br&gt;\n&gt; &gt; improvements.&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; If you&#39;re doing a broad crawl, you have the following\noptions&lt;br&gt;\n&gt; &gt; available&lt;br&gt;\n&gt; &gt; &gt; to you:&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; max-bytes-download&lt;br&gt;\n&gt; &gt; &gt; max-document-download&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; These options are not available in a domain scoped crawl\nwhich&lt;br&gt;\n&gt; &gt; seems to&lt;br&gt;\n&gt; &gt; &gt; be what it is you&#39;d like to do.&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; Tell us more about what it is that you&#39;d like.&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; The max-length-bytes options limits size of a particular\n  &lt;br&gt;\ndownload&lt;br&gt;\n&gt; &gt; only. &lt;br&gt;\n&gt; &gt; &gt; The max-size-bytes&nbsp; is upper-bound on the size of ARC\nfiles &lt;br&gt;\nwritten&lt;br&gt;\n&gt; &gt; (See&lt;br&gt;\n&gt; &gt; &gt; the code here &lt;a\n href=&quot;http://crawler.archive.org/xref/index.html%29.&quot;&gt;http://crawler.archive.org/xref/index.html).&lt;/a&gt;\n  &lt;br&gt;\n&gt; &gt; &lt;&lt;a href=&quot;http://crawler.archive.org/xref/index.html%29.&quot;&gt;http://crawler.archive.org/xref/index.html%29.&lt;/a&gt;&gt;&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; Yours,&lt;br&gt;\n&gt; &gt; &gt; St.Ack&lt;br&gt;\n&gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;The Processor max-length-bytes seems to work at some\nlevel as &lt;br&gt;\nthe&lt;br&gt;\n&gt; &gt; &gt; &gt;requests are reported as length-truncated in the\nlogs, but the&lt;br&gt;\n&gt; &gt; actual&lt;br&gt;\n&gt; &gt; &gt; &gt;HTML files still make it into the archive.&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;The Archive max-size-bytes appears to be ignored.\nLooking at &lt;br&gt;\nthe&lt;br&gt;\n&gt; &gt; &gt; &gt;code, it does not seem to be used by the\nARCWriterProcessor &lt;br&gt;\nclass&lt;br&gt;\n&gt; &gt; or&lt;br&gt;\n&gt; &gt; &gt; &gt;any other class for that matter...I just starting\ndigging &lt;br&gt;\nthrough&lt;br&gt;\n&gt; &gt; the&lt;br&gt;\n&gt; &gt; &gt; &gt;code earlier today.&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;I continue my experimentation and code reading, but\nfigured, &lt;br&gt;\nI&#39;d&lt;br&gt;\n&gt; &gt; &gt; &gt;check in to see if I am missing something very\nobvious or if&lt;br&gt;\n&gt; &gt; anyone&lt;br&gt;\n&gt; &gt; &gt; &gt;had experienced similar behavior.&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;Thanks in advance for your time,&lt;br&gt;\n&gt; &gt; &gt; &gt;Seb&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;Yahoo! Groups Links&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt; &gt; &gt; &lt;br&gt;\n&gt; &gt; &gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;\n------------------------------------------------------------------&lt;br&gt;\n------&lt;br&gt;\n&gt; &gt; *Yahoo! Groups Links*&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp; * To visit your group on the web, go to:&lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;a\n href=&quot;http://groups.yahoo.com/group/archive-crawler/&quot;&gt;http://groups.yahoo.com/group/archive-crawler/&lt;/a&gt;&lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp; * To unsubscribe from this group, send an email to:&lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;a class=&quot;moz-txt-link-abbreviated&quot; href=&quot;mailto:archive-crawler-unsubscribe@yahoogroups.com&quot;&gt;archive-crawler-unsubscribe@yahoogroups.com&lt;/a&gt;&lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;&lt;a class=&quot;moz-txt-link-freetext&quot; href=&quot;mailto:archive-crawler-unsubscribe@yahoogroups.com&quot;&gt;mailto:archive-crawler-unsubscribe@yahoogroups.com&lt;/a&gt;?&lt;br&gt;\nsubject=Unsubscribe&gt;&lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp; * Your use of Yahoo! Groups is subject to the Yahoo!\nTerms of&lt;br&gt;\n&gt; &gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Service &lt;&lt;a href=&quot;http://docs.yahoo.com/info/terms/&quot;&gt;http://docs.yahoo.com/info/terms/&lt;/a&gt;&gt;.&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n&gt; &gt;&lt;br&gt;\n  &lt;br&gt;\n  &lt;/tt&gt;\n\n&lt;/blockquote&gt;\n&lt;br&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\r\n--------------090502000804060305060207--\r\n\n"}}