{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"YbYGNFdhUg_mL0olebM60-pyG3MeK70yBrGIiSlfcAyQ2yQASodwJjrJ0CK0xYAHnQho3RykA3dU5a8EEBNMeSZQRSiHc8g","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Configuring a &quot;DomainScope&quot; crawl using DecidingScope","postDate":"1243282349","msgId":5854,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRBMUFGQkFELjUwMzAwMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDRBMTk5QkQzLjQwMzA5MDJAc3UzYW5hbHl0aWNzLmNvbT4=","referencesHeader":"PDRBMTk5QkQzLjQwMzA5MDJAc3UzYW5hbHl0aWNzLmNvbT4="},"prevInTopic":5852,"nextInTopic":0,"prevInTime":5853,"nextInTime":5855,"topicId":5852,"numMessagesInTopic":2,"msgSnippet":"... Should that be .* at the front? (A regex usually won t start with a * .) ... Yes, the prerequisite rule is required -- at least if you don t have the ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 30145 invoked from network); 25 May 2009 20:12:48 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m1.grp.re1.yahoo.com with QMQP; 25 May 2009 20:12:48 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta2.grp.sp2.yahoo.com with SMTP; 25 May 2009 20:12:48 -0000\r\nX-Received: (qmail 75601 invoked from network); 25 May 2009 20:12:30 -0000\r\nX-Received: from 70.137.133.136 (HELO ?10.0.10.194?) (70.137.133.136)\n  by relay00.pair.com with SMTP; 25 May 2009 20:12:30 -0000\r\nX-pair-Authenticated: 70.137.133.136\r\nMessage-ID: &lt;4A1AFBAD.5030002@...&gt;\r\nDate: Mon, 25 May 2009 13:12:29 -0700\r\nUser-Agent: Thunderbird 2.0.0.21 (Windows/20090302)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;4A199BD3.4030902@...&gt;\r\nIn-Reply-To: &lt;4A199BD3.4030902@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Configuring a &quot;DomainScope&quot; crawl using DecidingScope\r\nX-Yahoo-Group-Post: member; u=137285340; y=Jix22Jvglt6qdwsqTZs-8FPcEgronfAxso9LvAtbGfvo\r\nX-Yahoo-Profile: gojomo\r\n\r\nRichard Hill wrote:\n&gt; Hi All,\n&gt; \n&gt; I am just getting started with Heritrix, and was hoping for some advice \n&gt; with respect to limiting a crawl to within a single domain (and any \n&gt; sub-domains).\n&gt; \n&gt; Additionally, I only want html content, no images, css, js, pdf&#39;s etc (I \n&gt; appreciate I might be missing valid links - however at this point I \n&gt; don&#39;t need these). My seed list contains a single domain like:\n&gt; \n&gt; http://www.targetdomain.tld\n&gt; \n&gt; According to the docs, the crawl scope &quot;DomainScope&quot; is deprecated and \n&gt; &quot;DecidingScope&quot; should be used instead. My decide-rules are, in order:\n&gt; \n&gt; RejectDecideRule\n&gt; OnDomainsDecideRule\n&gt; MatchesFilePatternDecideRule\n&gt; PrerequisiteAcceptDecideRule\n&gt; \n&gt; As I understand it, RejectDecideRule sets the decision to REJECT, which \n&gt; being first on list serves as a default action. OnDomains returns ACCEPT \n&gt; if a sub-domain of the domains in the seed list, else PASS. My Matches \n&gt; file patterns is:\n&gt; \n&gt; *(?i)(&#92;.(doc|pdf|ppt|swf|js|css|jpg|jpeg|png|gif))$\n&gt; \n&gt; and returns REJECT on a match.\n\nShould that be &quot;.*&quot; at the front? (A regex usually won&#39;t start with a &#39;*&#39;.)\n\n&gt; I believe the prerequisite rule is required for DNS, robots.txt (the job \n&gt; doesn&#39;t seem to run without it). I am writing out using \n&gt; MirrorWriterProcessor.\n\nYes, the prerequisite rule is required -- at least if you don&#39;t have the \ntransclusion rule, which also allows the implied DNS/robots URIs to be \nfetched.\n\n&gt; Now the crawl seems to work _reasonably_ well - e.g no images, pdfs, or \n&gt; content from other domains etc. However, I still get a .css or two and \n&gt; also a number of javascript files (particularly scriptaculous stuff)\n&gt; \n&gt; Could it be that these stray css, js are somehow &quot;Prerequisites&quot; - and \n&gt; so are being ACCEPTED? If so why is that and what can I do to eliminate \n&gt; them? Or is there some other possible explanation?\n\nThat shouldn&#39;t be the case; it looks to me like your rules should \nprevent this, and only DNS and robots URIs should be considered \nprerequisities. However, it&#39;s easy to see in the crawl.log if a URI is \nconsidered a prerequisite -- the &#39;hops-path&#39; field, the string of \ncapital letters like &#39;L&#39; or &#39;LEX&#39; or &#39;LLP&#39; etc. -- will end with a &#39;P&#39;.\n\nThe only thing I can think of is that the URIs don&#39;t actually end &quot;.css&quot; \nor &quot;.js&quot; -- maybe they have a &quot;?query-string&quot; at the end? Looking at the \nMatchesFilePatternDecideRule, I see that despite its name and the \nexample patterns, it&#39;s comparing the full URI, not the \nURI-without-query-string. It should also be clear if this is the case \nfrom the crawl.log lines of the unwanted fetches.\n\n  - Gordon @ IA\n\n"}}