{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","replyTo":"LIST","senderId":"gySDB3C_ZEDzaWhr3ruRTLWNQX4lq33ArMOD32xShVgDomRCii_5G0RqGzj19BJXMTL2JuFk0eYQh0QE1VSZemmEj6KgxS9c","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] EasyGarden: Heritrix vs. Mercator vs. HTTrack","postDate":"1057000669","msgId":91,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PFBpbmUuTE5YLjQuMzMuMDMwNjMwMTE1ODM0MC4xNzc3MS0xMDAwMDBAaG9tZXNlcnZlci5hcmNoaXZlLm9yZz4=","inReplyToHeader":"PFBpbmUuTE5YLjQuMzMuMDMwNjMwMTEwMzQ4MC4yNzg2My0xMDAwMDBAaG9tZXNlcnZlci5hcmNoaXZlLm9yZz4="},"prevInTopic":90,"nextInTopic":0,"prevInTime":90,"nextInTime":92,"topicId":84,"numMessagesInTopic":6,"msgSnippet":"... Yes, Mercator is striping URLs after it finds chars like &,  , #, n, and etc. We have the code that skips this link striping, but it works only for & ","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 96588 invoked from network); 30 Jun 2003 19:17:50 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m2.grp.scd.yahoo.com with QMQP; 30 Jun 2003 19:17:50 -0000\r\nReceived: from unknown (HELO homeserver.archive.org) (209.237.233.202)\n  by mta2.grp.scd.yahoo.com with SMTP; 30 Jun 2003 19:17:50 -0000\r\nReceived: from localhost (igor@localhost)\n\tby homeserver.archive.org (8.11.6/8.11.6) with ESMTP id h5UJHoT12916\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 30 Jun 2003 12:17:50 -0700\r\nX-Authentication-Warning: homeserver.archive.org: igor owned process doing -bs\r\nDate: Mon, 30 Jun 2003 12:17:49 -0700 (PDT)\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nSubject: Re: [archive-crawler] EasyGarden: Heritrix vs. Mercator vs. HTTrack\r\nIn-Reply-To: &lt;Pine.LNX.4.33.0306301103480.27863-100000@...&gt;\r\nMessage-ID: &lt;Pine.LNX.4.33.0306301158340.17771-100000@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: TEXT/PLAIN; charset=US-ASCII\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nX-Yahoo-Group-Post: member; u=137477665\r\n\r\n&gt; -----------------------------\n&gt; Mercator did not find a page where there were spaces in the \n&gt; filename.  It seems to have chopped off the rest of the filename after the \n&gt; space, and when Mercator tried to find that file, it resulted in a 404:\n&gt; Mercator\n&gt; crawl08.archive.org/links/spaces\n&gt; Heritrix\n&gt; crawl08.archive.org/links/spaces%20in%20path.html\n&gt; This page was the only valid page that Heritrix got that Mercator did not.  \n\nYes, Mercator is striping URLs after it finds chars like &, &quot; &quot;, #, &#92;n, \nand etc.\nWe have the code that skips this link striping, but it works only for \n&quot;&&quot;\nAnyway, Mercator will strip URL &quot;spaces in path.html&quot; to \n&quot;spaces&quot; :(\n\n&gt; ---------------\n&gt; The remaining pages that Heritrix got that Mercator and HTTrack did not, \n&gt; all returned 404 response codes.  Furthermore, the one page difference \n&gt; between Mercator and HTTrack was that HTTrack got the robots.txt file and Mercator did not.\n\nMercator is not writing robots.txt files to arc files :(\nWe could get data from logs that include robots.txt, but then we will \nneed to get mime type from somewhere else.\nI will look into it.\n\ni.\n\n\n"}}