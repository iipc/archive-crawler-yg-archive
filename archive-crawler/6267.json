{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":186477060,"authorName":"lucky goyal","from":"lucky goyal &lt;lg_0811@...&gt;","profile":"lg_0811","replyTo":"LIST","senderId":"cmohDz-DG4V0KikrbBQz5_brMGpw_nDa0t5OcXLExjRs1fW5Jop-TILnvWQacsrElYFV5JyrujvRduWDHn8LSyg_dmTF8A","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Need help regarding Heritrix 1.14 on windows platform","postDate":"1262842246","msgId":6267,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PDY2NzMxNi45MzcwMS5xbUB3ZWI1NDUwNi5tYWlsLnJlMi55YWhvby5jb20+"},"prevInTopic":0,"nextInTopic":6268,"prevInTime":6266,"nextInTime":6268,"topicId":6267,"numMessagesInTopic":2,"msgSnippet":"Hi All,   I am trying to crawl some websites using heritrix 1.14 on a windows machine(Server 2008, 64-bit machine). I am using JAVA 6 (32-bit) for running","rawEmail":"Return-Path: &lt;lg_0811@...&gt;\r\nReceived: (qmail 68362 invoked from network); 7 Jan 2010 06:48:06 -0000\r\nReceived: from unknown (66.196.94.106)\n  by m12.grp.re1.yahoo.com with QMQP; 7 Jan 2010 06:48:06 -0000\r\nReceived: from unknown (HELO n40b.bullet.mail.sp1.yahoo.com) (66.163.168.154)\n  by mta2.grp.re1.yahoo.com with SMTP; 7 Jan 2010 06:48:06 -0000\r\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoogroups.com; s=lima; t=1262846701; bh=ttwLIXoeFLQX4cjVX/nWTmWN2F/xf3PSixDDccv3k78=; h=Received:Received:X-Sender:X-Apparently-To:X-Received:X-Received:X-Received:X-Received:Message-ID:X-YMail-OSG:X-Received:X-Mailer:Date:To:MIME-Version:Content-Type:X-Originating-IP:X-eGroups-Msg-Info:From:Subject:X-Yahoo-Group-Post:X-Yahoo-Profile:X-YGroups-SubInfo:Sender:X-Yahoo-Newman-Property:X-eGroups-Approved-By:X-eGroups-Auth; b=myhGEoHj0j7Ge85h0K3xmFSngRpHqOQxdC7AuayBDm/1/MlhdFGYDh1b/UFS6S7y62rM4OPpdBBsRE9ldkCJQ+YXSU7rkd0A+IC35nHCHS+Kd/A82E5DpfkgKkTnRlp7\r\nReceived: from [69.147.65.148] by n40.bullet.mail.sp1.yahoo.com with NNFMP; 07 Jan 2010 06:45:01 -0000\r\nReceived: from [98.137.34.184] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 07 Jan 2010 06:45:01 -0000\r\nX-Sender: lg_0811@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 86782 invoked from network); 7 Jan 2010 05:31:53 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m12.grp.re1.yahoo.com with QMQP; 7 Jan 2010 05:31:53 -0000\r\nX-Received: from unknown (HELO web54506.mail.re2.yahoo.com) (206.190.49.156)\n  by mta1.grp.sp2.yahoo.com with SMTP; 7 Jan 2010 05:31:52 -0000\r\nX-Received: (qmail 96253 invoked by uid 60001); 7 Jan 2010 05:30:46 -0000\r\nMessage-ID: &lt;667316.93701.qm@...&gt;\r\nX-YMail-OSG: Q42TTc0VM1m3VmZW9jx0UHRgQcQfk0t.1Vd3bmlav.ccaS8trQJHgyLZDnIP.LPpex9ESWCfAykZ9LLFvKIqctRMvDPCVzyE3Vo366L2YVw.ptdvDW3xqui7be0297uFsnM2SmOmA20o5OG9dJzL64IpZ5ePC57NhpyTvf3mERDt7wOJwKZgr.MFKcln73dHo37j1Oy4i.KhaTgs9hy_A3G_MQqgKCLBUGBNNNWuKUqUsPadtNcEmdIlo_RXfurpQZDOo9r4OJWvLbfCu.h52urW.KG1Y8KsTV5tVoBneK8Tg0yMR4ISHp88JvoBqTBPTdbbNiU8pONvc4PiUD_wiH8-\r\nX-Received: from [220.227.32.101] by web54506.mail.re2.yahoo.com via HTTP; Wed, 06 Jan 2010 21:30:46 PST\r\nX-Mailer: YahooMailClassic/9.0.20 YahooMailWebService/0.8.100.260964\r\nDate: Wed, 6 Jan 2010 21:30:46 -0800 (PST)\r\nTo: archive-crawler@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-1612033966-1262842246=:93701&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: lucky goyal &lt;lg_0811@...&gt;\r\nSubject: Need help regarding Heritrix 1.14 on windows platform\r\nX-Yahoo-Group-Post: member; u=186477060; y=XwTEK0Dj5Fi_ZKmPq7c7DznyaLDynVVxFveNKZ2jcjV_8A\r\nX-Yahoo-Profile: lg_0811\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 07 Jan 2010 06:44:59 -0000\r\n\r\n\r\n--0-1612033966-1262842246=:93701\r\nContent-Type: text/plain; charset=iso-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\nHi All,\n=A0\nI am trying to crawl=A0some=A0websites using heritrix 1.14 on a=\r\n\nwindows machine(Server 2008, 64-bit machine). I am using JAVA 6\n(32-bit) f=\r\nor running the heritrix instance. The heritrix is run from\ncommand line wit=\r\nh &quot;-nowui&quot; option enabled.=A0I am facing a few issues in\ncrawling. Any help=\r\n on these=A0shall be=A0greatly appreciated.\n=A0\n1)=A0My goal is to crawl a =\r\nbunch (20-25) of websites in a single\njob. The pages of my interest in thes=\r\ne websites are 2-4 links away from\nthe websites main page(home page). Since=\r\n I am giving the websites URL\nas the seed, the heritrix is fetching all lin=\r\nks its discovers from home\npage onwards. I am using deciding scope with a s=\r\nequence of decide rules\nas mentioned below.\n=A0\n=A0=A0=A0=A0=A0 &lt;newObject =\r\nname=3D&quot;decide-rules&quot; class=3D&quot;org.archive.crawler.deciderules.DecideRuleSe=\r\nquence&quot;&gt;\n=A0=A0=A0=A0=A0=A0=A0 &lt;map name=3D&quot;rules&quot;&gt;\n=A0=A0=A0=A0=A0=A0=A0=\r\n=A0=A0 &lt;newObject name=3D&quot;acceptAllRule&quot; class=3D&quot;org.archive.crawler.decid=\r\nerules.AcceptDecideRule&quot;&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;/newObject&gt;\n=A0=A0=\r\n=A0=A0=A0=A0=A0=A0=A0 &lt;newObject name=3D&quot;rejectNotOnDomainRule&quot; class=3D&quot;or=\r\ng.archive.crawler.deciderules.NotOnDomainsDecideRule&quot;&gt;\n=A0=A0=A0=A0=A0=A0=\r\n=A0=A0=A0=A0=A0 &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n=A0=A0=A0=A0=A0=\r\n=A0=A0=A0=A0=A0=A0 &lt;string name=3D&quot;surts-source-file&quot;/&gt;\n=A0=A0=A0=A0=A0=A0=\r\n=A0=A0=A0=A0=A0 &lt;boolean\n name=3D&quot;seeds-as-surt-prefixes&quot;&gt;true&lt;/boolean&gt;\n=\r\n=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;string name=3D&quot;surts-dump-file&quot;/&gt;\n=A0=A0=\r\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;boolean name=3D&quot;also-check-via&quot;&gt;false&lt;/boolean=\r\n&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;boolean name=3D&quot;rebuild-on-reconfig&quot;&gt;t=\r\nrue&lt;/boolean&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;/newObject&gt;\n=A0=A0=A0=A0=A0=A0=\r\n=A0=A0=A0 &lt;newObject name=3D&quot;rejectFilePatternRule&quot; class=3D&quot;org.archive.cr=\r\nawler.deciderules.MatchesFilePatternDecideRule&quot;&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=\r\n=A0=A0=A0 &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;\n=A0=A0=A0=A0=A0=A0=A0=\r\n=A0=A0=A0=A0 &lt;string\n name=3D&quot;use-preset-pattern&quot;&gt;Custom&lt;/string&gt;\n=A0=A0=A0=\r\n=A0=A0=A0=A0=A0=A0=A0=A0\n&lt;string\nname=3D&quot;regexp&quot;&gt;.*(?i)(//.(bmp|gif|jpe?g|p=\r\nng|tiff?|mid|mp2|mp3|mp4|wav|avi|mov|mpeg|ram|rm|smil|wmv|doc|odt|pdf|ppt|s=\r\nwf|mov|m4v|zip|sit|pub|File|exe|eps|ppt|pptx|fla|dmg|csv|png|css|pdf))$&lt;/st=\r\nring&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;/newObject&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 =\r\n&lt;newObject name=3D&quot;rejectIfTooManyHops&quot; class=3D&quot;org.archive.crawler.decide=\r\nrules.TooManyHopsDecideRule&quot;&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;integer na=\r\nme=3D&quot;max-hops&quot;&gt;5&lt;/integer&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;/newObject&gt;\n=A0=A0=\r\n=A0=A0=A0=A0=A0=A0=A0 &lt;newObject name=3D&quot;rejectIfPathological&quot;\n class=3D&quot;or=\r\ng.archive.crawler.deciderules.PathologicalPathDecideRule&quot;&gt;\n=A0=A0=A0=A0=A0=\r\n=A0=A0=A0=A0=A0=A0 &lt;integer name=3D&quot;max-repetitions&quot;&gt;3&lt;/integer&gt;\n=A0=A0=A0=\r\n=A0=A0=A0=A0=A0=A0 &lt;/newObject&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;newObject name=\r\n=3D&quot;rejectIfTooManyPathSegs&quot; class=3D&quot;org.archive.crawler.deciderules.TooMa=\r\nnyPathSegmentsDecideRule&quot;&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;integer name=\r\n=3D&quot;max-path-depth&quot;&gt;10&lt;/integer&gt;\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;/newObject&gt;\n=\r\n=A0=A0=A0=A0=A0=A0=A0=A0=A0 &lt;newObject name=3D&quot;acceptPreReqRule&quot; class=3D&quot;o=\r\nrg.archive.crawler.deciderules.PrerequisiteAcceptDecideRule&quot;&gt;\n=A0=A0=A0=A0=\r\n=A0=A0=A0=A0=A0 &lt;/newObject&gt;\n=A0=A0=A0=A0=A0=A0=A0\n &lt;/map&gt;\n=A0=A0=A0=A0=A0 =\r\n&lt;/newObject&gt;\n=A0\nAlthough I can see that in the mirror folder that no page =\r\noutside\nthe seed&#39;s domains and subdomains is been crawled, But still the\n&quot;p=\r\nrogress-statistics&quot; file shows a very large number of discovered\nURL&#39;s. Run=\r\nning this foccussed=A0crawl for approx. 5 days discovers around\n15=A0millio=\r\nn=A0URL&#39;s out of which only=A0approx 2 million are downloded in 5\ndays. dis=\r\ncovery of such a large number=A0of URL&#39;s from a set of 25 sites\nseems stran=\r\nge. Also I am not able to find a way to see the list of\ndiscovered and queu=\r\ned URI&#39;s. \n=A0\nCould someone please share his/her experience of similar beh=\r\naviour\nof any pointers to analysing the problem. Specially it would be real=\r\nly\nhelpful, if I could see the frontier log to see the list of\ndiscovered=\r\n=A0and queued URL&#39;s.\n=A0\n2)=A0Does=A0the list=A0of discovered URL&#39;s contain=\r\n out of scope URL&#39;s.\nOr=A0is the scope filter already applied before adding=\r\n the URL to\ndiscovered list.\n=A0\n3)=A0Does heritrix queue duplicate URL&#39;s a=\r\nlso.The webpages I intend to crawl, will contain a \n=A0=A0=A0 lot of duplic=\r\nate URL&#39;s. Is there any configuration setting to\nprevent duplicate URL&#39;s fr=\r\nom being queued=A0and crawled in a single crawl\njob run.=A0\n=A0\n4)=A0I can =\r\nsee a lot of &quot;FileNotFoundException@MirrorWriter&quot;=A0=A0errors\nin the &quot;local=\r\n-errors&quot; logs. On analysis, it was observed that the error\nis because the U=\r\nRL contains some special characters which are not\nallowed=A0in file names i=\r\nn windows, for example : &lt; &gt; &#92; ? | *=A0&quot;\netc. I have tried the character ma=\r\np setting as proposed in heritrix\nwebUI. But it does not seem to yield the =\r\nresults. Please let me\nknow=A0some solution to this problem. It is really i=\r\nmportant for my\nexercise as a lot of the desired URL&#39;s have one or another =\r\nspecial\ncharacters in the name.\n=A0\n5) I would also need to crawl a few Aja=\r\nx based websites. As per my\ncurrent understanding, this is not possible wit=\r\nh heritrix. Is there any\navalable plugin=A0or another mechanism to crawl su=\r\nch Ajax based websites\nusing heritrix. Any help on this=A0will be really us=\r\neful. \n=A0\nI shall be=A0looking forward=A0to=A0help on above issues. Kindly=\r\n also\nlet me know if I am not on the right forum, and advice me if I need t=\r\no\npost it some place else also.\n=A0\nThanks and Regards,\nLucky Goyal \n\n\n\n   =\r\n   \r\n--0-1612033966-1262842246=:93701\r\nContent-Type: text/html; charset=iso-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n&lt;table cellspacing=3D&quot;0&quot; cellpadding=3D&quot;0&quot; border=3D&quot;0&quot; &gt;&lt;tr&gt;&lt;td valign=3D&quot;=\r\ntop&quot; style=3D&quot;font: inherit;&quot;&gt;&lt;div&gt;Hi All,&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;I a=\r\nm trying to crawl&nbsp;some&nbsp;websites using heritrix 1.14 on a\nwindows =\r\nmachine(Server 2008, 64-bit machine). I am using JAVA 6\n(32-bit) for runnin=\r\ng the heritrix instance. The heritrix is run from\ncommand line with &quot;-nowui=\r\n&quot; option enabled.&nbsp;I am facing a few issues in\ncrawling. Any help on th=\r\nese&nbsp;shall be&nbsp;greatly appreciated.&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;1)=\r\n&nbsp;My goal is to crawl a bunch (20-25) of websites in a single\njob. The =\r\npages of my interest in these websites are 2-4 links away from\nthe websites=\r\n main page(home page). Since I am giving the websites URL\nas the seed, the =\r\nheritrix is fetching all links its discovers from home\npage onwards. I am u=\r\nsing deciding scope with a sequence of decide rules\nas mentioned below.&lt;/di=\r\nv&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject name=\r\n=3D&quot;decide-rules&quot; class=3D&quot;org.archive.crawler.deciderules.DecideRuleSequen=\r\nce&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;map name=3D&quot;rules=\r\n&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObje=\r\nct name=3D&quot;acceptAllRule&quot; class=3D&quot;org.archive.crawler.deciderules.AcceptDe=\r\ncideRule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt=\r\n;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &=\r\nlt;newObject name=3D&quot;rejectNotOnDomainRule&quot; class=3D&quot;org.archive.crawler.de=\r\nciderules.NotOnDomainsDecideRule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;string name=3D&quot;decision&quot;&gt;REJECT&lt;=\r\n/string&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n&nbsp; &lt;string name=3D&quot;surts-source-file&quot;/&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;boolean\n name=3D&quot;seeds-as-=\r\nsurt-prefixes&quot;&gt;true&lt;/boolean&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb=\r\nsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;string name=3D&quot;surts-dump-file&quot;/&gt;&lt;=\r\nbr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;b=\r\noolean name=3D&quot;also-check-via&quot;&gt;false&lt;/boolean&gt;&lt;br&gt;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;boolean name=3D&quot;rebu=\r\nild-on-reconfig&quot;&gt;true&lt;/boolean&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp; &lt;newObject name=3D&quot;rejectFilePatternRule&quot; clas=\r\ns=3D&quot;org.archive.crawler.deciderules.MatchesFilePatternDecideRule&quot;&gt;&lt;br&gt;&=\r\nnbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;strin=\r\ng name=3D&quot;decision&quot;&gt;REJECT&lt;/string&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nb=\r\nsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;string\n name=3D&quot;use-preset-patt=\r\nern&quot;&gt;Custom&lt;/string&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n&nbsp;&nbsp;&nbsp;&nbsp;\n&lt;string\nname=3D&quot;regexp&quot;&gt;.*(?i)(//.(bmp|gif|j=\r\npe?g|png|tiff?|mid|mp2|mp3|mp4|wav|avi|mov|mpeg|ram|rm|smil|wmv|doc|odt|pdf=\r\n|ppt|swf|mov|m4v|zip|sit|pub|File|exe|eps|ppt|pptx|fla|dmg|csv|png|css|pdf)=\r\n)$&lt;/string&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs=\r\np; &lt;newObject name=3D&quot;rejectIfTooManyHops&quot; class=3D&quot;org.archive.crawler.=\r\ndeciderules.TooManyHopsDecideRule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb=\r\nsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;integer name=3D&quot;max-hops&quot;&gt;5&lt;/in=\r\nteger&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/ne=\r\nwObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;n=\r\newObject name=3D&quot;rejectIfPathological&quot;\n class=3D&quot;org.archive.crawler.decide=\r\nrules.PathologicalPathDecideRule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs=\r\np;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;integer name=3D&quot;max-repetitions&quot;&gt;3&=\r\nlt;/integer&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &=\r\nlt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\r\n &lt;newObject name=3D&quot;rejectIfTooManyPathSegs&quot; class=3D&quot;org.archive.crawle=\r\nr.deciderules.TooManyPathSegmentsDecideRule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp=\r\n;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;integer name=3D&quot;max-path-de=\r\npth&quot;&gt;10&lt;/integer&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb=\r\nsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&=\r\nnbsp;&nbsp; &lt;newObject name=3D&quot;acceptPreReqRule&quot; class=3D&quot;org.archive.cr=\r\nawler.deciderules.PrerequisiteAcceptDecideRule&quot;&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&n=\r\nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/newObject&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;=\r\n&nbsp;&nbsp;&nbsp;&nbsp;\n &lt;/map&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &l=\r\nt;/newObject&gt;&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;Although I can see that in th=\r\ne mirror folder that no page outside\nthe seed&#39;s domains and subdomains is b=\r\neen crawled, But still the\n&quot;progress-statistics&quot; file shows a very large nu=\r\nmber of discovered\nURL&#39;s. Running this foccussed&nbsp;crawl for approx. 5 d=\r\nays discovers around\n15&nbsp;million&nbsp;URL&#39;s out of which only&nbsp;appr=\r\nox 2 million are downloded in 5\ndays. discovery of such a large number&nbsp=\r\n;of URL&#39;s from a set of 25 sites\nseems strange. Also I am not able to find =\r\na way to see the list of\ndiscovered and queued URI&#39;s. &lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/d=\r\niv&gt;\n&lt;div&gt;Could someone please share his/her experience of similar behaviour=\r\n\nof any pointers to analysing the problem. Specially it would be really\nhel=\r\npful, if I could see the frontier log to see the list of\ndiscovered&nbsp;an=\r\nd queued URL&#39;s.&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;2)&nbsp;Does&nbsp;the list&nbs=\r\np;of discovered URL&#39;s contain out of scope URL&#39;s.\nOr&nbsp;is the scope filt=\r\ner already applied before adding the URL to\ndiscovered list.&lt;/div&gt;\n&lt;div&gt;&nb=\r\nsp;&lt;/div&gt;\n&lt;div&gt;3)&nbsp;Does heritrix queue duplicate URL&#39;s also.The webpage=\r\ns I intend to crawl, will contain a &lt;/div&gt;\n&lt;div&gt;&nbsp;&nbsp;&nbsp; lot of d=\r\nuplicate URL&#39;s. Is there any configuration setting to\nprevent duplicate URL=\r\n&#39;s from being queued&nbsp;and crawled in a single crawl\njob run.&nbsp;&lt;/div=\r\n&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;4)&nbsp;I can see a lot of &quot;FileNotFoundException@=\r\nMirrorWriter&quot;&nbsp;&nbsp;errors\nin the &quot;local-errors&quot; logs. On analysis, it=\r\n was observed that the error\nis because the URL contains some special chara=\r\ncters which are not\nallowed&nbsp;in file names in windows, for example : &l=\r\nt; &gt; &#92; ? | *&nbsp;&quot;\netc. I have tried the character map setting as propo=\r\nsed in heritrix\nwebUI. But it does not seem to yield the results. Please le=\r\nt me\nknow&nbsp;some solution to this problem. It is really important for my=\r\n\nexercise as a lot of the desired URL&#39;s have one or another special\ncharact=\r\ners in the name.&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;5) I would also need to crawl=\r\n a few Ajax based websites. As per my\ncurrent understanding, this is not po=\r\nssible with heritrix. Is there any\navalable plugin&nbsp;or another mechanis=\r\nm to crawl such Ajax based websites\nusing heritrix. Any help on this&nbsp;w=\r\nill be really useful. &lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;I shall be&nbsp;looking=\r\n forward&nbsp;to&nbsp;help on above issues. Kindly also\nlet me know if I am=\r\n not on the right forum, and advice me if I need to\npost it some place else=\r\n also.&lt;/div&gt;\n&lt;div&gt;&nbsp;&lt;/div&gt;\n&lt;div&gt;Thanks and Regards,&lt;/div&gt;\n&lt;div&gt;Lucky Go=\r\nyal &lt;br&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;br&gt;\n\n      \r\n--0-1612033966-1262842246=:93701--\r\n\n"}}