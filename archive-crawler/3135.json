{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"9rtES66qZzOJpMDRjZCUqCWkWRgUgGvet0Kov0v4wl15BN1ssomzjlCwqmgUkts1yBf9u55n_sJ9oXm3LrL2zqUYRXO_wVPu","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Parallelizing crawler","postDate":"1154367859","msgId":3135,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0Q0U0MTczLjIwODAzMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGVhYjcwcis1cDkyQGVHcm91cHMuY29tPg==","referencesHeader":"PGVhYjcwcis1cDkyQGVHcm91cHMuY29tPg=="},"prevInTopic":3133,"nextInTopic":0,"prevInTime":3134,"nextInTime":3136,"topicId":3043,"numMessagesInTopic":16,"msgSnippet":"... Ok.  And if no UI, you d monitor the crawler via JMX? You ve see the HCC library: http://crawler.archive.org/hcc/?  Might be of use to you getting your","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 56187 invoked from network); 31 Jul 2006 17:43:58 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m40.grp.scd.yahoo.com with QMQP; 31 Jul 2006 17:43:58 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta6.grp.scd.yahoo.com with SMTP; 31 Jul 2006 17:43:58 -0000\r\nReceived: from [192.168.1.106] ([192.168.1.106])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k6VGR4T11911;\n\tMon, 31 Jul 2006 09:27:04 -0700\r\nMessage-ID: &lt;44CE4173.2080305@...&gt;\r\nDate: Mon, 31 Jul 2006 10:44:19 -0700\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.8.0.4) Gecko/20060516 SeaMonkey/1.0.2\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;eab70r+5p92@...&gt;\r\nIn-Reply-To: &lt;eab70r+5p92@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: Parallelizing crawler\r\nX-Yahoo-Group-Post: member; u=168599281; y=Zaxx93ZlVndToYrZxYHtFkMta74ZLOaS7TfP3Uytm_yjtCLX4JjBY7vT\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nmolzbh wrote:\n&gt;\n&gt; Thanks St.Ack . Your feedback is helping us a lot. By chores I meant\n&gt; to have cutsom Order.xml to load the crawl job, and not maintain a\n&gt; web console, I did not mean it in terms of Crawler functionality but\n&gt; in terms of deployment.\n&gt;\n\n\n\n\n\n\nOk.  And if no UI, you&#39;d monitor the crawler via JMX?\n\nYou&#39;ve see the HCC library: http://crawler.archive.org/hcc/?  Might be \nof use to you getting your cluster started, distributing out the \norder.xml to run.\n\n&gt;\n&gt; I went through the UBI article, I guess we had figured this scheme,\n&gt; but this is a peer I/O per URL, wouldn&#39;t you agree ?\n&gt;\n\n\n\n\n\nPardon me again.  Are you saying that its a peer-to-peer system (yes) \nand that it requires I/O per URL (Probably)?\n\n\n\n&gt; Also, depart\n&gt; from this the churn in the number of agents is expected to be pretty\n&gt; low, in which case I am wondering if it is a better option to share\n&gt; the Already Seen list upon joining of an agent, as then the URL would\n&gt; almost never be re-fetched. I agree that a central Seen List is a bad\n&gt; option.\n&gt;\n\n\n\n\n\n\n\n\n\n\nSo, as part of the join-the-cluster protocol, somehow, the new peer \nwould get downloads of already-seen from its adjacent agents (or \nappropriate portions of already-seen)?  This might be possible (if the \nalready-seen is  not done with a bloom-filter).  The resultant \nalready-seen though would be polluted with already-seen slices that fall \noutside the local agents&#39; scope (making lookups take longer).  If you \ndid as the ubicrawler paper suggests, querying the next hosts along the \nunit circle, if an URL was already-seen, then the new agent&#39;s \nalready-seen would contain pertinent already-seens only (You might check \nthe local already-seen first before going to the net and, after going to \nthe net, make a mark in the local DB for each URL already-seen so you \ndon&#39; t have to go to net again -- a lazy population of the local \nalready-seen).\n\nA concern would be the interagent communication, that it would tend to \noverwhelm (The ubicrawler folks make observation on how intercrawler \ntraffic changes with number of agents.  See &#39;Comunication Overhead&#39;).\nthat they are fetching more pages, and not to a design bottleneck&#39;).\n\nThe above talks about a new agent joining the cluster. Avoiding \nduplicates in this scenario seems easy enough. Avoiding duplicates when \na server crashes seems the harder problem.  In the single-crawler case, \nwe&#39;ve already made concession that refetch is unavoidable; its just a \nmatter of degrees (If using checkpointing, the degree is greater than \nwhen using the recover.log).  In the clustered case, what to do when a \ncrash?  Do we reset the complete cluster at the last cluster checkpoint \n-- all cluster-members whether crashed or not -- so there&#39;ll be a replay \nof the interagent swap of URLs or do we just recover the crashed member \nand hope that we&#39;ll be passed the lost URLs at some time again in the \nfuture?\n\nEnough for now.\nYours,\nSt.Ack\n\n\n\n\n\n&gt;  \n\n\n"}}