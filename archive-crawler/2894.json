{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"bQ_32jWPxDpPT7_pqogSA0cLyR3YfPs1mHM7k3zIZfR8vBgGhWwSU5oTCMEhPs2OHwJxUxFE3H9fwY6WONllwtPsbiy8trds","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] getting urls..","postDate":"1149142506","msgId":2894,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0N0U4NUVBLjIwODAxMDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGU1bHV0dCs0ajRtQGVHcm91cHMuY29tPg==","referencesHeader":"PGU1bHV0dCs0ajRtQGVHcm91cHMuY29tPg=="},"prevInTopic":2892,"nextInTopic":2895,"prevInTime":2893,"nextInTime":2895,"topicId":2892,"numMessagesInTopic":6,"msgSnippet":"... You could create your own LinksWriter and use it in place of ARCWriter. Your LinkWriter would open a file and per CrawlURI, it d dump the CrawlURI URI and","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 37515 invoked from network); 1 Jun 2006 06:14:13 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m30.grp.scd.yahoo.com with QMQP; 1 Jun 2006 06:14:13 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta2.grp.scd.yahoo.com with SMTP; 1 Jun 2006 06:14:12 -0000\r\nReceived: from [192.168.1.105] ([192.168.1.105])\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id k5150Hw24030;\n\tWed, 31 May 2006 22:00:17 -0700\r\nMessage-ID: &lt;447E85EA.2080105@...&gt;\r\nDate: Wed, 31 May 2006 23:15:06 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.8.0.1) Gecko/20060127 SeaMonkey/1.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e5lutt+4j4m@...&gt;\r\nIn-Reply-To: &lt;e5lutt+4j4m@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] getting urls..\r\nX-Yahoo-Group-Post: member; u=168599281; y=dwemhhYvG0SJQONoChf5jI0sXnWSFJqvO21cS6NhLPOlerORbjhCYF_o\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\ncallforshadab wrote:\n&gt; Hi All,\n&gt; Is there anyway to save links without saving the contents of the\n&gt; crawl??\n\nYou could create your own LinksWriter and use it in place of ARCWriter.\n\nYour LinkWriter would open a file and per CrawlURI, it&#39;d dump the \nCrawlURI URI and all its outlinks.  The outlinks are put into the \nCrawlURI by the extractors.  See LinksScoper for example code on how to \nget the links out of the CrawlURI: \nhttp://crawler.archive.org/xref/org/archive/crawler/postprocessor/LinksScoper.html#135.\n\n&gt; I understood that if we are using the bdbfrontier, it stores\n&gt; all links and the domain queues in *.jdb files. Can i extract all\n&gt; domains that it has in its list alongwith the urls belonging to that\n&gt; domain.\n&gt;\nYou could use bdbje db dump tools to get at the bdbje db content but it \nwouldn&#39;t be satisfactory.  The Bdb has only the URIs that are in scope \nand it only contains hashes of URIs, not the URIs themselves.\nSt.Ack\n\n"}}