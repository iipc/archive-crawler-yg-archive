{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":278782393,"authorName":"Paul Jack","from":"Paul Jack &lt;pjack@...&gt;","profile":"poetbeware","replyTo":"LIST","senderId":"fvgFbiV9rGScHl3MWynOo8qnic_Cu2HGr5gKBRqU5QwwzUZqEXn7xHd2OOKBNA5B6_xqHqurNTaoOdtzLznbd9Ijqw4","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Advice from the experts (please)","postDate":"1185813449","msgId":4477,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDYzNzkxQUM4LTI2OUEtNEM1My1BRDcyLUZDODNFQ0QyNUY4NUBhcmNoaXZlLm9yZz4=","inReplyToHeader":"PGY4OHJoMitibTNjQGVHcm91cHMuY29tPg==","referencesHeader":"PGY4OHJoMitibTNjQGVHcm91cHMuY29tPg=="},"prevInTopic":4464,"nextInTopic":0,"prevInTime":4476,"nextInTime":4478,"topicId":4464,"numMessagesInTopic":2,"msgSnippet":"Hi, I m unclear on what you mean by assigning categories to your URLs. However, the solution you describe (subclassing BdbFrontier) seems workable. Honestly,","rawEmail":"Return-Path: &lt;pjack@...&gt;\r\nX-Sender: pjack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 45391 invoked from network); 30 Jul 2007 16:37:53 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m43.grp.scd.yahoo.com with QMQP; 30 Jul 2007 16:37:53 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta10.grp.scd.yahoo.com with SMTP; 30 Jul 2007 16:37:53 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id F1C13141563A7\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 30 Jul 2007 09:37:30 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 10874-05-85 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 30 Jul 2007 09:37:30 -0700 (PDT)\r\nReceived: from [172.16.1.39] (adsl-71-131-24-221.dsl.sntc01.pacbell.net [71.131.24.221])\n\tby mail.archive.org (Postfix) with ESMTP id 66937141563A5\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 30 Jul 2007 09:37:30 -0700 (PDT)\r\nMime-Version: 1.0 (Apple Message framework v752.3)\r\nIn-Reply-To: &lt;f88rh2+bm3c@...&gt;\r\nReferences: &lt;f88rh2+bm3c@...&gt;\r\nContent-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed\r\nMessage-Id: &lt;63791AC8-269A-4C53-AD72-FC83ECD25F85@...&gt;\r\nContent-Transfer-Encoding: 7bit\r\nDate: Mon, 30 Jul 2007 09:37:29 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nX-Mailer: Apple Mail (2.752.3)\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Paul Jack &lt;pjack@...&gt;\r\nSubject: Re: [archive-crawler] Advice from the experts (please)\r\nX-Yahoo-Group-Post: member; u=278782393; y=MxgxZ7NlHIAGu-W7yBiCeBhKkvAYfQoj8Q51zsHaKAbYFaHqBA\r\nX-Yahoo-Profile: poetbeware\r\n\r\nHi,\n\nI&#39;m unclear on what you mean by &quot;assigning categories&quot; to your URLs.  \nHowever, the solution you describe (subclassing BdbFrontier) seems  \nworkable. Honestly, though, a few million URLs in a recovery file  \nshouldn&#39;t be that big a deal. You could still manage them in a  \ndatabase, and write a little routine to dump them in the proper  \nformat before you start the crawl.\n\nIf you do pursue subclassing BdbFrontier, let us know how it works...\n\n-Paul\n\n\nOn Jul 25, 2007, at 5:57 PM, _threeputt wrote:\n\n&gt; First of all, let me congratulate you on a robust, extendable project.\n&gt; My compliments...\n&gt;\n&gt; I&#39;m starting a project that will crawl millions of pre-configured\n&gt; urls. We want to have a category assigned to each url so we only want\n&gt; to crawl text and images from each site without following any external\n&gt; links. This will therefore require using DomainScoped crawling only\n&gt; (Right?).\n&gt;\n&gt; I am therefore under the impression that each url to be crawled should\n&gt; be treated as a seed. Is this true?\n&gt;\n&gt; If so, how do I best supply those (millions of) urls to the crawler?\n&gt; I could use the &quot;Start the crawl with a recovery log&quot; option that I\n&gt; read about here:\n&gt; http://webteam.archive.org/confluence/display/Heritrix/Feed+URLs+in \n&gt; +bulk+to+a+crawler,\n&gt; but that would require that an extremely large flat file be managed,\n&gt; and I feel like having rows in a table for the urls and their\n&gt; (potentially) multiple categories would be a more workable solution.\n&gt; I could, I suppose, look up each each url in the database as I process\n&gt; them in order to get their categories, but that would require they\n&gt; exist in the database and then be exported to the &quot;recovery file&quot;.\n&gt;\n&gt; I&#39;ve only been perusing the code for a day or so, but it seems like a\n&gt; better solution might be to sub-class BdbFrontier, create a thread\n&gt; that watches the WorkQueue, and then batches up urls from the database\n&gt; as the number of items on the queue diminishes.\n&gt;\n&gt; Does that sound like a good solution? Do you have any better  \n&gt; suggestions.\n&gt;\n&gt; Thank you so much for your time.\n&gt;\n&gt; Glenn Bullock\n&gt;\n&gt; Software Engineer\n&gt; Surf Recon, Inc\n&gt; www.surfrecon.com\n&gt;\n&gt;\n&gt; \n\n\n"}}