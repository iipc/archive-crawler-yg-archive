{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":152219847,"authorName":"cash_05","from":"&quot;cash_05&quot; &lt;cash_05@...&gt;","profile":"cash_05","replyTo":"LIST","senderId":"XDbVSOssTdjtwP4nP9SgS3UM9yMFMWe5xT_MHbwe4SKXA_2C6BhYndJfyWuit58qi1NjjdjGQQMm17H51oGK7u2_M78","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: Help! What always NOTCRAWLED","postDate":"1114745296","msgId":1767,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGQ0czlrZys3dGh0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQyNzE4MkE3LjgwNTAwMDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":1766,"nextInTopic":1768,"prevInTime":1766,"nextInTime":1768,"topicId":1760,"numMessagesInTopic":10,"msgSnippet":"Dear Gordon, ... The same machine running heritrix can access those sites with web browser. However it is behind a squid proxy server. Proxy setting was ","rawEmail":"Return-Path: &lt;cash_05@...&gt;\r\nX-Sender: cash_05@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 12266 invoked from network); 29 Apr 2005 03:28:26 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m26.grp.scd.yahoo.com with QMQP; 29 Apr 2005 03:28:26 -0000\r\nReceived: from unknown (HELO n10a.bulk.scd.yahoo.com) (66.94.237.44)\n  by mta2.grp.scd.yahoo.com with SMTP; 29 Apr 2005 03:28:26 -0000\r\nReceived: from [66.218.69.5] by n10.bulk.scd.yahoo.com with NNFMP; 29 Apr 2005 03:28:19 -0000\r\nReceived: from [66.218.66.99] by mailer5.bulk.scd.yahoo.com with NNFMP; 29 Apr 2005 03:28:19 -0000\r\nDate: Fri, 29 Apr 2005 03:28:16 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;d4s9kg+7tht@...&gt;\r\nIn-Reply-To: &lt;427182A7.8050002@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 1641\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: &quot;cash_05&quot; &lt;cash_05@...&gt;\r\nSubject: Re: Help! What always NOTCRAWLED\r\nX-Yahoo-Group-Post: member; u=152219847\r\nX-Yahoo-Profile: cash_05\r\n\r\nDear Gordon,\n \n&gt; The server(s) may be completely unreachable -- so Heritrix isn&#39;t\n&gt; even getting a 404 for /robots.txt. From the same machine on which\n&gt; Heritrix is running, can you access the sites with a web browser?\n&gt; (Do you need to use a web proxy to do so?)\n\nThe same machine running heritrix can access those sites with web\nbrowser. However it is behind a squid proxy server. Proxy setting was\ndefined in global profile.\n\nIs it the proxy that causing this problem? If so any work around?\n\n&gt; Alternative, something about the crawl configuration may be preventing\n&gt; /robots.txt from being fetched. If you have changed the scope, added\n&gt; filters, or reduced the allowable &#39;max-trans-hops&#39; for your crawl,\n&gt; this might be the case.\n\nI didnt change much on default setting, only reduce those delay time\nto half the default setting. Instead, for the last try i increase\nmax-trans-hop and link to 5000. So craw job will not stary if no\nrobots.txt been fetch at the first hand? If so how about those site\nthat didn&#39;t contain a robots.txt file?\n\n&gt; Does the crawl show as &#39;finished&#39; very quickly? If so, your crawl\n&gt; configuration is probably to blame -- the /robots.txt URLs were never\n&gt; tried.\n&gt; \n&gt; If the crawl shows as running for a while, but makes no progress, it&#39;s\n&gt; retrying the /robots.txt multiple times with pauses between, but\nfailing --\n&gt; in which case it&#39;s probably your network. (After it is done with\nretries,\n&gt; the /robots.txt URLs will show in the crawl.log with whatever error\n&gt; caused their last failure.)\n&gt; \n\nThe craw takes some times to finish, about 7-10 minutes. It show\nfinished on the status.\n\nAgain, thanks for help.\n\n\n\n\n"}}