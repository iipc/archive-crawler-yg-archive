{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":326070334,"authorName":"ruyanbo","from":"&quot;ruyanbo&quot; &lt;ruyanbo@...&gt;","profile":"ruyanbo","replyTo":"LIST","senderId":"ZqI8ihuDDHU2ew-JluaAQc1Mwbd_NUhlX7QLr1gC6LfujsvyBWKQtkhtmnJYhNClIJHNVqdQ_KSMDnH_pc0NkZ5XYEE","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: How to add a URL into the retry list?","postDate":"1191952361","msgId":4593,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGZlZ2Y1OSs4Y2NpQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ3MEE4RkU5LjgwMzA5MDJAYXJjaGl2ZS5vcmc+"},"prevInTopic":4589,"nextInTopic":0,"prevInTime":4592,"nextInTime":4594,"topicId":4588,"numMessagesInTopic":3,"msgSnippet":"The methods you suggested works. But it needs the user to look at the crawl log or crawled content and identify URLs that need to be re-crawled. Is it possible","rawEmail":"Return-Path: &lt;ruyanbo@...&gt;\r\nX-Sender: ruyanbo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 15684 invoked from network); 9 Oct 2007 17:52:42 -0000\r\nReceived: from unknown (69.147.108.201)\n  by m47.grp.scd.yahoo.com with QMQP; 9 Oct 2007 17:52:42 -0000\r\nReceived: from unknown (HELO n28b.bullet.sp1.yahoo.com) (209.131.38.247)\n  by mta2.grp.re1.yahoo.com with SMTP; 9 Oct 2007 17:52:42 -0000\r\nReceived: from [216.252.122.218] by n28.bullet.sp1.yahoo.com with NNFMP; 09 Oct 2007 17:52:42 -0000\r\nReceived: from [209.73.164.86] by t3.bullet.sp1.yahoo.com with NNFMP; 09 Oct 2007 17:52:42 -0000\r\nReceived: from [66.218.66.89] by t8.bullet.scd.yahoo.com with NNFMP; 09 Oct 2007 17:52:42 -0000\r\nDate: Tue, 09 Oct 2007 17:52:41 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;fegf59+8cci@...&gt;\r\nIn-Reply-To: &lt;470A8FE9.8030902@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;ruyanbo&quot; &lt;ruyanbo@...&gt;\r\nSubject: Re: How to add a URL into the retry list?\r\nX-Yahoo-Group-Post: member; u=326070334; y=CW7vHZTjXMcSXFPkeoSHoeZkPnZDD_VkSQeJB7hP0Eroxg\r\nX-Yahoo-Profile: ruyanbo\r\n\r\nThe methods you suggested works. \n\nBut it needs the user to look at the cra=\r\nwl log or crawled content and\nidentify URLs that need to be re-crawled. \n\nI=\r\ns it possible to change the fetcher or frontier to force heritrix\nautomatic=\r\nally retry the URLs, whose response status is 302, and save\nthe result of s=\r\necond fetch?\n\nSo the procedure is\n\n1. try URL #1 without cookie and get 302=\r\n redirection to URL #2\n2. put URL #1 into the retry queue. Do not save the =\r\nresult of URL #2\n3. retry URL #1 with the cookie. If the response is 200 OK=\r\n, save the\ncontent of URL #1. If the response is still 302, then save the c=\r\nontent\nof URL #2 (redirection)\n\nHow can I do it?\n\nthanks\n\n\n\n--- In archive-=\r\ncrawler@yahoogroups.com, Igor Ranitovic &lt;igor@...&gt; wrote:\n&gt;\n&gt; Quick and dir=\r\nt solution would be to add a fake seed at the top of the \n&gt; seed list. Mayb=\r\ne something like:\n&gt; \n&gt; http://www.myjones.com/code/signup.php?fakeparam\n&gt; h=\r\nttp://www.myjones.com/code/signup.php\n&gt; http://www.myjones.com/code/\n&gt; \n&gt; h=\r\nttp://www.myjones.com/code/ when cookie is set is probably same as \n&gt; http:=\r\n//www.myjones.com/code/?cook=3Dt when cookie is not set. If that is \n&gt; OK, =\r\nthen you can just leave it as it is and forget about the fake seed.\n&gt; \n&gt; \n&gt;=\r\n Another solution is to prepare cookies file and load it before crawling \n&gt;=\r\n starts. See HTTP Fetcher load-cookies-from-file and\nsave-cookies-to-file \n=\r\n&gt; options.\n&gt; \n&gt; If you want to re-fetch an URI, you can do that via JMX com=\r\nmand line \n&gt; tool. Example (I did not try it):\n&gt; \n&gt; JAVA_BIN=3D/usr/bin/jav=\r\na\n&gt; JMX_JAR=3D/heritrix/bin/cmdline-jmxclient-0.10.5.jar\n&gt; JMX_CONTROLROLE=\r\n=3DcontrolRole:letmein\n&gt; JMX_PORT=3D8849\n&gt; \n&gt; JMX_CMD=3D&quot;$JAVA_BIN -jar $JM=\r\nX_JAR $JMX_CONTROLROLE&quot;\n&gt; \n&gt; crawler=3Dcrawlerhostname\n&gt; \n&gt; mother=3D$($JMX=\r\n_CMD $crawler:$JMX_PORT 2&gt;&1 | grep &#39;mother&#39;)\n&gt; \n&gt; cmd=3D&quot;importUri=3Dhttp:=\r\n//www.myjones.com/code/,true,true&quot;\n&gt; \n&gt; $JMX_CMD $crawler:$JMX_PORT $mother=\r\n $cmd\n&gt; \n&gt; -------------------\n&gt; Comments from the crawl service mbean:\n&gt; i=\r\nmportUri: Add passed URL to the frontier\n&gt;   Parameters 3, return type=3Dja=\r\nva.lang.Void\n&gt;    name=3Durl type=3Djava.lang.String URL to add to the fron=\r\ntier\n&gt;    name=3DforceFetch type=3Djava.lang.Boolean True if URL is to be f=\r\norce\nfetched\n&gt;    name=3Dseed type=3Djava.lang.Boolean True if URL is a see=\r\nd\n&gt; --------------------\n&gt; \n&gt; \n&gt; Hope this helps.\n&gt; \n&gt; Take care,\n&gt; i.\n&gt; \n&gt;=\r\n \n&gt; \n&gt; \n&gt; \n&gt; ruyanbo wrote:\n&gt; &gt; I have a problem to use cookie and retry a =\r\nURL. \n&gt; &gt; \n&gt; &gt; 1. If I use http://www.myjones.com/code/signup.php as the se=\r\ned and set\n&gt; &gt; the crawler to only the seed. This URL will get an HTTP resp=\r\nonse\n&gt; &gt; status 302 and be redirected to\n&gt; &gt; http://www.myjones.com/code/si=\r\ngnup.php?cook=3Dt, because there is no\n&gt; &gt; cookie setted.\n&gt; &gt; \n&gt; &gt; 2. Same =\r\nthing happens for http://www.myjones.com/code/. It will get\n&gt; &gt; 302 and be =\r\nredirected to http://www.myjones.com/code/?cook=3Dt\n&gt; &gt; \n&gt; &gt; 3. If I use bo=\r\nth http://www.myjones.com/code/ and\n&gt; &gt; http://www.myjones.com/code/signup.=\r\nphp as the seeds and set the\n&gt; &gt; crawler to save and load cookie from file,=\r\n\n&gt; &gt; http://www.myjones.com/code/ still get 302 and be redirected to\n&gt; &gt; ht=\r\ntp://www.myjones.com/code/?cook=3Dt, but\n&gt; &gt; http://www.myjones.com/code/si=\r\ngnup.php will be crawled successfully\n&gt; &gt; (200 OK). \n&gt; &gt; \n&gt; &gt; Can anybody t=\r\nell me how to retry http://www.myjones.com/code/ ?\n&gt; &gt; \n&gt; &gt; Said in another=\r\n way. Now the situation is\n&gt; &gt; \n&gt; &gt; URL #1 and #2 are in the seeds list.\n&gt; =\r\n&gt; \n&gt; &gt; URL #1 (with no cookie) got 302 and be redirected to URL #3 (set\ncoo=\r\nkie)\n&gt; &gt; \n&gt; &gt; URL #2 (load cookie) got 200 OK\n&gt; &gt; \n&gt; &gt; Can I retry URL #1 w=\r\nith the cookie, so it can be correctly crawled?\n&gt; &gt; \n&gt; &gt; thanks\n&gt; &gt; \n&gt; &gt; \n&gt;=\r\n &gt; \n&gt; &gt;  \n&gt; &gt; Yahoo! Groups Links\n&gt; &gt; \n&gt; &gt; \n&gt; &gt;\n&gt;\n\n\n\n"}}