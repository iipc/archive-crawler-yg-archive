{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"HpeekU4_H1fs1Z89PaA3-_9Lq_Sz4BvWyCZORF2kDWh4BkSolRSPlzgYKARh1CFwTRePPZuSzCX6sd3e-39sGbmbxVOkAtPW","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Concern about crawl running from uncompressed recover.gz","postDate":"1196089812","msgId":4746,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3NEFFMUQ0LjcwOTAyMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDg5MmY2YTM0MDcxMTIyMDUyMng2MzJhNGZiYnEyMTliYTY5ZWFlOTZkM2M5QG1haWwuZ21haWwuY29tPg==","referencesHeader":"PGZpMmdmcStpaTFpQGVHcm91cHMuY29tPiA8NDc0NTREMDkuNDA0MDIwNEBhcmNoaXZlLm9yZz4gPDg5MmY2YTM0MDcxMTIyMDUyMng2MzJhNGZiYnEyMTliYTY5ZWFlOTZkM2M5QG1haWwuZ21haWwuY29tPg=="},"prevInTopic":4740,"nextInTopic":0,"prevInTime":4745,"nextInTime":4747,"topicId":4736,"numMessagesInTopic":4,"msgSnippet":"Hi Mike, Yes, you can remove the seeds via the Web UI (WUI). If the seeds list is too large to be shown via the WUI, then you can just replace the seeds file","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 62118 invoked from network); 26 Nov 2007 15:10:32 -0000\r\nX-Received: from unknown (66.218.67.95)\n  by m42.grp.scd.yahoo.com with QMQP; 26 Nov 2007 15:10:32 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta16.grp.scd.yahoo.com with SMTP; 26 Nov 2007 15:10:32 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 22C374D\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 26 Nov 2007 07:13:51 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id g3-Ds1Ndc-KH for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 26 Nov 2007 07:13:48 -0800 (PST)\r\nX-Received: from [127.0.0.1] (nor75-24-88-170-99-175.fbx.proxad.net [88.170.99.175])\n\tby mail.archive.org (Postfix) with ESMTP id 2ADA670\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 26 Nov 2007 07:13:47 -0800 (PST)\r\nMessage-ID: &lt;474AE1D4.7090202@...&gt;\r\nDate: Mon, 26 Nov 2007 07:10:12 -0800\r\nUser-Agent: Thunderbird 2.0.0.9 (Windows/20071031)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;fi2gfq+ii1i@...&gt; &lt;47454D09.4040204@...&gt; &lt;892f6a340711220522x632a4fbbq219ba69eae96d3c9@...&gt;\r\nIn-Reply-To: &lt;892f6a340711220522x632a4fbbq219ba69eae96d3c9@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Concern about crawl running from uncompressed\n recover.gz\r\nX-Yahoo-Group-Post: member; u=137477665; y=wTSWrNmk5W-Q5deQodHCsslB0nql-pNzQrFdJY8vei4vjfuOBw\r\nX-Yahoo-Profile: iranitovic\r\n\r\nHi Mike,\n\nYes, you can remove the seeds via the Web UI (WUI). If the seeds list is \ntoo large to be shown via the WUI, then you can just replace the seeds \nfile with an empty one using the file system.\n\nJust FYI, see recover-retain-failures option. If this option is set to \n&#39;true&#39; than failed URIs would be retired after the recovery.\n\nHope this helps.\n\nTake care,\ni.\n\n&gt; I am using the same seed list, but in thinking about this further many\n&gt; of the seeds failed to be resolved. Looked like a temporary dns lookup\n&gt; issue. All the seeds in the recover crawl were &#39; HTTP-200-Success-OK&#39;\n&gt; in the seeds report. This alone would result in many URIs discovered\n&gt; as the crawl progresses that would not have been located before.\n&gt; \n&gt; If I were to start the crawl off exactly as before are you saying that\n&gt; I would create the job from initial crawl job, but remove all entries\n&gt; from the seeds list in the &#39;Settings&#39; view? Not that I would do that\n&gt; now as I want all the seeds to be resolved, but just trying to\n&gt; understand if that is what you meant by &#39;an empty seed list&#39;.\n&gt; \n&gt; Thanks,\n&gt; Mike\n&gt; \n&gt; On Nov 22, 2007 4:34 AM, Igor Ranitovic &lt;igor@...&gt; wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; Hi Mike,\n&gt;&gt;\n&gt;&gt;  How big is the seed list? I believe, but I am not sure, that during the\n&gt;&gt;  recovery the seeds would be re-scheduled if you use the original seeds\n&gt;&gt;  list. It would be easy to check by exploring the current recover log.\n&gt;&gt;  Another reason for the larger queued list can be due the newly scheduled\n&gt;&gt;  prerequisites like dns and robots.txt URIs. However, I would think that\n&gt;&gt;  would not cause the queued list to triple in size.\n&gt;&gt;\n&gt;&gt;  In general, when recovering the crawl from the recover log, I have been\n&gt;&gt;  using the empty seeds list. When the seeds list is used for scoping, I\n&gt;&gt;  would create a separate scope file to mimic wanted scope.\n&gt;&gt;\n&gt;&gt;  I hope this helps.\n&gt;&gt;  i.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;  &gt; Here are the stats taken from the last entries in the\n&gt;&gt;  &gt; progress-statistics.log\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; When the original crawl was terminated at day 19 into the crawl:\n&gt;&gt;  &gt; discovered queued downloaded\n&gt;&gt;  &gt; 4465380 372767 2900208\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Now that the &#39;recoverd&#39; crawl has been running for only 4 hours:\n&gt;&gt;  &gt; disovered queued downloaded\n&gt;&gt;  &gt; 4469048 1462892 3935\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Estimated Time for the &#39;recovered&#39; crawl to complete as displayed on\n&gt;&gt;  &gt; the UI:\n&gt;&gt;  &gt; Time\n&gt;&gt;  &gt; 4h21m43s elapsed\n&gt;&gt;  &gt; 67d5h53m16s remaining (estimated)\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; I am a little concerned that it may be recrawling URL already crawled\n&gt;&gt;  &gt; by the initial crawl. It seems odd that the queued number (1462892) is\n&gt;&gt;  &gt; so high when after 19 days in the initial crawl the queued number was\n&gt;&gt;  &gt; only 372767.\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Thanks for any help,\n&gt;&gt;  &gt; Mike\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt; Yahoo! Groups Links\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;  &gt;\n&gt;&gt;\n&gt;&gt;  \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}