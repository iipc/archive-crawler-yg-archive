{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"i9Q2x60ibGNApS14aYDGDcq-p8aBT1eJD9G6tTfuNz93UwVc-bh5rdrf6cHhe1Bvb9vVPfy02YnAf6VYGDpDfLxhTNMVPs0","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Recrawl?","postDate":"1351030635","msgId":7831,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwODcxNzZCLjEwMjA4MDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MEZFRDZBQ0FAdzJrMy1ic3BleDE+","referencesHeader":"PDc0Qzk3RTdERjVBNzc4NEQ5OTcyMTdGRjc1RDEyMTY2MEZFRDZBQ0FAdzJrMy1ic3BleDE+"},"prevInTopic":7828,"nextInTopic":0,"prevInTime":7830,"nextInTime":7832,"topicId":7828,"numMessagesInTopic":2,"msgSnippet":"... There s no current way to essentially grant an in-progress job permission to revisit an entire host/range-of-URIs. As you note, you d have to tell it","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 39081 invoked from network); 23 Oct 2012 22:17:18 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m10.grp.sp2.yahoo.com with QMQP; 23 Oct 2012 22:17:18 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta3.grp.sp2.yahoo.com with SMTP; 23 Oct 2012 22:17:17 -0000\r\nX-Received: (qmail 5422 invoked by uid 0); 23 Oct 2012 22:17:16 -0000\r\nX-Received: from 70.36.143.78 (HELO silverbook.local) (70.36.143.78)\n  by relay00.pair.com with SMTP; 23 Oct 2012 22:17:16 -0000\r\nX-pair-Authenticated: 70.36.143.78\r\nMessage-ID: &lt;5087176B.1020800@...&gt;\r\nDate: Tue, 23 Oct 2012 15:17:15 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:16.0) Gecko/20121010 Thunderbird/16.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;Coram, Roger&quot; &lt;Roger.Coram@...&gt;\r\nReferences: &lt;74C97E7DF5A7784D997217FF75D121660FED6ACA@w2k3-bspex1&gt;\r\nIn-Reply-To: &lt;74C97E7DF5A7784D997217FF75D121660FED6ACA@w2k3-bspex1&gt;\r\nContent-Type: text/plain; charset=UTF-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Recrawl?\r\nX-Yahoo-Group-Post: member; u=137285340; y=0GLXPhH98TaoS_b0MPVGUNIPxlnZ1UD8xy578ai06KJu\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 10/23/12 6:13 AM, Coram, Roger wrote:\n&gt; Is it possible to recrawl a host within a single job (without a relaunch)?\n&gt;\n&gt; I know you can use a “.force” file to recrawl a single URL but is there\n&gt; a way of saying, for example, all URLs matching “http://(uk,bl,” should\n&gt; be re-downloaded if seen again in X days?\n&gt;\n&gt; I guess this is similar to the “Continuous Crawling” discussions that\n&gt; have occurred but I’m not sure whether any of this was ever implemented?\n\nThere&#39;s no current way to essentially grant an in-progress job \npermission to revisit an entire host/range-of-URIs. As you note, you&#39;d \nhave to tell it specific URIs to revisit, using the &#39;force&#39; option which \nskips uniqueness-testing. (You could do this as a batch operation from \nthe crawl.log records of previous URIs visited.)\n\nAlso, the example ReschedulingProcessor show a way to use the H3 \nfrontier&#39;s new &#39;rescheduleTime&#39; feature to automate this: it allows a \nURI to be automatically re-enqueued at some exact interval of time after \nits prior fetch completes. (It might still have to wait its turn.)\n\nStill, that&#39;s not based on &quot;if seen again&quot; -- it just happens after the \ninterval... even if the URI is no longer linked from elsewhere. \n(ReschedulingProcessor could be further refined to only keep doing it \nfor non-error responses; to make the interval sensitive to media-types, \nsizes, or apparent changes; etc. -- but is for now just a crude demo of \nthe most simple option.)\n\nAlternatively, it wouldn&#39;t be too hard to make an alternative version of \nthe BdbUriUniqFilter that allowed forgetting of specific URIs or \nhosts/ranges of URIs. This could be in occasional bulk/batch operations \n(eg: &quot;at the one 1 week point, forget everything we&#39;ve seen at site A, \nallowing its URIs to be queued again if rediscovered&quot;) or perhaps even \nwith per-URI expirations (eg: &quot;avoid reenqueuing this URI for the next \nweek, but if it&#39;s discovered after that, allow it to be reenqueued&quot;).\n\nThat would then allow other policies like: after a host appears to \n&#39;finish&#39; (queue reaches empty and stays empty for some interval), and/or \nsome other interval has passed, restore the eligibility of all URIs on \nthat host (ie: forget they were visited), and reenqueue the root page, \ncausing a repeat crawl (skipping those that aren&#39;t still reachable).\n\n(You couldn&#39;t easily base this hypothetical new capability on the \nBloomUriUniqFilter, because its underlying structure doesn&#39;t support \nremovals.)\n\nSo if this is an occasional need, you may just want to automate the \n&#39;force&#39; workaround or tinker with the ReschedulingProcessor... but if a \ndesired common mode of operation, look into enhancing the BdbUriUniqFilter.\n\nHope this helps,\n\n- Gordon\n\n\n\n&gt; Thanks,\n&gt;\n&gt; Roger\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}