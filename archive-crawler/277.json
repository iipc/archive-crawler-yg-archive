{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163894630,"authorName":"steensc42","from":"&quot;steensc42&quot; &lt;steensc42@...&gt;","profile":"steensc42","replyTo":"LIST","senderId":"sS4zyhf5MbJ9RjZT8oYuZXw17SS2rR28sVDJt_9e7IobUQgTpVhng4RCQpOgW9qlOuwCTIm9i54GqEEckgsY-HdW9XUzTWs","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Extending ARC Re: [archive-crawler] Re: Crawler-guided form/authentication entry","postDate":"1075457284","msgId":277,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGJ2ZGFlNCtsMHNyQGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQwMTVDQUFDLjkwMzA4MDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":271,"nextInTopic":281,"prevInTime":276,"nextInTime":278,"topicId":235,"numMessagesInTopic":8,"msgSnippet":"... problems. ... that ... all ... identifier ... problem ... and ... Hmm - I really liked the simplicity of the existing index. ... away ... used, ... naming","rawEmail":"Return-Path: &lt;steensc42@...&gt;\r\nX-Sender: steensc42@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 93881 invoked from network); 30 Jan 2004 10:10:20 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m18.grp.scd.yahoo.com with QMQP; 30 Jan 2004 10:10:20 -0000\r\nReceived: from unknown (HELO n1.grp.scd.yahoo.com) (66.218.66.64)\n  by mta1.grp.scd.yahoo.com with SMTP; 30 Jan 2004 10:10:20 -0000\r\nReceived: from [66.218.67.135] by n1.grp.scd.yahoo.com with NNFMP; 30 Jan 2004 10:08:06 -0000\r\nDate: Fri, 30 Jan 2004 10:08:04 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;bvdae4+l0sr@...&gt;\r\nIn-Reply-To: &lt;4015CAAC.9030808@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 8940\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-eGroups-Remote-IP: 66.218.66.64\r\nFrom: &quot;steensc42&quot; &lt;steensc42@...&gt;\r\nSubject: Extending ARC Re: [archive-crawler] Re: Crawler-guided form/authentication entry\r\nX-Yahoo-Group-Post: member; u=163894630\r\nX-Yahoo-Profile: steensc42\r\n\r\n--- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@a...&gt; \nwrote:\n&gt; steensc42 wrote:\n&gt; &gt; I agree that the safe approach is to include everything into the \n&gt; &gt; checksum.\n&gt; &gt; From a retrieval point of view this might however pose some \nproblems.\n&gt; &gt; How do we locate the correct record in the archive to return as \n&gt; &gt; response to a post request?\n&gt; &gt; It would be nice if we could convert the post request into a key \nthat \n&gt; &gt; we can locate \n&gt; &gt; in an index of archived url-objects. If this key is dependent on \nall \n&gt; &gt; request headers\n&gt; &gt; it will only work for specific browsers, referrer combinations.\n&gt; &gt; I guess the answer here is to accept that the &lt;capture-\nidentifier&gt; \n&gt; &gt; can not be\n&gt; &gt; used directly for indexing purposes - and leave the retrieval \nproblem \n&gt; &gt; to the\n&gt; &gt; implementation of the indexing mechanism.\n&gt; \n&gt; A possible index/access mechanism when considering form submissions\n&gt; would be to perform a fuzzy match against the archived data fields.\n&gt; \n&gt; Given a particular request -- URI, query-string, post-data, accept\n&gt; header, user-agent, etc. -- break each of those fields into tokens \nand\n&gt; look for the closest matches in the archive, in a full-text search\n&gt; style.\n&gt; \n&gt; The top ranked hit would be shown -- with the necessary UI\n&gt; disclaimers that the access tool cannot perfectly simulate\n&gt; the original form, but only provide captures from similar\n&gt; submissions.\n\nHmm - I really liked the simplicity of the existing index.\n\n&gt; \n&gt; &gt; I would really like the option to:\n&gt; &gt; \t- place the metadata record at an arbitrary position relative \n&gt; &gt; to the other capture records.\n&gt; &gt; \t- create more than one metadata record per capture.\n&gt; &gt; \t\n&gt; &gt; An example of a usage scenario with metadata records positioned \naway \n&gt; &gt; from the other capture records:\n&gt; &gt; \tAfter creation of an ARC file, a (time consuming) \n&gt; &gt; postprocessing of some \n&gt; &gt; \tof the collected data is performed.\n&gt; &gt; \tThe information extracted by this process is written as \n&gt; &gt; metadata records appended to \n&gt; &gt; \tthe original ARC file.\n&gt; \n&gt; I understand this goal, and I don&#39;t think there needs to be a\n&gt; strict requirement that the request, response, and metadata records\n&gt; are contiguous.\n&gt; \n&gt; However, your postprocessing scenario is more like what we at the\n&gt; Archive have usually kept in separate, but similar, &#39;DAT&#39; files.\n&gt; \n&gt; Even if the exact same &#39;metadata:&#39; labelling and record format are \nused,\n&gt; it could be beneficial to always keep postprocessing files distinct\n&gt; from original crawl files, perhaps via a different extension or \nnaming\n&gt; scheme. (&quot;.marc&quot;?)\n\nOur basic strategy is to store all preservation-deserving information \nin arc files.\n\nAs I understand DAT files, they only contain information that can be \nderived from\ndata already present in the original ARC file ? \nWe do however expect that some of our postprocessing will add new \ninformation\nnot already present in the original arc file. \n\n&gt; \n&gt; &gt; We would like to use this format to record frequent (daily) \ncaptures \n&gt; &gt; of selected sites.\n&gt; &gt; In this scenario we would need to be able to refer to duplicates \n&gt; &gt; across crawls.\n&gt; &gt; A procedure that ensures that master records are rewritten \n&gt; &gt; occasionally would of\n&gt; &gt; course need to be implemented.\n&gt; \n&gt; Yes, the ability to refer back to any previous (and reliably stored)\n&gt; capture would be ideal.\n&gt; \n&gt; Perhaps even: to support diff-encoding of the most recent version,\n&gt; when it is only a small change from the previous version.\n\nYep - diff encoding might be a real nice feature in this scenario.\nLars Christensen at statsbiblioteket (www.statsbiblioteket.dk) \nhas done some initial analysis of the potential benefit of diff-\nencoding.\nHe looked at 5 different sites over a period of 38 days.\n4 of the 5 sites would only benefit marginally from a diff encoding,\nthe objects typically did not change at all.\nOne site had a timestamp on approximately 50% of the pages as the \nonly changes between captures. This site would benefit from a diff \nencoding - another alternative would be just to accept the page as a \nduplicate\nin the archive, and just reference a version with an older timestamp.\n\n\n&gt; \n&gt; &gt; We have another usage scenario we would like this format to \nsupport.\n&gt; &gt; Some of the data formats in the archive may become \n&gt; &gt; obsolete/unsupported over time. \n&gt; &gt; One approach for handling this problem is storage of applications \n&gt; &gt; capable of handling/converting\n&gt; &gt; these formats. Another approach we want to support is \ntransformation \n&gt; &gt; of (important) data at risk\n&gt; &gt; to formats still supported. An example might be transformation \nfrom \n&gt; &gt; one graphic file format (say gif),\n&gt; &gt; to another format (say tif). This might be achieved \nusing &quot;transform&quot; \n&gt; &gt; to mark a transformed version:\n&gt; &gt; \n&gt; &gt; transform:http://&lt;url&gt; &lt;dest-ip&gt; &lt;timestamp&gt; &lt;mime&gt; &lt;response-\nsize&gt; \n&gt; &gt; &lt;capture-identifier&gt;\n&gt; &gt; &lt;DATA - transformed version of original data record&gt;\n&gt; &gt; metadata:transform:http://&lt;url&gt; &lt;dest-ip&gt; &lt;timestamp&gt; &lt;mime&gt; \n&gt; &gt; &lt;metadata-size&gt; &lt;capture-identifier&gt;\n&gt; &gt; &lt;DATA - metadata describing details about the transformation \n(from \n&gt; &gt; format, to format, application, etc)&gt;\n&gt; &gt; \n&gt; &gt; Each time a url-object is transformed a new pair of (transform, \n&gt; &gt; metadata:transform) records\n&gt; &gt; are created, allowing sequences of format transformations.\n&gt; &gt; \n&gt; &gt; Comments?\n&gt; \n&gt; Makes sense to me. I think we would generally prefer to archive\n&gt; transformation routines that could always be applied on-the-fly, but\n&gt; batch converts into new archived fromats may also be required.\n&gt; \n&gt; Here again, even if the internal format is identical, I might make\n&gt; such tranformed content distinguishable from original crawler\n&gt; output by a naming convention. (&quot;.tarc&quot;?)\n&gt; \n\n&gt; A new issue: how could we expand the beginning of the ARC format\n&gt; to better identify the context (software, project, settings) in\n&gt; which the following captures were collected?\n&gt; \n&gt; I&#39;ve been thinking that either (1) the first &#39;filedesc:&#39; record \ncould\n&gt; be expanded to include both a free-form operator comment and\n&gt; some number of standard fields; or (2) multiple pseudo-records,\n&gt; like the first, could describe the ARC or crawl as a whole.\n&gt; \n&gt; In particular, I would be willing to put a few K of crawl\n&gt; descriptive info into every ARC, or perhaps full crawl\n&gt; software and config info into the first ARC and pointers\n&gt; back into all subsequent ARCs.\n\nWe agree that we need a way to store capture information.\nWe have until now considered this information part of the\nmetadata scheme. This gives a nice separation between capture \ninformation\nand the physical placement of data records.\n\nOur current thoughts on this issue are as follows.\n\nCapture information is stored as metadata about an url-object. \nA naming scheme is needed for the url, we would probably use \nsomething like\nwww.netarkivet.dk/captures/date-2004-12-12/count-0/info.html\n\n(www.netarkivet.dk is the home page of the Danish internet \npreservation initiative)\n\n\nThe metadata record for this url-object must contain general capture \ninformation,\nin a format that does not depend on a specific crawler.\n\nAn example:\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;netArkiv:harvestSetup&gt;\n&lt;netArkiv:contentDesc&gt;&lt;/&gt; \n&lt;netArkiv:seedlist&gt;\n           &lt;netArkiv:globalFormat type=&quot;exclude&quot;&gt; \n\t&lt;netArkiv:mimetype&gt;&lt;/&gt;\n\t&lt;netArkiv:mimetype&gt;&lt;/&gt;\n           &lt;/netArkiv:globalFormat&gt;\n           &lt;netArkiv:globalRobot&gt;&lt;/&gt; \n           &lt;netArkiv:globallinkDepth&gt;&lt;/&gt; \n           &lt;netArkiv:globalExternellinkdata&gt;yes&lt;/&gt; \n           &lt;netArkiv:item&gt;\n\t&lt;netArkiv:itemType&gt;&lt;/&gt; \n                      &lt;netArkiv:URL&gt;&lt;/&gt;\n                      &lt;netArkiv:itemOwner&gt;(company)name, SE-nr/cprnr, \nadresse&lt;/&gt;                      \n\t  &lt;netArkiv:login cryptpassword=&quot;XdfgkJJ&quot;&gt;&lt;/&gt; \n\t  &lt;netArkiv:format type=&quot;include&quot;&gt; \n\t                &lt;netArkiv:mimetype&gt;&lt;/&gt;\n\t                &lt;netArkiv:mimetype&gt;&lt;/&gt;\n           \t   &lt;/netArkiv:format&gt;\n   &lt;netArkiv:robot&gt;&lt;/&gt; \n   &lt;netArkiv:linkDepth&gt;&lt;/&gt;    \n   &lt;netArkiv:contractRef&gt;&lt;/&gt; \n            &lt;/netArkiv:item&gt;  \n&lt;/netArkiv:seedlist&gt;\n&lt;netArkiv:harvestInstitution&gt;&lt;/&gt; \n&lt;netArkiv:harvestEmail Name=&quot;&quot; digitalSignature=&quot;&quot;&gt;&lt;/&gt; \n&lt;netArkiv:harvestSource&gt;&lt;/&gt; \n&lt;/netArkiv:harvestSetup&gt;\n\n\n&lt;netArkiv:harvesterRundata&gt;\n\t&lt;netArkiv:startTimestamp&gt; \n\t&lt;netArkiv:endTimestamp&gt; \n\t&lt;netArkiv:harvestStatus&gt; \n\t&lt;netArkiv:numberofFiles&gt; \n\t&lt;netArkiv:numberofMB&gt; \n&lt;/netArkiv:harvestRundata&gt;\n\n\nNotice that apart from this metadata record we also harvest the page \ninfo.html.\nIf we want to include harvester specific information, it would be \nnatural to add links \nfrom info.html to \n\tharverster setup (order.xml for heritrix)\n\tharverster code \n\tetc. \nand ensure these links are included in the harvest.\n\n\nThe metadata associated with each individual url-object may now use \nthe capture-url (id) to identify\nthe capture it belongs to:\n\n&lt;netArkiv:object&gt;\n\t&lt;netArkiv:harvestRef harvesttimeRef=200412120101 \nurl=www.netarkivet.dk/captures/date-2004-12-12/count-0/info.html &gt;&lt;&#92;&gt; \n&lt;netArkiv:object&gt;\n\n\n\n&gt; \n&gt; - Gordon\n\n\n"}}