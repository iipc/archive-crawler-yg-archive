{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"2rPs-sB_xGeGDlAGeSaEZkhbgeUFs89A5Ihs4t2GGsmeT2FQ1yTEy9OmGkMOtt4tRKfM5zyPMMs0Wb7fXFCToSxsTpfVTZKP","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] General statistics","postDate":"1161121377","msgId":3462,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1MzU0RTYxLjUwNTAwMDlAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ1MzQ5RjdFLjgwNDAyMDZAY2VzY2EuZXM+","referencesHeader":"PDY4QzIyMTg1REI5MENBNDFBNUFDQkQ4RTgzNEM1RUNEMDM0Njc0RDRAZ29vZnkud3Bha2Iua2Iubmw+IDw0NTM0OUY3RS44MDQwMjA2QGNlc2NhLmVzPg=="},"prevInTopic":3456,"nextInTopic":0,"prevInTime":3461,"nextInTime":3463,"topicId":3447,"numMessagesInTopic":8,"msgSnippet":"Take a look at make_reports.pl. It is a perl script that creates some basic reports from a single crawl.log. If you are comfortable with perl it would be easy","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 26514 invoked from network); 17 Oct 2006 21:44:23 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m39.grp.scd.yahoo.com with QMQP; 17 Oct 2006 21:44:23 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta2.grp.scd.yahoo.com with SMTP; 17 Oct 2006 21:44:23 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 412CD14156E89\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 17 Oct 2006 14:43:01 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 06565-02-19 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tTue, 17 Oct 2006 14:43:00 -0700 (PDT)\r\nReceived: from [127.0.0.1] (c-71-198-60-165.hsd1.ca.comcast.net [71.198.60.165])\n\tby mail.archive.org (Postfix) with ESMTP id E20631415FF32\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 17 Oct 2006 14:43:00 -0700 (PDT)\r\nMessage-ID: &lt;45354E61.5050009@...&gt;\r\nDate: Tue, 17 Oct 2006 14:42:57 -0700\r\nUser-Agent: Thunderbird 1.5.0.7 (Windows/20060909)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;68C22185DB90CA41A5ACBD8E834C5ECD034674D4@...&gt; &lt;45349F7E.8040206@...&gt;\r\nIn-Reply-To: &lt;45349F7E.8040206@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] General statistics\r\nX-Yahoo-Group-Post: member; u=137477665; y=5YL8N1VStHMHOtFbdtLA2INz5jmUQSmsERchWIStax71FA7kWw\r\nX-Yahoo-Profile: iranitovic\r\n\r\nTake a look at make_reports.pl. It is a perl script that creates some \nbasic reports from a single crawl.log. If you are comfortable with perl \nit would be easy to change it to read more than one crawl.log and create \ncombined reports.\n\nTake care,\ni.\n\n\nNatalia Torres wrote:\n&gt; Thanks Barts,\n&gt; \n&gt; I know about the information about each jop separatly, but I want to \n&gt; work with all this files at the same time.\n&gt; I want to make full reports and statics including the information of all \n&gt; crawled jobs as one.\n&gt; \n&gt; I need data like:\n&gt; \n&gt; Total Jobs Crawled: 25\n&gt; Total Seeds Crawled: 150\n&gt; Total Seeds Crawled from Job1: 150\n&gt; Total Seeds Crawled from Job2: 150\n&gt; ....\n&gt; Total Hosts Crawled: 18\n&gt; Total Hosts Crawled from Job1: 18\n&gt; Total Hosts Crawled from Job12: 18\n&gt; ...\n&gt; Total Documents Crawled: 889\n&gt; Total Documents Crawled from Job1: 8\n&gt; Total Documents Crawled from Job2: 58\n&gt; ....\n&gt; \n&gt; Total Raw Data Size in Bytes: 34829016 (33 MB)\n&gt; Total Raw Data Size in Bytes from Job1: 3 (xx MB)\n&gt; Total Raw Data Size in Bytes from Job2: 16 (xx MB)\n&gt; .....\n&gt; \n&gt; and show some graphs\n&gt; \n&gt; Natalia\n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt; \n\n\n"}}