{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":58465472,"authorName":"Michael Giles","from":"&quot;Michael Giles&quot; &lt;mgiles@...&gt;","profile":"michael_a_giles","replyTo":"LIST","senderId":"rwjxoz-g7bVo-HBLnHG5vDhyNRQaEYG-Mtilm9SsC2FOxj2ROH_ExU1t22Zie5AKi4gx0CJ5BBCCmuNjHFMzNYclpzbWfz6Nvs23_a2Lw64","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Running multiple directed crawls...","postDate":"1213200132","msgId":5304,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PGcyb3N1NCs0ZTdiQGVHcm91cHMuY29tPg=="},"prevInTopic":0,"nextInTopic":0,"prevInTime":5303,"nextInTime":5305,"topicId":5304,"numMessagesInTopic":1,"msgSnippet":"I have a couple of questions.  Here s the scenario: I am running directed crawls (i.e. only looking for certain pages on a given site) across a small number of","rawEmail":"Return-Path: &lt;mgiles@...&gt;\r\nReceived: (qmail 48667 invoked from network); 11 Jun 2008 16:03:52 -0000\r\nReceived: from unknown (66.218.67.94)\n  by m57.grp.scd.yahoo.com with QMQP; 11 Jun 2008 16:03:52 -0000\r\nReceived: from unknown (HELO n20c.bullet.scd.yahoo.com) (66.218.67.23)\n  by mta15.grp.scd.yahoo.com with SMTP; 11 Jun 2008 16:03:52 -0000\r\nReceived: from [209.73.164.86] by n20.bullet.scd.yahoo.com with NNFMP; 11 Jun 2008 16:03:52 -0000\r\nReceived: from [66.218.66.81] by t8.bullet.scd.yahoo.com with NNFMP; 11 Jun 2008 16:03:52 -0000\r\nX-Sender: mgiles@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 43266 invoked from network); 11 Jun 2008 16:02:15 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m35.grp.scd.yahoo.com with QMQP; 11 Jun 2008 16:02:15 -0000\r\nX-Received: from unknown (HELO n17.bullet.mail.re1.yahoo.com) (69.147.102.100)\n  by mta17.grp.scd.yahoo.com with SMTP; 11 Jun 2008 16:02:15 -0000\r\nX-Received: from [68.142.237.89] by n17.bullet.mail.re1.yahoo.com with NNFMP; 11 Jun 2008 16:02:14 -0000\r\nX-Received: from [66.218.69.4] by t5.bullet.re3.yahoo.com with NNFMP; 11 Jun 2008 16:02:14 -0000\r\nX-Received: from [66.218.66.79] by t4.bullet.scd.yahoo.com with NNFMP; 11 Jun 2008 16:02:14 -0000\r\nDate: Wed, 11 Jun 2008 16:02:12 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;g2osu4+4e7b@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Michael Giles&quot; &lt;mgiles@...&gt;\r\nSubject: Running multiple directed crawls...\r\nX-Yahoo-Group-Post: member; u=58465472; y=9QxOB77BbrXRbAuXuQPRzdDYsAIqQVuragMAtWH00Fy11D_Q2KCPoags\r\nX-Yahoo-Profile: michael_a_giles\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 11 Jun 2008 16:03:51 -0000\r\n\r\nI have a couple of questions.  Here&#39;s the scenario:\n\nI am running directed =\r\ncrawls (i.e. only looking for certain pages on a\ngiven site) across a small=\r\n number of sites (e.g. 100&#39;s).  Each site\nhas it&#39;s own regex rules to filte=\r\nr the urls to crawl and the urls to\nsave to disk.  Currently, each site has=\r\n an order.xml and seeds.txt file.\n\nI&#39;d like to farm these jobs out to a clu=\r\nster of heritrix servers so\nthat they could all execute in parallel.  What&#39;=\r\ns the best way to do\nthis?  I was picturing putting jobs on a distributed q=\r\nueue and having\nworkers pick them up, but it seems like HCC is the recommen=\r\nded way to\ndo this instead.  If that is true, is there any more documentati=\r\non on\nHCC than on the site?  For instance, I can&#39;t figure out how to take a=\r\nn\norder.xml and seeds.txt file and turn them into an OrderJar.  Any help\nth=\r\nere?  Or am I off-base using an order.xml for each site?  Is there a\nbetter=\r\n way to handle site-specific regex rules?  Would be nice if\nthere was...\n\nM=\r\ny second question is around re-crawling the same sites.  I&#39;d like to\nrun re=\r\ngular crawls for new/modified content on these sites.  Is there\na way to do=\r\n that without incurring a full crawl (i.e. based on some\nstate from the pre=\r\nvious crawl)?\n\nThanks!\n\n-Mike\n\n\n"}}