{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"he4SadKYdxk5Sl5AZrPgF5ANt2dJ8eJH5QdEsjLe9hdgMAy_MZ_rqujJLTYPhSMFAN-4D3ca7c1IvB_hN2cXby9uWCdqFOk","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix WriterProcessor","postDate":"1298416231","msgId":7037,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENjQ0MjY3LjQwNzA4MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDlDQzhENkU0ODA0RTIzNEE4MjdEMTc4MkFFM0RCNzc4MkRFRTFEQjMyM0BFWC1NQUlMLUhZRDEtMS5hbnQuYW1hem9uLmNvbT4=","referencesHeader":"PDlDQzhENkU0ODA0RTIzNEE4MjdEMTc4MkFFM0RCNzc4MkRFREE1Q0ZBM0BFWC1NQUlMLUhZRDEtMS5hbnQuYW1hem9uLmNvbT4gPDRENjM1RkRBLjQwMzA5MDdAYXJjaGl2ZS5vcmc+IDw5Q0M4RDZFNDgwNEUyMzRBODI3RDE3ODJBRTNEQjc3ODJERUUxREIzMjNARVgtTUFJTC1IWUQxLTEuYW50LmFtYXpvbi5jb20+"},"prevInTopic":7033,"nextInTopic":7042,"prevInTime":7036,"nextInTime":7038,"topicId":7016,"numMessagesInTopic":8,"msgSnippet":"... Then you are actually fetching the URL twice! Once in our FetchHTTP module (which does not use the java.net.URL class, because it does not offer enough","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 88536 invoked from network); 22 Feb 2011 23:10:36 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m12.grp.re1.yahoo.com with QMQP; 22 Feb 2011 23:10:36 -0000\r\nX-Received: from unknown (HELO relay01.pair.com) (209.68.5.15)\n  by mta2.grp.re1.yahoo.com with SMTP; 22 Feb 2011 23:10:36 -0000\r\nX-Received: (qmail 26395 invoked by uid 0); 22 Feb 2011 23:10:32 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay01.pair.com with SMTP; 22 Feb 2011 23:10:32 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4D644267.4070806@...&gt;\r\nDate: Tue, 22 Feb 2011 15:10:31 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;Dhake, Pankaj&quot; &lt;pdhake@...&gt;\r\nReferences: &lt;9CC8D6E4804E234A827D1782AE3DB7782DEDA5CFA3@...&gt; &lt;4D635FDA.4030907@...&gt; &lt;9CC8D6E4804E234A827D1782AE3DB7782DEE1DB323@...&gt;\r\nIn-Reply-To: &lt;9CC8D6E4804E234A827D1782AE3DB7782DEE1DB323@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix WriterProcessor\r\nX-Yahoo-Group-Post: member; u=137285340; y=PZiRINq9wh_WHkUjMNnz2sW6WVzUzGemg0E56f1jV5Y3\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 2/21/11 11:57 PM, Dhake, Pankaj wrote:\n&gt; Thanks for your reply.\n&gt;\n&gt;  &gt;There&#39;s no &#39;openstream()&#39; method on CrawlURI; can you be more specific\n&gt;  &gt;about what you&#39;re doing?\n&gt;\n&gt; I am using the openstream() available with the java.net.URL class. On\n&gt; the CrawlURI I use the getURI() method to get a String. Then I form a\n&gt; URL using the String obtained and then use the openstream() function on\n&gt; this URL to get an inputstream.\n\nThen you are actually fetching the URL twice! Once in our FetchHTTP \nmodule (which does not use the java.net.URL class, because it does not \noffer enough control and raw access) and then again in your writer.\n\n&gt; The only thing I am currently doing in my WriterProcessor is to use the\n&gt; above procedure to get an inputstream which then is used to write to the\n&gt; local files. I am not making use of ReplayInputStream, but the errors do\n&gt; disappear on removing my writer so am confused!!\n&gt;\n&gt;  &gt;If you remove your writer, and just do a short test crawl without\n&gt;  &gt;writing anything, do the above errors go away?\n&gt;\n&gt; When I remove my writer then the errors do go away.\n\nWhere is your writer relative to other Processors? Does it do anything \nother that p[en the URL and write its contents?\n\nDo you have another standard writer present, as well? (When you remove \nyour writer, do you put back the standard writer, or run with no writers?)\n\nHave you done any other reordering/insertion of Processors compared to \nthe example default configuration?\n\nI don&#39;t yet see a pattern or have a good theory as to what is happening, \nbut on the chance that we&#39;re not cleaning things up properly in the case \nwhere custom additions are used instead of usual classes, I&#39;d really \nlike to figure out exactly what&#39;s triggering the problem.\n\n&gt;  &gt;Are the errors always on DNS URIs, as in your example above, or all URI\n&gt;  &gt;types?\n&gt;\n&gt; Yes the errors are only on DNS URIs.\n&gt;\n&gt;  &gt;Is it exactly your seeds + the robots.txt URIs that succeed, then\n&gt;  &gt;no other URIs? (So, exactly 2 times the number of seeds are shown as\n&gt;  &gt;fetch successes?)\n&gt;\n&gt; The seeds are the only oneï¿½s that succeed. However the robots.txt do\n&gt; succeed for some other URIs.\n\nThis is unclear. If all the seeds succeed, then all the DNS and \nrobots.txt for those same hosts must have succeeded.\n\nIf some other (non-seed-host) robots.txt succeed, then the DNS for that \nsame host must have succeeded.\n\nI would try a very small number of seeds and watch exactly which (and \nhow many) URIs of other types, and on other hosts, succeed.\n\nAlso, still wondering:\n\n&gt;&gt; Are you using the Heritrix 3.0 original release as your base, or more\n&gt;&gt; recent code from the project SVN/dev-builds?\n\n\n- Gordon @ IA\n\n\n\n&gt; Thanking You,\n&gt;\n&gt; Pankaj.\n&gt;\n&gt; *From:*archive-crawler@yahoogroups.com\n&gt; [mailto:archive-crawler@yahoogroups.com] *On Behalf Of *Gordon Mohr\n&gt; *Sent:* Tuesday, February 22, 2011 12:34 PM\n&gt; *To:* archive-crawler@yahoogroups.com\n&gt; *Cc:* Dhake, Pankaj\n&gt; *Subject:* Re: [archive-crawler] Heritrix WriterProcessor\n&gt;\n&gt; On 2/21/11 9:56 PM, Dhake, Pankaj wrote:\n&gt;  &gt; Hi all, I have wriitten a WriterProcessor which serves my needs to\n&gt;  &gt; replace the ARCWriterProcessor for Heritrix 3.0. However, when I run\n&gt;  &gt; Heritrix 3.0 with my WriterProcessor then it correctly downloads the\n&gt;  &gt; contents of the seeds I supply and also the robots.txt files. However\n&gt;  &gt; it does not download the contents for the URLs generated after the\n&gt;  &gt; seed. The log file gives the following message for all URLs generated\n&gt;  &gt; after the seeds:\n&gt;  &gt;\n&gt;  &gt; 2011-02-21 11:07:44.708 SEVERE thread-14\n&gt; org.archive.modules.fetcher.FetchDNS.storeDNSRecord() Failed store of\n&gt; DNS Record for dns:search.yahoo.com\n&gt;  &gt; java.io.IOException: RIS already open for ToeThread #15:\n&gt; dns:search.yahoo.com\n&gt;  &gt; at org.archive.io.RecordingInputStream.open(RecordingInputStream.java:84)\n&gt;  &gt; at org.archive.util.Recorder.inputWrap(Recorder.java:144)\n&gt;  &gt; at org.archive.modules.fetcher.FetchDNS.recordDNS(FetchDNS.java:271)\n&gt;  &gt; at org.archive.modules.fetcher.FetchDNS.storeDNSRecord(FetchDNS.java:216)\n&gt;  &gt; at org.archive.modules.fetcher.FetchDNS.innerProcess(FetchDNS.java:171)\n&gt;  &gt; at org.archive.modules.Processor.innerProcessResult(Processor.java:177)\n&gt;  &gt; at org.archive.modules.Processor.process(Processor.java:144)\n&gt;  &gt; at org.archive.modules.ProcessorChain.process(ProcessorChain.java:131)\n&gt;  &gt; at org.archive.crawler.framework.ToeThread.run(ToeThread.java:146)\n&gt;  &gt;\n&gt;  &gt; It says that RIS is already open. However in my code I have not made\n&gt;  &gt; use of the ReplayInputStream at all and am just making use of the\n&gt;  &gt; CrawlURI and then using the openstream() function to download the\n&gt;  &gt; contents. I am not quite clear about the tasks that are done in\n&gt;  &gt; innerprocess() function of the ARCWriterProcessor. So am not able to\n&gt;  &gt; find the bug in my code.\n&gt;\n&gt; There&#39;s no &#39;openstream()&#39; method on CrawlURI; can you be more specific\n&gt; about what you&#39;re doing?\n&gt;\n&gt; (If you&#39;re getting the data from a fetch for analysis/writing, then you\n&gt; are in some way or another opening an InputStream or ReplayCharSequence\n&gt; from the Recorder data, and it needs to be cleanly closed in any\n&gt; eventuality.)\n&gt;\n&gt; If you remove your writer, and just do a short test crawl without\n&gt; writing anything, do the above errors go away?\n&gt;\n&gt; Are the errors always on DNS URIs, as in your example above, or all URI\n&gt; types? Is it exactly your seeds + the robots.txt URIs that succeed, then\n&gt; no other URIs? (So, exactly 2 times the number of seeds are shown as\n&gt; fetch successes?)\n&gt;\n&gt; Are you using the Heritrix 3.0 original release as your base, or more\n&gt; recent code from the project SVN/dev-builds?\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}