{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":234926651,"authorName":"Frank McCown","from":"Frank McCown &lt;fmccown@...&gt;","profile":"mccownf","replyTo":"LIST","senderId":"m80Hbshz63_icQfEvubTBTBF6MUI3f6cLPhwE0-tW6Z-3leB0jBkyk2v0wiA0Ykb8562z7v_T_R2i_UAcmWvCW1nhXanJ48J","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Limiting crawl to single host/domain","postDate":"1151015209","msgId":2972,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ0OUIxOTI5LjMwNDA2MDFAY3Mub2R1LmVkdT4=","inReplyToHeader":"PDQ0OUIwRDAwLjgwNjA5MDNAYXJjaGl2ZS5vcmc+","referencesHeader":"PGU3OXFrbStpZnZtQGVHcm91cHMuY29tPiA8ZmVhZGI1NWMwNjA2MjAyMTUxeTJhMDdhZGQ4b2U4MWIyZmMxYWNiYTdiNDZAbWFpbC5nbWFpbC5jb20+IDw0NDk5OUQ3Qy4yMDQwNjA3QGNzLm9kdS5lZHU+IDw0NDlBRTQ1OS40MDAwNkBjcy5vZHUuZWR1PiA8NDQ5QjBEMDAuODA2MDkwM0BhcmNoaXZlLm9yZz4="},"prevInTopic":2971,"nextInTopic":0,"prevInTime":2971,"nextInTime":2973,"topicId":2953,"numMessagesInTopic":11,"msgSnippet":"... This solution worked perfectly (I m using version 1.8.0).  All URLs matched http://www.cs.odu.edu/~fmccown/webdev/* except for the robots.txt and dns","rawEmail":"Return-Path: &lt;fmccown@...&gt;\r\nX-Sender: fmccown@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 3682 invoked from network); 22 Jun 2006 22:27:16 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m23.grp.scd.yahoo.com with QMQP; 22 Jun 2006 22:27:16 -0000\r\nReceived: from unknown (HELO cartero.cs.odu.edu) (128.82.4.9)\n  by mta10.grp.scd.yahoo.com with SMTP; 22 Jun 2006 22:27:14 -0000\r\nReceived: from [128.82.7.106] (bang.seven.research.odu.edu [128.82.7.106])\n\tby cartero.cs.odu.edu (8.13.6/8.12.10) with ESMTP id k5MMO6GR016183\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Thu, 22 Jun 2006 18:24:15 -0400 (EDT)\r\nMessage-ID: &lt;449B1929.3040601@...&gt;\r\nDate: Thu, 22 Jun 2006 18:26:49 -0400\r\nUser-Agent: Mozilla Thunderbird 1.0 (Windows/20041206)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;e79qkm+ifvm@...&gt; &lt;feadb55c0606202151y2a07add8oe81b2fc1acba7b46@...&gt; &lt;44999D7C.2040607@...&gt; &lt;449AE459.40006@...&gt; &lt;449B0D00.8060903@...&gt;\r\nIn-Reply-To: &lt;449B0D00.8060903@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Frank McCown &lt;fmccown@...&gt;\r\nSubject: Re: [archive-crawler] Limiting crawl to single host/domain\r\nX-Yahoo-Group-Post: member; u=234926651; y=-yLUJkoHhK0wjt0u8bwhKPAywvIgzFYK_pEhEVjRAFNVVQ\r\nX-Yahoo-Profile: mccownf\r\n\r\nGordon Mohr wrote:\n&gt; My recommendation for specifying what you want to crawl is to use the\n&gt; DecidingScope with the following rules:\n&gt; \n&gt; RejectAllDecideRule (by default)\n&gt; SurtPrefixedDecideRule (ACCEPT, use SURTs implied by seeds)\n&gt; PrerequisiteDecideRule\n\nThis solution worked perfectly (I&#39;m using version 1.8.0).  All URLs \nmatched http://www.cs.odu.edu/~fmccown/webdev/* except for the \nrobots.txt and dns requests.  Thanks, Gordon!  I appreciate Igor and \nMichael&#39;s responses too.\n\n\n&gt; PS: One other FYI: I know you wrote a tool to crawl the Wayback Machine\n&gt; a while back; we&#39;re finally about to change the various public WM error\n&gt; pages to return non-200 HTTP error codes, in the next few days. So keep\n&gt; that in mind if you were using some other way to detect WM errors.\n\nThanks for the heads-up.\n\nFrank\n\n\n"}}