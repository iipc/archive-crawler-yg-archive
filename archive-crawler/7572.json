{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"GKc_Y7H3GC-46VfltfVw4sjj8aQc0YhqlR53H3Dm1leVWF-ZRtIDSYph3XYp6QMUJ79o2Rl6T_w4sfqNcbzFGBIrEi_m4QE","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Re: Crawling Layer by Layer","postDate":"1327574843","msgId":7572,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMjEyRjNCLjkwMTA2MDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGpmcXRnZCtzc2syQGVHcm91cHMuY29tPg==","referencesHeader":"PGpmcXRnZCtzc2syQGVHcm91cHMuY29tPg=="},"prevInTopic":7571,"nextInTopic":7575,"prevInTime":7571,"nextInTime":7573,"topicId":7518,"numMessagesInTopic":6,"msgSnippet":"Yes, Heritrix considers certain implied prerequisites as if they were discovered when considering the URI that required them. Thus they could be 1-2 P","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 6165 invoked from network); 26 Jan 2012 10:47:27 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m10.grp.sp2.yahoo.com with QMQP; 26 Jan 2012 10:47:27 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta3.grp.sp2.yahoo.com with SMTP; 26 Jan 2012 10:47:27 -0000\r\nX-Received: (qmail 40593 invoked by uid 0); 26 Jan 2012 10:47:25 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay02.pair.com with SMTP; 26 Jan 2012 10:47:25 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4F212F3B.9010601@...&gt;\r\nDate: Thu, 26 Jan 2012 02:47:23 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: ngzikai92 &lt;ngzikai92@...&gt;\r\nReferences: &lt;jfqtgd+ssk2@...&gt;\r\nIn-Reply-To: &lt;jfqtgd+ssk2@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Crawling Layer by Layer\r\nX-Yahoo-Group-Post: member; u=137285340; y=zUyFSs5iUsg8-ZxUp7U39tgvCzyvNRCp32lRetLM6Suk\r\nX-Yahoo-Profile: gojomo\r\n\r\nYes, Heritrix considers certain implied prerequisites as if they were \n&#39;discovered&#39; when considering the URI that required them. Thus they \ncould be 1-2 &#39;P&#39; hops, in the simple hops-accounting.\n\nIf you place a PrerequisiteAcceptDecideRule near the (or at the very) \nend of your scope DecideRules, it will &#39;rescue&#39; any necessary &#39;P&#39;s that \nyour earlier rules REJECTed. The default H3 configuration includes such \na rule, just in case.\n\nYour technique will allow you to clearly separate URIs into categories \nby their hops from your starting points.\n\nBut, since some URIs/paths can only be retrieved slowly (due to either \nserver slowness or the requirements of being polite), this sort of \ncrawling will tend to proceed much more slowly.\n\n(For example: Consider you have 1 seed. It has 100 links to 100 \ndifferent sites, and 100 links to the same site. The first 101 outlinks \n-- 1 to each site -- can be crawled very fast, assuming all sites are \nup. The next 99 from the last site, when crawling politely, will take \n100 times as long, and you won&#39;t be done with &quot;level 1&quot; until then.)\n\nThat may be fine for your purpose; I just wanted to further \nillustrate/emphasize the tradeoff.\n\n- Gordon\n\n\n\nOn 1/25/12 10:57 PM, ngzikai92 wrote:\n&gt; Hi,\n&gt;\n&gt; First of all, thanks alot for the replies.\n&gt;\n&gt; In order to facilitate the crawling of layer by layer, I have been playing with the hops filter for awhile now.\n&gt;\n&gt; Because I wanted to crawl layer by layer, I have been configuring my hop filter with a value of &#39;1&#39;. My idea is to start with a seed url, and crawl it with a hop filter of 1 (meaning the uris will be 1 layer down from the seed), extract the results of the crawl.log, and start a new crawl with those links I extracted from the first crawl as the seeds.\n&gt;\n&gt; I have been using the TooManyHopsDecideRule. However, I something  interesting happens when I tried it out. When I limit the hops to 1, my crawl.log has a lot of links with a status code of &#39;-63&#39; (which means a prerequisite could not be resolved) even though they are 1 hop away from the link. I retried crawling the same seed, this time with a hop limit of 2 and interestingly, the uris which I was unable to crawl previously could be crawled now. This lead me to believe (correct me if I am wrong) that DNS requests are also considered a &#39;hop&#39;. My question is that is are there any configurations (or perhaps some other decide rule?) that I can use so that such uris could also be crawled?\n&gt;\n&gt; Once again, thanks for the help give.\n&gt;\n&gt; Regards,\n&gt; Zi Kai\n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@...&gt;  wrote:\n&gt;&gt;\n&gt;&gt; The one thing I would add is that the &#39;layers&#39; as encountered by the\n&gt;&gt; crawler may not be the ones you are most interested in, because there\n&gt;&gt; are many link-hop paths from your seed to each URI, and which path is\n&gt;&gt; followed first by the crawler is affected by lots of factors.\n&gt;&gt;\n&gt;&gt; For example, you might reach one URI via an &#39;LLLL&#39; path -- four\n&gt;&gt; navigational outinks in a row -- as reported in the crawl.log.\n&gt;&gt;\n&gt;&gt; But, that doesn&#39;t mean there isn&#39;t also a shorter &#39;LL&#39; path that *could*\n&gt;&gt; have been followed. Only that for the crawler, with all of its various\n&gt;&gt; ordering tendencies and the delays it encountered, happened to discover\n&gt;&gt; the &#39;LLLL&#39; path first. (If the first &#39;L&#39; of that short &#39;LL&#39; path was on\n&gt;&gt; a slow host, or on a host that already had hours or days worth of URIs\n&gt;&gt; queued up, then by the time it is crawled, the other &#39;LLLL&#39; path has\n&gt;&gt; already finished. Thus the &#39;LL&#39; discovery path is rejected as providing\n&gt;&gt; a non-unique URI, and never enters the crawler queues/crawl.log.)\n&gt;&gt;\n&gt;&gt; If you want a &#39;web graph&#39; of all paths (or all shortest paths) between\n&gt;&gt; URIs, that&#39;s usually calculated via a big post-crawl analysis.\n&gt;&gt;\n&gt;&gt; - Gordon\n&gt;&gt;\n&gt;&gt; On 1/20/12 12:04 PM, Noah Levitt wrote:\n&gt;&gt;&gt; Hello Ng Zi Kai,\n&gt;&gt;&gt;\n&gt;&gt;&gt; You can determine the number of hops from seed by looking at the\n&gt;&gt;&gt; path-from-seed, which is the 5th column in crawl.log. The number of\n&gt;&gt;&gt; letters in that field is the number of hops from seed (unless it&#39;s\n&gt;&gt;&gt; more than 50 hops in which case it will be abbreviated).\n&gt;&gt;&gt;\n&gt;&gt;&gt; It is possible to write separate crawl logs for the different numbers\n&gt;&gt;&gt; of hops, but it&#39;s not simple to configure. My suggestion would be to\n&gt;&gt;&gt; split the crawl log up in a postprocessing step, by examining the\n&gt;&gt;&gt; path-from-seed for each crawled url.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Noah\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 2012-01-12 19:09 , ngzikai92 wrote:\n&gt;&gt;&gt;&gt; Hi,\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; This might be a little confusing, and I am just starting out.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; If we think about Heritrix, we can see it like a &#39;tree structure&#39;\n&gt;&gt;&gt;&gt; where the seed url is at the top layer, the additional urls that\n&gt;&gt;&gt;&gt; are found from the root at the second layer, and the urls that are\n&gt;&gt;&gt;&gt; found from the the additional urls at the third layer, and so on.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Currently, based on what I can get from Heritrix&#39;s crawl.log, I am\n&gt;&gt;&gt;&gt; only able to see the list of urls Heritrix found/visits as a list,\n&gt;&gt;&gt;&gt; but I am not able to determine the layers of the &#39;tree structure&#39;\n&gt;&gt;&gt;&gt; from there.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; I would like to know, if it is possible to customise Heritrix in\n&gt;&gt;&gt;&gt; such a way that I can crawl layer by layer and separating the urls\n&gt;&gt;&gt;&gt; obtained from each layer into separate log files?\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Much thanks and regards, Ng Zi Kai\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}