{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr (Internet Archive)","from":"&quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"2FC6EMP9rFOsNbnbxNGB2W-gonpgSH5iaoigFpRFExA8okY3kY5TvBUWVhg8OT13e311c5LXK4utU-iibuZrT9HQfKqROvAHNN7xBWG59QljDCcC2pdrQ2WtTaQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: Redirections OK but not extracting actual pages","postDate":"1098308389","msgId":1103,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxNzZEQjI1LjgwNjA2MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGNsNjJkayt0Z21zQGVHcm91cHMuY29tPg==","referencesHeader":"PGNsNjJkayt0Z21zQGVHcm91cHMuY29tPg=="},"prevInTopic":1102,"nextInTopic":1104,"prevInTime":1102,"nextInTime":1104,"topicId":1097,"numMessagesInTopic":11,"msgSnippet":"Probably, by not having our HTTPClient overlays for recording the received content in effect (found before the HTTPClient defaults), all processing subsequent","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 95367 invoked from network); 20 Oct 2004 21:40:04 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m24.grp.scd.yahoo.com with QMQP; 20 Oct 2004 21:40:04 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta3.grp.scd.yahoo.com with SMTP; 20 Oct 2004 21:40:04 -0000\r\nReceived: (qmail 21001 invoked by uid 100); 20 Oct 2004 21:28:22 -0000\r\nReceived: from unknown (HELO ?207.241.238.179?) (gojomo@...@207.241.238.179)\n  by mail-dev.archive.org with SMTP; 20 Oct 2004 21:28:22 -0000\r\nMessage-ID: &lt;4176DB25.8060607@...&gt;\r\nDate: Wed, 20 Oct 2004 14:39:49 -0700\r\nUser-Agent: Mozilla Thunderbird 0.8 (X11/20040913)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;cl62dk+tgms@...&gt;\r\nIn-Reply-To: &lt;cl62dk+tgms@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.1 required=7.0 tests=AWL autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: &quot;Gordon Mohr (Internet Archive)&quot; &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Redirections OK but not extracting actual\n pages\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nProbably, by not having our HTTPClient overlays for recording the\nreceived content in effect (found before the HTTPClient defaults),\nall processing subsequent to HTTP fetching (including extraction and\nlogging) saw no content.\n\nWe could add a sanity check at the end of HTTP fetching: is the\ncontent length reported by the HTTPClient library consistent with\nthe content length reported by the HTTP recording tools? If not,\na more helpful error could logged.\n\n- Gordon @ IA\n\njkilleen74 wrote:\n&gt; \n&gt; Thanks a million to Stack and Niklas - because I&#39;d had to update to \n&gt; the proxy-friendly patch, I&#39;d had to mess around with my setup, jars \n&gt; etc, and was actually running extracted classes rather than directly \n&gt; from heritrix-1.0.4.jar. As everything seemed to start and run fine \n&gt; without errors, I hadn&#39;t thought it was a classpath problem.\n&gt; \n&gt; But, having started a fresh directory, just putting in the jars I \n&gt; absolutely needed and generally tidying up, and running directly from \n&gt; a proxy-friendly-updated heritrix-1.0.4.jar, the problem seems to \n&gt; have been resolved - at least, I now have a crawl running happily \n&gt; into the thousands.\n&gt; \n&gt; Schoolboy error on my part, although slightly worrying at \n&gt; how &#39;silent&#39; the problem was. Thanks again, though,\n&gt; \n&gt; Jim\n&gt; \n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; \n&gt;&gt;Niklas Vargensten wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;Hey, I had the same problem a while back. No data, except for DNS. \n&gt;&gt;&gt;In my case it seemed to depend on my putting some additional .jar \n&gt; \n&gt; files in the \n&gt; \n&gt;&gt;&gt;heritrix directory, and as the heritrix startup script includes \n&gt; \n&gt; all jars with \n&gt; \n&gt;&gt;&gt;($HERITRIX_HOME)/*.jar in addition to ($HERITRIX_HOME)/lib/*.jar \n&gt; \n&gt; when \n&gt; \n&gt;&gt;&gt;running, it got \n&gt;&gt;&gt;confused, somehow (probably because my jar files contained some \n&gt; \n&gt; libs used by \n&gt; \n&gt;&gt;&gt;heritrix). \n&gt;&gt;&gt;It seemed really strange, but I tried everything, and \n&gt;&gt;&gt;the problem was not solved until I moved my jar files to another \n&gt; \n&gt; directory. I \n&gt; \n&gt;&gt;&gt;can&#39;t really tell if that was really it, but heritrix has worked \n&gt; \n&gt; perfect \n&gt; \n&gt;&gt;&gt;since then, anyway.\n&gt;&gt;&gt;\n&gt;&gt;&gt;This was just a wildcard, but if this is the case for you too, the \n&gt; \n&gt; startup \n&gt; \n&gt;&gt;&gt;script would really be in need of a re-write...\n&gt;&gt;&gt;/ Niklas\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;Thanks Niklas for jumping in.\n&gt;&gt;\n&gt;&gt;The heritrix.jar needs to be found before any of the supporting \n&gt; \n&gt; jars \n&gt; \n&gt;&gt;because it overlays classes from the httpclient jar. \n&gt;&gt;\n&gt;&gt;Looking back though the list, others have reported a problem \n&gt; \n&gt; similar to \n&gt; \n&gt;&gt;Jim&#39;s fixed by rejiggling the classpath:\n&gt;&gt;\n&gt;&gt;http://groups.yahoo.com/group/archive-crawler/message/772\n&gt;&gt;\n&gt;&gt;We have yet to do a windows start script.  Technically its an \n&gt;&gt;unsupported platform but it seems like experience has it that it \n&gt;&gt;generally works for people.  If anyone wants to donate a windows \n&gt; \n&gt; start \n&gt; \n&gt;&gt;script, I can add it in.\n&gt;&gt;\n&gt;&gt;Thanks,\n&gt;&gt;St.Ack\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt;On Wednesday 20 October 2004 16.37, jkilleen74 wrote:\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;I&#39;m running Heritrix 1.0.4 on a PC (Windows XP); I&#39;m having a \n&gt; \n&gt; problem\n&gt; \n&gt;&gt;&gt;&gt;where any crawl jobs seem to start fine - retrieve the DNS and\n&gt;&gt;&gt;&gt;robots.txt as appropriate, redirect as appropriate etc. But once \n&gt; \n&gt; they\n&gt; \n&gt;&gt;&gt;&gt;hit an actual page e.g. somewebsite/index.html, the crawl comes \n&gt; \n&gt; to an\n&gt; \n&gt;&gt;&gt;&gt;end. It&#39;s as if the page contained no data or links, yet I know \n&gt; \n&gt; this\n&gt; \n&gt;&gt;&gt;&gt;is not the case (I can browse to it through IE perfectly \n&gt; \n&gt; normally.)\n&gt; \n&gt;&gt;&gt;&gt;In my crawl.log, I see something like the following e.g.:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;20041020142929438     1         62 dns:www.st-andrews.ac.uk P\n&gt;&gt;&gt;&gt;http://www.st-andrews.ac.uk/study.shtml text/dns #000 0 - -\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;20041020142930190   404          0 http://www.st-\n&gt;&gt;&gt;&gt;andrews.ac.uk/robots.txt P http://www.st-andrews.ac.uk/study.shtml\n&gt;&gt;&gt;&gt;text/html #000 251 - -\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;20041020142931601   200          0 http://www.st-\n&gt;&gt;&gt;&gt;andrews.ac.uk/study.shtml - - text/html #000 142 - 3t\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Note the figures in the second column, for amount of data \n&gt; \n&gt; downloaded.\n&gt; \n&gt;&gt;&gt;&gt;For the front page (study.shtml), it seems to have downloaded zero\n&gt;&gt;&gt;&gt;data. Yet the crawl completes without any obvious errors or \n&gt; \n&gt; alerts -\n&gt; \n&gt;&gt;&gt;&gt;it seems to have found this front page, then decided that not only\n&gt;&gt;&gt;&gt;are there no links to follow, but not even any data to retrieve. \n&gt; \n&gt; The\n&gt; \n&gt;&gt;&gt;&gt;same pattern is seen whether I respect or ignore robots.txt, for \n&gt; \n&gt; any\n&gt; \n&gt;&gt;&gt;&gt;number of sites I&#39;ve tried to examine so far. I&#39;m not applying any\n&gt;&gt;&gt;&gt;unusual filters, just a standard crawl.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;I&#39;m running this behind a firewall, so am using the Proxy server\n&gt;&gt;&gt;&gt;patch proved a while back, which seems to be working (without it, \n&gt; \n&gt; any\n&gt; \n&gt;&gt;&gt;&gt;crawl job just stalled on the very first step.)\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;I&#39;m probably missing something obvious, but any suggestions\n&gt;&gt;&gt;&gt;appreciated!\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Jim K\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;   \n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; \n&gt;&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; \n&gt; \n&gt; \n\n\n"}}