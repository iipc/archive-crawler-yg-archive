{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":254719850,"authorName":"steve@archive.org","from":"&quot;steve@...&quot; &lt;steve@...&gt;","profile":"stearcorg","replyTo":"LIST","senderId":"3DBuip2e-2KC-yri3keF2PplaTDqBt7uhzJeT-G9PFfiWhfXXWVtwyd1Ygb0aYkvzho9IBaFvAMutKdecEZf0cTvhfoj4HjJuVGDXnGY","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Two issues for crawling","postDate":"1261153656","msgId":6232,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRCMkJBRDc4LjYwMzA1MDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGhnZmgwZStpMmI3QGVHcm91cHMuY29tPg==","referencesHeader":"PGhnZmgwZStpMmI3QGVHcm91cHMuY29tPg=="},"prevInTopic":6230,"nextInTopic":6233,"prevInTime":6231,"nextInTime":6233,"topicId":6230,"numMessagesInTopic":3,"msgSnippet":"hi Tony, first, which version of Heritrix are you running, and on what platform? re (a) crawl a URL at most once, you ll need to tell subsequent crawls to","rawEmail":"Return-Path: &lt;steve@...&gt;\r\nX-Sender: steve@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 3778 invoked from network); 18 Dec 2009 16:27:38 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m14.grp.re1.yahoo.com with QMQP; 18 Dec 2009 16:27:38 -0000\r\nX-Received: from unknown (HELO mail.archive.org) (207.241.231.239)\n  by mta2.grp.re1.yahoo.com with SMTP; 18 Dec 2009 16:27:38 -0000\r\nX-Received: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 83FF02D96A8\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 18 Dec 2009 08:27:37 -0800 (PST)\r\nX-Received: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 8JdKhewQK9zb for &lt;archive-crawler@yahoogroups.com&gt;;\n\tFri, 18 Dec 2009 08:27:36 -0800 (PST)\r\nX-Received: from takomaki-2.local (unknown [208.70.28.180])\n\tby mail.archive.org (Postfix) with ESMTPSA id DF6222D969A\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 18 Dec 2009 08:27:36 -0800 (PST)\r\nMessage-ID: &lt;4B2BAD78.6030506@...&gt;\r\nDate: Fri, 18 Dec 2009 08:27:36 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.5) Gecko/20091204 Thunderbird/3.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;hgfh0e+i2b7@...&gt;\r\nIn-Reply-To: &lt;hgfh0e+i2b7@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 2:3:4:0:0\r\nFrom: &quot;steve@...&quot; &lt;steve@...&gt;\r\nSubject: Re: [archive-crawler] Two issues for crawling\r\nX-Yahoo-Group-Post: member; u=254719850; y=WPS2b07Ufy0aZRToAUoN45Qs5Fr0w6EDOfzP4-pjv83O\r\nX-Yahoo-Profile: stearcorg\r\n\r\nhi Tony,\n\nfirst, which version of Heritrix are you running, and on what platform?\n\nre (a) crawl a URL at most once, you&#39;ll need to tell subsequent crawls\nto ignore a specified set of URLs. with H3 you can do this by adding\nthe URLs you wish to NOT be crawled to the _negative-surts.dump_\nfile i believe, which will be REJECTed from the scope (via\nSurtPrefixedDecideRule bean) by the default profile CXML\n(crawler-beans.cxml). i believe you can accomplish the same thing in H1\nby specifying a &quot;surts-dump-file&quot; with a REJECT SurtPrefixedDecideRule,\nhowever the default in H1 is to ACCEPT URLs in that file, so you&#39;ll\nneed a new rule. see the Heritrix Glossary for more about SURT prefixes:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/Glossary\n\nre (b) adding seeds after crawl launch. with H3, you can drop a seeds\nfile into the &quot;action&quot; directory. see the user&#39;s guide for more info:\n\nhttps://webarchive.jira.com/wiki/display/Heritrix/Action+Directory\n\ni&#39;m not sure how to do this in H1, and couldn&#39;t find anything about\nit in a quick search of the mailing list and the issue tracker. perhaps\nsomeone else on the list has done this?\n\n\n/steve@...\n\n\nOn 12/18/09 1:11 AM, tony871209 wrote:\n&gt; how can i\n&gt; a.crawl a URL at most once ( i do not want a update-checking followed\n&gt; with the 2nd time crawl)\n&gt; b.add seeds after crawl launch\n\n"}}