{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":6903103,"authorName":"Tom Emerson","from":"Tom Emerson &lt;Tree@...&gt;","profile":"tree02139","replyTo":"LIST","senderId":"BRTrPIzL8SDpjuk6GqZ0wV5OgPvdbI_3mKnK7YKtw45rnBD3vYZUGzayxlZrHq4_-ggoCc-YHKXnoKJr7f2iOC7hB6ryUvo","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Re: [archive-crawler] Limiting crawls (mostly) to HTML","postDate":"1093008109","msgId":854,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDE2Njc3LjY0MjM3LjU3Nzc4LjUxODYxNUB0aXBoYXJlcy5iYXNpc3RlY2gubmV0Pg==","inReplyToHeader":"PFNNVFAxLUJBU0lTVEVDSFlzVnIwMDAwMmVlMUBzbXRwMS1iYXNpc3RlY2g+","referencesHeader":"PFNNVFAxLUJBU0lTVEVDSFlzVnIwMDAwMmVlMUBzbXRwMS1iYXNpc3RlY2g+"},"prevInTopic":849,"nextInTopic":855,"prevInTime":853,"nextInTime":855,"topicId":841,"numMessagesInTopic":14,"msgSnippet":"... This is the next logical step, and you implemented it the same way I would. There are two reasons why I didn t do it this way though: 1. The (assumed)","rawEmail":"Return-Path: &lt;Tree@...&gt;\r\nX-Sender: Tree@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 1331 invoked from network); 20 Aug 2004 13:21:57 -0000\r\nReceived: from unknown (66.218.66.218)\n  by m23.grp.scd.yahoo.com with QMQP; 20 Aug 2004 13:21:57 -0000\r\nReceived: from unknown (HELO mailserver.basistech.com) (199.88.205.4)\n  by mta3.grp.scd.yahoo.com with SMTP; 20 Aug 2004 13:21:56 -0000\r\nReceived: from postfix.basistech.com ([10.1.3.65] RDNS failed) by mailserver.basistech.com with Microsoft SMTPSVC(6.0.3790.0);\n\t Fri, 20 Aug 2004 09:21:49 -0400\r\nReceived: by postfix.basistech.com (Postfix, from userid 5007)\n\tid 4ACA62C5795; Fri, 20 Aug 2004 09:21:49 -0400 (EDT)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\nMessage-ID: &lt;16677.64237.57778.518615@...&gt;\r\nDate: Fri, 20 Aug 2004 09:21:49 -0400\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;SMTP1-BASISTECHYsVr00002ee1@smtp1-basistech&gt;\r\nReferences: &lt;SMTP1-BASISTECHYsVr00002ee1@smtp1-basistech&gt;\r\nX-Mailer: VM 7.18 under Emacs 21.2.1\r\nReturn-Path: tree@...\r\nX-OriginalArrivalTime: 20 Aug 2004 13:21:49.0452 (UTC) FILETIME=[A63FCCC0:01C486B8]\r\nX-eGroups-Remote-IP: 199.88.205.4\r\nFrom: Tom Emerson &lt;Tree@...&gt;\r\nReply-To: tree@...\r\nSubject: Re: Re: [archive-crawler] Limiting crawls (mostly) to HTML\r\nX-Yahoo-Group-Post: member; u=6903103\r\nX-Yahoo-Profile: tree02139\r\n\r\nzhousp writes:\n&gt; I change a little in org.archive.crawler.writer.ARCWriterProcessor and can\n&gt; let user to add some ContentType string. only the content-type list in the\n&gt; user defined values will be save to hard disk. When turn on expert setting,will find\n&gt; some property under ARCWriter setting. Don&#39;t set this value means accept\n&gt; all pages.\n\nThis is the next logical step, and you implemented it the same way I\nwould. There are two reasons why I didn&#39;t do it this way though:\n\n 1. The (assumed) slowdown on iterating over the list of acceptable\n    content types for *every* retrieved document.\n\n 2. The inability to specify wild-cards (either MIME-based &#39;text/*&#39; or\n    regular expressions), as you pointed out in your original\n    response.\n\nThe first issue is mitigated somewhat by the RegExp exclude filter,\ninsofar as it minimizes the number of content types that are\nrequested, though I think it is still a concern if the list of\nacceptable content types is long.\n\nI was thinking that building a trie from the acceptable types would\nwork well --- O(n) worst case on lookup. Actually now that I think\nabout it, converting the list to a single regular expression would\nwork well and get essential the same runtime behavior once it&#39;s\ncompiled...\n\nFood for thought.\n\n    -tree\n\n-- \nTom Emerson                                          Basis Technology Corp.\nSoftware Architect                                 http://www.basistech.com\n  &quot;Beware the lollipop of mediocrity: lick it once and you suck forever&quot;\n\n"}}