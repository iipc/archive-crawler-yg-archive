{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"u95b9uUAEU3NCPXTlOH8MZ7fqxKzSqWbqns6lSv4vJUqAzoN37944A9Oa7Xs3ErW2e0niiLLPV4pWKRUwu99ZC5qPHpPOqE","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] Heritrix WriterProcessor","postDate":"1298931648","msgId":7045,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRENkMxRkMwLjEwNjA3MDRAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDlDQzhENkU0ODA0RTIzNEE4MjdEMTc4MkFFM0RCNzc4MkRFRTNCOENGNkBFWC1NQUlMLUhZRDEtMS5hbnQuYW1hem9uLmNvbT4=","referencesHeader":"PDlDQzhENkU0ODA0RTIzNEE4MjdEMTc4MkFFM0RCNzc4MkRFRTNCOENGNkBFWC1NQUlMLUhZRDEtMS5hbnQuYW1hem9uLmNvbT4="},"prevInTopic":7042,"nextInTopic":0,"prevInTime":7044,"nextInTime":7046,"topicId":7016,"numMessagesInTopic":8,"msgSnippet":"... Have you changed it so it s no longer using java.net.URL (and thus fetching the material twice)? (That s probably not the immediate cause of the problem,","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 19932 invoked from network); 28 Feb 2011 22:20:50 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m7.grp.sp2.yahoo.com with QMQP; 28 Feb 2011 22:20:50 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta1.grp.sp2.yahoo.com with SMTP; 28 Feb 2011 22:20:50 -0000\r\nX-Received: (qmail 50178 invoked by uid 0); 28 Feb 2011 22:20:49 -0000\r\nX-Received: from 208.70.27.190 (HELO silverbook.local) (208.70.27.190)\n  by relay03.pair.com with SMTP; 28 Feb 2011 22:20:49 -0000\r\nX-pair-Authenticated: 208.70.27.190\r\nMessage-ID: &lt;4D6C1FC0.1060704@...&gt;\r\nDate: Mon, 28 Feb 2011 14:20:48 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.13) Gecko/20101207 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nCc: &quot;Dhake, Pankaj&quot; &lt;pdhake@...&gt;\r\nReferences: &lt;9CC8D6E4804E234A827D1782AE3DB7782DEE3B8CF6@...&gt;\r\nIn-Reply-To: &lt;9CC8D6E4804E234A827D1782AE3DB7782DEE3B8CF6@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Heritrix WriterProcessor\r\nX-Yahoo-Group-Post: member; u=137285340; y=lGZHNwYPfzBUOw_9frsuk1PT7Mp4SSlS2QxIJ-MA8HnY\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 2/28/11 12:56 AM, Dhake, Pankaj wrote:\n&gt;\n&gt; Hi, Thanks for your reply. Could not yet get the solution to this\n&gt; problem.\n&gt;\n&gt;&gt; Where is your writer relative to other Processors? Does it do\n&gt;&gt; anything other that p[en the URL and write its contents?\n&gt;\n&gt; My writer is the first in the DispositionChain Processors which\n&gt; consists of 3 processors, S3WriterProcessor, CandidatesProcessor,\n&gt; DispositionProcessor. No my writer does nothing other than take the\n&gt; URL stirng and write its contents.\n\nHave you changed it so it&#39;s no longer using java.net.URL (and thus\nfetching the material twice)? (That&#39;s probably not the immediate cause \nof the problem, but if you were to access the already-fetched material \nin the same manner as the default writers, that would eliminate one \nsalient difference between your broken scenario and the working \nscenario, and eliminate unnecessary extra hits to target sites as well.)\n\n&gt;&gt; Do you have another standard writer present, as well? (When you\n&gt;&gt; remove your writer, do you put back the standard writer, or run\n&gt;&gt; with no writers?)\n&gt; Is I have the standard ARCWriterProcessor and when I put it back then\n&gt; no errors are produced.\n\nIt would be useful to also know what happens with *no* writers present\n(neither your writer nor the default).\n\n&gt;&gt; Have you done any other reordering/insertion of Processors compared\n&gt;&gt; to the example default configuration?\n&gt; NO I have not done any reordering but only replaced the\n&gt; ARCWriterProcessor by my S3WriterProcessor.\n&gt;\n&gt;&gt; This is unclear. If all the seeds succeed, then all the DNS and\n&gt;&gt; robots.txt for those same hosts must have succeeded. If some other\n&gt;&gt; (non-seed-host) robots.txt succeed, then the DNS for that same host\n&gt;&gt; must have succeeded. I would try a very small number of seeds and\n&gt;&gt; watch exactly which (and how many) URIs of other types, and on\n&gt;&gt; other hosts, succeed.\n&gt; For the seeds both the robots.txt and dns do succeed and it also\n&gt; works for most other URL. I get around 5 dns errors, which I had\n&gt; mentioned, while about 60 success. To debug this error, I did output\n&gt; the URI strings from the S3WriterProcessor file. However the uris for\n&gt; which the DNS error is reported are not in this output and hence may\n&gt; be not passed to my S3WriterProcessor. Just wondering could this be\n&gt; because the same URI is extracted from two different files and then\n&gt; passed on to the FetchDNS Processor more than once, which gives the\n&gt; error because the ReplayInputStream is alredy opened for that URI?\n\nIn general DNS URI will only be retried because (1) it failed a previous\nattempt; or (2) the expiration interval since its last request has\npassed. So it should not be a matter of the DNS URI being \ntriggered/enqueued from two different places.\n\nI suggest trying a single seed for each of the three possible \nconfigurations (no writer, default ARCWriterProcessor, your Writer) and \ncarefully examine the crawl.log, alerts, and heritrix_out.log for \ndifferences, and the first difference in behavior.\n\n&gt;&gt;&gt; Are you using the Heritrix 3.0 original release as your base, or\n&gt;&gt;&gt; more recent code from the project SVN/dev-builds?\n&gt; I am using the original release as the base.\n\nOK, thanks. If none of the above help resolve the issue, it would also \nbe worth trying your code in a dev-build/SVN-trunk. (I don&#39;t know \nspecifically any recent fixes that are relevant, and there are other \nrisks with using dev builds that haven&#39;t been fully stabilized, but if \nit fixes the problem it narrows the issues to consider.\n\n- Gordon @ IA\n\n\n\n\n&gt; Thanking you, Pankaj.\n&gt;\n&gt;\n&gt; -----Original Message----- From: Gordon Mohr\n&gt; [mailto:gojomo@...] Sent: Wednesday, February 23, 2011 4:41\n&gt; AM To: archive-crawler@yahoogroups.com Cc: Dhake, Pankaj Subject: Re:\n&gt; [archive-crawler] Heritrix WriterProcessor\n&gt;\n&gt; On 2/21/11 11:57 PM, Dhake, Pankaj wrote:\n&gt;&gt; Thanks for your reply.\n&gt;&gt;\n&gt;&gt;&gt; There&#39;s no &#39;openstream()&#39; method on CrawlURI; can you be more\n&gt;&gt;&gt; specific about what you&#39;re doing?\n&gt;&gt;\n&gt;&gt; I am using the openstream() available with the java.net.URL class.\n&gt;&gt; On the CrawlURI I use the getURI() method to get a String. Then I\n&gt;&gt; form a URL using the String obtained and then use the openstream()\n&gt;&gt; function on this URL to get an inputstream.\n&gt;\n&gt; Then you are actually fetching the URL twice! Once in our FetchHTTP\n&gt; module (which does not use the java.net.URL class, because it does\n&gt; not offer enough control and raw access) and then again in your\n&gt; writer.\n&gt;\n&gt;&gt; The only thing I am currently doing in my WriterProcessor is to use\n&gt;&gt; the above procedure to get an inputstream which then is used to\n&gt;&gt; write to the local files. I am not making use of ReplayInputStream,\n&gt;&gt; but the errors do disappear on removing my writer so am confused!!\n&gt;&gt;\n&gt;&gt;&gt; If you remove your writer, and just do a short test crawl\n&gt;&gt;&gt; without writing anything, do the above errors go away?\n&gt;&gt;\n&gt;&gt; When I remove my writer then the errors do go away.\n&gt;\n&gt; Where is your writer relative to other Processors? Does it do\n&gt; anything other that p[en the URL and write its contents?\n&gt;\n&gt; Do you have another standard writer present, as well? (When you\n&gt; remove your writer, do you put back the standard writer, or run with\n&gt; no writers?)\n&gt;\n&gt; Have you done any other reordering/insertion of Processors compared\n&gt; to the example default configuration?\n&gt;\n&gt; I don&#39;t yet see a pattern or have a good theory as to what is\n&gt; happening, but on the chance that we&#39;re not cleaning things up\n&gt; properly in the case where custom additions are used instead of usual\n&gt; classes, I&#39;d really like to figure out exactly what&#39;s triggering the\n&gt; problem.\n&gt;\n&gt;&gt;&gt; Are the errors always on DNS URIs, as in your example above, or\n&gt;&gt;&gt; all URI types?\n&gt;&gt;\n&gt;&gt; Yes the errors are only on DNS URIs.\n&gt;&gt;\n&gt;&gt;&gt; Is it exactly your seeds + the robots.txt URIs that succeed,\n&gt;&gt;&gt; then no other URIs? (So, exactly 2 times the number of seeds are\n&gt;&gt;&gt; shown as fetch successes?)\n&gt;&gt;\n&gt;&gt; The seeds are the only one&#39;s that succeed. However the robots.txt\n&gt;&gt; do succeed for some other URIs.\n&gt;\n&gt; This is unclear. If all the seeds succeed, then all the DNS and\n&gt; robots.txt for those same hosts must have succeeded.\n&gt;\n&gt; If some other (non-seed-host) robots.txt succeed, then the DNS for\n&gt; that same host must have succeeded.\n&gt;\n&gt; I would try a very small number of seeds and watch exactly which\n&gt; (and how many) URIs of other types, and on other hosts, succeed.\n&gt;\n&gt; Also, still wondering:\n&gt;\n&gt;&gt;&gt; Are you using the Heritrix 3.0 original release as your base, or\n&gt;&gt;&gt; more recent code from the project SVN/dev-builds?\n&gt;\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt;\n&gt;\n&gt;&gt; Thanking You,\n&gt;&gt;\n&gt;&gt; Pankaj.\n&gt;&gt;\n&gt;&gt; *From:*archive-crawler@yahoogroups.com\n&gt;&gt; [mailto:archive-crawler@yahoogroups.com] *On Behalf Of *Gordon\n&gt;&gt; Mohr *Sent:* Tuesday, February 22, 2011 12:34 PM *To:*\n&gt;&gt; archive-crawler@yahoogroups.com *Cc:* Dhake, Pankaj *Subject:* Re:\n&gt;&gt; [archive-crawler] Heritrix WriterProcessor\n&gt;&gt;\n&gt;&gt; On 2/21/11 9:56 PM, Dhake, Pankaj wrote:\n&gt;&gt;&gt; Hi all, I have wriitten a WriterProcessor which serves my needs\n&gt;&gt;&gt; to replace the ARCWriterProcessor for Heritrix 3.0. However, when\n&gt;&gt;&gt; I run Heritrix 3.0 with my WriterProcessor then it correctly\n&gt;&gt;&gt; downloads the contents of the seeds I supply and also the\n&gt;&gt;&gt; robots.txt files. However it does not download the contents for\n&gt;&gt;&gt; the URLs generated after the seed. The log file gives the\n&gt;&gt;&gt; following message for all URLs generated after the seeds:\n&gt;&gt;&gt;\n&gt;&gt;&gt; 2011-02-21 11:07:44.708 SEVERE thread-14\n&gt;&gt; org.archive.modules.fetcher.FetchDNS.storeDNSRecord() Failed store\n&gt;&gt; of DNS Record for dns:search.yahoo.com\n&gt;&gt;&gt; java.io.IOException: RIS already open for ToeThread #15:\n&gt;&gt; dns:search.yahoo.com\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.io.RecordingInputStream.open(RecordingInputStream.java:84)\n&gt;&gt;\n&gt;&gt;&gt;\n&gt; at org.archive.util.Recorder.inputWrap(Recorder.java:144)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.modules.fetcher.FetchDNS.recordDNS(FetchDNS.java:271)\n&gt;&gt;\n&gt;&gt;&gt;\n&gt; at\n&gt; org.archive.modules.fetcher.FetchDNS.storeDNSRecord(FetchDNS.java:216)\n&gt;&gt;\n&gt;  at\n&gt; org.archive.modules.fetcher.FetchDNS.innerProcess(FetchDNS.java:171)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.modules.Processor.innerProcessResult(Processor.java:177)\n&gt;&gt;\n&gt;&gt;&gt;\n&gt; at org.archive.modules.Processor.process(Processor.java:144)\n&gt;&gt;&gt; at\n&gt;&gt;&gt; org.archive.modules.ProcessorChain.process(ProcessorChain.java:131)\n&gt;&gt;\n&gt;&gt;&gt;\n&gt; at org.archive.crawler.framework.ToeThread.run(ToeThread.java:146)\n&gt;&gt;&gt;\n&gt;&gt;&gt; It says that RIS is already open. However in my code I have not\n&gt;&gt;&gt; made use of the ReplayInputStream at all and am just making use\n&gt;&gt;&gt; of the CrawlURI and then using the openstream() function to\n&gt;&gt;&gt; download the contents. I am not quite clear about the tasks that\n&gt;&gt;&gt; are done in innerprocess() function of the ARCWriterProcessor. So\n&gt;&gt;&gt; am not able to find the bug in my code.\n&gt;&gt;\n&gt;&gt; There&#39;s no &#39;openstream()&#39; method on CrawlURI; can you be more\n&gt;&gt; specific about what you&#39;re doing?\n&gt;&gt;\n&gt;&gt; (If you&#39;re getting the data from a fetch for analysis/writing, then\n&gt;&gt; you are in some way or another opening an InputStream or\n&gt;&gt; ReplayCharSequence from the Recorder data, and it needs to be\n&gt;&gt; cleanly closed in any eventuality.)\n&gt;&gt;\n&gt;&gt; If you remove your writer, and just do a short test crawl without\n&gt;&gt; writing anything, do the above errors go away?\n&gt;&gt;\n&gt;&gt; Are the errors always on DNS URIs, as in your example above, or all\n&gt;&gt; URI types? Is it exactly your seeds + the robots.txt URIs that\n&gt;&gt; succeed, then no other URIs? (So, exactly 2 times the number of\n&gt;&gt; seeds are shown as fetch successes?)\n&gt;&gt;\n&gt;&gt; Are you using the Heritrix 3.0 original release as your base, or\n&gt;&gt; more recent code from the project SVN/dev-builds?\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}