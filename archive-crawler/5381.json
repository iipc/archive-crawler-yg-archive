{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":334763322,"authorName":"Christian Krumm","from":"Christian Krumm &lt;signore.chrissi@...&gt;","profile":"chuk_ol","replyTo":"LIST","senderId":"PjE7yfloqINpNgd6b2uJh9gWHlMTlCigz3ifbgB5dS0SO7DXBtYMYsVKlA42H-v0CSnPjjbYG33EiDkOXFCz5i7buuxGJhSpz15ZDs51U_APqpnOvjoi","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Newbie - downloading only HTML pages","postDate":"1217587491","msgId":5381,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4OTJFOTIzLjkwNDA4QGdvb2dsZW1haWwuY29tPg==","inReplyToHeader":"PGc2c2tsOStvbTA0QGVHcm91cHMuY29tPg==","referencesHeader":"PGc2c2tsOStvbTA0QGVHcm91cHMuY29tPg=="},"prevInTopic":5379,"nextInTopic":5382,"prevInTime":5380,"nextInTime":5382,"topicId":5339,"numMessagesInTopic":9,"msgSnippet":"... Sure. Here it is. I hope this is what you have been looking for. For preventing non HTML-Documents to be downloaded I ve added three DecideRules to the","rawEmail":"Return-Path: &lt;signore.chrissi@...&gt;\r\nX-Sender: signore.chrissi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 72453 invoked from network); 1 Aug 2008 10:45:03 -0000\r\nX-Received: from unknown (66.218.67.97)\n  by m46.grp.scd.yahoo.com with QMQP; 1 Aug 2008 10:45:03 -0000\r\nX-Received: from unknown (HELO smtpmx10.uni-oldenburg.de) (134.106.87.110)\n  by mta18.grp.scd.yahoo.com with SMTP; 1 Aug 2008 10:45:02 -0000\r\nX-Received: from [192.168.2.27] (p57B6E72C.dip.t-dialin.net [87.182.231.44])\n\t(authenticated bits=0)\n\tby smtpmx10.uni-oldenburg.de (8.13.1/8.13.1) with ESMTP id m71AioaA003281\n\t(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Fri, 1 Aug 2008 12:44:58 +0200\r\nMessage-ID: &lt;4892E923.90408@...&gt;\r\nDate: Fri, 01 Aug 2008 12:44:51 +0200\r\nUser-Agent: Thunderbird 2.0.0.16 (X11/20080724)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g6skl9+om04@...&gt;\r\nIn-Reply-To: &lt;g6skl9+om04@...&gt;\r\nX-Enigmail-Version: 0.95.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\nX-PMX-Version: 5.4.3.345767, Antispam-Engine: 2.6.0.325393, Antispam-Data: 2008.8.1.102234\r\nX-PerlMx-Spam: Gauge=IIIIIII, Probability=7%, Report=&#39;BODY_SIZE_7000_7999 0, RDNS_DYNAMIC 0, RDNS_SUSP 0, RDNS_SUSP_SPECIFIC 0, __BOUNCE_CHALLENGE_SUBJ 0, __C230066_P5 0, __CP_MEDIA_BODY 0, __CP_URI_IN_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __FRAUD_419_WEBMAIL 0, __FRAUD_419_WEBMAIL_FROM 0, __HAS_MSGID 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __SANE_MSGID 0, __USER_AGENT 0&#39;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Christian Krumm &lt;signore.chrissi@...&gt;\r\nSubject: Re: [archive-crawler] Re: Newbie - downloading only HTML pages\r\nX-Yahoo-Group-Post: member; u=334763322; y=_Rc_NscM_vJAkOayVUhfxafLtV4B0oedTfQw23RLWPcduA\r\nX-Yahoo-Profile: chuk_ol\r\n\r\nivar_sr schrieb:\n&gt; Christian,\n&gt; \n&gt; Could you provide me the syntax (the complete definition in \n&gt; global.sheet) or custom sheet for this definition please? I have tried \n&gt; different possibilities but got some error.\n&gt; \n&gt; thank you.\n&gt; \n&gt; \n\nSure.\nHere it is. I hope this is what you have been looking for.\nFor preventing non HTML-Documents to be downloaded I&#39;ve added three\nDecideRules to the mid-fetch-rules. First I&#39;ll reject all URIs. Then\nI&#39;ll only accept URIs which have text/html or application/xhtml\nContent-Type using a ContentTypeMatchesRegExpDecideRule, so only\nHTML-Sites will be downloaded. Finally all Prerequisites are ruled in.\n\nIt&#39;s important that the ContentTypeMatchesRegExpDecideRule be placed\nimmediately before PrerequisiteAcceptDecideRule; otherwise the DNS and\nrobots.txt prerequisites will be rejected since they won&#39;t match the regexp.\n\nNever the less URIs of images, videos etc. will be handeled by further\nProcessors so I may get the whole link-tree without downloading all\nressources. The DecideRules within mid-fetch-rules only prevents the\ncontent to be downloaded. I&#39;m not sure if it skips further processing to\nthe first PostProcessor if the URI is rejected by the mid-fetch-rules.\nPerhaps someone may clarrify this.\n\nSo to be sure, that none image, or video is written by your\nWriteProcessor you may have to add the\nContentTypeMatchesRegExpDecideRule to your writers too.\nInstead of ContentTypeMatchesRegExpDecideRule you may use a\nMatchesFilePatternDecideRule to rule all .html-File in.\n\n\nI&#39;ve attached my global.sheet for reference. In this profile I&#39;m\nnot archiving any ressource in an ARC or database because here I&#39;m\ncurrently only interested in the number of URIs with desired characeristics.\n\nHope this helps\nChristian\n\nroot=map, java.lang.Object\nroot:metadata=primary, org.archive.modules.writer.DefaultMetadataProvider\nroot:metadata:description=string, CHARISMA - Bridge-Page-Heuristic\nroot:metadata:http-user-agent=string, Mozilla/5.0 (compatible;\nheritrix/@VERSION@ +@OPERATOR_CONTACT_URL@)\nroot:metadata:operator-contact-url=string, http://www.offis.de\nroot:metadata:operator-from=string, CKrumm@...\nroot:metadata:operator-name=string, Christian Krumm\nroot:metadata:organization=string, OFFIS MI\nroot:metadata:robots-honoring-policy=primary,\norg.archive.modules.net.RobotsHonoringPolicy\nroot:metadata:robots-honoring-policy:user-agents=list, java.lang.String\nroot:loggerModule=primary, org.archive.crawler.framework.CrawlerLoggerModule\nroot:seeds=primary, org.archive.modules.seeds.SeedModuleImpl\nroot:scope=object, org.archive.modules.deciderules.DecideRuleSequence\nroot:scope:rules=list, org.archive.modules.deciderules.DecideRule\nroot:scope:rules:0=object, org.archive.modules.deciderules.AcceptDecideRule\nroot:scope:rules:1=object,\norg.archive.modules.deciderules.TransclusionDecideRule\nroot:scope:rules:2=object,\norg.archive.modules.deciderules.PathologicalPathDecideRule\nroot:scope:rules:3=object,\norg.archive.modules.deciderules.TooManyPathSegmentsDecideRule\nroot:scope:rules:4=object,\norg.archive.modules.deciderules.PrerequisiteAcceptDecideRule\nroot:uriUniqFilter=primary, org.archive.crawler.util.BdbUriUniqFilter\nroot:queue-assignment-policy=primary,\norg.archive.crawler.frontier.SurtAuthorityQueueAssignmentPolicy\nroot:server-cache=primary, org.archive.modules.net.BdbServerCache\nroot:credential-store=primary,\norg.archive.modules.credential.CredentialStore\nroot:controller=primary, org.archive.crawler.framework.CrawlControllerImpl\nroot:controller:checkpointer-period=int, 1\nroot:controller:frontier=primary, org.archive.crawler.frontier.BdbFrontier\nroot:controller:frontier:queue-precedence-policy=object,\norg.archive.crawler.frontier.precedence.HighestUriQueuePrecedencePolicy\nroot:controller:frontier:rules=list,\norg.archive.modules.canonicalize.CanonicalizationRule\nroot:controller:frontier:rules:0=object,\norg.archive.modules.canonicalize.LowercaseRule\nroot:controller:frontier:rules:1=object,\norg.archive.modules.canonicalize.StripUserinfoRule\nroot:controller:frontier:rules:2=object,\norg.archive.modules.canonicalize.StripWWWNRule\nroot:controller:frontier:rules:3=object,\norg.archive.modules.canonicalize.StripSessionIDs\nroot:controller:frontier:rules:4=object,\norg.archive.modules.canonicalize.StripSessionCFIDs\nroot:controller:frontier:rules:5=object,\norg.archive.modules.canonicalize.FixupQueryStr\nroot:controller:frontier:scope=reference, root:scope\nroot:controller:frontier:uri-precedence-policy=object,\nde.offis.mi.charisma.crawler.evaluation.precedence.CustomUriPrecedencePolicy\nroot:controller:max-bytes-download=long, 5368709120\nroot:controller:max-time-sec=long, 432000\nroot:controller:pause-at-finish=boolean, true\nroot:controller:pause-at-start=boolean, true\nroot:controller:processors=map, org.archive.modules.Processor\nroot:controller:processors:Preselector=object,\norg.archive.crawler.prefetch.Preselector\nroot:controller:processors:Preselector:scope=reference, root:scope\nroot:controller:processors:Preprocessor=object,\norg.archive.crawler.prefetch.PreconditionEnforcer\nroot:controller:processors:DNS=object, org.archive.modules.fetcher.FetchDNS\nroot:controller:processors:HTTP=object,\norg.archive.modules.fetcher.FetchHTTP\nroot:controller:processors:HTTP:accept-headers=list, java.lang.String\nroot:controller:processors:HTTP:midfetch-rules=object,\norg.archive.modules.deciderules.DecideRuleSequence\nroot:controller:processors:HTTP:midfetch-rules:rules=list,\norg.archive.modules.deciderules.DecideRule\nroot:controller:processors:HTTP:midfetch-rules:rules:0=object,\norg.archive.modules.deciderules.RejectDecideRule\nroot:controller:processors:HTTP:midfetch-rules:rules:1=object,\norg.archive.modules.deciderules.ContentTypeMatchesRegExpDecideRule\nroot:controller:processors:HTTP:midfetch-rules:rules:1:regexp=pattern,\n(?i)(text/html|application/xhtml)\nroot:controller:processors:HTTP:midfetch-rules:rules:2=object,\norg.archive.modules.deciderules.PrerequisiteAcceptDecideRule\nroot:controller:processors:ExtractorHTTP=object,\norg.archive.modules.extractor.ExtractorHTTP\nroot:controller:processors:LinkExtractor=object,\nde.offis.mi.charisma.crawler.extraction.LinkExtractor\nroot:controller:processors:GeoParser=object,\nde.offis.mi.charisma.crawler.geo.GeoParserProcessor\nroot:controller:processors:Evaluation=object,\nde.offis.mi.charisma.crawler.evaluation.EvaluationProcessor\nroot:controller:processors:Evaluation:evaluators:0:do-cut=boolean, true\nroot:controller:processors:Updater=object,\norg.archive.crawler.postprocessor.CrawlStateUpdater\nroot:controller:processors:LinksScoper=object,\norg.archive.crawler.postprocessor.LinksScoper\nroot:controller:processors:LinksScoper:logger-module=auto, root:loggerModule\nroot:controller:processors:LinksScoper:scope=reference, root:scope\nroot:controller:processors:BequeathProcessor=object,\nde.offis.mi.charisma.crawler.evaluation.BequeathProcessor\nroot:controller:processors:Scheduler=object,\norg.archive.crawler.postprocessor.FrontierScheduler\nroot:controller:statistics-tracker=object,\norg.archive.crawler.framework.StatisticsTrackerImpl\n\n"}}