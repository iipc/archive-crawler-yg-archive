{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"QBk4aIC9uuXnoIhlMbvbxFtUB2r0w1UcPt9wO-qfPetMnmjGXBdA0GOpVR3MSaUM9Iq-VgzV97z8nVtv3w_7ZQ","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: can anyone  explain some concepts of Crawl Scope?","postDate":"1104360507","msgId":1311,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQxRDMzNDNCLjYwMzA3MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGNxZzFqNCsybGQ0QGVHcm91cHMuY29tPg==","referencesHeader":"PGNxZzFqNCsybGQ0QGVHcm91cHMuY29tPg=="},"prevInTopic":1310,"nextInTopic":1346,"prevInTime":1310,"nextInTime":1312,"topicId":1302,"numMessagesInTopic":11,"msgSnippet":"... It ll do better with 2gigs of RAM.  The RAM-to-#Hosts is probably not linear. The crawler goes into pause when it hits an OOME. It ll last longer a bit","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 72026 invoked from network); 29 Dec 2004 22:55:51 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m22.grp.scd.yahoo.com with QMQP; 29 Dec 2004 22:55:51 -0000\r\nReceived: from unknown (HELO ia00524.archive.org) (207.241.224.172)\n  by mta6.grp.scd.yahoo.com with SMTP; 29 Dec 2004 22:55:51 -0000\r\nReceived: (qmail 13584 invoked by uid 100); 29 Dec 2004 22:41:26 -0000\r\nReceived: from debord.archive.org (HELO ?207.241.238.140?) (stack@...@207.241.238.140)\n  by mail-dev.archive.org with SMTP; 29 Dec 2004 22:41:26 -0000\r\nMessage-ID: &lt;41D3343B.6030705@...&gt;\r\nDate: Wed, 29 Dec 2004 14:48:27 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.3) Gecko/20041007 Debian/1.7.3-5\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;cqg1j4+2ld4@...&gt;\r\nIn-Reply-To: &lt;cqg1j4+2ld4@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Spam-DCC: : \r\nX-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on ia00524.archive.org\r\nX-Spam-Level: \r\nX-Spam-Status: No, hits=0.8 required=6.5 tests=AWL autolearn=no version=2.63\r\nX-eGroups-Remote-IP: 207.241.224.172\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: can anyone  explain some concepts of Crawl\n Scope?\r\nX-Yahoo-Group-Post: member; u=168599281\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nbjhong02 wrote:\n\n&gt; ...\n&gt; &gt; But be aware that Heritrix currently doesn&#39;t do a good job\n&gt; broadcrawling\n&gt; &gt; currently (See\n&gt; &gt;\n&gt; http://crawler.archive.org/articles/releasenotes.html#1_0_0_limitatio\n&gt; ns).\n&gt;\n&gt; in the limitations, I found:\n&gt; With the default settings, and an assignment of a 256MB Java heap to\n&gt; the Heritrix process, crawling which discovers up to 10 000 hosts,\n&gt; and schedules over 6 000 000 URIs, should be possible. Discovery of\n&gt; higher numbers of URIs/hosts will likely trigger out-of-memory\n&gt; problems unless a larger java heap was assigned at startup.\n&gt;\n&gt; Is that to say, it can discover up to 40,000 hosts, if I assign\n&gt; 1025MB Java heap (on a PC with 2GB physical memory).\n&gt; I wonder if there&#39;s any strategy to stop crawl before triggering out-\n&gt; of-memory problems. or we can set some parameters to limit the\n&gt; number of discovered URIs/hosts.\n\nIt&#39;ll do better with 2gigs of RAM.  The RAM-to-#Hosts is probably not \nlinear.\n\nThe crawler goes into pause when it hits an OOME.\n\nIt&#39;ll last longer a bit longer if you  set &#39;hold-queues&#39; to true -- an \nexpert settting -- and if you set down the &#39;host-queues-memory-capacity&#39; \nfrom 200 to 20.\n\n(Memory usage should be much better in heritrix 1.4 -- its the main \nfocus of this release -- which should be available soon: I&#39;d guess a \n~month or so).\n\nYours,\nSt.Ack\n\n&gt;\n&gt; thanks\n&gt;\n\n"}}