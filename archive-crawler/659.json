{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":6903103,"authorName":"Tom Emerson","from":"Tom Emerson &lt;Tree@...&gt;","profile":"tree02139","replyTo":"LIST","senderId":"l3j3M3zJOyUlquGfdgFikprF6N_AnfGorxtaPqA22A6hgxwgHcK9qd2wCMqTadbPWX6FKe85bchkGanN9l4Qim9eetwE4d0","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Compression of ARC files [Was: add some\tmethod\ton\torg.archive.io.arc.ARCReader]","postDate":"1089978875","msgId":659,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDE2NjMxLjQ5NjU5LjI1NDQzNy45NzM1NDNAdGlwaGFyZXMuYmFzaXN0ZWNoLm5ldD4=","inReplyToHeader":"PDEwODk5NjcwMjcuMTkzMy44Ni5jYW1lbEBwYzc3MC5zYi5zdGF0c2JpYmxpb3Rla2V0LmRrPg==","referencesHeader":"PDIwMDQwNzE0MDQwMi5pNkU0MmhaMjA3NjVAcG9sbHV4LnN0YXRzYmlibGlvdGVrZXQuZGs+CTwxMDg5NzkwODExLjE5MzMuMjEuY2FtZWxAcGM3NzAuc2Iuc3RhdHNiaWJsaW90ZWtldC5kaz4JPDQwRjU1RkFCLjUwNjA5MDZAYXJjaGl2ZS5vcmc+CTwxMDg5ODczOTUwLjE5MzMuNTQuY2FtZWxAcGM3NzAuc2Iuc3RhdHNiaWJsaW90ZWtldC5kaz4JPDQwRjZDMjA1LjkwNDAzMDJAYXJjaGl2ZS5vcmc+CTwxMDg5OTY3MDI3LjE5MzMuODYuY2FtZWxAcGM3NzAuc2Iuc3RhdHNiaWJsaW90ZWtldC5kaz4="},"prevInTopic":658,"nextInTopic":660,"prevInTime":658,"nextInTime":660,"topicId":629,"numMessagesInTopic":20,"msgSnippet":"... So configure your crawls to not compress the ARC files and you get the results you want. Remember too that the data stored in the arc file, except for the","rawEmail":"Return-Path: &lt;Tree@...&gt;\r\nX-Sender: Tree@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 46658 invoked from network); 16 Jul 2004 11:54:38 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m25.grp.scd.yahoo.com with QMQP; 16 Jul 2004 11:54:38 -0000\r\nReceived: from unknown (HELO mailserver.basistech.com) (199.88.205.4)\n  by mta5.grp.scd.yahoo.com with SMTP; 16 Jul 2004 11:54:38 -0000\r\nReceived: from postfix.basistech.com ([10.1.3.65] RDNS failed) by mailserver.basistech.com with Microsoft SMTPSVC(6.0.3790.0);\n\t Fri, 16 Jul 2004 07:54:35 -0400\r\nReceived: by postfix.basistech.com (Postfix, from userid 5007)\n\tid 6A931186932; Fri, 16 Jul 2004 07:54:35 -0400 (EDT)\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\nMessage-ID: &lt;16631.49659.254437.973543@...&gt;\r\nDate: Fri, 16 Jul 2004 07:54:35 -0400\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;1089967027.1933.86.camel@...&gt;\r\nReferences: &lt;200407140402.i6E42hZ20765@...&gt;\n\t&lt;1089790811.1933.21.camel@...&gt;\n\t&lt;40F55FAB.5060906@...&gt;\n\t&lt;1089873950.1933.54.camel@...&gt;\n\t&lt;40F6C205.9040302@...&gt;\n\t&lt;1089967027.1933.86.camel@...&gt;\r\nX-Mailer: VM 7.18 under Emacs 21.2.1\r\nReturn-Path: tree@...\r\nX-OriginalArrivalTime: 16 Jul 2004 11:54:35.0779 (UTC) FILETIME=[AA476D30:01C46B2B]\r\nX-eGroups-Remote-IP: 199.88.205.4\r\nFrom: Tom Emerson &lt;Tree@...&gt;\r\nReply-To: tree@...\r\nSubject: Re: [archive-crawler] Compression of ARC files [Was: add some\tmethod\ton\n\torg.archive.io.arc.ARCReader]\r\nX-Yahoo-Group-Post: member; u=6903103\r\nX-Yahoo-Profile: tree02139\r\n\r\nLars Clausen writes:\n&gt; I&#39;m not saying that zlib is any worse or better than other compression\n&gt; algorithms in comprehensibility.  It&#39;s that if you just get the file,\n&gt; it&#39;s binary junk, not the immediately comprehensible ASCII-based ARC\n&gt; format that had me convinced would be readable 100 years from now.\n\nSo configure your crawls to not compress the ARC files and you get the\nresults you want.\n\nRemember too that the data stored in the arc file, except for the URL\nheaders in each member, can be binary junk depending on the content\ntype. What guarantee will you have that PDF files or ShockWave movies\nwill be readable in 100 years?\n\nAnyway, if you want to look at the raw contents of your compressed\nARCs, just use zmore:\n\n(0) tree% zmore ARN-20040708211401-00000.arc.gz\n\n&gt; One could argue that the same holds for hardware or file system\n&gt; compression, but I would say that in those cases, if you can read the\n&gt; files, you get it out of compressed mode.  Only if a tape/disk was found\n&gt; so long from now that none of the original hardware is available and\n&gt; some kind of other scan is set up would the compression be a problem.\n\nAnd I would argue that it is far more likely that the hardware would\nhave deteriorated and be unusable. For that matter if we have media\nnow (besides good old paper) that exists in 100 years we&#39;ll be lucky:\nit won&#39;t matter at all whether the data is compressed or not if it\ncannot be read.\n\n&gt; One way to make compression a little more workable would be to not\n&gt; compress the meatdata lines.  That way, there wouldn&#39;t be any confusion\n&gt; about the length of the block, and it would be possible to understand\n&gt; the structure of the file without using zlib.  It could (and should) be\n&gt; indicated in the header what compression style is used.  \n\nThere is no confusion about the length of the block now, since each\ncompressed member in the ARC file is a complete gzip stream: you just\ndecode it until done. The noise related to overrunning the end offset\nis a side-effect of the Java implementation of zlib, not the\ncompression format itself.\n\nOn thing you could do is take a page from the PDF format: a\ncross-reference table exists at the end of the file that gives\ninformation about the sections in the rest of the file. The same could\nbe done for ARCs, giving the start offsets of each member in the\nARC. That way a reader could quickly scan the offset table and not\nneed to worry about the overruns.\n\nThe problem with doing that is you loose the nice property that the\ncompressed ARC file can be read with the standard GZIP command-line\ntools since you&#39;ve just expanded the format. It may be the case that\nit would &quot;just work&quot;, given the offset section would lack a valid GZIP\nheader and would probably be ignored, but it would need to be tried.\n\n    tree\n\n-- \nTom Emerson                                          Basis Technology Corp.\nSoftware Architect                                 http://www.basistech.com\n  &quot;Beware the lollipop of mediocrity: lick it once and you suck forever&quot;\n\n"}}