{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"pgzrCZqiiDSjpa0IGWfAa5NMXK1kSPxpdtWd2ugQ44NBzHbWgCGPD_c7rq6ghri-xjk_LyWjv_-oT-tu0NqnQ_BFXf9bkA8","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] (Memory leak experience?) Re: Crawl rate decreasing with time?","postDate":"1311812550","msgId":7237,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRFMzBBQkM2LjEwMzA2MDFAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGowcHZkOSsxcm5jQGVHcm91cHMuY29tPg==","referencesHeader":"PGowcHZkOSsxcm5jQGVHcm91cHMuY29tPg=="},"prevInTopic":7235,"nextInTopic":0,"prevInTime":7236,"nextInTime":7238,"topicId":7213,"numMessagesInTopic":9,"msgSnippet":"The NullPointerException is likely an occurrence of this bug, fixed in the latest development builds: https://webarchive.jira.com/browse/HER-1772 It would be","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 58969 invoked from network); 28 Jul 2011 00:22:34 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m2.grp.sp2.yahoo.com with QMQP; 28 Jul 2011 00:22:34 -0000\r\nX-Received: from unknown (HELO relay00.pair.com) (209.68.5.9)\n  by mta3.grp.sp2.yahoo.com with SMTP; 28 Jul 2011 00:22:34 -0000\r\nX-Received: (qmail 93948 invoked by uid 0); 28 Jul 2011 00:22:31 -0000\r\nX-Received: from 76.218.213.38 (HELO silverbook.local) (76.218.213.38)\n  by relay00.pair.com with SMTP; 28 Jul 2011 00:22:31 -0000\r\nX-pair-Authenticated: 76.218.213.38\r\nMessage-ID: &lt;4E30ABC6.1030601@...&gt;\r\nDate: Wed, 27 Jul 2011 17:22:30 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:5.0) Gecko/20110624 Thunderbird/5.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;j0pvd9+1rnc@...&gt;\r\nIn-Reply-To: &lt;j0pvd9+1rnc@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] (Memory leak experience?) Re: Crawl rate decreasing\n with time?\r\nX-Yahoo-Group-Post: member; u=137285340; y=_DUmKFFiTMvTsBV7BFf05VcvrLaUlOQbPrEwupGKzcTZ\r\nX-Yahoo-Profile: gojomo\r\n\r\nThe NullPointerException is likely an occurrence of this bug, fixed in \nthe latest development builds:\n\nhttps://webarchive.jira.com/browse/HER-1772\n\nIt would be unrelated to any speed issues you&#39;re facing.\n\nIt&#39;s hard to comment on your speed issues. They would only be \nmemory-leak related if the Java process were growing so much, and in \ncompetition with other memory-using processes on your machine, to cause \nconstant virtual-memory swapping. That doesn&#39;t seem to be the case from \nyour screenshots, but I have more experience interpreting Linux \nmonitoring tools for that situation. (It&#39;s also not likely; I haven&#39;t \nheard of general allocation leaks in the Mac JVM, and leaks in the Java \nheap space would result an OutOfMemoryError more often than swapping.)\n\nTo diagnose further would require taking a closer look at what the Java \nthreads are doing (via the in-UI &#39;threads report&#39; or other JVM \nmonitoring tools); the status of frontier queues (via either the 1-line \nsummary on the &#39;reports&#39; page or the full frontier report); and watching \nthe machine&#39;s CPU/disk-IO/bandwidth usage over time and relative to the \nmachine&#39;s full capacity.\n\nA Mac with a single hard drive (as it appears you&#39;re using) is *not* a \ntypical or recommended setup for max-speed crawling, though it should \nwork up to a certain rate/size of crawl.\n\n- Gordon\n\nOn 7/27/11 2:19 PM, helloitsmaxine wrote:\n&gt; Sorry I just realized my last question was really vague. The null pointer exception alerts look like this:\n&gt;\n&gt; Time: \t Jul. 27, 2011 21:17:07 GMT\n&gt; Level: \t WARNING\n&gt; Message: \t\n&gt; ExtractorHTML: NullPointerException (in thread &#39;ToeThread #20: http://freemusicarchive.org/&#39;; in processor &#39;ExtractorHTML&#39;)\n&gt; Exception: \t\n&gt; java.lang.NullPointerException\n&gt; Stacktrace: java.lang.NullPointerException\n&gt; \tat org.archive.crawler.extractor.ExtractorHTML.processGeneralTag(ExtractorHTML.java:439)\n&gt; \tat org.archive.crawler.extractor.ExtractorHTML.extract(ExtractorHTML.java:667)\n&gt; \tat org.archive.crawler.extractor.ExtractorHTML.extract(ExtractorHTML.java:613)\n&gt; \tat org.archive.crawler.extractor.Extractor.innerProcess(Extractor.java:67)\n&gt; \tat org.archive.crawler.framework.Processor.process(Processor.java:109)\n&gt; \tat org.archive.crawler.framework.ToeThread.processCrawlUri(ToeThread.java:306)\n&gt; \tat org.archive.crawler.framework.ToeThread.run(ToeThread.java:154)\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, &quot;helloitsmaxine&quot;&lt;itsmaxine@...&gt;  wrote:\n&gt;&gt;\n&gt;&gt; I&#39;m still pretty stuck on this problem and am wondering if anyone in general has found memory leaks. Also what is a null pointer message in the alerts section usually indicative of? For reference I&#39;m running Heritrix 1.14.4.\n&gt;&gt;\n&gt;&gt; --- In archive-crawler@yahoogroups.com, &quot;helloitsmaxine&quot;&lt;itsmaxine@&gt;  wrote:\n&gt;&gt;&gt;\n&gt;&gt;&gt; Hey Gordon,\n&gt;&gt;&gt; I&#39;ve looked into your suggestions though nothing in the reports jumped out really readily but will continue to investigate.\n&gt;&gt;&gt;\n&gt;&gt;&gt; However I wanted to ask you about a possibility that others have been mentioning--could it be possible that this slow-down is due to a memory leak? Is there a way to test for that? Someone suggested that I stop the crawl, restart it on a fresh jvm, and see what happens. I paused the crawl, started it from the recovery log in a new jvm, and the crawl rates shot right back up to where they started. Would this be indicative of a memory leak? If so, it seems like this problem is pretty inevitable since it has happened pretty much every time without fail. Just wanted to know what you (or anyone else) thinks about this?\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks!\n&gt;&gt;&gt;\n&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@&gt;  wrote:\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Looks like you&#39;re on a 4GB Mac with a 2GB heap, so there shouldn&#39;t be a\n&gt;&gt;&gt;&gt; swapping problem.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; 250 threads may be a lot for a 2GB heap. There are no firm rules, but a\n&gt;&gt;&gt;&gt; very rough rule I&#39;ve used is to take the heap size, deduct the 60% used\n&gt;&gt;&gt;&gt; by default by the BerkeleyDB-JE-based structures, then divide the\n&gt;&gt;&gt;&gt; remaining value (800MB in your case) by ~5MB/thread to get a plausible\n&gt;&gt;&gt;&gt; thread count.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; *If* the problem is threads waiting for unresponsive hosts, then\n&gt;&gt;&gt;&gt; reducing soTimeout may help a little. The one-line reports on the\n&gt;&gt;&gt;&gt; reports page, or the longer &#39;threads report&#39;, may give a hint if this is\n&gt;&gt;&gt;&gt; the block. But you can&#39;t make soTimeout too small, there are real delays\n&gt;&gt;&gt;&gt; for busy networks/server for which you may not want to miss that URL\n&gt;&gt;&gt;&gt; entirely. It&#39;s a tradeoff you&#39;ll have to decide.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Carefully watching the crawl and aggressively removing URLs from\n&gt;&gt;&gt;&gt; unwanted/unresponsive sites (eg by adding new scope limitations during\n&gt;&gt;&gt;&gt; the crawl) may offer a better response to slowdowns due to unresponsive\n&gt;&gt;&gt;&gt; sites.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; If threads spending their time on slow/big resources are an issue,\n&gt;&gt;&gt;&gt; reducing the timeout-seconds or maximum size to download settings could\n&gt;&gt;&gt;&gt; help a little. The threads report and crawl log might indicate if this\n&gt;&gt;&gt;&gt; is a contributing factor.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; As a Mac I&#39;m guessing there&#39;s just a single hard drive. That&#39;s going to\n&gt;&gt;&gt;&gt; cap performance, with all queueing/lookups/scratch-files/content-writing\n&gt;&gt;&gt;&gt; competing for the single disk.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; When the tradeoffs associated with the Bloom option may be right for\n&gt;&gt;&gt;&gt; your project is something you&#39;ll have to weigh.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; - Gordon\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; On 7/21/11 4:41 PM, helloitsmaxine wrote:\n&gt;&gt;&gt;&gt;&gt; I&#39;m referring to what it says on the Activity Monitor, though\n&gt;&gt;&gt;&gt;&gt; admittedly I&#39;m not sure what the relevance of it is. Here are\n&gt;&gt;&gt;&gt;&gt; screencaps of the console/Activity Monitor (2 panes):\n&gt;&gt;&gt;&gt;&gt; http://img39.imageshack.us/img39/2212/ss25h36m.jpg\n&gt;&gt;&gt;&gt;&gt; http://img29.imageshack.us/img29/9120/ss25h36mcpu.jpg\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; The swap reported is 4.4gb at this point-about 25h into the\n&gt;&gt;&gt;&gt;&gt; crawl...is that a large enough amount to warrant scaling back on\n&gt;&gt;&gt;&gt;&gt; heap?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; The HTTP timeout-seconds value I have now is 1200 (that&#39;s 20\n&gt;&gt;&gt;&gt;&gt; mins...seems long?) and the sotimeout-ms is 20,000 (20s, I guess that\n&gt;&gt;&gt;&gt;&gt; makes sense). Would it help to reduce both or just the sotimeout? How\n&gt;&gt;&gt;&gt;&gt; much of a reduction would you suggest? Could I make it as low as 1\n&gt;&gt;&gt;&gt;&gt; second? Or is maybe 5-10 better?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Currently it looks like URI&#39;s crawled is still under a million,\n&gt;&gt;&gt;&gt;&gt; though I would eventually like to grow it to the tens of\n&gt;&gt;&gt;&gt;&gt; millions--would looking into the BloomUriUniqFilter be worth it at\n&gt;&gt;&gt;&gt;&gt; this point?\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Thanks for your suggestions; I really appreciate it!\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr&lt;gojomo@&gt;\n&gt;&gt;&gt;&gt;&gt; wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; What do you mean by &quot;VM size&quot;? (What tool is reporting that\n&gt;&gt;&gt;&gt;&gt;&gt; number?)\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; It would be very atypical for the heap size or JavaVM process\n&gt;&gt;&gt;&gt;&gt;&gt; address space to be 150 gigabytes.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; What is the hardware like? (RAM, CPU, disk count/speed)\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; If all the threads are working on something, and the &#39;congestion\n&gt;&gt;&gt;&gt;&gt;&gt; ratio&#39; is high, then the problem is not that there&#39;s too little\n&gt;&gt;&gt;&gt;&gt;&gt; that&#39;s eligible to crawl politely.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Some top possibilities:\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; � Java process size has grown larger than RAM and excessive\n&gt;&gt;&gt;&gt;&gt;&gt; swapping is occurring. Due to Java&#39;s pattern of memory access, you\n&gt;&gt;&gt;&gt;&gt;&gt; essentially never want to be using swap. If &#39;top&#39; or &#39;vmstat&#39; show\n&gt;&gt;&gt;&gt;&gt;&gt; any swap being used, add RAM or scale back the heap so that it\n&gt;&gt;&gt;&gt;&gt;&gt; isn&#39;t.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; � most threads are making fetches against unresponsive sites, which\n&gt;&gt;&gt;&gt;&gt;&gt; can take a long time to timeout. Large crawls that hit giant\n&gt;&gt;&gt;&gt;&gt;&gt; link-lists to defunct/fake sites can experience this. More threads,\n&gt;&gt;&gt;&gt;&gt;&gt; shortening the FetchHTTP soTimeout, and manually pruning the bad\n&gt;&gt;&gt;&gt;&gt;&gt; URIs can help.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; � the crawl has grown so large that accesses to the\n&gt;&gt;&gt;&gt;&gt;&gt; data-structures which overflow to disk (notably the default\n&gt;&gt;&gt;&gt;&gt;&gt; &#39;already-seen&#39; BdbUriUniqFilter) are now dominating its time usage.\n&gt;&gt;&gt;&gt;&gt;&gt; On broader and larger crawls -- expected to grow to more than a few\n&gt;&gt;&gt;&gt;&gt;&gt; tens of millions of URIs -- you may want to consider the\n&gt;&gt;&gt;&gt;&gt;&gt; BloomUriUniqFilter, though it has other RAM costs and imprecision\n&gt;&gt;&gt;&gt;&gt;&gt; caveats.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Hope this helps,\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; - Gordon\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; On 7/21/11 3:59 PM, helloitsmaxine wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Some other details I&#39;ve noticed are that according to the\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; activity monitor, the VM size is about 150gb, and CPU usage\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; during the crawl is typically very low, with over 95% idle.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Is that weird? If there is so much CPU and VM available, would\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; it just make sense to keep increasing memory allocation and #\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; threads to keep crawl rates up? Anyone have experience with this\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; sort of thing?\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com,\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; &quot;helloitsmaxine&quot;&lt;itsmaxine@&gt;    wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On my crawls I&#39;ve noticed that they generally have been\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; starting out at pretty good rates, ie. 1500kb/s, but then after\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; a few hours, this goes down to 0-50kb/s and pretty much stays\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; there.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; After reading about some other people&#39;s same problems I tried a\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; few tweaks: - increasing JVM memory to 1024 - increasing #\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; threads from 50 to 100 - increasing in/out recording buffers\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; However the problem still persists.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Some information about my crawl is: - using Heritrix 1.4 - the\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; max heap is being used (ie. current and max are the same) - the\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; threads are all pretty much active (99-100 out of 100 active at\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; a time) - very high congestion ratio, (ie. 7000) - so far it\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; has crawled 674003 URI&#39;s in about 20h.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Does anyone have insight into how I can prevent this problem?\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I&#39;ve read this:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/making+a+busy+crawl+go+faster\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; and a few other posts but am not sure what applies to my situation and\n&gt;&gt;&gt;&gt;&gt;&gt; what would be the best next steps to take. Or if there&#39;s any other\n&gt;&gt;&gt;&gt;&gt;&gt; information that might be helpful let me know!\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks!\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}