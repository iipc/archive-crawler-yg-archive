{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":163406187,"authorName":"Kristinn Sigur√∞sson","from":"=?iso-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;","profile":"kristsi25","replyTo":"LIST","senderId":"AGxrQIw4394_rz5hoGAvaCjiSnu4Ff96a6-j6RTR-txd8VFw0_OCwfDEKGD10AzIj0xEK91Dmtz0HKByybOd1deJp0r6CbaWCSnJvUpVjP6XJ930YyBCkncGXV7ZhZdB","spamInfo":{"isSpam":false,"reason":"0"},"subject":"RE: [archive-crawler] Advice needed on how to (properly) structure new Heritrix modify and delete functionality","postDate":"1153297207","msgId":3084,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDYwNzE5MDgyMDE0LjlCRDQ3MTQxNTZBOTZAbWFpbC5hcmNoaXZlLm9yZz4=","inReplyToHeader":"PDQ0QkQzNDQ0LjkwNTAzMDZAYXJjaGl2ZS5vcmc+"},"prevInTopic":3080,"nextInTopic":3085,"prevInTime":3083,"nextInTime":3085,"topicId":3063,"numMessagesInTopic":32,"msgSnippet":"... Just out of curiosity, what kind of timeframe are you looking at? ... Sounds like the DeDuplicator on steroids. Good stuff. Any thoughts on how to rule","rawEmail":"Return-Path: &lt;kris@...&gt;\r\nX-Sender: kris@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 84885 invoked from network); 19 Jul 2006 08:21:46 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m34.grp.scd.yahoo.com with QMQP; 19 Jul 2006 08:21:46 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.227.188)\n  by mta1.grp.scd.yahoo.com with SMTP; 19 Jul 2006 08:21:46 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id A6FFF14156B42\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 19 Jul 2006 01:20:15 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 07814-01-9 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 19 Jul 2006 01:20:15 -0700 (PDT)\r\nReceived: from FORRITUN1 (forritun-1.bok.hi.is [130.208.152.80])\n\tby mail.archive.org (Postfix) with ESMTP id 9BD4714156A96\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 19 Jul 2006 01:20:14 -0700 (PDT)\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nDate: Wed, 19 Jul 2006 08:20:07 -0000\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Microsoft Office Outlook, Build 11.0.5510\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.2869\r\nThread-Index: AcaqowUvd/juCTN1TJu7roLjPwlkqQAaA5rA\r\nIn-Reply-To: &lt;44BD3444.9050306@...&gt;\r\nMessage-Id: &lt;20060719082014.9BD4714156A96@...&gt;\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: =?iso-8859-1?Q?Kristinn_Sigur=F0sson?= &lt;kris@...&gt;\r\nSubject: RE: [archive-crawler] Advice needed on how to (properly) structure new Heritrix modify and delete functionality\r\nX-Yahoo-Group-Post: member; u=163406187; y=5TWT4LtH1elOgz6JPtL8c0Ux_v45nI83EIPwAkm3h__HCSTy\r\nX-Yahoo-Profile: kristsi25\r\n\r\n \n\n&gt; -----Original Message-----\n&gt; From: archive-crawler@yahoogroups.com \n&gt; =\r\n[mailto:archive-crawler@yahoogroups.com] On Behalf Of Michael Stack\n&gt; Sent:=\r\n 18. j=FAl=ED 2006 19:20\n&gt; To: archive-crawler@yahoogroups.com\n&gt; Subject: R=\r\ne: [archive-crawler] Advice needed on how to \n&gt; (properly) structure new He=\r\nritrix modify and delete functionality\n&gt; \n&gt; We&#39;ve been talking here at the =\r\nArchive on how to broach the \n&gt; topics under discussion here: duplicate/nea=\r\nr-duplicate \n&gt; detection, change-detection, junk-avoidance, etc. Here are a=\r\n \n&gt; few notes on what we&#39;ve been thinking. \n&gt; They pertain to Karl and Kris=\r\n&#39;s discussion so I&#39;ll add them \n&gt; in here under the same subject (Implement=\r\nation is probably \n&gt; too far off to solve Karl&#39;s needs).\n\nJust out of curio=\r\nsity, what kind of timeframe are you looking at?\n\n&gt; As per Kris&#39;s experienc=\r\ne, crawl-time duplicate/change \n&gt; detection is hard. We&#39;re thinking, like K=\r\nris, that at first, \n&gt; we&#39;ll do this by post-processing crawls. The product=\r\n of \n&gt; post-processing will be distilled data-structures that the \n&gt; crawle=\r\nr can do fast lookups against as its crawling to ask \n&gt; such questions as: =\r\n&quot;Is this a duplicate?&quot;; &quot;Is this a near \n&gt; duplicate?&quot;; &quot;Is this page junk?=\r\n&quot;; &quot;Is this page \n&gt; &#39;important&#39;?&quot;; and &quot;Has this page changed?&quot;. Or, more l=\r\nikely, \n&gt; we&#39;ll aggregate all URL evaluation &#39;plugins&#39; and a crawler \n&gt; wil=\r\nl simply ask, &quot;Should I crawl this?&quot; and get back an \n&gt; &#39;evaluation&#39; that f=\r\nalls somewhere between 0 and 1 with a \n&gt; configurable threshold in the craw=\r\nler saying what level of \n&gt; evaluations it&#39;ll pursue.\n\nSounds like the DeDu=\r\nplicator on steroids. Good stuff. Any thoughts on how to\nrule things as jun=\r\nk? That might be one of the most interesting applications\nof this.\n\n&gt; We&#39;re=\r\n thinking we need to build a pluggable bulk-processing \n&gt; infrastructure to=\r\n do the post-crawl analysis and we&#39;ll need \n&gt; to build a fast lookup servic=\r\ne to field crawl-time queries.\n&gt; \n&gt; For the bulk-processing platform, hadoo=\r\np -- \n&gt; lucene.apache.org/hadoop -- and parts of nutch are what we&#39;re \n&gt; co=\r\nnsidering. We&#39;ve experience using both in our nutchwax, \n&gt; building full-te=\r\nxt indices out of ARCs. For example, \n&gt; evaluating exact duplicates, mirror=\r\n-detection, and \n&gt; rate-of-change detection could be done as mapreduce jobs=\r\n \n&gt; written to run on hadoop where the ingest is the Heritrix \n&gt; crawl.log.=\r\n Also, Nutch has mapreduce jobs we&#39;ll want to \n&gt; model/exploit. Nutch has j=\r\nobs to strip markup from HTML (or \n&gt; PDF, etc.), extract anchor text, calcu=\r\nlate inlinks, and it \n&gt; keeps a running database of all URLs ever seen each=\r\n of which \n&gt; could serve as input to mapreduce jobs that evaluate \n&gt; &#39;impor=\r\ntance&#39;, &#39;change&#39;, or whether or not a page is &#39;junk&#39;. \n&gt; Nutch won&#39;t work f=\r\nor us out of the box -- for instance, we&#39;ll \n&gt; need to make the extractors =\r\nused by Heritrix sync with the \n&gt; extractors used by Nutch in post-processi=\r\nng and we&#39;ll need to \n&gt; add notions of &#39;history&#39; to the crawl database -- b=\r\nut its a start.\n&gt; \n&gt; For the lookup service, we&#39;re thinking it&#39;ll have to b=\r\ne able \n&gt; to service a cluster of crawlers. Bdbje probably won&#39;t be \n&gt; fast=\r\n enough. Kris&#39;s lucene experiment looks interesting. We \n&gt; also need to loo=\r\nk at hashing External Memory Alogrithms \n&gt; similar to the one discussed in =\r\nthe Mercator paper.\n\nLucene seems to scale fine to at least 10 million URLs=\r\n. Above that (in my\nexperience) I/O becomes an issue, especially as it star=\r\nts to contend with\nthe BdbFrontier&#39;s datastructures.\n\n&gt; Blue-skying it, if =\r\nwe could agree on a bulk processing \n&gt; platform, such as hadoop, and if we =\r\ncould agree on how to \n&gt; package the fast-lookups -- e.g. \n&gt; all implement =\r\na Lookup Interface that takes a (shrunken?) \n&gt; CrawlURI returning an object=\r\n that had a &#39;score&#39; and \n&gt; &#39;rationale&#39; -- then it seems like we could colla=\r\nborate/split-the-work.\n\nJust how big do you expect the indexes to get? Also=\r\n, I&#39;ve purposely gone\nafter &#39;high yield&#39; documents (i.e. non text) that ser=\r\nves to limit the number\nof entries in the index and the number of lookups a=\r\nt crawl time. I take it\nyou are looking for a more complete solution?\n\n- Kr=\r\nis\n\n\n"}}