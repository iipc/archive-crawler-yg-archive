{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":200981851,"authorName":"Thirumalai Veerasamy","from":"&quot;Thirumalai Veerasamy&quot; &lt;thirumalai.veerasamy@...&gt;","profile":"thirumalaikv","replyTo":"LIST","senderId":"p_3LTWU38bfPs286KZG1AwooFbvpJP-xucJOiC3-KMJQ5zbQMQpvBPLEsBgRrtqIn-dZkUlvPEXCiyg5ND3BhklV-cV6ljUs-dfmsN3aZyaO_YT1TK50CADWYYm6Qw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"How do I Change crawler behaviour?","postDate":"1140723523","msgId":2706,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGYxZGExYjhhMDYwMjIzMTEzOGk0MzJkNmQ5ZWlmNjBlMTYwOWE3MjRlNGZjQG1haWwuZ21haWwuY29tPg=="},"prevInTopic":0,"nextInTopic":2708,"prevInTime":2705,"nextInTime":2707,"topicId":2706,"numMessagesInTopic":5,"msgSnippet":"Hi, I wanted to crawl through my entire web application. The reasons 1. Test for broken link 2. Record the crawling as script and play it again 3. Find out the","rawEmail":"Return-Path: &lt;thirumalai.veerasamy@...&gt;\r\nX-Sender: thirumalai.veerasamy@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 60028 invoked from network); 23 Feb 2006 19:38:46 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m30.grp.scd.yahoo.com with QMQP; 23 Feb 2006 19:38:46 -0000\r\nReceived: from unknown (HELO uproxy.gmail.com) (66.249.92.205)\n  by mta10.grp.scd.yahoo.com with SMTP; 23 Feb 2006 19:38:46 -0000\r\nReceived: by uproxy.gmail.com with SMTP id u2so58296uge\n        for &lt;archive-crawler@yahoogroups.com&gt;; Thu, 23 Feb 2006 11:38:43 -0800 (PST)\r\nReceived: by 10.66.248.15 with SMTP id v15mr2410737ugh;\n        Thu, 23 Feb 2006 11:38:43 -0800 (PST)\r\nReceived: by 10.67.19.8 with HTTP; Thu, 23 Feb 2006 11:38:43 -0800 (PST)\r\nMessage-ID: &lt;f1da1b8a0602231138i432d6d9eif60e1609a724e4fc@...&gt;\r\nDate: Thu, 23 Feb 2006 14:38:43 -0500\r\nTo: archive-crawler@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Thirumalai Veerasamy&quot; &lt;thirumalai.veerasamy@...&gt;\r\nSubject: How do I Change crawler behaviour?\r\nX-Yahoo-Group-Post: member; u=200981851; y=x_lKlg2zRL3QIMxIkz-ZGBFHBUo4CNguX1T_N0bLgXEfJjZIvwT2\r\nX-Yahoo-Profile: thirumalaikv\r\n\r\nHi,\n\n   I wanted to crawl through my entire web application.\n\n  The reasons=\r\n\n\n  1. Test for broken link\n  2. Record the crawling as script and play it =\r\nagain\n  3. Find out the elapsed time for individual pages\n\n  I wrote a cust=\r\nom component using apache httpconnection. Now I wanted\nto save each web pag=\r\ne, so that I can go through individual page and\nverify whether the page is =\r\ndisplay what it is supposed to display.\n\n  I found archive crawler can be u=\r\nsed to save the pages using\nMirrorWriterProcessor. I also feel that archive=\r\n crawler is bit heavy\napp to crawl just 200-400 pages of my application.\n\n =\r\n I have few more questions before I can go deep into this crawler.\n\n  1. Ca=\r\nn I skip certain pages?\n      i I want to crawl only urls in that particula=\r\nr domain.\n(http://localhost),\n      ii. skip urls which satisfies some regu=\r\nlar expression.\n      iii remove some paramter from the url and check wheth=\r\ner it is\nalready crawled (the web app add a paramter which doesn&#39;t change t=\r\nhe\nbehaviour of the page, but creates different urls for the same page\nwhic=\r\nh I don&#39;t want to crawl).\n   2. Find out the elapsed time for each page.\n  =\r\n 3. Save html, images, etc., into a folder only if it is not already\ndownlo=\r\naded.\n   4. Support for proxy\n\n  Please let me know your inputs.\n\n   If the=\r\nse questions are already answered, pleaes let me know the\nfaq/help section.=\r\n\n\nRegards\nThiru\n\n"}}