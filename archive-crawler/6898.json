{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":328606787,"authorName":"Maciej Grela","from":"Maciej Grela &lt;maciej.grela@...&gt;","replyTo":"LIST","senderId":"gEnQX39CcoXhI0tWmvphKh3qtqY0btxAjsJruL6zEGHW5_hP9mysf__TarG0mpLW672cSwCpYqlpl74wazfA2Wx9RONMw_epHRPBrw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Downloading PDF files from a site using heritrix 3.0","postDate":"1292110130","msgId":6898,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEFBTkxrVGlrOHRtVjE5PUgxT3RLcUpaS0dGd18rd3NiU0NfdV9CaENjT21OMEBtYWlsLmdtYWlsLmNvbT4=","inReplyToHeader":"PDREMDE3NkI3LjcwNzAxMDdAYXJjaGl2ZS5vcmc+","referencesHeader":"PEFBTkxrVGlrcENNWlZpUXlUZVQ1bVZRcl9jbWcxSjdKcjJ1OWNCdkJIUjRuZEBtYWlsLmdtYWlsLmNvbT4JPDRDRkVBMDc1LjQwNTAxMDJAYXJjaGl2ZS5vcmc+CTxBQU5Ma1RpbjlTN04ycz1DVkZSdXRucFhaWnNRUjRoWVlzR2hWNDhTRC01Q3JAbWFpbC5nbWFpbC5jb20+CTw0RDAxNzZCNy43MDcwMTA3QGFyY2hpdmUub3JnPg=="},"prevInTopic":6890,"nextInTopic":6901,"prevInTime":6897,"nextInTime":6899,"topicId":6866,"numMessagesInTopic":7,"msgSnippet":"2010/12/10 Gordon Mohr  ... and crawl.log). ... Where to check for timeouts ? Haven t found any hints of them happening in the logfiles","rawEmail":"Return-Path: &lt;maciej.grela@...&gt;\r\nX-Sender: maciej.grela@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 86907 invoked from network); 11 Dec 2010 23:28:53 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m3.grp.sp2.yahoo.com with QMQP; 11 Dec 2010 23:28:53 -0000\r\nX-Received: from unknown (HELO mail-qw0-f50.google.com) (209.85.216.50)\n  by mta2.grp.sp2.yahoo.com with SMTP; 11 Dec 2010 23:28:52 -0000\r\nX-Received: by qwd6 with SMTP id 6so5347410qwd.37\n        for &lt;archive-crawler@yahoogroups.com&gt;; Sat, 11 Dec 2010 15:28:52 -0800 (PST)\r\nMIME-Version: 1.0\r\nX-Received: by 10.229.99.84 with SMTP id t20mr2176038qcn.120.1292110130630; Sat,\n 11 Dec 2010 15:28:50 -0800 (PST)\r\nX-Received: by 10.229.128.31 with HTTP; Sat, 11 Dec 2010 15:28:50 -0800 (PST)\r\nIn-Reply-To: &lt;4D0176B7.7070107@...&gt;\r\nReferences: &lt;AANLkTikpCMZViQyTeT5mVQr_cmg1J7Jr2u9cBvBHR4nd@...&gt;\n\t&lt;4CFEA075.4050102@...&gt;\n\t&lt;AANLkTin9S7N2s=CVFRutnpXZZsQR4hYYsGhV48SD-5Cr@...&gt;\n\t&lt;4D0176B7.7070107@...&gt;\r\nDate: Sun, 12 Dec 2010 00:27:50 +0059\r\nMessage-ID: &lt;AANLkTik8tmV19=H1OtKqJZKGFw_+wsbSC_u_BhCcOmN0@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nContent-Type: multipart/mixed; boundary=00163628531458d60b04972ad560\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Maciej Grela &lt;maciej.grela@...&gt;\r\nSubject: Re: [archive-crawler] Downloading PDF files from a site using\n heritrix 3.0\r\nX-Yahoo-Group-Post: member; u=328606787\r\n\r\n\r\n--00163628531458d60b04972ad560\r\nContent-Type: multipart/alternative; boundary=00163628531458d5dc04972ad55e\r\n\r\n\r\n--00163628531458d5dc04972ad55e\r\nContent-Type: text/plain; charset=UTF-8\r\n\r\n2010/12/10 Gordon Mohr &lt;gojomo@...&gt;\n\n&gt;\n&gt;\n&gt; Did your crawl of the site finish cleanly? (I just tried a test crawl of\n&gt; the same site, and many of the fetch attempts are timing out -- which\n&gt; could be network/server issues, or the site&#39;s own attempts to slow/block\n&gt; crawling.)\n&gt;\n&gt; Yes.\n\n&gt;  Are there any errors in the crawl.log or nonfatal-errors.log that refer\n&gt; to the URIs of interest?\n&gt;\n&gt;  No. To be sure I&#39;ve ran the crawl again today (attached crawler-beans.xml\nand crawl.log).\n\n\n\n&gt;  http://bip.kprm.gov.pl/g2/2010_11/3678_fileot.pdf\n&gt;\n&gt; However, currently many timeouts on page fetches are preventing my test\n&gt; crawl from reaching that PDF. Perhaps the same thing is happening for\n&gt; your crawls, and the proper fix is to both make the crawler more\n&gt; patient, and wait longer for the crawl to finish.\n&gt;\n&gt;\nWhere to check for timeouts ? Haven&#39;t found any hints of them happening in\nthe logfiles generated for this crawl.\n\n\n\n&gt;  So, you may want to adjust the FetchHTTP &#39;soTimeoutMs&#39; setting, to wait\n&gt; longer than 20000ms for a socket reply. (I&#39;ve increased it to 60000 and\n&gt; I&#39;m still seeing timeouts form the site.) Setting &#39;timeoutSeconds&#39; to\n&gt; more than 1200 (20 minutes) may also be necessary.\n&gt;\n&gt; Hope this helps,\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt;\n\nTried to run with timeoutSeconds to 2400 and soTimeoutMs to 60000 but the\nresults are the same.\n\nBest regards,\nMaciej Grela\n\r\n--00163628531458d5dc04972ad55e\r\nContent-Type: text/html; charset=UTF-8\r\nContent-Transfer-Encoding: quoted-printable\r\n\r\n2010/12/10 Gordon Mohr &lt;span dir=3D&quot;ltr&quot;&gt;&lt;&lt;a href=3D&quot;mailto:gojomo@archi=\r\nve.org&quot; target=3D&quot;_blank&quot;&gt;gojomo@...&lt;/a&gt;&gt;&lt;/span&gt;&lt;br&gt;&lt;div class=\r\n=3D&quot;gmail_quote&quot;&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin: 0pt 0pt=\r\n 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;&quot;&gt;=\r\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;div style=3D&quot;background-color: rgb(255, 255, 255);&quot;&gt;\n&lt;span=\r\n&gt;=C2=A0&lt;/span&gt;\n\n\n&lt;div&gt;\n  &lt;div&gt;\n\n\n    &lt;div&gt;\n      \n      \n      &lt;p&gt;Did your =\r\ncrawl of the site finish cleanly? (I just tried a test crawl of &lt;br&gt;\nthe sa=\r\nme site, and many of the fetch attempts are timing out -- which &lt;br&gt;\ncould =\r\nbe network/server issues, or the site&#39;s own attempts to slow/block &lt;br&gt;=\r\n\ncrawling.)&lt;br&gt;\n&lt;br&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;Yes. &lt;br&gt;=\r\n&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quote&quot; style=3D&quot;margin: 0pt 0pt 0pt 0.8ex;=\r\n border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;&quot;&gt;&lt;div style=\r\n=3D&quot;background-color: rgb(255, 255, 255);&quot;&gt;\n\n&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;\nAre there a=\r\nny errors in the crawl.log or nonfatal-errors.log that refer &lt;br&gt;\nto the UR=\r\nIs of interest?&lt;br&gt;\n&lt;br&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;=C2=\r\n=A0No. To be sure I&#39;ve ran the crawl again today (attached crawler-bean=\r\ns.xml and crawl.log).&lt;br&gt;&lt;br&gt;&lt;/div&gt;=C2=A0&lt;br&gt;&lt;blockquote class=3D&quot;gmail_quo=\r\nte&quot; style=3D&quot;margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204=\r\n, 204); padding-left: 1ex;&quot;&gt;\n\n&lt;div style=3D&quot;background-color: rgb(255, 255,=\r\n 255);&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;\n&lt;a href=3D&quot;http://bip.kprm.gov.pl/g2/2010_11/3=\r\n678_fileot.pdf&quot; target=3D&quot;_blank&quot;&gt;http://bip.kprm.gov.pl/g2/2010_11/3678_fi=\r\nleot.pdf&lt;/a&gt;&lt;br&gt;\n&lt;br&gt;&lt;/div&gt;\nHowever, currently many timeouts on page fetche=\r\ns are preventing my test &lt;br&gt;\ncrawl from reaching that PDF. Perhaps the sam=\r\ne thing is happening for &lt;br&gt;\nyour crawls, and the proper fix is to both ma=\r\nke the crawler more &lt;br&gt;\npatient, and wait longer for the crawl to finish.&lt;=\r\nbr&gt;\n&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;br&gt;Where to check for ti=\r\nmeouts ? Haven&#39;t found any hints of them happening in the logfiles gene=\r\nrated for this crawl.&lt;br&gt;&lt;br&gt;=C2=A0&lt;br&gt;&lt;/div&gt;&lt;blockquote class=3D&quot;gmail_quo=\r\nte&quot; style=3D&quot;margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204=\r\n, 204); padding-left: 1ex;&quot;&gt;\n\n&lt;div style=3D&quot;background-color: rgb(255, 255,=\r\n 255);&quot;&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;\nSo, you may want to adjust the FetchHTTP &#39;soTim=\r\neoutMs&#39; setting, to wait &lt;br&gt;\nlonger than 20000ms for a socket reply. (=\r\nI&#39;ve increased it to 60000 and &lt;br&gt;\nI&#39;m still seeing timeouts form =\r\nthe site.) Setting &#39;timeoutSeconds&#39; to &lt;br&gt;\nmore than 1200 (20 minu=\r\ntes) may also be necessary.&lt;br&gt;\n&lt;br&gt;\nHope this helps,&lt;br&gt;\n&lt;br&gt;\n- Gordon @ I=\r\nA&lt;div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;=\r\n&lt;br&gt;&lt;/div&gt;&lt;br&gt;Tried to run with timeoutSeconds to 2400 and soTimeoutMs to 6=\r\n0000 but the results are the same.&lt;br&gt;&lt;br&gt;Best regards,&lt;br&gt;\n\nMaciej Grela&lt;b=\r\nr&gt;&lt;br&gt;\n\r\n--00163628531458d5dc04972ad55e--\r\n\n\r\n--00163628531458d60b04972ad560\r\nContent-Type: text/plain; charset=US-ASCII; name=&quot;crawl.log&quot;\r\nContent-Disposition: attachment; filename=&quot;crawl.log&quot;\r\nX-Attachment-Id: f_ghl2ka641\r\n\r\n[ Attachment content not displayed ]\r\n--00163628531458d60b04972ad560\r\nContent-Type: application/octet-stream; name=&quot;crawler-beans.cxml&quot;\r\nContent-Disposition: attachment; filename=&quot;crawler-beans.cxml&quot;\r\nX-Attachment-Id: f_ghl1uzyh0\r\n\r\n[ Attachment content not displayed ]\r\n--00163628531458d60b04972ad560--\r\n\n"}}