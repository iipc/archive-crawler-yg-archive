{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":214587980,"authorName":"Christian Kohlschuetter","from":"Christian Kohlschuetter &lt;ck-heritrix@...&gt;","replyTo":"LIST","senderId":"fdDC_HOtJOStQZqiZzrP_Yi1oIiD6Ww9m7uEpJScI8qVFtLWwN4ODjNG0f3hNQxS-zcBfdvsGFz2dov7RDJP1Ub3GdW-w2qp3gkyzSItsco3OuLoYYBT8A","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Optimize URL scheduling?","postDate":"1111139748","msgId":1671,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDUwMzE4MTA1NS40ODQ0MC5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4=","inReplyToHeader":"PDQyMzlFOUI5LjYwMjAyMDhAYXJjaGl2ZS5vcmc+","referencesHeader":"PDIwMDUwMzE3MTQzNC41MTgzMC5jay1oZXJpdHJpeEBuZXdzY2x1Yi5kZT4gPDQyMzlFOUI5LjYwMjAyMDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":1667,"nextInTopic":1672,"prevInTime":1670,"nextInTime":1672,"topicId":1666,"numMessagesInTopic":4,"msgSnippet":"... Yes, in the worst case, things are not accelerated (but it should never get measurably slower than with the current approach). I think the big advantage ","rawEmail":"Return-Path: &lt;ck-heritrix@...&gt;\r\nX-Sender: ck-heritrix@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 33720 invoked from network); 18 Mar 2005 09:57:15 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m14.grp.scd.yahoo.com with QMQP; 18 Mar 2005 09:57:15 -0000\r\nReceived: from unknown (HELO mail.newsclub.de) (130.75.2.42)\n  by mta4.grp.scd.yahoo.com with SMTP; 18 Mar 2005 09:57:14 -0000\r\nReceived: (qmail 7966 invoked by uid 2002); 18 Mar 2005 09:57:13 -0000\r\nReceived: from ck-heritrix@... by nhf3.rrzn.uni-hannover.de by uid 207 with qmail-scanner-1.21 \n (clamscan: 0.67. spamassassin: 2.63.  Clear:RC:0(130.75.87.112):SA:0(-2.6/5.0):. \n Processed in 1.809702 secs); 18 Mar 2005 09:57:13 -0000\r\nX-Spam-Status: No, hits=-2.6 required=5.0\r\nReceived: from pc112.l3s.uni-hannover.de (HELO mail.newsclub.de) (webmail@...@130.75.87.112)\n  by nhf3.rrzn.uni-hannover.de with RC4-MD5 encrypted SMTP; 18 Mar 2005 09:57:11 -0000\r\nOrganization: NewsClub\r\nTo: archive-crawler@yahoogroups.com\r\nDate: Fri, 18 Mar 2005 10:55:48 +0100\r\nUser-Agent: KMail/1.7.2\r\nReferences: &lt;200503171434.51830.ck-heritrix@...&gt; &lt;4239E9B9.6020208@...&gt;\r\nIn-Reply-To: &lt;4239E9B9.6020208@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n  charset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nMessage-Id: &lt;200503181055.48440.ck-heritrix@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0\r\nFrom: Christian Kohlschuetter &lt;ck-heritrix@...&gt;\r\nSubject: Re: [archive-crawler] Optimize URL scheduling?\r\nX-Yahoo-Group-Post: member; u=214587980\r\n\r\nOn Thursday 17 March 2005 21:34, stack wrote:\n&gt; Christian Kohlschuetter wro=\r\nte:\n&gt; &gt; Dear all,\n&gt; &gt; ...\n&gt; &gt;\n&gt; &gt; I think, decoupling the crawling and link=\r\n-enqueueing stages (=3D making\n&gt; &gt; them run\n&gt; &gt; in parallel), could reduce =\r\nthe overall delay and improve crawling\n&gt; &gt; performance: Couldn&#39;t the Postse=\r\nlector simply enqueue all extracted Links\n&gt; &gt; (including duplicates -- ie. =\r\nno deduping overhead) to a Thread-Local,\n&gt; &gt; disk-backed queue (&quot;postproces=\r\ns-queue&quot;)? Another Thread/pool of\n&gt; &gt; Threads would\n&gt; &gt; then dequeue items =\r\nfrom that queue and try to schedule them (doing the\n&gt; &gt; already-seen test a=\r\nnd enqueue them into the ready-queue).\n&gt;\n&gt; Its an idea worth trying.  We ma=\r\ny just end up moving the traffic jam to\n&gt; a new location.  On the other han=\r\nd, there may be some rough balance\n&gt; struck -- between toethread pool and d=\r\nequeuing pool -- where overall\n&gt; throughput is raised (I&#39;ve added your sugg=\r\nestion Christian to the RFE\n&gt; that covers the phenomeon you describe: &#39;Thre=\r\nads underutilized: decrease\n&gt; frontier lock contention&#39;:\n&gt; http://sourcefor=\r\nge.net/tracker/?group_id=3D73833&atid=3D539099&func=3Ddetail&aid=3D\n&gt;965395=\r\n).\n&gt;\n&gt; St.Ack\n\nYes, in the worst case, things are not accelerated (but it s=\r\nhould never get \nmeasurably slower than with the current approach). I think=\r\n the big advantage \nhere is due to the fact that broad crawls generate an e=\r\nxponentially growing \namount of new pages which are added to the ready-queu=\r\ne -- the ready-queue \nwill always have enough pages to fetch, even if the p=\r\nostprocess-queue is \nflushed/dequeued only from time to time. Of course in =\r\nthe beginning, when \nonly a few URLs are in the ready-queue, the postproces=\r\ns-queue has to be \nflushed more often than later on (but for breadth-first =\r\ncrawling, this is a \nspecial case only).\n\nChristian\n-- \nChristian Kohlsch=\r\n=FCtter\nmailto: ck -at- NewsClub.de\n\n"}}