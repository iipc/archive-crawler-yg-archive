{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":500983475,"authorName":"david_pane1","from":"&quot;david_pane1&quot; &lt;dpane@...&gt;","profile":"david_pane1","replyTo":"LIST","senderId":"c52AeAtfa5BIhugRmX59JrBx0SNCCXX2KIAyRlMozjXYLw-iLJe2cgw-cpe0n0oOJbOKw182-rTivSqPOBu7I-upKoMeYDc","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: questions before we restart the crawl","postDate":"1327678325","msgId":7578,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGpmdWcxbCs4ZzF2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDRGMjIzRkE5LjUwNjA4MDBAZ21haWwuY29tPg=="},"prevInTopic":7574,"nextInTopic":7582,"prevInTime":7577,"nextInTime":7579,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"Kenji, You understood correctly 25M pages/day for 5 machines.  Right now, with three instances running, we are seeing around 17M pages per day.  This makes me","rawEmail":"Return-Path: &lt;dpane@...&gt;\r\nX-Sender: dpane@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 44684 invoked from network); 27 Jan 2012 15:32:06 -0000\r\nX-Received: from unknown (98.137.34.45)\n  by m8.grp.sp2.yahoo.com with QMQP; 27 Jan 2012 15:32:06 -0000\r\nX-Received: from unknown (HELO ng11-ip2.bullet.mail.bf1.yahoo.com) (98.139.165.88)\n  by mta2.grp.sp2.yahoo.com with SMTP; 27 Jan 2012 15:32:06 -0000\r\nX-Received: from [98.139.164.126] by ng11.bullet.mail.bf1.yahoo.com with NNFMP; 27 Jan 2012 15:32:06 -0000\r\nX-Received: from [69.147.65.150] by tg7.bullet.mail.bf1.yahoo.com with NNFMP; 27 Jan 2012 15:32:06 -0000\r\nX-Received: from [98.137.34.73] by t7.bullet.mail.sp1.yahoo.com with NNFMP; 27 Jan 2012 15:32:05 -0000\r\nDate: Fri, 27 Jan 2012 15:32:05 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;jfug1l+8g1v@...&gt;\r\nIn-Reply-To: &lt;4F223FA9.5060800@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;david_pane1&quot; &lt;dpane@...&gt;\r\nSubject: Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=500983475; y=pLj76Q3l827iW5GnlRJuLXWQUE-2eKlC-horJxr2JE3xOND0eunCbA\r\nX-Yahoo-Profile: david_pane1\r\n\r\n\n\nKenji,\n\nYou understood correctly 25M pages/day for 5 machines.  Right now=\r\n, with three instances running, we are seeing around 17M pages per day.  Th=\r\nis makes me think that we may be saturating our network throughput.  \n\nBut,=\r\n we have seen, at the beginning of the crawl, 250-300 URIs/second (an avera=\r\nge of 52M pages/day over the first 3 days of the crawl. Once we get past th=\r\ne first few days, the crawl slows.  During mid crawl, we found that stoppin=\r\ng the crawl and restarting it improves the throughput but never back to 50M=\r\n pages/day. \n\nHow many queues do you need active per thread?  Wouldn&#39;t Heri=\r\ntrix activate more queues if there are enough threads to handle them?  We c=\r\nertainly have a lot of queues.\n\nAs an example, on one instance we are seein=\r\ng a rate of 93.25 URIs/sec average.\n\nLoad:\n1176 active of 1176 threads; 3,4=\r\n56.5 congestion ration 13688365 deepest queue; 62 average depth\n\nThreads:\n1=\r\n176 threads: 1176 ABOUT_TO_BEGIN_PROCESSOR; 911 warcWriter, 262 fetchHttp, =\r\n2 candidates, 1 extractorHtml\n\nFrontier:\n\nRUN 12955728 URI queues: 4302 act=\r\nive (1200 in-process; 994 ready; 2108 snoozed); 11429799 inactive; 0 inelig=\r\nible; 0 retired; 1521627 exhausted.\n\n\n--David\n\n\n--- In archive-crawler@yaho=\r\nogroups.com, Kenji Nagahashi &lt;knagahashi@...&gt; wrote:\n&gt;\n&gt; David,\n&gt; \n&gt; I unde=\r\nrstood &quot;25M + page/day&quot; was total for 5 machines, as you wrote \n&gt; &quot;(these n=\r\numbers are totals of all 5 instances combined)&quot;. Did you mean \n&gt; 25M+ page/=\r\nday/instance? If so, my &quot;100 threads&quot; comment is pointless. \n&gt; Please disre=\r\ngard it.\n&gt; \n&gt; I&#39;m running broad crawl that captures everything linked: imag=\r\nes, script, \n&gt; CSS, PDF, Excel, ... even mpeg4 videos. Our Heritrix 3 runs =\r\non 8GB \n&gt; memory + 4 core virtual machine (KVM), with 100 threads. it goes =\r\n\n&gt; ~60URI/s on average (per instance). Probably we could go as high as 150 =\r\n\n&gt; threads to get higher crawl speed, but it comes with higher risk of \n&gt; d=\r\nying of OutOfMemoryError, empirically. Crawl speed is also limited by \n&gt; lo=\r\nwer disk I/O performance of VMs. 100 seems to be a good number for us.\n&gt; \n&gt;=\r\n Yes, increasing threads brings significant increase in crawl speed, to \n&gt; =\r\ncertain extent. If you don&#39;t have enough &quot;active queues,&quot; threads are \n&gt; ju=\r\nst wasted. There are other bottleneck, too, and it can change over time.\n&gt; =\r\n\n&gt; At least we&#39;re getting sustained 60URI/s level of speed with 100 \n&gt; thre=\r\nads. With 1,200 threads and enough active queues, you should be \n&gt; getting =\r\ncrawl speed much much higher than that (I&#39;ve never been able to \n&gt; run my c=\r\nrawler with 1200 threads, though!)\n&gt; \n&gt; --Kenji\n&gt; \n&gt; (1/26/12 10:31 AM), da=\r\nvid_pane1 wrote:\n&gt; &gt; Kenji,\n&gt; &gt;\n&gt; &gt; Are you saying that you can get 25M pag=\r\nes per day on 100 threads and 1\n&gt; &gt; instance or 25M URIs/day? Do you captur=\r\ne all images, pdfs, and\n&gt; &gt; supporting page documents or are you just captu=\r\nring html pages?\n&gt; &gt;\n&gt; &gt; My test crawls before running our large crawl show=\r\ned significant\n&gt; &gt; increase in the number of pages captured when we increas=\r\ned the number of\n&gt; &gt; threads.\n&gt; &gt;\n&gt; &gt; --David\n&gt; &gt;\n&gt; &gt; --- In archive-crawle=\r\nr@yahoogroups.com\n&gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nag=\r\nahashi\n&gt; &gt; &lt;knagahashi@&gt; wrote:\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Hi,\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; May be a bi=\r\nt off-topic, but 25M/day with 5 machine is average 58/s per\n&gt; &gt;  &gt; machine.=\r\n Since I know Heritrix-3 can crawl at this speed with just 100\n&gt; &gt;  &gt; ToeTh=\r\nreads, I wonder if most of your 1200 ToeThreads are idle.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; Whi=\r\nle why you don&#39;t get much higher speed with 1200 threads is a big\n&gt; &gt;  &gt; qu=\r\nestion, it may make sense to cut down the number of ToeThreads if\n&gt; &gt;  &gt; yo=\r\nu&#39;re okay with current crawl speed. Less threads will make H3 less\n&gt; &gt;  &gt; s=\r\nusceptible to memory problems... Just a thought.\n&gt; &gt;  &gt;\n&gt; &gt;  &gt; --Kenji\n&gt; &gt; =\r\n &gt;\n&gt; &gt;  &gt; (1/20/12 9:54 PM), David Pane wrote:\n&gt; &gt;  &gt; &gt; Gordon,\n&gt; &gt;  &gt; &gt;\n&gt; =\r\n&gt;  &gt; &gt; Thank you for your response. And I am sorry for the overwhelming amo=\r\nunt\n&gt; &gt;  &gt; &gt; of information...I think I am a little overwhelmed.... and fee=\r\nling the\n&gt; &gt;  &gt; &gt; pressure.\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; 1) Our Bloom filter configura=\r\ntion:\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; &lt;bean id=3D&quot;uriUniqFilter&quot;\n&gt; &gt;  &gt; &gt; class=3D&quot;org.ar=\r\nchive.crawler.util.BloomUriUniqFilter&quot;&gt;\n&gt; &gt;  &gt; &gt; &lt;property name=3D&quot;bloomFil=\r\nter&quot;&gt;\n&gt; &gt;  &gt; &gt; &lt;bean class=3D&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&gt; &gt;  &gt; &gt; =\r\n&lt;constructor-arg value=3D&quot;400000000&quot;/&gt;\n&gt; &gt;  &gt; &gt; &lt;constructor-arg value=3D&quot;3=\r\n0&quot;/&gt;\n&gt; &gt;  &gt; &gt; &lt;/bean&gt;\n&gt; &gt;  &gt; &gt; &lt;/property&gt;\n&gt; &gt;  &gt; &gt; &lt;/bean&gt;\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt;=\r\n &gt; 2) We are writing the crawl data to a NAS configured with RAID 6.\n&gt; &gt; We=\r\n did\n&gt; &gt;  &gt; &gt; see some problems with disk errors on the NAS earlier in the =\r\ncrawl\n&gt; &gt; (late\n&gt; &gt;  &gt; &gt; Dec ). I recently found this out. We were/are runn=\r\ning in a degraded\n&gt; &gt;  &gt; &gt; raid state - a few of the disks have been replac=\r\ned and the RAID is\n&gt; &gt; being\n&gt; &gt;  &gt; &gt; rebuilt. We didn&#39;t see any block devi=\r\nce errors in the logs on the NAS\n&gt; &gt;  &gt; &gt; so the write failures we saw are =\r\nprobably not related to the\n&gt; &gt; rebuild. We\n&gt; &gt;  &gt; &gt; did see some network h=\r\niccups (no outright failures) in the logs.\n&gt; &gt;  &gt; &gt; So, this may be the cul=\r\nprit for some of the\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; 3) Yes, we have been cross-feeding U=\r\nRIs.\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; --David\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt; On 1/21/12 12:22 AM, Gordo=\r\nn Mohr wrote:\n&gt; &gt;  &gt; &gt; &gt; You&#39;ve provided an overwhelming amount of informat=\r\nion and we may be\n&gt; &gt;  &gt; &gt; &gt; dealing with multiple issues, some of which ha=\r\nve roots going back\n&gt; &gt;  &gt; &gt; &gt; earlier than the diagnostic data we now have=\r\n available.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; A few key points of emphasis:\n&gt; &gt;  &gt; &gt; &gt;\n=\r\n&gt; &gt;  &gt; &gt; &gt; - we&#39;ve not run crawls with 1200 threads before, or on hardware\n=\r\n&gt; &gt;  &gt; &gt; &gt; similar to yours, so our experience is only vaguely suggestive\n&gt;=\r\n &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; - it&#39;s not the lower thread counts that are the real s=\r\nource of\n&gt; &gt;  &gt; &gt; &gt; concern; you can even adjust the number of threads mid-=\r\ncrawl. It&#39;s\n&gt; &gt;  &gt; &gt; &gt; that the error that killed the threads almost certai=\r\nnly left a queue\n&gt; &gt;  &gt; &gt; &gt; in a &#39;phantom&#39; state where no progress would be=\r\n made crawling its\n&gt; &gt;  &gt; &gt; &gt; URIs, each time it happened, on each resume l=\r\neading to the\n&gt; &gt; current state.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; - without having und=\r\nerstood and fixed whatever software or system\n&gt; &gt;  &gt; &gt; &gt; problems caused th=\r\ne earliest/most-foundational errors in your crawl,\n&gt; &gt;  &gt; &gt; &gt; it&#39;s impossib=\r\nle to say how likely they are to recur.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; With that in =\r\nmind, I&#39;ll try to provide quick answers to your other\n&gt; &gt;  &gt; &gt; &gt; questions.=\r\n..\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; On 1/20/12 4:20 PM, David Pane wrote:\n&gt; &gt;  &gt; &gt; &gt;&gt;\n=\r\n&gt; &gt;  &gt; &gt; &gt;&gt; We have collected about 550 million pages along with the images=\r\n and\n&gt; &gt;  &gt; &gt; &gt;&gt; supporting documents on our 5 instance crawl that was star=\r\nted\n&gt; &gt; Dec. 23rd.\n&gt; &gt;  &gt; &gt; &gt;&gt; Although we are please with the amount of da=\r\nta we captured to\n&gt; &gt; date, we\n&gt; &gt;  &gt; &gt; &gt;&gt; are very concerned about the sta=\r\nte of the Heritrix instances. If\n&gt; &gt; fact,\n&gt; &gt;  &gt; &gt; &gt;&gt; we aren&#39;t very confi=\r\ndent that the instances will last until the\n&gt; &gt; end of\n&gt; &gt;  &gt; &gt; &gt;&gt; February=\r\n. We are now running on a total of over 500 less threads\n&gt; &gt; than\n&gt; &gt;  &gt; &gt; =\r\n&gt;&gt; the configured 1200 threads/instance.\n&gt; &gt;  &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; 0 - not ru=\r\nnning right now.\n&gt; &gt;  &gt; &gt; &gt;&gt; 1 - running on 1198 ( 2 less)\n&gt; &gt;  &gt; &gt; &gt;&gt; 2 - =\r\nrunning on 931 (269 less)\n&gt; &gt;  &gt; &gt; &gt;&gt; 3 - running on 987 (213 less)\n&gt; &gt;  &gt; =\r\n&gt; &gt;&gt; 4 - running on 1170 (30 less)\n&gt; &gt;  &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; Since we are ser=\r\niously considering throwing away this past\n&gt; &gt; month&#39;s work\n&gt; &gt;  &gt; &gt; &gt;&gt; and=\r\n starting over, we would like to pick your brain on some\n&gt; &gt; strategies\n&gt; &gt;=\r\n  &gt; &gt; &gt;&gt; that will help us avoid getting into this situation again. We were=\r\n\n&gt; &gt;  &gt; &gt; &gt;&gt; hoping to be done crawling by the end of February so this\n&gt; &gt; =\r\nrestart will\n&gt; &gt;  &gt; &gt; &gt;&gt; put us behind schedule.\n&gt; &gt;  &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; 1)=\r\n Can we continue from here but with &quot;clean&quot; Heritrix instances?\n&gt; &gt;  &gt; &gt; &gt;&gt;=\r\n\n&gt; &gt;  &gt; &gt; &gt;&gt; Is there a way that we can continue from the this point forwar=\r\nd, but\n&gt; &gt;  &gt; &gt; &gt;&gt; start with Heritrix instances that will not be corrupt d=\r\nue to sever\n&gt; &gt;  &gt; &gt; &gt;&gt; error? (e.g. using the\n&gt; &gt;  &gt; &gt; &gt;&gt; https://webarchi=\r\nve.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &gt; &lt;https://webarchive.ji=\r\nra.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt; &gt;  &gt; &gt; &lt;https://webarchive.j=\r\nira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &gt; &lt;https://webarchive.jira.c=\r\nom/wiki/display/Heritrix/Crawl+Recovery&gt;&gt; ) If\n&gt; &gt;  &gt; &gt; &gt;&gt; so, would you re=\r\ncommend doing this? You mentioned that this could be\n&gt; &gt;  &gt; &gt; &gt;&gt; time consu=\r\nming. Each of our instances has downloaded around 170M\n&gt; &gt; URIs,\n&gt; &gt;  &gt; &gt; &gt;=\r\n&gt; they have over 700M queued URIs, what is your time estimate for\n&gt; &gt;  &gt; &gt; =\r\n&gt;&gt; something this large?\n&gt; &gt;  &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; We are willing to sacrific=\r\ne a few days to get our crawler to a clean\n&gt; &gt;  &gt; &gt; &gt;&gt; state again so we ca=\r\nn crawl for another 30 days at the pace we\n&gt; &gt; have been\n&gt; &gt;  &gt; &gt; &gt;&gt; crawli=\r\nng.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; You can do a big &#39;frontier-recover&#39; log replay to=\r\n avoid\n&gt; &gt; recrawling the\n&gt; &gt;  &gt; &gt; &gt; same URIs, and approximate the earlier=\r\n queue state. Splitting/filters\n&gt; &gt;  &gt; &gt; &gt; the logs manually beforehand as =\r\nalluded to in the wiki page can speed\n&gt; &gt;  &gt; &gt; &gt; this process somewhat... b=\r\nut given the size of all your log-segments\n&gt; &gt;  &gt; &gt; &gt; that log grooming bef=\r\norehand is itself likely to be a lengthy\n&gt; &gt; process.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;=\r\n I don&#39;t think we&#39;ve ever done it with logs of 170M crawled / 870M\n&gt; &gt;  &gt; &gt;=\r\n &gt; discovered before, nor on any hardware comparable to yours. So it&#39;s\n&gt; &gt; =\r\n &gt; &gt; &gt; impossible to project its duration in your environment. It&#39;s\n&gt; &gt; tak=\r\nen 2-3\n&gt; &gt;  &gt; &gt; &gt; days for us on smaller crawls, slower hardware.\n&gt; &gt;  &gt; &gt; =\r\n&gt;\n&gt; &gt;  &gt; &gt; &gt; An added complication is that this older frontier-recover-log =\r\nreplay\n&gt; &gt;  &gt; &gt; &gt; technique happens in its own thread separate from the che=\r\nckpointing\n&gt; &gt;  &gt; &gt; &gt; process, so it is not, itself, accurately checkpointe=\r\nd during the\n&gt; &gt; long\n&gt; &gt;  &gt; &gt; &gt; reload process.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; At n=\r\nearly 1B discovered URIs per node, even if you are using the\n&gt; &gt;  &gt; &gt; &gt; alt=\r\nernate BloomUriUniqFilter, if you are using it at its default size\n&gt; &gt;  &gt; &gt;=\r\n &gt; (~500MB) it will now be heavily saturated and thus returning many\n&gt; &gt;  &gt;=\r\n &gt; &gt; false-positives causing truly unique URIs to be rejected as\n&gt; &gt;  &gt; &gt; &gt;=\r\n duplicates. (If you&#39;re using a significantly larger filter, you may\n&gt; &gt;  &gt;=\r\n &gt; &gt; not yet be at a high false-positive rate: you&#39;d have to do the bloom\n&gt;=\r\n &gt;  &gt; &gt; &gt; filter math. If you&#39;re still using BdbUriUniqFilter, you&#39;re way w=\r\nay\n&gt; &gt;  &gt; &gt; &gt; past the point where its disk seeks have usually made it too =\r\nslow for\n&gt; &gt;  &gt; &gt; &gt; our purposes.)\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; 2) What can be do=\r\nne to avoid corrupting the Heritrix instances?\n&gt; &gt;  &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; - Wh=\r\nat kind of strategies might we take to keep the crawl error\n&gt; &gt; free?\n&gt; &gt;  =\r\n&gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; - Do you think the SEVER errors that we have seen are\n&gt; =\r\n&gt; deterministic or\n&gt; &gt;  &gt; &gt; &gt;&gt; random (e.g., triggered by occasional flaky =\r\nnetwork conditions,\n&gt; &gt; disks,\n&gt; &gt;  &gt; &gt; &gt;&gt; race conditions, or whatever)?\n&gt;=\r\n &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; Hard to say. The main thing I could suggest is watch v=\r\nery closely and\n&gt; &gt;  &gt; &gt; &gt; when a SEVERE error occurs, prioritize diagnosin=\r\ng and resolving the\n&gt; &gt;  &gt; &gt; &gt; cause while the info is fresh.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; =\r\n&gt;  &gt; &gt; &gt;&gt; - Do you believe that we can reliably backup to the previous\n&gt; &gt; =\r\ncheckpoint\n&gt; &gt;  &gt; &gt; &gt;&gt; if we watch the logs and stop as soon as we see the =\r\nfirst SEVER\n&gt; &gt; error?\n&gt; &gt;  &gt; &gt; &gt;&gt; If we do this, do you speculate that the=\r\n same SEVER will occur\n&gt; &gt; again?\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; Resuming from the l=\r\natest checkpoint before an error believed to\n&gt; &gt;  &gt; &gt; &gt; corrupt the on-disk=\r\n state will be the best strategy.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; If we never figure =\r\nout the real cause, but run the same software on\n&gt; &gt;  &gt; &gt; &gt; the same machin=\r\ne, yes, I expect the same problem will recur!\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; - Is t=\r\nhere any reason why a Heritrix instance that is run while\n&gt; &gt; binded\n&gt; &gt;  &gt;=\r\n &gt; &gt;&gt; to one ip address can&#39;t be resumed binded to a different ip address?\n=\r\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; Only the web UI to my knowledge binds to a chosen add=\r\nress, and it is\n&gt; &gt;  &gt; &gt; &gt; common to have it bind to all. I don&#39;t expect th=\r\ne outbound requests\n&gt; &gt;  &gt; &gt; &gt; would be hurt by a machine changing its IP a=\r\nddress while the\n&gt; &gt; crawl was\n&gt; &gt;  &gt; &gt; &gt; running, but I would run a test t=\r\no be sure if that was an important,\n&gt; &gt;  &gt; &gt; &gt; expected transition.\n&gt; &gt;  &gt; =\r\n&gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; 3) Should we configure the crawler with more instances and =\r\nswitch\n&gt; &gt;  &gt; &gt; &gt;&gt; between them?\n&gt; &gt;  &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; We have seen that =\r\nwe can run a single instance to 100M pages +\n&gt; &gt;  &gt; &gt; &gt;&gt; supporting images =\r\nand documents. Perhaps this means that we need\n&gt; &gt; 10 or\n&gt; &gt;  &gt; &gt; &gt;&gt; more i=\r\nnstances instead of 5. That raises the possibility of\n&gt; &gt; running 2\n&gt; &gt;  &gt; =\r\n&gt; &gt;&gt; instances per machine. If we could run 2, or even 4, instances on a\n&gt; =\r\n&gt;  &gt; &gt; &gt;&gt; single machine, they would each run half as long.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt; =\r\n &gt; &gt; &gt; I don&#39;t think the problems as reported are specifically due to one\n&gt;=\r\n &gt;  &gt; &gt; &gt; node&#39;s progress growing beyond a certain size, but it might be th=\r\ne\n&gt; &gt;  &gt; &gt; &gt; case that giant instances are more likely to suffer from, and =\r\nharder\n&gt; &gt;  &gt; &gt; &gt; to recover from, single glitches (eg a single disk error)=\r\n. On the\n&gt; &gt;  &gt; &gt; &gt; other hand, many instances introduce more redundant ove=\r\nrhead costs\n&gt; &gt;  &gt; &gt; &gt; (certain data structures, cross-feeding URIs if you&#39;=\r\nre doing\n&gt; &gt; that, etc.).\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; - Can you suggest a way to=\r\n start/stop instances from a script so\n&gt; &gt; we can\n&gt; &gt;  &gt; &gt; &gt;&gt; change betwee=\r\nn instances automatically?\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; Not a mode I&#39;ve thought mu=\r\nch about.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; - Have you seen frequent starting / stoppi=\r\nng of instances introduce\n&gt; &gt;  &gt; &gt; &gt;&gt; instability?\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; No=\r\n... but it might make you notice latent issues sooner.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; =\r\n&gt;&gt; 4) Crawl slows but restarting seems to improve the speed again.\n&gt; &gt;  &gt; &gt;=\r\n &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; We noticed that the all of our instances would initially ru=\r\nn at\n&gt; &gt; a fast\n&gt; &gt;  &gt; &gt; &gt;&gt; pace. We would collect an average of 25M + page=\r\ns/day for 2-3\n&gt; &gt; days and\n&gt; &gt;  &gt; &gt; &gt;&gt; then the crawl would slow down to 10=\r\nM pages/day over the next\n&gt; &gt; few days.\n&gt; &gt;  &gt; &gt; &gt;&gt; (these numbers are tota=\r\nls of all 5 instances combined). When we\n&gt; &gt;  &gt; &gt; &gt;&gt; restarted the instance=\r\ns, the average pages would improve back to\n&gt; &gt; 25M +\n&gt; &gt;  &gt; &gt; &gt;&gt; pages/day.=\r\n The total crawled numbers (TiB) also reflected the\n&gt; &gt; slow down.\n&gt; &gt;  &gt; &gt;=\r\n &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; - Is this something that others have experienced as well?\n&gt;=\r\n &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; I don&#39;t recall hearing other reports of speed boosts a=\r\nfter\n&gt; &gt;  &gt; &gt; &gt; checkpoint-resumes but others may have more experience.\n&gt; &gt;=\r\n  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; 5) We are capturing tweets from twitter, harvesting the=\r\n urls and\n&gt; &gt; want to\n&gt; &gt;  &gt; &gt; &gt;&gt; crawl those urls within 1 day of receivin=\r\ng the tweet. Can you\n&gt; &gt; recommend\n&gt; &gt;  &gt; &gt; &gt;&gt; a strategy for doing this wi=\r\nth the 5 instances we are running?\n&gt; &gt;  &gt; &gt; &gt;&gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; - Do we need to =\r\nrun a separate crawler dedicated to this? If so,\n&gt; &gt; can you\n&gt; &gt;  &gt; &gt; &gt;&gt; su=\r\nggest a way to crawl out from the tweeted urls but when we get\n&gt; &gt;  &gt; &gt; &gt;&gt; =\r\nadditional urls from the tweets, quickly change focus to these urls\n&gt; &gt;  &gt; =\r\n&gt; &gt;&gt; instead of the ones branching out. When adding urls as seeds,\n&gt; &gt; can =\r\nyou\n&gt; &gt;  &gt; &gt; &gt;&gt; set a high priority to crawl those before the discovered ur=\r\nls?\n&gt; &gt; Do you\n&gt; &gt;  &gt; &gt; &gt;&gt; recommend maybe setting up a specific crawl for =\r\nthese urls and\n&gt; &gt; then only\n&gt; &gt;  &gt; &gt; &gt;&gt; crawl a few hopes from the seeds -=\r\n injecting the urls from the\n&gt; &gt; tweets as\n&gt; &gt;  &gt; &gt; &gt;&gt; seeds?\n&gt; &gt;  &gt; &gt; &gt;\n&gt; =\r\n&gt;  &gt; &gt; &gt; Dedicating a special script or crawler to URIs that come from such=\r\n a\n&gt; &gt;  &gt; &gt; &gt; constrained source (Twitter feeds), or that need to be crawle=\r\nd in a\n&gt; &gt;  &gt; &gt; &gt; special timeframe, or according to other special limits (=\r\nfewer hops),\n&gt; &gt;  &gt; &gt; &gt; could make sense.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; It would ta=\r\nke some customization of the queueing-policy or\n&gt; &gt;  &gt; &gt; &gt; &#39;precedence&#39; fea=\r\ntures of Heritrix to allow URIs added mid-crawl to be\n&gt; &gt;  &gt; &gt; &gt; prioritize=\r\nd above those already discovered and queued. The most\n&gt; &gt; simple\n&gt; &gt;  &gt; &gt; &gt;=\r\n possible customization might be a UriPrecedencePolicy that takes all\n&gt; &gt;  =\r\n&gt; &gt; &gt; zero-hop URIs (which all seeds and most direct-fed URIs would be) and=\r\n\n&gt; &gt;  &gt; &gt; &gt; gives them a higher precedence (lower precedence number) than a=\r\nll\n&gt; &gt;  &gt; &gt; &gt; other URIs.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt;&gt; 6) I think the answer is n=\r\no for this question, but I will ask it\n&gt; &gt; anyway.\n&gt; &gt;  &gt; &gt; &gt;&gt; If you have =\r\na Heritrix instance that is configured for 1200\n&gt; &gt; threads on\n&gt; &gt;  &gt; &gt; &gt;&gt; =\r\none machine, can you recover from a checkpoint from that 1200 thread\n&gt; &gt;  &gt;=\r\n &gt; &gt;&gt; configuration on a different machine with an Heritrix instance\n&gt; &gt; th=\r\nat is\n&gt; &gt;  &gt; &gt; &gt;&gt; configured for less threads (e.g. the default 25 threads)=\r\n?\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; Yes - there&#39;s no need to keep the thread count the =\r\nsame after a\n&gt; &gt;  &gt; &gt; &gt; resume. None of the checkpoint structures (or usual=\r\n disk structures)\n&gt; &gt;  &gt; &gt; &gt; are based on the number of worker threads (&#39;To=\r\neThreads&#39;)... as\n&gt; &gt;  &gt; &gt; &gt; mentioned above you can even vary the number of=\r\n threads in a running\n&gt; &gt;  &gt; &gt; &gt; crawl.\n&gt; &gt;  &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt; &gt; - Gordon\n&gt; &gt; =\r\n &gt; &gt; &gt;\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt; &gt;\n&gt; &gt;  &gt;\n&gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}