{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":194371199,"authorName":"bjhong02","from":"&quot;bjhong02&quot; &lt;bjhong02@...&gt;","profile":"bjhong02","replyTo":"LIST","senderId":"pFzy5o9ML6wx_TwSeCJbvqqvbt5NsNzvcKSAVOx-X5dwQGWwK42Frs-D6bwuXdkzgfsZvUzlBcOT2eHRhdLCyNHyhp78Ig","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: can anyone  explain some concepts of Crawl Scope?","postDate":"1103858084","msgId":1307,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGNxZzFqNCsybGQ0QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQxQ0IzQUQ1LjEwMDA3MDhAYXJjaGl2ZS5vcmc+"},"prevInTopic":1306,"nextInTopic":1310,"prevInTime":1306,"nextInTime":1308,"topicId":1302,"numMessagesInTopic":11,"msgSnippet":"... (unlimited), ... am i ... There is ... about ... 0 (or ... broadcrawling ... http://crawler.archive.org/articles/releasenotes.html#1_0_0_limitatio ns). in","rawEmail":"Return-Path: &lt;bjhong02@...&gt;\r\nX-Sender: bjhong02@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 86788 invoked from network); 24 Dec 2004 04:16:40 -0000\r\nReceived: from unknown (66.218.66.172)\n  by m25.grp.scd.yahoo.com with QMQP; 24 Dec 2004 04:16:40 -0000\r\nReceived: from unknown (HELO n19a.bulk.scd.yahoo.com) (66.94.237.48)\n  by mta4.grp.scd.yahoo.com with SMTP; 24 Dec 2004 04:16:39 -0000\r\nReceived: from [66.218.69.6] by n19.bulk.scd.yahoo.com with NNFMP; 24 Dec 2004 04:15:51 -0000\r\nReceived: from [66.218.67.136] by mailer6.bulk.scd.yahoo.com with NNFMP; 24 Dec 2004 03:58:02 -0000\r\nDate: Fri, 24 Dec 2004 03:14:44 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;cqg1j4+2ld4@...&gt;\r\nIn-Reply-To: &lt;41CB3AD5.1000708@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1\r\nContent-Length: 1255\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nX-eGroups-Remote-IP: 66.94.237.48\r\nFrom: &quot;bjhong02&quot; &lt;bjhong02@...&gt;\r\nSubject: Re: can anyone  explain some concepts of Crawl Scope?\r\nX-Yahoo-Group-Post: member; u=194371199\r\nX-Yahoo-Profile: bjhong02\r\n\r\n\n&gt; &gt; 1. if I use the BraodScope, set the max-link-hops to 0  \n(unlimited),\n&gt; &gt; and with some representative seeds, I can crawl the whole web. \nam i\n&gt; &gt; right?\n&gt; \n&gt; Looking at the code, 0 is zero link hops, not unlimited hops.  \nThere is \n&gt; no way of specifying unlimited (We need to be more consistent \nabout \n&gt; no-limit values across all settings).  Choose something other than \n0 (or \n&gt; -1) to do your broad-crawl.\n&gt; \n&gt; But be aware that Heritrix currently doesn&#39;t do a good job \nbroadcrawling \n&gt; currently (See \n&gt; \nhttp://crawler.archive.org/articles/releasenotes.html#1_0_0_limitatio\nns).\n\nin the limitations, I found:\nWith the default settings, and an assignment of a 256MB Java heap to \nthe Heritrix process, crawling which discovers up to 10 000 hosts, \nand schedules over 6 000 000 URIs, should be possible. Discovery of \nhigher numbers of URIs/hosts will likely trigger out-of-memory \nproblems unless a larger java heap was assigned at startup.\n\nIs that to say, it can discover up to 40,000 hosts, if I assign \n1025MB Java heap (on a PC with 2GB physical memory).\nI wonder if there&#39;s any strategy to stop crawl before triggering out-\nof-memory problems. or we can set some parameters to limit the \nnumber of discovered URIs/hosts.\n\nthanks\n \n\n\n\n\n\n\n\n\n\n\n"}}