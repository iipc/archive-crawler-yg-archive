{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":165458231,"authorName":"Bjarne Andersen","from":"Bjarne Andersen &lt;bja@...&gt;","profile":"bjarne_dk2000","replyTo":"LIST","senderId":"SMWnJq44faTU8At8EHUgxCCQIQh9Jw5uhpyXlF-OZJBieySTbnfipIYhnykEsivVEXJTcHtsX5R3uuhiZ6pcBCHc-jBQXtjtt-Yh2FzJVQw","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Duplicates in repeated domain crawls","postDate":"1172046329","msgId":3828,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1REMwMUY5LjIwMDA3QHN0YXRzYmlibGlvdGVrZXQuZGs+","inReplyToHeader":"PGVxdmVkcCtrZTgxQGVHcm91cHMuY29tPg==","referencesHeader":"PGVxdmVkcCtrZTgxQGVHcm91cHMuY29tPg=="},"prevInTopic":3819,"nextInTopic":0,"prevInTime":3827,"nextInTime":3829,"topicId":3819,"numMessagesInTopic":2,"msgSnippet":"In Denmark we implemented the islandic DeDuplicator heritrix module last autumn and has run one .dk-TLD crawl with that mechanism. That crawl was 11Tb with 3Tb","rawEmail":"Return-Path: &lt;bja@...&gt;\r\nX-Sender: bja@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 91546 invoked from network); 21 Feb 2007 08:25:31 -0000\r\nReceived: from unknown (66.218.66.70)\n  by m28.grp.scd.yahoo.com with QMQP; 21 Feb 2007 08:25:31 -0000\r\nReceived: from unknown (HELO luna.statsbiblioteket.dk) (130.225.24.87)\n  by mta12.grp.scd.yahoo.com with SMTP; 21 Feb 2007 08:25:30 -0000\r\nReceived: from [172.18.251.249] (pc975.sb.statsbiblioteket.dk [172.18.251.249])\n by luna.statsbiblioteket.dk\n (iPlanet Messaging Server 5.2 HotFix 1.16 (built May 14 2003))\n with ESMTP id &lt;0JDT00MEQ0QH10@...&gt; for\n archive-crawler@yahoogroups.com; Wed, 21 Feb 2007 09:25:29 +0100 (MET)\r\nDate: Wed, 21 Feb 2007 09:25:29 +0100\r\nIn-reply-to: &lt;eqvedp+ke81@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-id: &lt;45DC01F9.20007@...&gt;\r\nOrganization: Statsbiblioteket\r\nMIME-version: 1.0\r\nContent-type: multipart/mixed; boundary=&quot;Boundary_(ID_MIrnqq3sgtg+jFfBgjkpFA)&quot;\r\nX-Accept-Language: en-us, en\r\nUser-Agent: Mozilla Thunderbird 1.0 (X11/20041206)\r\nReferences: &lt;eqvedp+ke81@...&gt;\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Bjarne Andersen &lt;bja@...&gt;\r\nSubject: Re: [archive-crawler] Duplicates in repeated domain crawls\r\nX-Yahoo-Group-Post: member; u=165458231; y=2O61fn6VsF2V3tuCyfbtgNPE1tzbPXUQiMwcJ9H8fhcXJIspABBhcA\r\nX-Yahoo-Profile: bjarne_dk2000\r\n\r\n\r\n--Boundary_(ID_MIrnqq3sgtg+jFfBgjkpFA)\r\nContent-type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-transfer-encoding: 7BIT\r\n\r\nIn Denmark we implemented the islandic DeDuplicator heritrix module last autumn and has run one .dk-TLD crawl with that mechanism. That \ncrawl was 11Tb with 3Tb of duplicates (27%)\n\nWe did only deduplication one mimetypes not matching &#39;text/*&#39; - so e.g. HTML was not deduplicated. This was for performance only. With \nsmaller crawls you could deduplicate everything.\n\nI know Island (Kristinn) is running deduplication on weekly crawls with very high duplicate percentages (80-90%)\n\nbest\n-- \nBjarne Andersen\nDaily Manager - netarchive.dk\n\nState & University Library\nUniversitetsparken\nDK-8000 Aarhus C\nT: +45 89462165 - C: +45 25662353\nCVR/SE 10100682 - EAN 5798000791084\nhttp://netarchive.dk\n\n\n&gt; \n&gt; \n&gt; Hello,\n&gt; \n&gt; I am curious if anyone might have compiled any stats with regards to\n&gt; how much duplicate content is likely to appear in repeat domain\n&gt; crawls. In our case, we are crawling a Government domain that is\n&gt; approximately 1,500 seeds and 2TB in size. We plan to crawl twice a\n&gt; year and obviously a lot of content will not change across these crawls\n&gt; resulting in many duplicates (I&#39;ve noticed some sites have not been\n&gt; updated since 2004!).\n&gt; \n&gt; It&#39;d be great to hear if anyone has analyzed this type of question\n&gt; before (particularly with Government domains). We would like to use\n&gt; this info to estimate how much space / bandwidth will be needed when\n&gt; repeating domain crawls with a &quot;smart&quot; crawler that uses deduplication.\n&gt; \n&gt; cheers\n&gt; Adam\n&gt; \n&gt; \n\n\n\r\n--Boundary_(ID_MIrnqq3sgtg+jFfBgjkpFA)\r\nContent-type: text/x-vcard; charset=utf-8; name=bja.vcf\r\nContent-disposition: attachment; filename=bja.vcf\r\n\r\n[ Attachment content not displayed ]\r\n--Boundary_(ID_MIrnqq3sgtg+jFfBgjkpFA)--\r\n\n"}}