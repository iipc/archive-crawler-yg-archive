{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":452070983,"authorName":"arundudee gmail","from":"arundudee gmail &lt;arundudee@...&gt;","profile":"arundudee","replyTo":"LIST","senderId":"y7keAK97dODhkethevFZSiTZI5X0_jA_dXlNTBVKuRNsctJI1scpaSuPgHKI3V5Rde60BJmkdCM_L9OaDeEFL_9xLTKZwLLjjahV0Q","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] crawl only seed urls in Heritrix","postDate":"1294287611","msgId":6976,"canDelete":false,"contentTrasformed":false,"systemMessage":true,"headers":{"messageIdInHeader":"PDREMjU0MkZCLjIwNDA5MDdAZ21haWwuY29tPg==","inReplyToHeader":"PDREMjRCNUJCLjYwNDA4QGFyY2hpdmUub3JnPg==","referencesHeader":"PGlmdjF2ZCt0bzd0QGVHcm91cHMuY29tPiA8NEQyMzcxODAuOTA3MDYwOEBhcmNoaXZlLm9yZz4gPDREMjM5MUVBLjgwNjA3MDFAYXJjaGl2ZS5vcmc+IDw0RDIzRjBENS42MDkwMDA5QGdtYWlsLmNvbT4gPDREMjQwRUNDLjEwMzA0MDNAYXJjaGl2ZS5vcmc+IDw0RDI0NzE3RS42MDUwMTA2QGdtYWlsLmNvbT4gPDREMjRCNUJCLjYwNDA4QGFyY2hpdmUub3JnPg=="},"prevInTopic":6972,"nextInTopic":0,"prevInTime":6975,"nextInTime":6977,"topicId":6955,"numMessagesInTopic":9,"msgSnippet":"Noah- 1)my scale is 5 million in starting and then scale to few billion. 2)Is there any restriction on numbers of urls seed file can have.i gave around 50K to","rawEmail":"Return-Path: &lt;arundudee@...&gt;\r\nReceived: (qmail 65774 invoked from network); 11 Jan 2011 18:53:58 -0000\r\nReceived: from unknown (66.196.94.105)\n  by m14.grp.re1.yahoo.com with QMQP; 11 Jan 2011 18:53:58 -0000\r\nReceived: from unknown (HELO n38b.bullet.mail.sp1.yahoo.com) (66.163.168.152)\n  by mta1.grp.re1.yahoo.com with SMTP; 11 Jan 2011 18:53:57 -0000\r\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoogroups.com; s=lima; t=1294772036; bh=UqZxCME48hVT4lj0CCC4+HfXNSq6qbka1DArxwyWgBI=; h=Received:Received:X-Sender:X-Apparently-To:X-Received:X-Received:X-Received:X-Received:X-Received:X-Received:Message-ID:Date:User-Agent:MIME-Version:To:Cc:References:In-Reply-To:Content-Type:Content-Transfer-Encoding:X-Originating-IP:X-eGroups-Msg-Info:From:Subject:X-Yahoo-Group-Post:X-Yahoo-Profile:X-YGroups-SubInfo:Sender:X-Yahoo-Newman-Property:X-eGroups-Approved-By:X-eGroups-Auth; b=tKbP4gfLx8UCXpY8mv+7XOjf6a1GTaNivYkh+9h5gjBlRlgHo/hHKqYUPEydzZdzzYhQB3ig3lMLUDPDCHJn6ZvubZWczh/nvfKpAzvfhncdiJcpaevBid7dgbtB+WnM\r\nReceived: from [69.147.65.171] by n38.bullet.mail.sp1.yahoo.com with NNFMP; 11 Jan 2011 18:53:56 -0000\r\nReceived: from [98.137.34.73] by t13.bullet.mail.sp1.yahoo.com with NNFMP; 11 Jan 2011 18:53:56 -0000\r\nX-Sender: arundudee@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 41731 invoked from network); 6 Jan 2011 04:20:43 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m6.grp.sp2.yahoo.com with QMQP; 6 Jan 2011 04:20:43 -0000\r\nX-Received: from unknown (HELO mail-gw0-f44.google.com) (74.125.83.44)\n  by mta3.grp.sp2.yahoo.com with SMTP; 6 Jan 2011 04:20:43 -0000\r\nX-Received: by gwj17 with SMTP id 17so6784139gwj.3\n        for &lt;archive-crawler@yahoogroups.com&gt;; Wed, 05 Jan 2011 20:20:43 -0800 (PST)\r\nX-Received: by 10.236.103.38 with SMTP id e26mr5292162yhg.88.1294287643231;\n        Wed, 05 Jan 2011 20:20:43 -0800 (PST)\r\nX-Received: from [172.23.96.167] ([115.113.53.206])\n        by mx.google.com with ESMTPS id i39sm124779yhd.11.2011.01.05.20.20.40\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Wed, 05 Jan 2011 20:20:42 -0800 (PST)\r\nMessage-ID: &lt;4D2542FB.2040907@...&gt;\r\nDate: Thu, 06 Jan 2011 09:50:11 +0530\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.13) Gecko/20101208 Lightning/1.0b2 Thunderbird/3.1.7\r\nMIME-Version: 1.0\r\nTo: Gordon Mohr &lt;gojomo@...&gt;\r\nCc: Noah Levitt &lt;nlevitt@...&gt;, archive-crawler@yahoogroups.com\r\nReferences: &lt;ifv1vd+to7t@...&gt; &lt;4D237180.9070608@...&gt; &lt;4D2391EA.8060701@...&gt; &lt;4D23F0D5.6090009@...&gt; &lt;4D240ECC.1030403@...&gt; &lt;4D24717E.6050106@...&gt; &lt;4D24B5BB.60408@...&gt;\r\nIn-Reply-To: &lt;4D24B5BB.60408@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: arundudee gmail &lt;arundudee@...&gt;\r\nSubject: Re: [archive-crawler] crawl only seed urls in Heritrix\r\nX-Yahoo-Group-Post: member; u=452070983; y=5k_WEeVW693MLUyyMCz0ol_is8gh0-QFMkt-NdLbkcB-CJOp\r\nX-Yahoo-Profile: arundudee\r\nX-Yahoo-Newman-Property: groups-system\r\nX-eGroups-Approved-By: gojomo &lt;gojomo@...&gt; via web; 11 Jan 2011 18:53:56 -0000\r\n\r\nNoah-\n1)my scale is 5 million in starting and then scale to few billion.\n2)Is there any restriction on numbers of urls seed file can have.i gave \naround 50K to test but it picked up top 2.5K only.\n\nThanks\nArun\nOn Wednesday 05 January 2011 11:47 PM, Gordon Mohr wrote:\n&gt; On 1/5/11 5:26 AM, arundudee gmail wrote:\n&gt;&gt; Thanks Noah , i was successful in setting up job with below mentioned\n&gt;&gt; steps and its working as desired.\n&gt;&gt; please can you help on point #2 as well :\n&gt;&gt; 2. I have one more question:let say i crawled 100 seeds urls and they\n&gt;&gt; are dumped at arc folder.Is there a way i can specify url and i can get\n&gt;&gt; response body of that page from dump... or is that possible through any\n&gt;&gt; other mechanism.\n&gt;\n&gt; In addition to Noah&#39;s answer, I would note that if you only have a \n&gt; fixed list of 100 URLs to crawl -- never wanting to extract links, or \n&gt; review HTTP headers, etc. -- then Heritrix may be overkill for your \n&gt; purposes. A command-line tool like &#39;wget&#39;, &#39;curl&#39;, etc. in a small \n&gt; loop may be plenty.\n&gt;\n&gt;&gt; I also noticed then when i run job , num of active threads remain zero :\n&gt;&gt; &#39;0 active of 50 threads &#39; how can i run all threads to speed up my\n&gt;&gt; crawling.\n&gt;\n&gt; See this page in the project wiki FAQ:\n&gt;\n&gt; &quot;Why is crawling slower than expected with a not-very-busy crawling \n&gt; machine?&quot;\n&gt;\n&gt; https://webarchive.jira.com/wiki/display/Heritrix/unexpectedly+slow+crawling+on+idle+crawler \n&gt;\n&gt;\n&gt; - Gordon @ IA\n&gt;\n&gt;&gt; Thank you so much\n&gt;&gt; Arun\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; On Wednesday 05 January 2011 11:55 AM, Noah Levitt wrote:\n&gt;&gt;&gt; You can add and remove decide rules in the Submodules tab, or edit\n&gt;&gt;&gt; order.xml directly.\n&gt;&gt;&gt;\n&gt;&gt;&gt; Noah\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; On 2011-01-04 20:17 , arundudee gmail wrote:\n&gt;&gt;&gt;&gt; Gordon ,Noah - Thanks a ton for reply.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; 1. I am newbie to heritrix please can you elaborate how to perform\n&gt;&gt;&gt;&gt; three steps you mentioned below.I am attaching a screenshot of\n&gt;&gt;&gt;&gt; setting page but i don&#39;t know how to remove these rules and where to\n&gt;&gt;&gt;&gt; add #3.\n&gt;&gt;&gt;&gt; 2. I have one more question:let say i crawled 100 seeds urls and they\n&gt;&gt;&gt;&gt; are dumped at arc folder.Is there a way i can specify url and i can\n&gt;&gt;&gt;&gt; get response body of that page from dump... or is that possible\n&gt;&gt;&gt;&gt; through any other mechanism.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Thanks once again.\n&gt;&gt;&gt;&gt; Arun\n&gt;&gt;&gt;&gt; On Wednesday 05 January 2011 03:02 AM, Gordon Mohr wrote:\n&gt;&gt;&gt;&gt;&gt; One extra note about Noah&#39;s suggestion: it assumes you&#39;re starting\n&gt;&gt;&gt;&gt;&gt; from the (recommended) &#39;deciding-default&#39; example scope, rather than\n&gt;&gt;&gt;&gt;&gt; using the (deprecated, legacy) &#39;BroadScope&#39; class.\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; (I think your previous max-links-hops and max-trans-hops settings\n&gt;&gt;&gt;&gt;&gt; *should* have worked, but since Noah&#39;s approach is the better base\n&gt;&gt;&gt;&gt;&gt; for the future, getting to the bottom of what might have gone wrong\n&gt;&gt;&gt;&gt;&gt; with the BroadScope approach isn&#39;t as useful as moving the\n&gt;&gt;&gt;&gt;&gt; &#39;deciding&#39; scope.)\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; - Gordon @ IA\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; On 1/4/11 11:14 AM, Noah Levitt wrote:\n&gt;&gt;&gt;&gt;&gt;&gt; Here&#39;s one way to crawl seeds only, and not even embedded images or\n&gt;&gt;&gt;&gt;&gt;&gt; anything, which seems to be what you want. Starting with the\n&gt;&gt;&gt;&gt;&gt;&gt; default order.xml...\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; 1. remove the scope rule acceptIfSurtPrefixed\n&gt;&gt;&gt;&gt;&gt;&gt; 2. remove the scope rule acceptIfTranscluded\n&gt;&gt;&gt;&gt;&gt;&gt; 3. add this scope rule right after rejectByDefault\n&gt;&gt;&gt;&gt;&gt;&gt; &lt;newObject name=&quot;acceptIfSeed&quot;\n&gt;&gt;&gt;&gt;&gt;&gt; class=&quot;org.archive.crawler.deciderules.SeedAcceptDecideRule&quot;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; &lt;/newObject&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Hope this works for you.\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Noah\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; On 2011-01-04 03:57 , arun wrote:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; i am using Heritrix 1.14.4.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; I have around million seed urls and i just want to crawl these\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; seed urls only.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Please any one can suggest me how to do that.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; one more problem i am facing is that.. i tried to crawl only seed\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; urls uing broadscope and then max-link-hops: 0 and max-trans-hops:\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 0 but its not restricting to seed ulrs it crawls other urls in\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; that page too.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Any help will be highly appreciated ..thanks a ton in advance.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;\n\n"}}