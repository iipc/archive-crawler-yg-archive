{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":212342429,"authorName":"Pranay Pandey","from":"Pranay Pandey &lt;sspranay@...&gt;","profile":"sspranay","replyTo":"LIST","senderId":"qFCZmz71q9HhWECj7-tTcj-76hpCpd9-pwwULicTo7udAXWfK8QOx5xNsC1EqEBsic9UM2A5o9pxI3PRFw69CobfobdhCneJAw","spamInfo":{"isSpam":false,"reason":"12"},"subject":"more fetches using parallel queues in H3.0","postDate":"1260388360","msgId":6197,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDkyMjk4Mi43MzIzNy5xbUB3ZWI0MzE0Mi5tYWlsLnNwMS55YWhvby5jb20+"},"prevInTopic":0,"nextInTopic":6198,"prevInTime":6196,"nextInTime":6198,"topicId":6197,"numMessagesInTopic":4,"msgSnippet":"Hi, I have a site that allows my crawler to be least polite. The site is huge and also capable of handling large traffic. I increased the number of parallel","rawEmail":"Return-Path: &lt;sspranay@...&gt;\r\nX-Sender: sspranay@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 82821 invoked from network); 9 Dec 2009 19:52:41 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m2.grp.sp2.yahoo.com with QMQP; 9 Dec 2009 19:52:41 -0000\r\nX-Received: from unknown (HELO web43142.mail.sp1.yahoo.com) (216.252.121.72)\n  by mta1.grp.sp2.yahoo.com with SMTP; 9 Dec 2009 19:52:41 -0000\r\nX-Received: (qmail 74116 invoked by uid 60001); 9 Dec 2009 19:52:40 -0000\r\nMessage-ID: &lt;922982.73237.qm@...&gt;\r\nX-YMail-OSG: CkGaFnoVM1kw88F1J9MGgA7vuCVoPEhBuA9qLbNGa5uY0ThxFxUuZ50.Nc.2whiblBX6jS7YAPG096YxlJOeTvZhSqATIX.04mH3mGmFWrEvrY2ACpr3GUyNuM0h5hoxoXkSSZ7YRxSwKiVatlF64DxXDHZNt7dnG.9zxTu9iXZSqhgva4tsrph86j8iPCaR2IiI.WCAWHNjIege8eny6SM1XbNerspSCE05exB9MZKh3De3woOAIptmceeYta.JLdNiiLeV.j9pUMO5p_IheP.8RN5qyvn3_CqCOSoUZsS1YvADwA_FeV3hximWtJ7oPdM-\r\nX-Received: from [204.194.77.3] by web43142.mail.sp1.yahoo.com via HTTP; Wed, 09 Dec 2009 11:52:40 PST\r\nX-Mailer: YahooMailClassic/9.0.19 YahooMailWebService/0.8.100.260964\r\nDate: Wed, 9 Dec 2009 11:52:40 -0800 (PST)\r\nTo: archive-crawler@yahoogroups.com\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative; boundary=&quot;0-1267771180-1260388360=:73237&quot;\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Pranay Pandey &lt;sspranay@...&gt;\r\nSubject: more fetches using parallel queues in H3.0\r\nX-Yahoo-Group-Post: member; u=212342429; y=PJW-CrYaj9gZZvuKiOA6mYbXyAbZTxjogn3Cw_43BaL9vYg\r\nX-Yahoo-Profile: sspranay\r\n\r\n\r\n--0-1267771180-1260388360=:73237\r\nContent-Type: text/plain; charset=us-ascii\r\n\r\nHi,\n\nI have a site that allows my crawler to be least polite. The site is huge and also capable of handling large traffic. \nI increased the number of parallel queues from 1 to 10, but no visible improvement in crawl speed - still around 0.28 docs/sec against 0.19 docs/sec with 1 queue. \n\nI have set the heapSize to 2GB and at no point of time more than 1G was being used.\nI have also loosened all the other politeness factors.\nIf not 10 times, the crawl should have been faster by at least 5 times with 10 queues. \nAny clues?\n\nThanks,\nPranay\n\n\n\n      \r\n--0-1267771180-1260388360=:73237\r\nContent-Type: text/html; charset=us-ascii\r\n\r\n&lt;table cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; border=&quot;0&quot; &gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; style=&quot;font: inherit;&quot;&gt;Hi,&lt;br&gt;&lt;br&gt;I have a site that allows my crawler to be least polite. The site is huge and also capable of handling large traffic. &lt;br&gt;I increased the number of parallel queues from 1 to 10, but no visible improvement in crawl speed - still around 0.28 docs/sec against 0.19 docs/sec with 1 queue. &lt;br&gt;&lt;br&gt;I have set the heapSize to 2GB and at no point of time more than 1G was being used.&lt;br&gt;I have also loosened all the other politeness factors.&lt;br&gt;If not 10 times, the crawl should have been faster by at least 5 times with 10 queues. &lt;br&gt;Any clues?&lt;br&gt;&lt;br&gt;Thanks,&lt;br&gt;Pranay&lt;br&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;br&gt;\n\n      \r\n--0-1267771180-1260388360=:73237--\r\n\n"}}