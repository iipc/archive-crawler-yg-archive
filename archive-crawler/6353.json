{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":366268246,"authorName":"Derek Pappas","from":"Derek Pappas &lt;derekepappas@...&gt;","profile":"depappas","replyTo":"LIST","senderId":"wJG_cumBCFM0QlJYr39b05iorDu0MEjd-MomthMVfMX_MNC37wd5-NM7jMlRx7HF_5Zj3QXRD0ybdD4Agy5lxccUClYb2WNxI1gtvQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Misc questions","postDate":"1265138866","msgId":6353,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PEMxRTExMzE4LUIzRTktNDU1Ny04MzI2LTNDODNDNjc2MkYwQUBnbWFpbC5jb20+","inReplyToHeader":"PDRCNjg1NUFFLjYwNDA3QGFyY2hpdmUub3JnPg==","referencesHeader":"PGhrNnQ5cSs4NXVkQGVHcm91cHMuY29tPiA8NEI2N0I5ODcuNTAyMDJAYXJjaGl2ZS5vcmc+IDxEMjgxOEU4Qi1FMUQxLTQxNEUtQTRBNC0xMzI5QzdEQTYyODFAZ21haWwuY29tPiA8NEI2ODU1QUUuNjA0MDdAYXJjaGl2ZS5vcmc+"},"prevInTopic":6351,"nextInTopic":6370,"prevInTime":6352,"nextInTime":6354,"topicId":6345,"numMessagesInTopic":9,"msgSnippet":"... Some times the check point hangs. Some times the check point can t be reloaded. We are using a complex regex to eliminate every file type except html. ... ","rawEmail":"Return-Path: &lt;derekepappas@...&gt;\r\nX-Sender: derekepappas@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 29752 invoked from network); 2 Feb 2010 19:28:56 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m14.grp.re1.yahoo.com with QMQP; 2 Feb 2010 19:28:56 -0000\r\nX-Received: from unknown (HELO fg-out-1718.google.com) (72.14.220.155)\n  by mta1.grp.sp2.yahoo.com with SMTP; 2 Feb 2010 19:28:56 -0000\r\nX-Received: by fg-out-1718.google.com with SMTP id 19so181788fgg.17\n        for &lt;archive-crawler@yahoogroups.com&gt;; Tue, 02 Feb 2010 11:27:55 -0800 (PST)\r\nX-Received: by 10.87.73.23 with SMTP id a23mr10526295fgl.76.1265138874989;\n        Tue, 02 Feb 2010 11:27:54 -0800 (PST)\r\nReturn-Path: &lt;derekepappas@...&gt;\r\nX-Received: from ?192.168.1.5? (c-24-6-97-60.hsd1.ca.comcast.net [24.6.97.60])\n        by mx.google.com with ESMTPS id 3sm11720585fge.1.2010.02.02.11.27.49\n        (version=TLSv1/SSLv3 cipher=RC4-MD5);\n        Tue, 02 Feb 2010 11:27:50 -0800 (PST)\r\nMessage-Id: &lt;C1E11318-B3E9-4557-8326-3C83C6762F0A@...&gt;\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;4B6855AE.60407@...&gt;\r\nContent-Type: text/plain; charset=WINDOWS-1252; format=flowed; delsp=yes\r\nContent-Transfer-Encoding: quoted-printable\r\nMime-Version: 1.0 (Apple Message framework v936)\r\nDate: Tue, 2 Feb 2010 11:27:46 -0800\r\nReferences: &lt;hk6t9q+85ud@...&gt; &lt;4B67B987.50202@...&gt; &lt;D2818E8B-E1D1-414E-A4A4-1329C7DA6281@...&gt; &lt;4B6855AE.60407@...&gt;\r\nX-Mailer: Apple Mail (2.936)\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Derek Pappas &lt;derekepappas@...&gt;\r\nSubject: Re: [archive-crawler] Misc questions\r\nX-Yahoo-Group-Post: member; u=366268246; y=_oAuuO83f_m0p10G_d0hWG7-6QFUzodEykqUJwHyAYG3Dps\r\nX-Yahoo-Profile: depappas\r\n\r\n\nOn Feb 2, 2010, at 8:41 AM, Gordon Mohr wrote:\n\n&gt; Derek Pappas wrote:\n&gt; &gt; =\r\nGordon,\n&gt; &gt;\n&gt; &gt; We are building a vertical search engine and are using Heri=\r\ntrix\n&gt; &gt;\n&gt; &gt; 1. As far as checking pointing is concerned is it supposed to =\r\nwork  \n&gt; in\n&gt; &gt; 1.xxx.\n&gt; &gt; My experience has been that it does not work as =\r\nadvertised.\n&gt;\n&gt; In H1, checkpointing should work, though it requires a full=\r\n pause, and\n&gt; if using custom/contributed components (such as processors) t=\r\nhey might\n&gt; not all support checkpointing. Can you be more specific about h=\r\now it  \n&gt; has\n&gt; failed?\n&gt;\n&gt;\nSome times the check point hangs.\nSome times th=\r\ne check point can&#39;t be reloaded.\nWe are using a complex regex to eliminate =\r\nevery file type except html.\n\n&gt; &gt; 2. Should we switch our production crawl =\r\nto 3.0?\n&gt;\n&gt; It depends on your comfort with H1, your need for H3 features, =\r\nand  \n&gt; your\n&gt; comfort with H3&#39;s changes, especially the changes in configu=\r\nration.\n&gt;\n&gt; If your crawl operators are Java developers or otherwise comfor=\r\ntable\n&gt; with editing XML configuration files, you might prefer some options=\r\n H3\n&gt; offers; if all&#39;s well with your current crawls and you like the H1\n&gt; =\r\nguided-forms crawl-configuration, you could stay with H1 a while.\n&gt;\n&gt; We&#39;ve=\r\n started to use H3 at the Internet Archive for our internal  \n&gt; crawls\n&gt; to=\r\n expand our own collections, but not most of our partner crawls.  \n&gt; That\n&gt;=\r\n will be changing over the course of 2010.\n&gt;\n&gt; &gt; 3. I did not receive an in=\r\nvite to the Internet Archive Heritrix\n&gt; &gt; meeting in San Francisco.\n&gt; &gt; How=\r\n do I qualify for an invite to the the meeting?\n&gt;\n&gt; Step 1 -- if you haven&#39;=\r\nt already done so -- is to fill out the  \n&gt; survey at\n&gt; &lt;http://www.surveym=\r\nonkey.com/s/PWGH959&gt; to express interest.\n&gt;\n&gt;\nI filled it out two months ag=\r\no.\n\n\n&gt; &gt; 4. We are planning on converting our semi manual flow to a Hadoop\n=\r\n&gt; &gt; automated\n&gt; &gt; flow and would like Hertrix to write directly to HDFS. Is=\r\n\n&gt; &gt; HDFSWriterProcessor\n&gt; &gt; going to be part of the main Heritrix release?=\r\n\n&gt;\n&gt; We&#39;d be happy to include it in the main release, but since at IA we\n&gt; =\r\ndon&#39;t expect to write directly to HDFS from our crawls, it&#39;d be best  \n&gt; if=\r\n\n&gt; an outside contributor took the lead on maintaining/supporting it.\n&gt;\nWe =\r\nare HDFS novices.\n\n&gt; &gt; 5. Is it possible to run more than one crawl job in =\r\n1.xxx or do we\n&gt; &gt; need to launch\n&gt; &gt; new instances of the crawler to do so=\r\n?\n&gt;\n&gt; It is *possible*, using the JMX operations, to launch more than one\n&gt;=\r\n &#39;Heritrix instance&#39; inside one JVM, and then each &#39;Heritrix instance&#39;\n&gt; ma=\r\ny run a separate crawl. However, this can be confusing because:\n&gt;\n&gt; - the w=\r\neb UI only reflects one &#39;Heritrix instance&#39; at a time, making  \n&gt; it\n&gt; hard=\r\ner to monitor all the crawls\n&gt; - you have to use JMX to get into this state=\r\n\n&gt; - until 1.14.3, running even just 2 crawls with default settings was\n&gt; h=\r\nighly likely to result in an OutOfMemoryError, because each would try\n&gt; to =\r\nallocate the default 60% of heap (totaling 120%) to their BDB\n&gt; environment=\r\ns. Only by resizing crawl parameters based on how many\n&gt; simultaneous crawl=\r\ns were expected would yield good results. (For  \n&gt; 1.14.3,\n&gt; a new BDB shar=\r\ned-cache feature avoids that largest risk, but still  \n&gt; most\n&gt; of our rule=\r\ns-of-thumb and defaults for crawl settings assume only one\n&gt; crawl is runni=\r\nng in the same JVM/machine.)\n&gt;\n&gt; So, you can do it if you know what you&#39;re =\r\ngetting into, and if your\n&gt; crawls are not very busy/broad, and you&#39;re able=\r\n to monitor outside the\n&gt; web UI, but it&#39;s not the preferred mode of operat=\r\nion.\n&gt;\n&gt; H3 handles multiple simultaneous jobs better; they can be launched=\r\n and\n&gt; monitored via the UI independently. You would still want to configur=\r\ne\n&gt; each crawl to allow for other simultaneous activity, though -- for\n&gt; ex=\r\nample fewer threads than if it were the only crawl running.\n&gt;\n&gt; Still, sinc=\r\ne the crawl job is the unit on which discovered URIs are\n&gt; remembered, ther=\r\ne are often efficiencies to running fewer, larger\n&gt; crawls. Can you say mor=\r\ne about why you want to run smaller independent\n&gt; simultaneous crawls? (Do =\r\nthe results need to isolated from each  \n&gt; other?)\n&gt;\n&gt;\nWe run seed lists th=\r\nat are for a targeted category and limit the  \ncrawls to the domain.\nWhat t=\r\nends to happen is that the crawls start out fast and then the  \nURI&#39;s/sec d=\r\negrades\nover time. We would then like to start another crawl. I don&#39;t know =\r\nif  \nthe slowdown is due\nto a head of line blocking problem in the queues o=\r\nr ... What is  \ninteresting is that there\nare millions of pages in the queu=\r\nes and the URI;s/sec are dropping  \nover time.\n\nAre the queues stored on di=\r\nsk or in RAM?\n\nI can send some png files showing the current crawler state =\r\nand the  \nbandwidth utilization\non the link if that would be interesting.\n\n=\r\n\n&gt; - Gordon @ IA\n&gt;\n&gt; &gt; Best,\n&gt; &gt;\n&gt; &gt; Derek\n&gt; &gt;\n&gt;\n&gt; \n\n\n"}}