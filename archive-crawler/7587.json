{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":264138474,"authorName":"Kenji Nagahashi","from":"Kenji Nagahashi &lt;knagahashi@...&gt;","profile":"kenznag","replyTo":"LIST","senderId":"mXjoMRFRhsgYwnwPv8311vQv01PlVCvgHsEyeJdua5T1k3YSpfsi6E2J9OSou463MlgOlj-D8QTKhu7Z8g90qwnkjuttOGEG6eJ4VYU","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: questions before we restart the crawl","postDate":"1327953695","msgId":7587,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDRGMjZGNzFGLjgwM0BnbWFpbC5jb20+","inReplyToHeader":"PGpnNmNzYytsNDBkQGVHcm91cHMuY29tPg==","referencesHeader":"PGpnNmNzYytsNDBkQGVHcm91cHMuY29tPg=="},"prevInTopic":7586,"nextInTopic":7588,"prevInTime":7586,"nextInTime":7588,"topicId":7527,"numMessagesInTopic":27,"msgSnippet":"David, Ah, then you must have poolMaxActive=1. Try increasing that number to allow Heritirx to write into multiple warc files in parallel. It should improve","rawEmail":"Return-Path: &lt;knagahashi@...&gt;\r\nX-Sender: knagahashi@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 91467 invoked from network); 30 Jan 2012 20:01:40 -0000\r\nX-Received: from unknown (98.137.34.46)\n  by m2.grp.sp2.yahoo.com with QMQP; 30 Jan 2012 20:01:40 -0000\r\nX-Received: from unknown (HELO mail-iy0-f172.google.com) (209.85.210.172)\n  by mta3.grp.sp2.yahoo.com with SMTP; 30 Jan 2012 20:01:39 -0000\r\nX-Received: by iagf6 with SMTP id f6so5878178iag.3\n        for &lt;archive-crawler@yahoogroups.com&gt;; Mon, 30 Jan 2012 12:01:39 -0800 (PST)\r\nX-Received: by 10.42.158.69 with SMTP id g5mr14964881icx.55.1327953699551;\n        Mon, 30 Jan 2012 12:01:39 -0800 (PST)\r\nReturn-Path: &lt;knagahashi@...&gt;\r\nX-Received: from kenji-mbp.local (router300.sf.archive.org. [208.70.27.190])\n        by mx.google.com with ESMTPS id ub10sm249656igb.7.2012.01.30.12.01.38\n        (version=TLSv1/SSLv3 cipher=OTHER);\n        Mon, 30 Jan 2012 12:01:38 -0800 (PST)\r\nMessage-ID: &lt;4F26F71F.803@...&gt;\r\nDate: Mon, 30 Jan 2012 12:01:35 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:9.0) Gecko/20111222 Thunderbird/9.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;jg6csc+l40d@...&gt;\r\nIn-Reply-To: &lt;jg6csc+l40d@...&gt;\r\nContent-Type: text/plain; charset=windows-1252; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Kenji Nagahashi &lt;knagahashi@...&gt;\r\nSubject: Re: [archive-crawler] Re: questions before we restart the crawl\r\nX-Yahoo-Group-Post: member; u=264138474; y=TSevWw64veCr1RIOBuRzSzNf2hrqPqPyWNuNxNTeoE3mFw\r\nX-Yahoo-Profile: kenznag\r\n\r\nDavid,\n\nAh, then you must have poolMaxActive=1. Try increasing that number to \nallow Heritirx to write into multiple warc files in parallel. It should \nimprove your throughput even if you&#39;re writing into single disk. I&#39;m \nusing 6. Only downside of poolMaxActive&gt;1 is crawled resources are \nscattered across multiple WARC files.\n\n--Kenji\n\n(1/30/12 7:27 AM), david_pane1 wrote:\n&gt; Kenji,\n&gt;\n&gt; I didn&#39;t change any of the values in WARCWriterProcessor. It is the default.\n&gt;\n&gt; &lt;bean id=&quot;warcWriter&quot;\n&gt; class=&quot;org.archive.modules.writer.WARCWriterProcessor&quot;&gt;\n&gt; &lt;!-- &lt;property name=&quot;compress&quot; value=&quot;true&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;prefix&quot; value=&quot;IAH&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;suffix&quot; value=&quot;${HOSTNAME}&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;maxFileSizeBytes&quot; value=&quot;1000000000&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;poolMaxActive&quot; value=&quot;1&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;MaxWaitForIdleMs&quot; value=&quot;500&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;skipIdenticalDigests&quot; value=&quot;false&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;maxTotalBytesToWrite&quot; value=&quot;0&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;directory&quot; value=&quot;${launchId}&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;storePaths&quot;&gt;\n&gt; &lt;list&gt;\n&gt; &lt;value&gt;warcs&lt;/value&gt;\n&gt; &lt;/list&gt;\n&gt; &lt;/property&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;writeRequests&quot; value=&quot;true&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;writeMetadata&quot; value=&quot;true&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;writeRevisitForIdenticalDigests&quot; value=&quot;true&quot; /&gt; --&gt;\n&gt; &lt;!-- &lt;property name=&quot;writeRevisitForNotModified&quot; value=&quot;true&quot; /&gt; --&gt;\n&gt; &lt;/bean&gt;\n&gt;\n&gt; --David\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt; &lt;knagahashi@...&gt; wrote:\n&gt;  &gt;\n&gt;  &gt; David,\n&gt;  &gt;\n&gt;  &gt; It is fairly common H3 goes 2x~3x faster at the beginning, where URI/s\n&gt;  &gt; figure includes lots of DNS queries, many queues are ready, state\n&gt;  &gt; database is smaller, etc. I often see my crawlers running at &gt;200URI/s,\n&gt;  &gt; too. (Also URI/s shown on the web UI is not really reliable.)\n&gt;  &gt;\n&gt;  &gt; It is difficult to tell how many queues are enough to keep all threads\n&gt;  &gt; busy, as there are many factors affecting crawl speed. Assuming 0.5 sec\n&gt;  &gt; processing time per URI and 100% concurrency, 1200 thread could do 2400\n&gt;  &gt; URI/s. On the other hand, assuming constant crawl delay of 3sec, 4000\n&gt;  &gt; active queues can only emit 1333 URI/s. In this case, active queue size\n&gt;  &gt; becomes the limiting factor. If processing time per URI becomes 1 sec,\n&gt;  &gt; then processing time becomes a bottleneck. Some queues get snoozed much\n&gt;  &gt; longer than 3 sec and it makes URI emit rate lower. I tried drawing a\n&gt;  &gt; active-queue to crawl-speed graph, but what I can say is that there is a\n&gt;  &gt; strong correlation between them (roughly, 5000 queues -&gt; 40URI/s, 8000\n&gt;  &gt; queue -&gt; 70URI/s ; variance is really big).\n&gt;  &gt;\n&gt;  &gt; Looking at your thread and frontier report, I noticed:\n&gt;  &gt;\n&gt;  &gt; - 994 queues are &quot;ready&quot;\n&gt;  &gt; - 911 threads are in warcWriter\n&gt;  &gt;\n&gt;  &gt; that is, you have enough active queues to keep threads busy, but threads\n&gt;  &gt; are spending too much time on writing WARCs to process URIs on time.\n&gt;  &gt; WarcWriter is the bottleneck in this case, probably because of small\n&gt;  &gt; number of concurrent writers? What is your warcWriter.poolMaxActive?\n&gt;  &gt;\n&gt;  &gt; --Kenji\n&gt;  &gt;\n&gt;  &gt; (1/27/12 7:32 AM), david_pane1 wrote:\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Kenji,\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; You understood correctly 25M pages/day for 5 machines. Right now, with\n&gt;  &gt; &gt; three instances running, we are seeing around 17M pages per day. This\n&gt;  &gt; &gt; makes me think that we may be saturating our network throughput.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; But, we have seen, at the beginning of the crawl, 250-300 URIs/second\n&gt;  &gt; &gt; (an average of 52M pages/day over the first 3 days of the crawl.\n&gt; Once we\n&gt;  &gt; &gt; get past the first few days, the crawl slows. During mid crawl, we\n&gt; found\n&gt;  &gt; &gt; that stopping the crawl and restarting it improves the throughput but\n&gt;  &gt; &gt; never back to 50M pages/day.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; How many queues do you need active per thread? Wouldn&#39;t Heritrix\n&gt;  &gt; &gt; activate more queues if there are enough threads to handle them? We\n&gt;  &gt; &gt; certainly have a lot of queues.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; As an example, on one instance we are seeing a rate of 93.25 URIs/sec\n&gt;  &gt; &gt; average.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Load:\n&gt;  &gt; &gt; 1176 active of 1176 threads; 3,456.5 congestion ration 13688365 deepest\n&gt;  &gt; &gt; queue; 62 average depth\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Threads:\n&gt;  &gt; &gt; 1176 threads: 1176 ABOUT_TO_BEGIN_PROCESSOR; 911 warcWriter, 262\n&gt;  &gt; &gt; fetchHttp, 2 candidates, 1 extractorHtml\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; Frontier:\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; RUN 12955728 URI queues: 4302 active (1200 in-process; 994 ready; 2108\n&gt;  &gt; &gt; snoozed); 11429799 inactive; 0 ineligible; 0 retired; 1521627\n&gt; exhausted.\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; --David\n&gt;  &gt; &gt;\n&gt;  &gt; &gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt;  &gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt;  &gt; &gt; &lt;knagahashi@&gt; wrote:\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; David,\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I understood &quot;25M + page/day&quot; was total for 5 machines, as you wrote\n&gt;  &gt; &gt; &gt; &quot;(these numbers are totals of all 5 instances combined)&quot;. Did you\n&gt; mean\n&gt;  &gt; &gt; &gt; 25M+ page/day/instance? If so, my &quot;100 threads&quot; comment is pointless.\n&gt;  &gt; &gt; &gt; Please disregard it.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; I&#39;m running broad crawl that captures everything linked: images,\n&gt; script,\n&gt;  &gt; &gt; &gt; CSS, PDF, Excel, ... even mpeg4 videos. Our Heritrix 3 runs on 8GB\n&gt;  &gt; &gt; &gt; memory + 4 core virtual machine (KVM), with 100 threads. it goes\n&gt;  &gt; &gt; &gt; ~60URI/s on average (per instance). Probably we could go as high\n&gt; as 150\n&gt;  &gt; &gt; &gt; threads to get higher crawl speed, but it comes with higher risk of\n&gt;  &gt; &gt; &gt; dying of OutOfMemoryError, empirically. Crawl speed is also\n&gt; limited by\n&gt;  &gt; &gt; &gt; lower disk I/O performance of VMs. 100 seems to be a good number\n&gt; for us.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; Yes, increasing threads brings significant increase in crawl\n&gt; speed, to\n&gt;  &gt; &gt; &gt; certain extent. If you don&#39;t have enough &quot;active queues,&quot; threads are\n&gt;  &gt; &gt; &gt; just wasted. There are other bottleneck, too, and it can change over\n&gt;  &gt; &gt; time.\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; At least we&#39;re getting sustained 60URI/s level of speed with 100\n&gt;  &gt; &gt; &gt; threads. With 1,200 threads and enough active queues, you should be\n&gt;  &gt; &gt; &gt; getting crawl speed much much higher than that (I&#39;ve never been\n&gt; able to\n&gt;  &gt; &gt; &gt; run my crawler with 1200 threads, though!)\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; --Kenji\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; (1/26/12 10:31 AM), david_pane1 wrote:\n&gt;  &gt; &gt; &gt; &gt; Kenji,\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; Are you saying that you can get 25M pages per day on 100\n&gt; threads and 1\n&gt;  &gt; &gt; &gt; &gt; instance or 25M URIs/day? Do you capture all images, pdfs, and\n&gt;  &gt; &gt; &gt; &gt; supporting page documents or are you just capturing html pages?\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; My test crawls before running our large crawl showed significant\n&gt;  &gt; &gt; &gt; &gt; increase in the number of pages captured when we increased the\n&gt;  &gt; &gt; number of\n&gt;  &gt; &gt; &gt; &gt; threads.\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; --David\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; --- In archive-crawler@yahoogroups.com\n&gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt;  &gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;\n&gt;  &gt; &gt; &gt; &gt; &lt;mailto:archive-crawler%40yahoogroups.com&gt;, Kenji Nagahashi\n&gt;  &gt; &gt; &gt; &gt; &lt;knagahashi@&gt; wrote:\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; Hi,\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; May be a bit off-topic, but 25M/day with 5 machine is average\n&gt;  &gt; &gt; 58/s per\n&gt;  &gt; &gt; &gt; &gt; &gt; machine. Since I know Heritrix-3 can crawl at this speed with\n&gt;  &gt; &gt; just 100\n&gt;  &gt; &gt; &gt; &gt; &gt; ToeThreads, I wonder if most of your 1200 ToeThreads are idle.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; While why you don&#39;t get much higher speed with 1200 threads\n&gt; is a big\n&gt;  &gt; &gt; &gt; &gt; &gt; question, it may make sense to cut down the number of\n&gt; ToeThreads if\n&gt;  &gt; &gt; &gt; &gt; &gt; you&#39;re okay with current crawl speed. Less threads will make\n&gt; H3 less\n&gt;  &gt; &gt; &gt; &gt; &gt; susceptible to memory problems... Just a thought.\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; --Kenji\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; (1/20/12 9:54 PM), David Pane wrote:\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; Gordon,\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; Thank you for your response. And I am sorry for the\n&gt;  &gt; &gt; overwhelming amount\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; of information...I think I am a little overwhelmed.... and\n&gt;  &gt; &gt; feeling the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; pressure.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; 1) Our Bloom filter configuration:\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;bean id=&quot;uriUniqFilter&quot;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; class=&quot;org.archive.crawler.util.BloomUriUniqFilter&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;property name=&quot;bloomFilter&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;bean class=&quot;org.archive.util.BloomFilter64bit&quot;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;constructor-arg value=&quot;400000000&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;constructor-arg value=&quot;30&quot;/&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;/property&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &lt;/bean&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; 2) We are writing the crawl data to a NAS configured with\n&gt; RAID 6.\n&gt;  &gt; &gt; &gt; &gt; We did\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; see some problems with disk errors on the NAS earlier in\n&gt; the crawl\n&gt;  &gt; &gt; &gt; &gt; (late\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; Dec ). I recently found this out. We were/are running in a\n&gt; degraded\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; raid state - a few of the disks have been replaced and the\n&gt; RAID is\n&gt;  &gt; &gt; &gt; &gt; being\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; rebuilt. We didn&#39;t see any block device errors in the logs on\n&gt;  &gt; &gt; the NAS\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; so the write failures we saw are probably not related to the\n&gt;  &gt; &gt; &gt; &gt; rebuild. We\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; did see some network hiccups (no outright failures) in the\n&gt; logs.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; So, this may be the culprit for some of the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; 3) Yes, we have been cross-feeding URIs.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; --David\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; On 1/21/12 12:22 AM, Gordon Mohr wrote:\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; You&#39;ve provided an overwhelming amount of information and we\n&gt;  &gt; &gt; may be\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; dealing with multiple issues, some of which have roots\n&gt; going back\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; earlier than the diagnostic data we now have available.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; A few key points of emphasis:\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; - we&#39;ve not run crawls with 1200 threads before, or on\n&gt; hardware\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; similar to yours, so our experience is only vaguely\n&gt; suggestive\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; - it&#39;s not the lower thread counts that are the real\n&gt; source of\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; concern; you can even adjust the number of threads mid-crawl.\n&gt;  &gt; &gt; It&#39;s\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; that the error that killed the threads almost certainly left\n&gt;  &gt; &gt; a queue\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; in a &#39;phantom&#39; state where no progress would be made\n&gt; crawling its\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; URIs, each time it happened, on each resume leading to the\n&gt;  &gt; &gt; &gt; &gt; current state.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; - without having understood and fixed whatever software\n&gt; or system\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; problems caused the earliest/most-foundational errors in your\n&gt;  &gt; &gt; crawl,\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; it&#39;s impossible to say how likely they are to recur.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; With that in mind, I&#39;ll try to provide quick answers to your\n&gt;  &gt; &gt; other\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; questions...\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; On 1/20/12 4:20 PM, David Pane wrote:\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; We have collected about 550 million pages along with the\n&gt;  &gt; &gt; images and\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; supporting documents on our 5 instance crawl that was\n&gt; started\n&gt;  &gt; &gt; &gt; &gt; Dec. 23rd.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; Although we are please with the amount of data we\n&gt; captured to\n&gt;  &gt; &gt; &gt; &gt; date, we\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; are very concerned about the state of the Heritrix\n&gt; instances. If\n&gt;  &gt; &gt; &gt; &gt; fact,\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; we aren&#39;t very confident that the instances will last\n&gt; until the\n&gt;  &gt; &gt; &gt; &gt; end of\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; February. We are now running on a total of over 500 less\n&gt; threads\n&gt;  &gt; &gt; &gt; &gt; than\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; the configured 1200 threads/instance.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 0 - not running right now.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 1 - running on 1198 ( 2 less)\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 2 - running on 931 (269 less)\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 3 - running on 987 (213 less)\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 4 - running on 1170 (30 less)\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; Since we are seriously considering throwing away this past\n&gt;  &gt; &gt; &gt; &gt; month&#39;s work\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; and starting over, we would like to pick your brain on some\n&gt;  &gt; &gt; &gt; &gt; strategies\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; that will help us avoid getting into this situation again.\n&gt;  &gt; &gt; We were\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; hoping to be done crawling by the end of February so this\n&gt;  &gt; &gt; &gt; &gt; restart will\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; put us behind schedule.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 1) Can we continue from here but with &quot;clean&quot; Heritrix\n&gt;  &gt; &gt; instances?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; Is there a way that we can continue from the this point\n&gt;  &gt; &gt; forward, but\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; start with Heritrix instances that will not be corrupt due\n&gt;  &gt; &gt; to sever\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; error? (e.g. using the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;\n&gt;  &gt; &gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery\n&gt; &lt;https://webarchive.jira.com/wiki/display/Heritrix/Crawl+Recovery&gt;&gt;&gt;&gt; ) If\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; so, would you recommend doing this? You mentioned that this\n&gt;  &gt; &gt; could be\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; time consuming. Each of our instances has downloaded\n&gt; around 170M\n&gt;  &gt; &gt; &gt; &gt; URIs,\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; they have over 700M queued URIs, what is your time\n&gt; estimate for\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; something this large?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; We are willing to sacrifice a few days to get our crawler to\n&gt;  &gt; &gt; a clean\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; state again so we can crawl for another 30 days at the\n&gt; pace we\n&gt;  &gt; &gt; &gt; &gt; have been\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; crawling.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; You can do a big &#39;frontier-recover&#39; log replay to avoid\n&gt;  &gt; &gt; &gt; &gt; recrawling the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; same URIs, and approximate the earlier queue state.\n&gt;  &gt; &gt; Splitting/filters\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; the logs manually beforehand as alluded to in the wiki page\n&gt;  &gt; &gt; can speed\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; this process somewhat... but given the size of all your\n&gt;  &gt; &gt; log-segments\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; that log grooming beforehand is itself likely to be a lengthy\n&gt;  &gt; &gt; &gt; &gt; process.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; I don&#39;t think we&#39;ve ever done it with logs of 170M\n&gt; crawled / 870M\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; discovered before, nor on any hardware comparable to yours.\n&gt;  &gt; &gt; So it&#39;s\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; impossible to project its duration in your environment. It&#39;s\n&gt;  &gt; &gt; &gt; &gt; taken 2-3\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; days for us on smaller crawls, slower hardware.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; An added complication is that this older frontier-recover-log\n&gt;  &gt; &gt; replay\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; technique happens in its own thread separate from the\n&gt;  &gt; &gt; checkpointing\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; process, so it is not, itself, accurately checkpointed\n&gt; during the\n&gt;  &gt; &gt; &gt; &gt; long\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; reload process.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; At nearly 1B discovered URIs per node, even if you are\n&gt; using the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; alternate BloomUriUniqFilter, if you are using it at its\n&gt;  &gt; &gt; default size\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; (~500MB) it will now be heavily saturated and thus\n&gt; returning many\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; false-positives causing truly unique URIs to be rejected as\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; duplicates. (If you&#39;re using a significantly larger filter,\n&gt;  &gt; &gt; you may\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; not yet be at a high false-positive rate: you&#39;d have to do\n&gt;  &gt; &gt; the bloom\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; filter math. If you&#39;re still using BdbUriUniqFilter, you&#39;re\n&gt;  &gt; &gt; way way\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; past the point where its disk seeks have usually made it too\n&gt;  &gt; &gt; slow for\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; our purposes.)\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 2) What can be done to avoid corrupting the Heritrix\n&gt; instances?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - What kind of strategies might we take to keep the\n&gt; crawl error\n&gt;  &gt; &gt; &gt; &gt; free?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Do you think the SEVER errors that we have seen are\n&gt;  &gt; &gt; &gt; &gt; deterministic or\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; random (e.g., triggered by occasional flaky network\n&gt; conditions,\n&gt;  &gt; &gt; &gt; &gt; disks,\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; race conditions, or whatever)?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Hard to say. The main thing I could suggest is watch very\n&gt;  &gt; &gt; closely and\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; when a SEVERE error occurs, prioritize diagnosing and\n&gt;  &gt; &gt; resolving the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; cause while the info is fresh.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Do you believe that we can reliably backup to the previous\n&gt;  &gt; &gt; &gt; &gt; checkpoint\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; if we watch the logs and stop as soon as we see the\n&gt; first SEVER\n&gt;  &gt; &gt; &gt; &gt; error?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; If we do this, do you speculate that the same SEVER will\n&gt; occur\n&gt;  &gt; &gt; &gt; &gt; again?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Resuming from the latest checkpoint before an error\n&gt; believed to\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; corrupt the on-disk state will be the best strategy.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; If we never figure out the real cause, but run the same\n&gt;  &gt; &gt; software on\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; the same machine, yes, I expect the same problem will recur!\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Is there any reason why a Heritrix instance that is\n&gt; run while\n&gt;  &gt; &gt; &gt; &gt; binded\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; to one ip address can&#39;t be resumed binded to a different ip\n&gt;  &gt; &gt; address?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Only the web UI to my knowledge binds to a chosen address,\n&gt;  &gt; &gt; and it is\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; common to have it bind to all. I don&#39;t expect the outbound\n&gt;  &gt; &gt; requests\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; would be hurt by a machine changing its IP address while the\n&gt;  &gt; &gt; &gt; &gt; crawl was\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; running, but I would run a test to be sure if that was an\n&gt;  &gt; &gt; important,\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; expected transition.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 3) Should we configure the crawler with more instances and\n&gt;  &gt; &gt; switch\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; between them?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; We have seen that we can run a single instance to 100M\n&gt; pages +\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; supporting images and documents. Perhaps this means that\n&gt; we need\n&gt;  &gt; &gt; &gt; &gt; 10 or\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; more instances instead of 5. That raises the possibility of\n&gt;  &gt; &gt; &gt; &gt; running 2\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; instances per machine. If we could run 2, or even 4,\n&gt;  &gt; &gt; instances on a\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; single machine, they would each run half as long.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; I don&#39;t think the problems as reported are specifically due\n&gt;  &gt; &gt; to one\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; node&#39;s progress growing beyond a certain size, but it might\n&gt;  &gt; &gt; be the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; case that giant instances are more likely to suffer from, and\n&gt;  &gt; &gt; harder\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; to recover from, single glitches (eg a single disk\n&gt; error). On the\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; other hand, many instances introduce more redundant overhead\n&gt;  &gt; &gt; costs\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; (certain data structures, cross-feeding URIs if you&#39;re doing\n&gt;  &gt; &gt; &gt; &gt; that, etc.).\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Can you suggest a way to start/stop instances from a\n&gt; script so\n&gt;  &gt; &gt; &gt; &gt; we can\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; change between instances automatically?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Not a mode I&#39;ve thought much about.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Have you seen frequent starting / stopping of instances\n&gt;  &gt; &gt; introduce\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; instability?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; No... but it might make you notice latent issues sooner.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 4) Crawl slows but restarting seems to improve the speed\n&gt; again.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; We noticed that the all of our instances would initially\n&gt; run at\n&gt;  &gt; &gt; &gt; &gt; a fast\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; pace. We would collect an average of 25M + pages/day for 2-3\n&gt;  &gt; &gt; &gt; &gt; days and\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; then the crawl would slow down to 10M pages/day over the\n&gt; next\n&gt;  &gt; &gt; &gt; &gt; few days.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; (these numbers are totals of all 5 instances combined).\n&gt; When we\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; restarted the instances, the average pages would improve\n&gt; back to\n&gt;  &gt; &gt; &gt; &gt; 25M +\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; pages/day. The total crawled numbers (TiB) also\n&gt; reflected the\n&gt;  &gt; &gt; &gt; &gt; slow down.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Is this something that others have experienced as well?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; I don&#39;t recall hearing other reports of speed boosts after\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; checkpoint-resumes but others may have more experience.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 5) We are capturing tweets from twitter, harvesting the\n&gt; urls and\n&gt;  &gt; &gt; &gt; &gt; want to\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; crawl those urls within 1 day of receiving the tweet.\n&gt; Can you\n&gt;  &gt; &gt; &gt; &gt; recommend\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; a strategy for doing this with the 5 instances we are\n&gt; running?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; - Do we need to run a separate crawler dedicated to\n&gt; this? If so,\n&gt;  &gt; &gt; &gt; &gt; can you\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; suggest a way to crawl out from the tweeted urls but\n&gt; when we get\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; additional urls from the tweets, quickly change focus to\n&gt;  &gt; &gt; these urls\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; instead of the ones branching out. When adding urls as\n&gt; seeds,\n&gt;  &gt; &gt; &gt; &gt; can you\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; set a high priority to crawl those before the discovered\n&gt; urls?\n&gt;  &gt; &gt; &gt; &gt; Do you\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; recommend maybe setting up a specific crawl for these\n&gt; urls and\n&gt;  &gt; &gt; &gt; &gt; then only\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; crawl a few hopes from the seeds - injecting the urls\n&gt; from the\n&gt;  &gt; &gt; &gt; &gt; tweets as\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; seeds?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Dedicating a special script or crawler to URIs that come from\n&gt;  &gt; &gt; such a\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; constrained source (Twitter feeds), or that need to be\n&gt;  &gt; &gt; crawled in a\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; special timeframe, or according to other special limits\n&gt;  &gt; &gt; (fewer hops),\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; could make sense.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; It would take some customization of the queueing-policy or\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; &#39;precedence&#39; features of Heritrix to allow URIs added\n&gt;  &gt; &gt; mid-crawl to be\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; prioritized above those already discovered and queued.\n&gt; The most\n&gt;  &gt; &gt; &gt; &gt; simple\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; possible customization might be a UriPrecedencePolicy that\n&gt;  &gt; &gt; takes all\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; zero-hop URIs (which all seeds and most direct-fed URIs would\n&gt;  &gt; &gt; be) and\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; gives them a higher precedence (lower precedence number)\n&gt; than all\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; other URIs.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; 6) I think the answer is no for this question, but I\n&gt; will ask it\n&gt;  &gt; &gt; &gt; &gt; anyway.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; If you have a Heritrix instance that is configured for 1200\n&gt;  &gt; &gt; &gt; &gt; threads on\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; one machine, can you recover from a checkpoint from that\n&gt;  &gt; &gt; 1200 thread\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; configuration on a different machine with an Heritrix\n&gt; instance\n&gt;  &gt; &gt; &gt; &gt; that is\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;&gt; configured for less threads (e.g. the default 25 threads)?\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; Yes - there&#39;s no need to keep the thread count the same\n&gt; after a\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; resume. None of the checkpoint structures (or usual disk\n&gt;  &gt; &gt; structures)\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; are based on the number of worker threads\n&gt; (&#39;ToeThreads&#39;)... as\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; mentioned above you can even vary the number of threads in a\n&gt;  &gt; &gt; running\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; crawl.\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt; - Gordon\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt; &gt;\n&gt;  &gt; &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt; &gt;\n&gt;  &gt;\n&gt;\n&gt; \n\n\n"}}