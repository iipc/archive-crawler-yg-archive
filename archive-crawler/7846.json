{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"a6r3yk508muJWI3_54d3ygCya30XfqOh2rCXsYI-C2QS9qtuSOkwTQcWpE5wSbOcveJFBUyN2ymbe1lgXjstMIW0sDsqbM0","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [archive-crawler] strange cycles with crawling speed","postDate":"1351636656","msgId":7846,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDUwOTA1NkIwLjkwNTA2MDVAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGs2b3BvZCtvb2hiQGVHcm91cHMuY29tPg==","referencesHeader":"PGs2b3BvZCtvb2hiQGVHcm91cHMuY29tPg=="},"prevInTopic":7845,"nextInTopic":7847,"prevInTime":7845,"nextInTime":7847,"topicId":7845,"numMessagesInTopic":6,"msgSnippet":"... The predominance of warcWriter indicates a traffic-jam at that stage. If you re writing all WARCs to a single disk, and perhaps even to a single open","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 53463 invoked from network); 30 Oct 2012 22:37:38 -0000\r\nX-Received: from unknown (98.137.35.162)\n  by m2.grp.sp2.yahoo.com with QMQP; 30 Oct 2012 22:37:38 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta6.grp.sp2.yahoo.com with SMTP; 30 Oct 2012 22:37:38 -0000\r\nX-Received: (qmail 57844 invoked by uid 0); 30 Oct 2012 22:37:37 -0000\r\nX-Received: from 70.36.143.78 (HELO silverbook.local) (70.36.143.78)\n  by relay02.pair.com with SMTP; 30 Oct 2012 22:37:37 -0000\r\nX-pair-Authenticated: 70.36.143.78\r\nMessage-ID: &lt;509056B0.9050605@...&gt;\r\nDate: Tue, 30 Oct 2012 15:37:36 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:16.0) Gecko/20121010 Thunderbird/16.0.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;k6opod+oohb@...&gt;\r\nIn-Reply-To: &lt;k6opod+oohb@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] strange cycles with crawling speed\r\nX-Yahoo-Group-Post: member; u=137285340; y=MFE7uy0OgHQKcOXIfJbOtQEnWQbCOCZ9Fxr1aeiMHUWx\r\nX-Yahoo-Profile: gojomo\r\n\r\nOn 10/30/12 7:55 AM, rjoberon80 wrote:\n&gt; While I was writing this post the crawler seems to stabilize and\n&gt; possibly the problem is solved. Still, I post it to find an\n&gt; explanation for the behavior and maybe for people which observe a\n&gt; similar behavior.\n&gt;\n&gt; My crawler is now running for 20 days and has crawled some 3.4TB. We\n&gt; had to buy new disk space and restart it from a checkpoint and since\n&gt; then it does not run smooth any longer. The only change I made\n&gt; (besides adding the hard disk) was to double the heap from 1GB to\n&gt; 2GB.\n&gt;\n&gt; The problem can best be described by having a look at this graph:\n&gt; http://www.flickr.com/photos/89400617@N02/8138463405/\n&gt;\n&gt; There is a cyclic change between high throughput and no throughput,\n&gt; e.g., between\n&gt;\n&gt; 76.45 URIs/sec (23.28 avg); 8794 KB/sec (2158 avg) 95 threads: 95\n&gt; ABOUT_TO_BEGIN_PROCESSOR; 75 fetchHttp, 14 extractorHtml, 6\n&gt; warcWriter\n&gt;\n&gt; and\n&gt;\n&gt; 0 URIs/sec (23.28 avg); 0 KB/sec (2159 avg) 95 threads: 95\n&gt; ABOUT_TO_BEGIN_PROCESSOR; 94 warcWriter, 1 fetchHttp\n&gt;\n&gt; (the averages in brackets are over the last 20 days)\n\nThe predominance of &#39;warcWriter&#39; indicates a traffic-jam at that stage. \nIf you&#39;re writing all WARCs to a single disk, and perhaps even to a \nsingle open file at once, then one extra-large (100s of MB, \nmultiple-GB?) response can sometimes cause a pile-up. It will be even \nworse if the &#39;scratch&#39; and &#39;warc&#39; destinations are the same disk volume.\n\nI have a hunch that certain JVM/OS/virtualization configurations \naffecting thread/IO/CPU scheduling might make such situations worse, but \nno firm theories or ways to further investigate.\n\nYou could use the &#39;threads report&#39; or other JVM stack dumps (via th \n&#39;jstack&#39; tool or sending SIGQUIT to the JVM process) to get a better \nidea of what threads are up to (or blocked on) during the slow periods. \nYou could use OS tools like &#39;iostat&#39; to get a better idea of disk usage \naround the episodes.\n\nSometimes even using larger JVM object heaps cause more noticeable \npauses due to occasional global garbage-collection cycles (though larger \nheaps should usually help with throughput... but by having fewer larger \nfull-GCs they stick out more).\n\nBut from the description I suspect concentrating on the available write \nIO for the warcWriter may offer the biggest help: writing multiple WARCs \nat a time to volumes that are independent of each other (using the \npoolMaxActive and storePaths settings of WARCWriterProcessor), and \nseparate from the &#39;scratch&#39; path.\n\n&gt; In the high times I reach values I never had before, e.g., 128.24\n&gt; URIs/sec (23.28 avg); 15162 KB/sec (2161 avg) and more. In the last\n&gt; 20 days the crawling was much more constant with a slow decrease (as\n&gt; can be seen here\n&gt; http://www.flickr.com/photos/89400617@N02/8138481733/).\n&gt;\n&gt; Does anybody have any idea what is happening and how I can fix it?\n&gt;\n&gt; I first thought about too many file descriptors but on average 360\n&gt; are open (max observed is 575) and the limit is 1024 (I increased it\n&gt; to 2048, though). The performance of the new hard disks should also\n&gt; not be the problem, I checked this with hdparm.\n\nRunning out of FDs can be fatal to a process and having a small max is \nhardly ever helpful, so we typically raise this to 32K, 64K, or as high \nas the OS allows before using a machine for crawling (or regret not \ndoing so if forgotten).\n\n&gt; Now the crawl seems to stabilize, see\n&gt; http://www.flickr.com/photos/89400617@N02/8138552692/ with 42.38\n&gt; URIs/sec (23.31 avg); 6627 KB/sec (2165 avg). An explanation of this\n&gt; behaviour would be useful. Could it be that after the restart all 100\n&gt; threads were fetching and writing together (strictly synchronized)\n&gt; and that the hard disk could not master the IO? And after some time\n&gt; they got off-sync and there is now a more distributed fetching and\n&gt; writing?\n\nHard to say. Just working on a different mix of URIs (smaller \nrespponses?) could make a big difference. Or maybe something about the \nJVM/OS/virtualization scheduling rebalanced/readjusted after a restart. \nYou also always want to make sure there&#39;s absolutely no virtual-memory \nswapping affecting a system running a Java process (including in ways \nthat may be invisible to virtualization-guests).\n\nOthers have sometimes noticed that a restarted crawl seems to have a \n&#39;faster progress&#39; honeymoon period, but the exact reasons why haven&#39;t \nbeen isolated.\n\nHope this helps,\n\n- Gordon\n\n\n&gt; I would appreciate any input!\n&gt;\n&gt; Best regards, Robert\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------\n&gt;\n&gt; Yahoo! Groups Links\n&gt;\n&gt;\n&gt;\n\n"}}