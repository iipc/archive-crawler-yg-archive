{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"&quot;Gordon Mohr&quot; &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"kpgSEaaAS1d2GlqBPVViqt0cD5INITDt42csly6gEMQTOVVyclIWj8U6DSFcA26LYA3fe5RAH3ma21mzaVRBRzzoyPYUAmtR5w","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: Testing web garden...","postDate":"1056480329","msgId":81,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDAwMGYwMWMzM2E4MCRkYjA0YjZkMCQ0OGYwZWRkMUBXT1JLU1RBVElPTjIxPg=="},"prevInTopic":78,"nextInTopic":0,"prevInTime":80,"nextInTime":82,"topicId":78,"numMessagesInTopic":2,"msgSnippet":"We re getting a good collection of tests at... http://crawl08.archive.org/index-2.html and http://crawl08.archive.org/newtest/ But, could we split them up as","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 75714 invoked from network); 24 Jun 2003 18:46:25 -0000\r\nReceived: from unknown (66.218.66.217)\n  by m5.grp.scd.yahoo.com with QMQP; 24 Jun 2003 18:46:25 -0000\r\nReceived: from unknown (HELO mail.archive.org) (209.237.232.56)\n  by mta2.grp.scd.yahoo.com with SMTP; 24 Jun 2003 18:46:24 -0000\r\nReceived: from WORKSTATION21 (b116-dyn-72.archive.org [209.237.240.72])\n\tby mail.archive.org (8.12.8/8.10.2) with SMTP id h5OHuLlQ011300\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Tue, 24 Jun 2003 10:56:36 -0700\r\nMessage-ID: &lt;000f01c33a80$db04b6d0$48f0edd1@WORKSTATION21&gt;\r\nTo: &lt;archive-crawler@yahoogroups.com&gt;\r\nSubject: Re: Testing web garden...\r\nDate: Tue, 24 Jun 2003 11:45:29 -0700\r\nMIME-Version: 1.0\r\nContent-Type: text/plain;\n\tcharset=&quot;iso-8859-1&quot;\r\nContent-Transfer-Encoding: 7bit\r\nX-Priority: 3\r\nX-MSMail-Priority: Normal\r\nX-Mailer: Microsoft Outlook Express 6.00.2800.1158\r\nX-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1165\r\nFrom: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\r\nX-Yahoo-Group-Post: member; u=137285340\r\nX-Yahoo-Profile: gojomo\r\n\r\nWe&#39;re getting a good collection of tests at...\n\n   http://crawl08.archive.org/index-2.html\nand \n   http://crawl08.archive.org/newtest/\n\nBut, could we split them up as mentioned in my previous message (below)?\n\nThis should take the form of separate &quot;Crawler Entry&quot; pages for each\nindependent test set. (There could also be an overall entry page for\n&quot;all&quot;.)\n\nAlso, the tests should be versioned in our sourceforge CVS as a \nnew module.  You&#39;ll probably have to use the CVS command-line tools\nto do this. Let&#39;s call the module &quot;CrawlGarden&quot;. \n\nAnother important distinction to make in the CVS versioning is \nseparating those tests that can be checked out anywhere (including\nother people&#39;s web test servers) and those that have hardcoded \ndomains. \n\nAs soon as there&#39;s a stable list of simple tests -- even if small -- we \nshould run Mercator against it to completion, and save the ARCs/logs.\n\nThen, this process should be further automated:\n (1) a single script that checks out the latest (or a particular\n     version) of the CrawlGarden, runs Mercator (or Heritrix) \n     against it, prints some summary of the results and/or \n     archives the ARCs/logs to a set place\n (2) a scheduled repeat of the above on certain evenings\n\n- Gordon\n\n----- Original Message ----- \nFrom: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\nTo: &lt;archive-crawler@yahoogroups.com&gt;\nSent: Wednesday, June 18, 2003 4:51 PM\nSubject: Re: Testing web garden...\n\n\n&gt; [moving discussion to arcive-crawler@yahoogroups.com]\n&gt; \n&gt; Looks good!\n&gt; \n&gt; I think we&#39;ll want to split up the tests into at least 3 non-overlapping\n&gt; (not cross-linked) sets:\n&gt; \n&gt;  (1) The easy: stuff every crawler must handle, in a clear way,\n&gt;      and we expect our crawler to handle soon. (Basic HTML and \n&gt;      embedded images.)\n&gt; \n&gt;  (2) The hard: stuff we&#39;d like to handle, perhaps very difficult\n&gt;      (eg javascript which builds a URI string), but ultimately\n&gt;      with a clearly right end result. \n&gt; \n&gt;  (3) The indeterminate: stuff that can only be handled by making\n&gt;      some policy decision which is not inherently right or wrong,\n&gt;      such as determining when we give up on a URI/site/path.\n&gt;      (The infinites fit here.)\n&gt; \n&gt; I like the infinitelybroad script. Some form of it would be useful for \n&gt; internal speed profiling, too. \n&gt; \n&gt; It would be ideal if its synthesized target hosts/URIs were \n&gt; deterministic, so that two crawler visits entering at the same\n&gt; place would be reproduceable/comparable (up to the point where\n&gt; they gave up). \n&gt; \n&gt; Also, if &quot;infinite&quot; is only up to &quot;www99999999&quot;, we can visit all\n&gt; those. :) 100 million pages isn&#39;t so many considering our eventual \n&gt; goals!\n&gt; \n&gt; - Gordon\n&gt; \n&gt; ----- Original Message ----- \n&gt; From: &quot;Parker Thompson&quot; &lt;parkert@...&gt;\n&gt; To: &quot;Gordon Mohr&quot; &lt;gojomo@...&gt;\n&gt; Cc: &lt;igor@...&gt;; &lt;jma@...&gt;\n&gt; Sent: Wednesday, June 18, 2003 4:29 PM\n&gt; Subject: Re: Testing web garden...\n&gt; \n&gt; \n&gt; &gt; Gordon,\n&gt; &gt; \n&gt; &gt; I&#39;ve started filling in some pages that we can use to test the crawl.  I\n&gt; &gt; copied ideas but not content from the searchtools pages, and we may be\n&gt; &gt; able to copy content pending a request I made to the maintainers of that\n&gt; &gt; site.\n&gt; &gt; \n&gt; &gt; Additionally, I wrote a perl script that I believe we can use to do the\n&gt; &gt; infintely broad/duplicate content test.  It&#39;s in the newtest/ directory:\n&gt; &gt; \n&gt; &gt;   http://crawl08.archive.org/newtest/\n&gt; &gt; \n&gt; &gt; but is non-functional there.  We&#39;ll need to set up a host with wildcard\n&gt; &gt; aliasing.  To see this in action check out:\n&gt; &gt; \n&gt; &gt;  http://parkert.com/infinitelybroad.cgi\n&gt; &gt; \n&gt; &gt; Please take a look at the framework and let me know if there&#39;s anything \n&gt; &gt; fundementally wrong with it, any top/second-level categories I&#39;ve \n&gt; &gt; omitted, etc.  \n&gt; &gt; \n&gt; &gt; Thanks,\n&gt; &gt; \n&gt; &gt; pt.\n&gt; &gt; -- \n&gt; &gt; Parker Thompson\n&gt; &gt; The Internet Archive\n&gt; &gt; 510.541.0125\n&gt; &gt; \n&gt; &gt; On Wed, 18 Jun 2003, Gordon Mohr wrote:\n&gt; &gt; \n&gt; &gt; |The test area at...\n&gt; &gt; |\n&gt; &gt; |http://crawl08.archive.org/index-2.html\n&gt; &gt; |\n&gt; &gt; |...came from the www.searchtools.com website, and covers a bunch\n&gt; &gt; |of the things we&#39;ll need to cover. \n&gt; &gt; |\n&gt; &gt; |I don&#39;t think we want to use the exact copy pages, though, \n&gt; &gt; |because (1) we may not have permission to do so, and since \n&gt; &gt; |we&#39;ll eventually want to have our test suite be CVS-versioned \n&gt; &gt; |and open-sourced, we need to be confident of our ability \n&gt; &gt; |to use/redistribute it; 2) some of their listed tests aren&#39;t yet \n&gt; &gt; |implemented; and (3) we&#39;ll want to add a lot more before we&#39;re \n&gt; &gt; |done. \n&gt; &gt; |\n&gt; &gt; |That site also has a useful &quot;Crawler Checklist&quot; which\n&gt; &gt; |covers some other issues we&#39;ll want to test for:\n&gt; &gt; |\n&gt; &gt; |http://www.searchtools.com/robots/robot-checklist.html\n&gt; &gt; |\n&gt; &gt; |So, Parker, until Judy&#39;s back it&#39;d be great for you to start \n&gt; &gt; |our own master index page, linking off to individual point tests \n&gt; &gt; |(inspired by those from search tools, funnellback, and elsewhere), \n&gt; &gt; |and devise a strategy for keeping this in CVS -- probably as\n&gt; &gt; |another module.\n&gt; &gt; |\n&gt; &gt; |Some free-form ideas on things to test are at:\n&gt; &gt; |\n&gt; &gt; |http://crawler.archive.org/cgi-bin/wiki.pl/wiki.pl?WebGarden\n&gt; &gt; |\n&gt; &gt; |- Gordon\n&gt; &gt; |\n&gt; &gt;\n&gt; \n\n"}}