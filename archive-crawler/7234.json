{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":289645082,"authorName":"helloitsmaxine","from":"&quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;","profile":"helloitsmaxine","replyTo":"LIST","senderId":"jOZDHr2oaWiF6LzHexfZLJy-aTaD9H-Mvolwr9cczk45pUxVui022FTv98LjGJOsN_z3i5cGLcjcL-sLZF1NReBaHrP4mlG1GBIHg8s","spamInfo":{"isSpam":false,"reason":"0"},"subject":"(Memory leak experience?) Re: Crawl rate decreasing with time?","postDate":"1311800436","msgId":7234,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGowcHU5aythN3Z2QGVHcm91cHMuY29tPg==","inReplyToHeader":"PGowazdlcitqMDRnQGVHcm91cHMuY29tPg=="},"prevInTopic":7223,"nextInTopic":7235,"prevInTime":7233,"nextInTime":7235,"topicId":7213,"numMessagesInTopic":9,"msgSnippet":"I m still pretty stuck on this problem and am wondering if anyone in general has found memory leaks. Also what is a null pointer message in the alerts section","rawEmail":"Return-Path: &lt;itsmaxine@...&gt;\r\nX-Sender: itsmaxine@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 9498 invoked from network); 27 Jul 2011 21:00:39 -0000\r\nX-Received: from unknown (66.196.94.106)\n  by m15.grp.re1.yahoo.com with QMQP; 27 Jul 2011 21:00:39 -0000\r\nX-Received: from unknown (HELO ng2-ip1.bullet.mail.ne1.yahoo.com) (98.138.214.252)\n  by mta2.grp.re1.yahoo.com with SMTP; 27 Jul 2011 21:00:39 -0000\r\nX-Received: from [98.138.217.178] by ng2.bullet.mail.ne1.yahoo.com with NNFMP; 27 Jul 2011 21:00:38 -0000\r\nX-Received: from [69.147.65.173] by tg3.bullet.mail.ne1.yahoo.com with NNFMP; 27 Jul 2011 21:00:38 -0000\r\nX-Received: from [98.137.34.119] by t15.bullet.mail.sp1.yahoo.com with NNFMP; 27 Jul 2011 21:00:38 -0000\r\nDate: Wed, 27 Jul 2011 21:00:36 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;j0pu9k+a7vv@...&gt;\r\nIn-Reply-To: &lt;j0k7er+j04g@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt;\r\nSubject: (Memory leak experience?) Re: Crawl rate decreasing with time?\r\nX-Yahoo-Group-Post: member; u=289645082; y=6zDQ_8fnM0i6O-mTHWrgsZVKw7DLbSRSg2i5R4YLK7sSCE9rWeAXj9Qwf8XYlxr5qGWKOXgDaKcylsA\r\nX-Yahoo-Profile: helloitsmaxine\r\n\r\nI&#39;m still pretty stuck on this problem and am wondering if anyone in genera=\r\nl has found memory leaks. Also what is a null pointer message in the alerts=\r\n section usually indicative of? For reference I&#39;m running Heritrix 1.14.4.\n=\r\n\n--- In archive-crawler@yahoogroups.com, &quot;helloitsmaxine&quot; &lt;itsmaxine@...&gt; w=\r\nrote:\n&gt;\n&gt; Hey Gordon,\n&gt; I&#39;ve looked into your suggestions though nothing in=\r\n the reports jumped out really readily but will continue to investigate.\n&gt; =\r\n\n&gt; However I wanted to ask you about a possibility that others have been me=\r\nntioning--could it be possible that this slow-down is due to a memory leak?=\r\n Is there a way to test for that? Someone suggested that I stop the crawl, =\r\nrestart it on a fresh jvm, and see what happens. I paused the crawl, starte=\r\nd it from the recovery log in a new jvm, and the crawl rates shot right bac=\r\nk up to where they started. Would this be indicative of a memory leak? If s=\r\no, it seems like this problem is pretty inevitable since it has happened pr=\r\netty much every time without fail. Just wanted to know what you (or anyone =\r\nelse) thinks about this?\n&gt; \n&gt; Thanks!\n&gt; \n&gt; --- In archive-crawler@yahoogrou=\r\nps.com, Gordon Mohr &lt;gojomo@&gt; wrote:\n&gt; &gt;\n&gt; &gt; Looks like you&#39;re on a 4GB Mac=\r\n with a 2GB heap, so there shouldn&#39;t be a\n&gt; &gt; swapping problem.\n&gt; &gt; \n&gt; &gt; 25=\r\n0 threads may be a lot for a 2GB heap. There are no firm rules, but a \n&gt; &gt; =\r\nvery rough rule I&#39;ve used is to take the heap size, deduct the 60% used \n&gt; =\r\n&gt; by default by the BerkeleyDB-JE-based structures, then divide the \n&gt; &gt; re=\r\nmaining value (800MB in your case) by ~5MB/thread to get a plausible \n&gt; &gt; t=\r\nhread count.\n&gt; &gt; \n&gt; &gt; *If* the problem is threads waiting for unresponsive =\r\nhosts, then \n&gt; &gt; reducing soTimeout may help a little. The one-line reports=\r\n on the \n&gt; &gt; reports page, or the longer &#39;threads report&#39;, may give a hint =\r\nif this is \n&gt; &gt; the block. But you can&#39;t make soTimeout too small, there ar=\r\ne real delays \n&gt; &gt; for busy networks/server for which you may not want to m=\r\niss that URL \n&gt; &gt; entirely. It&#39;s a tradeoff you&#39;ll have to decide.\n&gt; &gt; \n&gt; &gt;=\r\n Carefully watching the crawl and aggressively removing URLs from \n&gt; &gt; unwa=\r\nnted/unresponsive sites (eg by adding new scope limitations during \n&gt; &gt; the=\r\n crawl) may offer a better response to slowdowns due to unresponsive \n&gt; &gt; s=\r\nites.\n&gt; &gt; \n&gt; &gt; If threads spending their time on slow/big resources are an =\r\nissue, \n&gt; &gt; reducing the timeout-seconds or maximum size to download settin=\r\ngs could \n&gt; &gt; help a little. The threads report and crawl log might indicat=\r\ne if this \n&gt; &gt; is a contributing factor.\n&gt; &gt; \n&gt; &gt; As a Mac I&#39;m guessing the=\r\nre&#39;s just a single hard drive. That&#39;s going to \n&gt; &gt; cap performance, with a=\r\nll queueing/lookups/scratch-files/content-writing \n&gt; &gt; competing for the si=\r\nngle disk.\n&gt; &gt; \n&gt; &gt; When the tradeoffs associated with the Bloom option may=\r\n be right for \n&gt; &gt; your project is something you&#39;ll have to weigh.\n&gt; &gt; \n&gt; &gt;=\r\n - Gordon\n&gt; &gt; \n&gt; &gt; On 7/21/11 4:41 PM, helloitsmaxine wrote:\n&gt; &gt; &gt; I&#39;m refe=\r\nrring to what it says on the Activity Monitor, though\n&gt; &gt; &gt; admittedly I&#39;m =\r\nnot sure what the relevance of it is. Here are\n&gt; &gt; &gt; screencaps of the cons=\r\nole/Activity Monitor (2 panes):\n&gt; &gt; &gt; http://img39.imageshack.us/img39/2212=\r\n/ss25h36m.jpg\n&gt; &gt; &gt; http://img29.imageshack.us/img29/9120/ss25h36mcpu.jpg\n&gt;=\r\n &gt; &gt;\n&gt; &gt; &gt; The swap reported is 4.4gb at this point-about 25h into the\n&gt; &gt; =\r\n&gt; crawl...is that a large enough amount to warrant scaling back on\n&gt; &gt; &gt; he=\r\nap?\n&gt; &gt; &gt;\n&gt; &gt; &gt; The HTTP timeout-seconds value I have now is 1200 (that&#39;s 2=\r\n0\n&gt; &gt; &gt; mins...seems long?) and the sotimeout-ms is 20,000 (20s, I guess th=\r\nat\n&gt; &gt; &gt; makes sense). Would it help to reduce both or just the sotimeout? =\r\nHow\n&gt; &gt; &gt; much of a reduction would you suggest? Could I make it as low as =\r\n1\n&gt; &gt; &gt; second? Or is maybe 5-10 better?\n&gt; &gt; &gt;\n&gt; &gt; &gt; Currently it looks lik=\r\ne URI&#39;s crawled is still under a million,\n&gt; &gt; &gt; though I would eventually l=\r\nike to grow it to the tens of\n&gt; &gt; &gt; millions--would looking into the BloomU=\r\nriUniqFilter be worth it at\n&gt; &gt; &gt; this point?\n&gt; &gt; &gt;\n&gt; &gt; &gt; Thanks for your s=\r\nuggestions; I really appreciate it!\n&gt; &gt; &gt;\n&gt; &gt; &gt; --- In archive-crawler@yaho=\r\nogroups.com, Gordon Mohr&lt;gojomo@&gt;\n&gt; &gt; &gt; wrote:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; What do you me=\r\nan by &quot;VM size&quot;? (What tool is reporting that\n&gt; &gt; &gt;&gt; number?)\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;=\r\n It would be very atypical for the heap size or JavaVM process\n&gt; &gt; &gt;&gt; addre=\r\nss space to be 150 gigabytes.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; What is the hardware like? (RAM=\r\n, CPU, disk count/speed)\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; If all the threads are working on so=\r\nmething, and the &#39;congestion\n&gt; &gt; &gt;&gt; ratio&#39; is high, then the problem is not=\r\n that there&#39;s too little\n&gt; &gt; &gt;&gt; that&#39;s eligible to crawl politely.\n&gt; &gt; &gt;&gt;\n&gt;=\r\n &gt; &gt;&gt; Some top possibilities:\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; =95 Java process size has grown=\r\n larger than RAM and excessive\n&gt; &gt; &gt;&gt; swapping is occurring. Due to Java&#39;s =\r\npattern of memory access, you\n&gt; &gt; &gt;&gt; essentially never want to be using swa=\r\np. If &#39;top&#39; or &#39;vmstat&#39; show\n&gt; &gt; &gt;&gt; any swap being used, add RAM or scale b=\r\nack the heap so that it\n&gt; &gt; &gt;&gt; isn&#39;t.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; =95 most threads are ma=\r\nking fetches against unresponsive sites, which\n&gt; &gt; &gt;&gt; can take a long time =\r\nto timeout. Large crawls that hit giant\n&gt; &gt; &gt;&gt; link-lists to defunct/fake s=\r\nites can experience this. More threads,\n&gt; &gt; &gt;&gt; shortening the FetchHTTP soT=\r\nimeout, and manually pruning the bad\n&gt; &gt; &gt;&gt; URIs can help.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; =\r\n=95 the crawl has grown so large that accesses to the\n&gt; &gt; &gt;&gt; data-structure=\r\ns which overflow to disk (notably the default\n&gt; &gt; &gt;&gt; &#39;already-seen&#39; BdbUriU=\r\nniqFilter) are now dominating its time usage.\n&gt; &gt; &gt;&gt; On broader and larger =\r\ncrawls -- expected to grow to more than a few\n&gt; &gt; &gt;&gt; tens of millions of UR=\r\nIs -- you may want to consider the\n&gt; &gt; &gt;&gt; BloomUriUniqFilter, though it has=\r\n other RAM costs and imprecision\n&gt; &gt; &gt;&gt; caveats.\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; Hope this he=\r\nlps,\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; - Gordon\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt; On 7/21/11 3:59 PM, helloitsmaxin=\r\ne wrote:\n&gt; &gt; &gt;&gt;&gt; Some other details I&#39;ve noticed are that according to the\n=\r\n&gt; &gt; &gt;&gt;&gt; activity monitor, the VM size is about 150gb, and CPU usage\n&gt; &gt; &gt;&gt;&gt;=\r\n during the crawl is typically very low, with over 95% idle.\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;=\r\n&gt; Is that weird? If there is so much CPU and VM available, would\n&gt; &gt; &gt;&gt;&gt; it=\r\n just make sense to keep increasing memory allocation and #\n&gt; &gt; &gt;&gt;&gt; threads=\r\n to keep crawl rates up? Anyone have experience with this\n&gt; &gt; &gt;&gt;&gt; sort of t=\r\nhing?\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; --- In archive-crawler@yahoogroups.com,\n&gt; &gt; &gt;&gt;&gt; &quot;hell=\r\noitsmaxine&quot;&lt;itsmaxine@&gt;   wrote:\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt; On my crawls I&#39;ve notice=\r\nd that they generally have been\n&gt; &gt; &gt;&gt;&gt;&gt; starting out at pretty good rates,=\r\n ie. 1500kb/s, but then after\n&gt; &gt; &gt;&gt;&gt;&gt; a few hours, this goes down to 0-50k=\r\nb/s and pretty much stays\n&gt; &gt; &gt;&gt;&gt;&gt; there.\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt; After reading a=\r\nbout some other people&#39;s same problems I tried a\n&gt; &gt; &gt;&gt;&gt;&gt; few tweaks: - inc=\r\nreasing JVM memory to 1024 - increasing #\n&gt; &gt; &gt;&gt;&gt;&gt; threads from 50 to 100 -=\r\n increasing in/out recording buffers\n&gt; &gt; &gt;&gt;&gt;&gt; However the problem still per=\r\nsists.\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt; Some information about my crawl is: - using Heritr=\r\nix 1.4 - the\n&gt; &gt; &gt;&gt;&gt;&gt; max heap is being used (ie. current and max are the s=\r\name) - the\n&gt; &gt; &gt;&gt;&gt;&gt; threads are all pretty much active (99-100 out of 100 a=\r\nctive at\n&gt; &gt; &gt;&gt;&gt;&gt; a time) - very high congestion ratio, (ie. 7000) - so far=\r\n it\n&gt; &gt; &gt;&gt;&gt;&gt; has crawled 674003 URI&#39;s in about 20h.\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt; Does =\r\nanyone have insight into how I can prevent this problem?\n&gt; &gt; &gt;&gt;&gt;&gt; I&#39;ve read=\r\n this:\n&gt; &gt; &gt;&gt;&gt;&gt; https://webarchive.jira.com/wiki/display/Heritrix/making+a+=\r\nbusy+crawl+go+faster\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; and a few other =\r\nposts but am not sure what applies to my situation and\n&gt; &gt; &gt;&gt; what would be=\r\n the best next steps to take. Or if there&#39;s any other\n&gt; &gt; &gt;&gt; information th=\r\nat might be helpful let me know!\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;&gt; Thanks!\n&gt; &gt; &gt;&gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;=\r\n\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; ------------------------------------\n&gt; &gt; &gt;=\r\n&gt;&gt;\n&gt; &gt; &gt;&gt;&gt; Yahoo! Groups Links\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;&gt;\n&gt; &gt; &gt;&gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt;=\r\n &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt; ------------------------------------\n&gt; &gt; &gt;\n&gt; &gt; &gt; Yahoo! Gr=\r\noups Links\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt; &gt;\n&gt; &gt;\n&gt;\n\n\n\n"}}