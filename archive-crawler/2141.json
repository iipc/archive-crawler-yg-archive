{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"stack","from":"stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"HmJ20I8dTUaH8PHCnc-VEfVht_jwDVEsK4s69qTViDhr0EYkppniHANlgyjZh2uiJ7cANGYCNGRv0UXsVFMWYg","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: multimachine crawling","postDate":"1124762931","msgId":2141,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQzMEE4NTMzLjQwMzAwMDBAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGRlY2ZwZytidmlrQGVHcm91cHMuY29tPg==","referencesHeader":"PGRlY2ZwZytidmlrQGVHcm91cHMuY29tPg=="},"prevInTopic":2140,"nextInTopic":2143,"prevInTime":2140,"nextInTime":2142,"topicId":2118,"numMessagesInTopic":12,"msgSnippet":"... I think there needs to be one config. shared by all members of the cluster.  Otherwise, if we allow config. to vary by crawler, the cluster will fast","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 41185 invoked from network); 23 Aug 2005 02:13:15 -0000\r\nReceived: from unknown (66.218.66.167)\n  by m32.grp.scd.yahoo.com with QMQP; 23 Aug 2005 02:13:15 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.114)\n  by mta6.grp.scd.yahoo.com with SMTP; 23 Aug 2005 02:13:15 -0000\r\nReceived: from [192.168.1.100] ([192.168.1.100])\n\t(authenticated)\n\tby dns-eth1.duboce.net (8.10.2/8.10.2) with ESMTP id j7N12aW06523\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon, 22 Aug 2005 18:02:36 -0700\r\nMessage-ID: &lt;430A8533.4030000@...&gt;\r\nDate: Mon, 22 Aug 2005 19:08:51 -0700\r\nUser-Agent: Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.8) Gecko/20050511\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;decfpg+bvik@...&gt;\r\nIn-Reply-To: &lt;decfpg+bvik@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Re: multimachine crawling\r\nX-Yahoo-Group-Post: member; u=168599281; y=GL5VgX3P-ajFCGdd-VT3R5VAcWB6dfCO60R8rtmGQzlsSJrxQQ4o6sCp\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\nKaisa Kaunonen wrote:\n\n&gt; Hi,\n&gt; the question of distributed crawls looks very interesting. To\n&gt; participate in discussion, I have some general type comments on it.\n\n&gt;\n&gt; Do all machines in a cluster use the same profile for a crawl? Each\n&gt; machine may have different disk space/bandwidth/CPU so an identical\n&gt; profile for everyone may not be practical. What about one\n&gt; `super&#39; profile with some common options set so that each\n&gt; cluster member can adjust the performance related options. How many\n&gt; profiles should user write for a single distributed crawl?\n&gt;\nI think there needs to be one config. shared by all members of the \ncluster.  Otherwise, if we allow config. to vary by crawler, the cluster \nwill fast become unmanageable. \n\nBut as you point out, different hosts have different capacity.  Or, even \nif all crawlers are exactly the same, they&#39;ll each have a different \ncrawl experience as the crawl progresses (As you say later in your \nmail).  Though I&#39;ve just said that all crawler&#39;s should have the same \nconfig., it&#39;d be sweet if there was a &#39;dial&#39; that we could turn \nremotely: the crawler&#39;s &#39;capacity&#39; dial. We&#39;d aggregate a bunch of order \nfile attributes under crawler &#39;capacity&#39;.  Increasing a crawler&#39;s \ncapacity would ripple down through an order file in a way yet to be \nfigured changing attribute values up or down dependent on which way the \n&#39;dial&#39; was turned making the crawler carry more or less of the crawl \nscope.  This might be a little ambitious.  A more basic mechanism would \nincrease or decrease the faction of the total crawl scope a particular \nmachine is responsible for somehow moving queues between crawlers when \nthe the fraction is changed midcrawl.\n\n&gt; It&#39;s not quite clear to me how one divides a large unknown net\n&gt; space into non-overlapping partitions before crawling.\n\nWell, first all URLs would be tested to see if they fall within cluster \nscope.  Thereafter its a matter of dividing the work.  We have a basic \nmechanism for dividing the work in place at the moment.  See the \nCrawlSplitter processor: \nhttp://crawler.archive.org/xref/org/archive/crawler/prefetch/CrawlSplitter.html.  \nIn each crawler you set an alphabetical range for it to work on.  As \nURLs come out of the Frontier, it tests the URL against the specified \nrange.  If the URL falls within the range, we carry on and crawl it.  \nOtherwise, currently until we figure add in an intermachine \ncommunication mechanism, the URL is written out to a log for manual \ndistribution amongst crawlers according to range. A (smile) more \ninteresting splitting mechanism is the one used by the ubicrawler \nfolks.  They use Consistent Hashing.  Check it out: \nhttp://ubi.imc.pi.cnr.it/projects/ubicrawler/docs/it/unimi/dsi/ubix/ConsistentHashFunction.html \n(They kindly recently changed the license from GPL to LGPL on the java \nclass so we could use it should we choose to) or see the ubicrawler \npaper: http://vigna.dsi.unimi.it/ftp/papers/UbiCrawler.pdf.\n\n&gt;\n&gt; If you want to crawl whole *.com, you have 1) a large list of seeds\n&gt; divided between machines and 2) each machine has instructions to\n&gt; collect everything it meets in *.com starting from own seeds and\n&gt; excluding those URLs which are in subdomains defined by seeds in other\n&gt; machines (what else could you say?)\n\nSee above on dividing the work.\n\n&gt;\n&gt; This definition still leaves many URLs which can be collected from\n&gt; several cluster members. Maybe one should think carefully about the\n&gt; depth of each sub crawl. Less depth means less chance that some URLs\n&gt; are reached from more than one machine.\n&gt;\n&gt; What&#39;s the best way to divide seeds between machines: let Heritrix\n&gt; compute and decide or let the user assign them to cluster members. \n\nProbably giving all seeds to all clusters and letting the cluster figure \nit out would be best (We&#39;ll need to develop reporting for the cluster: \ne.g. asking the cluster if all seeds were crawled, etc.)\n\n...\n\nThanks for the input Kaisa.  Thanks for jumping in.\nSt.Ack\n\n&gt;\n&gt;\n&gt; --- In archive-crawler@yahoogroups.com, stack &lt;stack@a...&gt; wrote:\n&gt; &gt; I&#39;ve added some notes on multimachine crawling to the wiki, here:\n&gt; &gt;\n&gt; http://crawler.archive.org/cgi-bin/wiki.pl?Multima\n&gt; chineCrawl#practical. \n&gt; &gt; It&#39;d be great to hear people&#39;s feedback on how they think a &#39;hive\n&gt; of\n&gt; &gt; heritrices&#39; might coordinate to achieve a large-scale crawl.\n&gt; &gt; Yours,\n&gt; &gt; St.Ack\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt; YAHOO! GROUPS LINKS\n&gt;\n&gt;     *  Visit your group &quot;archive-crawler\n&gt;       &lt;http://groups.yahoo.com/group/archive-crawler&gt;&quot; on the web.\n&gt;        \n&gt;     *  To unsubscribe from this group, send an email to:\n&gt;        archive-crawler-unsubscribe@yahoogroups.com\n&gt;       &lt;mailto:archive-crawler-unsubscribe@yahoogroups.com?subject=Unsubscribe&gt;\n&gt;        \n&gt;     *  Your use of Yahoo! Groups is subject to the Yahoo! Terms of\n&gt;       Service &lt;http://docs.yahoo.com/info/terms/&gt;.\n&gt;\n&gt;\n&gt; ------------------------------------------------------------------------\n&gt;\n\n\n"}}