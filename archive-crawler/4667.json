{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"Pw8oV2ohySYcqXqWBAQRGsT0_veRMQL7MmyOa6j1mUsx3o1UpcK1zkGM1QlAq0UqxyKbdpilLeJiyT3g0w0yrrHCu4igCMQ","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: java.lang.OutOfMemoryError: GC overhead limit exceeded","postDate":"1194453499","msgId":4667,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ3MzFFOUZCLjMwNzAzMDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGZnczMyMSs3amhtQGVHcm91cHMuY29tPg==","referencesHeader":"PGZnczMyMSs3amhtQGVHcm91cHMuY29tPg=="},"prevInTopic":4664,"nextInTopic":4668,"prevInTime":4666,"nextInTime":4668,"topicId":4629,"numMessagesInTopic":9,"msgSnippet":"... As 4*35% = 140% of heap, if at any time the 4 contemporaneous crawls try to use their maximum bdb-cache allocation, you will get an OOME. ... It depends on","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 29903 invoked from network); 7 Nov 2007 16:38:24 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m55.grp.scd.yahoo.com with QMQP; 7 Nov 2007 16:38:24 -0000\r\nX-Received: from unknown (HELO relay02.pair.com) (209.68.5.16)\n  by mta17.grp.scd.yahoo.com with SMTP; 7 Nov 2007 16:38:24 -0000\r\nX-Received: (qmail 11488 invoked from network); 7 Nov 2007 16:38:23 -0000\r\nX-Received: from unknown (HELO ?10.0.10.102?) (unknown)\n  by unknown with SMTP; 7 Nov 2007 16:38:23 -0000\r\nX-pair-Authenticated: 70.137.138.31\r\nMessage-ID: &lt;4731E9FB.3070307@...&gt;\r\nDate: Wed, 07 Nov 2007 08:38:19 -0800\r\nUser-Agent: Thunderbird 2.0.0.6 (Windows/20070728)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;fgs321+7jhm@...&gt;\r\nIn-Reply-To: &lt;fgs321+7jhm@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: java.lang.OutOfMemoryError: GC overhead\n limit exceeded\r\nX-Yahoo-Group-Post: member; u=137285340; y=-TZaeeLA3LCDpzjxq5rN1fGyjletOUK5cO9AuYFSpNde\r\nX-Yahoo-Profile: gojomo\r\n\r\nhinoglu wrote:\n&gt; hi, \n&gt; modified my job management script in a way that will not allow more\n&gt; than 4 \n&gt; instances at the same time and changed the bdb-cache-percent value to 35\n&gt; in profile settings. \n\nAs 4*35% = 140% of heap, if at any time the 4 contemporaneous crawls try \nto use their maximum bdb-cache allocation, you will get an OOME.\n\n&gt; heritrix is running for almost 2 days with -Xmx1100M parameter, and\n&gt; got no errors so far. \n&gt; now, for a few hours no instances other than the Heritrix instance \n&gt; (org.archive.crawler:jmxport=8849,name=Heritrix,type=CrawlService,guiport=8080,host=myhost)\n&gt; \n&gt; are running, and on the console of the intance ui, i have these values:\n&gt; \n&gt; Memory\n&gt; 527876 KB used\n&gt; 667200 KB current heap\n&gt; 1084736 KB max heap\n&gt; \n&gt; though nothing other than the heritrix instance is running, why\n&gt; are values for heap and used memory so high? \n&gt; \n&gt; this is from the top command&#39;s output: \n&gt; 22971 nobody  18   0 1419m 569m 4608 S    0 28.1  34:09.13 java\n&gt; \n&gt; with a 2Gb memory hardware, java or heritrix indeed seems to have\n&gt; allocated around\n&gt; 500Mbs. is that normal?\n\nIt depends on the size of your crawls, but that doesn&#39;t seem excessive. \nMuch of that might be dead objects waiting to be garbage collected, \nwhich can remain part of &#39;used&#39; memory indefinitely if memory isn&#39;t \ntight. Also, Heritrix in places makes use of of &#39;soft&#39; references, which \nalso keep objects alive until memory is tight.\n\nThe previous jmap output you provided suggesting ~1900 instances of \ncommon per-crawl objects still causes me some concern that there may be \na reference leak in your pattern of instance/job startup/shutdown, and \nI&#39;ve made an issue for that:\n\n   http://webteam.archive.org/jira/browse/HER-1307\n\nBut, the total memory used in your situation isn&#39;t on its own worrisome.\n\n- Gordon @ IA\n\n"}}