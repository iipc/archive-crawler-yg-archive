{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"6zracfcuPhytxBWMntg5N_Lmi01CsoH4fxnrCFAkvlaJqAG6F6aOVCQhJ479qOiB5Eye9kQPGYyvnwzywKOYgFnkfpaSknA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Re: fetch only pdf data","postDate":"1184784985","msgId":4444,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2OUU2MjU5LjIwMzAwMDZAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGY3bGwwOSs3cmJvQGVHcm91cHMuY29tPg==","referencesHeader":"PGY3bGwwOSs3cmJvQGVHcm91cHMuY29tPg=="},"prevInTopic":4443,"nextInTopic":4586,"prevInTime":4443,"nextInTime":4445,"topicId":4442,"numMessagesInTopic":4,"msgSnippet":"This will indeed restrict the crawler to only fetching .pdf files -- but that also means it won t find any others that aren t directly added as seeds. I","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 34257 invoked from network); 18 Jul 2007 18:55:42 -0000\r\nReceived: from unknown (66.218.66.71)\n  by m49.grp.scd.yahoo.com with QMQP; 18 Jul 2007 18:55:42 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta13.grp.scd.yahoo.com with SMTP; 18 Jul 2007 18:55:42 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id 58F8214156AA6\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 18 Jul 2007 11:55:41 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 25563-10-87 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tWed, 18 Jul 2007 11:55:40 -0700 (PDT)\r\nReceived: from [192.168.1.203] (c-76-102-230-209.hsd1.ca.comcast.net [76.102.230.209])\n\tby mail.archive.org (Postfix) with ESMTP id DF745141569B5\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Wed, 18 Jul 2007 11:55:40 -0700 (PDT)\r\nMessage-ID: &lt;469E6259.2030006@...&gt;\r\nDate: Wed, 18 Jul 2007 11:56:25 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (X11/20070604)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f7ll09+7rbo@...&gt;\r\nIn-Reply-To: &lt;f7ll09+7rbo@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: fetch only pdf data\r\nX-Yahoo-Group-Post: member; u=137285340; y=j7bqXBWe9bBGzZrzbxTt2iDYQx5bl_39TAJ3dtYzLhfS\r\nX-Yahoo-Profile: gojomo\r\n\r\nThis will indeed restrict the crawler to only fetching &quot;.pdf&quot; files -- \nbut that also means it won&#39;t find any others that aren&#39;t directly added \nas seeds.\n\nI suspect Martin will want to crawl and link-extract any file type that \nmight contain links, but only *save* discovered PDFs.\n\nThis would mean:\n  - changing the &#39;scope&#39; with decide-rules that reject URLs that won&#39;t \ncontain links (like image file extensions)\n  - adding rules like Joey suggests to the writing processor \n(ArcWriterProcessor in usual configurations) that cause it to reject \n(not process) all non-PDF documents\n\nHope this helps,\n\n- Gordon @ IA\n\njoeyfreund wrote:\n&gt; Add two filters in the decide rules -\n&gt; 1. org.archive.crawler.deciderules.RejectDecideRule \n&gt; 2. org.archive.crawler.deciderules.MatchesFilePatternDecideRule\n&gt; \n&gt; Go to &quot;settings&quot; and set the second rule to accept .pdf files.\n&gt; \n&gt; Joey\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, &quot;mkammerlander&quot; \n&gt; &lt;Martin.Kammerlander@...&gt; wrote:\n&gt;&gt; Hi\n&gt;&gt;\n&gt;&gt; First of all: I&#39;m new on heritrix.\n&gt;&gt;\n&gt;&gt; I have a few seed urls and I want to do a crawling based on this\n&gt;&gt; seeds. Now I&#39;m not sure how to handle it that only web sites that \n&gt; are\n&gt;&gt; pdf&#39;s will be fetched...all other &#39;mime&#39; types should be ignored.\n&gt;&gt;\n&gt;&gt; Any ideas which configuration I have to take for reaching this?\n&gt;&gt;\n&gt;&gt; thx\n&gt;&gt; martin\n&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; \n&gt;  \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n\n"}}