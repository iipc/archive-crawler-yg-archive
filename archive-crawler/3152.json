{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":251489418,"authorName":"goblin_cz","from":"&quot;goblin_cz&quot; &lt;adam.brokes@...&gt;","profile":"goblin_cz","replyTo":"LIST","senderId":"WVZ8fslQhknOZ_DOJ-P_64UmPBSEKdf2lUjS8sJycX0yDJ2_Br4gynHee3nX6iGxDp3gHWSyzdJ9Q0dG3XjTWqdFqQGCH4toZQKh","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: small amount of crawled documents and -6 error in logs","postDate":"1154593235","msgId":3152,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGVhc2JraitscDY4QGVHcm91cHMuY29tPg==","inReplyToHeader":"PDQ0Q0ZGMTEyLjQwMTAwMDFAYXJjaGl2ZS5vcmc+"},"prevInTopic":3144,"nextInTopic":3153,"prevInTime":3151,"nextInTime":3153,"topicId":3140,"numMessagesInTopic":5,"msgSnippet":"... Yes, they are. ... settings ... I will explore that. ... begin ... would be ... I am using 1.8 version. I add surt prefix (+http://(cz,) to my seeds.txt","rawEmail":"Return-Path: &lt;adam.brokes@...&gt;\r\nX-Sender: adam.brokes@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 55418 invoked from network); 3 Aug 2006 08:21:46 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m36.grp.scd.yahoo.com with QMQP; 3 Aug 2006 08:21:46 -0000\r\nReceived: from unknown (HELO n24.bullet.scd.yahoo.com) (66.94.237.53)\n  by mta10.grp.scd.yahoo.com with SMTP; 3 Aug 2006 08:21:46 -0000\r\nReceived: from [66.218.69.3] by n24.bullet.scd.yahoo.com with NNFMP; 03 Aug 2006 08:20:35 -0000\r\nReceived: from [66.218.66.78] by t3.bullet.scd.yahoo.com with NNFMP; 03 Aug 2006 08:20:35 -0000\r\nDate: Thu, 03 Aug 2006 08:20:35 -0000\r\nTo: archive-crawler@yahoogroups.com\r\nMessage-ID: &lt;easbkj+lp68@...&gt;\r\nIn-Reply-To: &lt;44CFF112.4010001@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;goblin_cz&quot; &lt;adam.brokes@...&gt;\r\nSubject: Re: small amount of crawled documents and -6 error in logs\r\nX-Yahoo-Group-Post: member; u=251489418; y=DMYhCO4D5sdatjuzMyaLouG8nVKo6JMiXt-Xun0Ss8Y8V765\r\nX-Yahoo-Profile: goblin_cz\r\n\r\n&gt; Normally, this would mean that the hostname DNS lookup for those URLs \n&gt; =\r\nfailed. With no successful DNS lookup, the URL cannot be fetched.\n&gt; \n&gt; Are =\r\nthese same URLs visitable in a browser?\n\nYes, they are.\n&gt; \n&gt; If you crawl a=\r\n handful of these URLs separately, with all other\nsettings \n&gt; the same, do =\r\nthey succeed? (Even if not, it may then be easier to \n&gt; examine all the log=\r\ns to see what happened -- there may be additional \n&gt; errors in local-errors=\r\n.log indicating what step went wrong.)\n\nI will explore that. \n\n&gt; Which vers=\r\nion of Heritrix are you using? Since 1.6 (and maybe earlier), \n&gt; a SURTs so=\r\nurce file may include either (1) actual URIs, which are \n&gt; converted to imp=\r\nlied SURTs for scoping purposes, or (2) lines that\nbegin \n&gt; &#39;+&#39; with litera=\r\nl SURT prefixes (such as &quot;+http://(cz,&quot; -- which\nwould be \n&gt; a sufficient p=\r\nrefix to accept all discovered .CZ URLs as in-scope).\n\nI am using 1.8 versi=\r\non. I add surt prefix (+http://(cz,) to my\nseeds.txt and make job based on =\r\nyour advices.\n\n&gt; - We generally recommend using a DecidingScope with a seri=\r\nes of rules \n&gt; rather than the classic scopes (such as SurtPrefixScope)\n\nI =\r\nuse DecidingScope and BdbFrontier with UnitCostAssignmentPolicy set\nto 5000=\r\n. How then I find sites that exceed the limit?\n\nNow I am starting the crawl=\r\n.. We will see what happens ;]\n\nThanks you a lot..\n\n\n\n\n\n"}}