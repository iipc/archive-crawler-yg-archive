{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137477665,"authorName":"Igor Ranitovic","from":"Igor Ranitovic &lt;igor@...&gt;","profile":"iranitovic","replyTo":"LIST","senderId":"FxrOY_nY3RlUEeCGTdiddQgNFQeI9gUBk1N61oIiRfrXHl2e4RasSMsMjPcMEZ_vLoSE-FlyTbXOrSDkGHJVu-73z1In6a8t","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Issue with the max-retries option under BdbFrontier","postDate":"1183985881","msgId":4421,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ2OTIzMEQ5LjIwOTAwMDhAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDQ2OTIxRTRFLjgwOTAwMDRAc3RhdHNiaWJsaW90ZWtldC5kaz4=","referencesHeader":"PGY2dDNrcytiZTduQGVHcm91cHMuY29tPiA8NDY5MjFFNEUuODA5MDAwNEBzdGF0c2JpYmxpb3Rla2V0LmRrPg=="},"prevInTopic":4420,"nextInTopic":4425,"prevInTime":4420,"nextInTime":4422,"topicId":4419,"numMessagesInTopic":7,"msgSnippet":"Notice in crawl.log that every seed has at least 3 retries (3t in annotations). Every time an URI is deferred, the count of retries goes up. Seed will be","rawEmail":"Return-Path: &lt;igor@...&gt;\r\nX-Sender: igor@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 68001 invoked from network); 9 Jul 2007 12:58:40 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m54.grp.scd.yahoo.com with QMQP; 9 Jul 2007 12:58:40 -0000\r\nReceived: from unknown (HELO mail.archive.org) (207.241.233.246)\n  by mta10.grp.scd.yahoo.com with SMTP; 9 Jul 2007 12:58:40 -0000\r\nReceived: from localhost (localhost [127.0.0.1])\n\tby mail.archive.org (Postfix) with ESMTP id BA2FB1416BC4C\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  9 Jul 2007 05:58:38 -0700 (PDT)\r\nReceived: from mail.archive.org ([127.0.0.1])\n\tby localhost (mail.archive.org [127.0.0.1]) (amavisd-new, port 10024)\n\twith LMTP id 15732-01-2 for &lt;archive-crawler@yahoogroups.com&gt;;\n\tMon, 9 Jul 2007 05:58:38 -0700 (PDT)\r\nReceived: from [127.0.0.1] (nor75-24-88-170-99-175.fbx.proxad.net [88.170.99.175])\n\tby mail.archive.org (Postfix) with ESMTP id 03479141601B8\n\tfor &lt;archive-crawler@yahoogroups.com&gt;; Mon,  9 Jul 2007 05:58:37 -0700 (PDT)\r\nMessage-ID: &lt;469230D9.2090008@...&gt;\r\nDate: Mon, 09 Jul 2007 05:58:01 -0700\r\nUser-Agent: Thunderbird 1.5.0.12 (Windows/20070509)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;f6t3ks+be7n@...&gt; &lt;46921E4E.8090004@...&gt;\r\nIn-Reply-To: &lt;46921E4E.8090004@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-Virus-Scanned: Debian amavisd-new at archive.org\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Igor Ranitovic &lt;igor@...&gt;\r\nSubject: Re: [archive-crawler] Issue with the max-retries option under BdbFrontier\r\nX-Yahoo-Group-Post: member; u=137477665; y=-nEiKXSFz6WmQfw9UK-dGu5oWVMIFlJBIVRJh2pK9NLJ6EBq3w\r\nX-Yahoo-Profile: iranitovic\r\n\r\nNotice in crawl.log that every seed has at least 3 retries (3t in \nannotations). Every time an URI is deferred, the count of retries goes \nup. Seed will be deferred at least twice because of missing \nprerequisites (dns and robots.txt). Later in the crawl you will have \nsome other URIs being deferred because of prerequisites but that depends \nof your configuration of dns and robots.txt lookups.\n\nI would not consider this a bug but that is just me. If you have \ndifferent opinion please let u know. BTW, I am not sure if it is easy to \nchange prerequisite defers not to count against max retries count.\n\nTake care,\ni.\n\n\n\nBjarne Andersen wrote:\n&gt; I&#39;ve had that problem as well - it seems the crawler won&#39;t crawl with a setting for max-retries lower that 3. Could be a bug?\n&gt; \n&gt; best\n&gt; \n\n\n"}}