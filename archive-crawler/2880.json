{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":205318215,"authorName":"Greg Kempe","from":"&quot;Greg Kempe&quot; &lt;thelonghotsummer@...&gt;","profile":"gregkza","replyTo":"LIST","senderId":"u6mBWrdcDGhjyd9aCbHJsfhLz61na3I_7BTQgbXOMSwy0GJlLZSWS1riJuLaiON5PI9fPd1LSi08x1FJvIPN7zQdslgTE7ZnpH0yJBgHWkI","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Carrying the already-seen list between crawls","postDate":"1148510215","msgId":2880,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PGI4MTI1OWEwMDYwNTI0MTUzNnRmMGRiMDkxc2VlYzEwNWU4MDRkMzczMjdAbWFpbC5nbWFpbC5jb20+","inReplyToHeader":"PDQ0NkUwN0I2LjgwNTA4MDNAYXJjaGl2ZS5vcmc+","referencesHeader":"PGI4MTI1OWEwMDYwNDE4MTgwN2w3NThiNmJiM3NiZWJhMWNkNWQ2MjhlMDFiQG1haWwuZ21haWwuY29tPgkgPDQ0NDVEOENBLjMwMjA1MDdAYXJjaGl2ZS5vcmc+CSA8YjgxMjU5YTAwNjA1MTkxMDA4dTIyOWNkN2Zka2ZmMjgyZmZmZmY3ZmVjMTVAbWFpbC5nbWFpbC5jb20+CSA8NDQ2RTA3QjYuODA1MDgwM0BhcmNoaXZlLm9yZz4="},"prevInTopic":2871,"nextInTopic":2881,"prevInTime":2879,"nextInTime":2881,"topicId":2791,"numMessagesInTopic":7,"msgSnippet":"... That s a fair point. My primary usecase is to recover from unclean shutdowns when a checkpoint either failed, or isn t possible because heritrix is stuck","rawEmail":"Return-Path: &lt;thelonghotsummer@...&gt;\r\nX-Sender: thelonghotsummer@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 38814 invoked from network); 24 May 2006 22:37:34 -0000\r\nReceived: from unknown (66.218.66.166)\n  by m15.grp.scd.yahoo.com with QMQP; 24 May 2006 22:37:32 -0000\r\nReceived: from unknown (HELO py-out-1112.google.com) (64.233.166.176)\n  by mta5.grp.scd.yahoo.com with SMTP; 24 May 2006 22:37:06 -0000\r\nReceived: by py-out-1112.google.com with SMTP id m51so2331216pye\n        for &lt;archive-crawler@yahoogroups.com&gt;; Wed, 24 May 2006 15:36:56 -0700 (PDT)\r\nReceived: by 10.35.66.12 with SMTP id t12mr1299376pyk;\n        Wed, 24 May 2006 15:36:55 -0700 (PDT)\r\nReceived: by 10.35.76.16 with HTTP; Wed, 24 May 2006 15:36:55 -0700 (PDT)\r\nMessage-ID: &lt;b81259a00605241536tf0db091seec105e804d37327@...&gt;\r\nDate: Wed, 24 May 2006 15:36:55 -0700\r\nTo: archive-crawler@yahoogroups.com\r\nIn-Reply-To: &lt;446E07B6.8050803@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Disposition: inline\r\nReferences: &lt;b81259a00604181807l758b6bb3sbeba1cd5d628e01b@...&gt;\n\t &lt;4445D8CA.3020507@...&gt;\n\t &lt;b81259a00605191008u229cd7fdkff282fffff7fec15@...&gt;\n\t &lt;446E07B6.8050803@...&gt;\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: &quot;Greg Kempe&quot; &lt;thelonghotsummer@...&gt;\r\nSubject: Re: [archive-crawler] Carrying the already-seen list between crawls\r\nX-Yahoo-Group-Post: member; u=205318215; y=iqnsZuyec42NgpLl18GVUBJeeUOD9ruElmk845p5s13u_A\r\nX-Yahoo-Profile: gregkza\r\n\r\nOn 19/05/06, Michael Stack &lt;stack@...&gt; wrote:\n&gt; Greg Kempe wrote:\n&gt;=\r\n &gt; ...\n&gt; &gt; On a related note, how possible is it for a BdbFrontier to recov=\r\ner\n&gt; &gt; using only the bdb environment? That is, how important is the\n&gt; &gt; or=\r\ng.archive.crawler.frontier.BdbFrontier.serialized file created\n&gt; &gt; during a=\r\n checkpoint (or any of the other .serialized files, for that\n&gt; &gt; matter)? I=\r\nf the crawler could resume using only the bdb environment,\n&gt; &gt; explicit che=\r\nckpoints could be optional. In such a case things like\n&gt; &gt; statistics may b=\r\ne lost, but at least the already-seen list and\n&gt; &gt; frontier would be recove=\r\nrable. The alternative -- using recover.gz --\n&gt; &gt; loses the same data but i=\r\ns more cumbersome to resume from.\n&gt; &gt;\n&gt; Do you think such an option would b=\r\ne of general use?  Running\n&gt; checkpointing costs little if you use the &#39;Fas=\r\nt checkpointing&#39; mode.  At\n&gt; least that is our impression.  Is yours differ=\r\nent?  Run it often.  With\n&gt; checkpointing you can be &#39;certain&#39; you&#39;re resum=\r\ning from a wholesome state.\n\nThat&#39;s a fair point. My primary usecase is to =\r\nrecover from unclean\nshutdowns when a checkpoint either failed, or isn&#39;t po=\r\nssible because\nheritrix is stuck doing other things.\n\nFor instance, I found=\r\n that checkpoints fail mid-way if the crawl is a\nrecovery and the recovery =\r\nthread is still reading frontier entries.\nThe recovery frontier reader modi=\r\nfies one of the maps and a concurrent\nmodification exception results (see l=\r\nog snippet below).  Since the\ncrawler starts after it has a small part of t=\r\nhe froniter, the\nsituation isn&#39;t that much of an edge case.\n\n05/23/2006 23:=\r\n18:26 +0000 FINE\norg.archive.crawler.framework.CrawlController sendCheckpoi=\r\nntEvent Sent\nCHECKPOINTING\n05/23/2006 23:18:26 +0000 FINE\norg.archive.crawl=\r\ner.framework.CrawlController checkpoint Rotating log\nfiles.\n05/23/2006 23:1=\r\n8:26 +0000 FINE\norg.archive.crawler.framework.CrawlController checkpoint Bi=\r\ngMaps.\njava.util.ConcurrentModificationException\n        at java.util.HashM=\r\nap$HashIterator.nextEntry(HashMap.java:787)\n        at java.util.HashMap$Ke=\r\nyIterator.next(HashMap.java:823)\n        at org.archive.util.CachedBdbMap.s=\r\nync(CachedBdbMap.java:496)\n        at org.archive.crawler.framework.CrawlCo=\r\nntroller.checkpointBigMaps(CrawlController.java:1966)\n        at org.archiv=\r\ne.crawler.framework.CrawlController.checkpoint(CrawlController.java:1181)\n =\r\n       at org.archive.crawler.framework.Checkpointer$CheckpointingThread.ru=\r\nn(Checkpointer.java:194)\n05/23/2006 23:18:50 +0000 INFO\norg.archive.crawler=\r\n.framework.Checkpointer$CheckpointingThread run\nFinished\n\n&gt; One idea we&#39;ve =\r\nkicked around is having the recover.gz log start over at\n&gt; each checkpoint.=\r\n  After a checkpoint recovery, optionally you could\n&gt; replay the recovery.g=\r\nz to bring the crawler even closer to the crash point.\n\nThat would be usefu=\r\nl. If the jmx importUris method used the same logic\nto read a recover log a=\r\ns the frontier recovery code does, you could\nuse it to add both frontier ur=\r\nis and already-seen uris.\n\nGreg\n\n"}}