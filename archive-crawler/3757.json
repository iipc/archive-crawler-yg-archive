{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":168599281,"authorName":"Michael Stack","from":"Michael Stack &lt;stack@...&gt;","profile":"stackarchiveorg","replyTo":"LIST","senderId":"TddxLWLnqy3AO1POZ5LLvGFzpLtI74tc76S6ET1naEhvzZAON3ERBljtV4iBFdsS8T10oTFjakjxAdpHL0kr-EGvzWvnzwEu","spamInfo":{"isSpam":false,"reason":"3"},"subject":"Re: [archive-crawler] Crawler with proxy FetchHTTP  Class problem (proxy problem)","postDate":"1169744062","msgId":3757,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ1QjhFMEJFLjQwNjA1MDdAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PGVwOXFxbys1anBsQGVHcm91cHMuY29tPg==","referencesHeader":"PGVwOXFxbys1anBsQGVHcm91cHMuY29tPg=="},"prevInTopic":3754,"nextInTopic":0,"prevInTime":3756,"nextInTime":3758,"topicId":3754,"numMessagesInTopic":2,"msgSnippet":"I d suggest you send a patch rather than whole java source file.  A patch spotlights your changes whereas changes are hard to find in a complete source listing","rawEmail":"Return-Path: &lt;stack@...&gt;\r\nX-Sender: stack@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nReceived: (qmail 19376 invoked from network); 25 Jan 2007 16:55:54 -0000\r\nReceived: from unknown (66.218.67.33)\n  by m34.grp.scd.yahoo.com with QMQP; 25 Jan 2007 16:55:54 -0000\r\nReceived: from unknown (HELO dns.duboce.net) (63.203.238.117)\n  by mta7.grp.scd.yahoo.com with SMTP; 25 Jan 2007 16:55:50 -0000\r\nReceived: by dns.duboce.net (Postfix, from userid 1008)\n\tid E74B6C564; Thu, 25 Jan 2007 07:34:28 -0800 (PST)\r\nX-Spam-Checker-Version: SpamAssassin 3.1.4 (2006-07-26) on dns.duboce.net\r\nX-Spam-Level: \r\nX-Spam-Status: No, score=-0.7 required=5.0 tests=ALL_TRUSTED,AWL,BAYES_00,\n\tHTML_30_40,HTML_BADTAG_30_40,HTML_MESSAGE,HTML_OBFUSCATE_05_10,\n\tHTML_TINY_FONT,INFO_TLD,INTERRUPTUS,WEIRD_QUOTING autolearn=no \n\tversion=3.1.4\r\nReceived: from [192.168.1.107] (debord.duboce.net [192.168.1.107])\n\tby dns.duboce.net (Postfix) with ESMTP id CE7A7C256;\n\tThu, 25 Jan 2007 07:34:04 -0800 (PST)\r\nMessage-ID: &lt;45B8E0BE.4060507@...&gt;\r\nDate: Thu, 25 Jan 2007 08:54:22 -0800\r\nUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.2pre) Gecko/20070111 SeaMonkey/1.1\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;ep9qqo+5jpl@...&gt;\r\nIn-Reply-To: &lt;ep9qqo+5jpl@...&gt;\r\nContent-Type: multipart/alternative;\n boundary=&quot;------------060401000807090800020507&quot;\r\nX-eGroups-Msg-Info: 2:3:4:0\r\nFrom: Michael Stack &lt;stack@...&gt;\r\nSubject: Re: [archive-crawler] Crawler with proxy FetchHTTP  Class problem\n (proxy problem)\r\nX-Yahoo-Group-Post: member; u=168599281; y=yxk0GM7WmvDJx18k4iuq23DhFvYYtR8ccjDdvF2dOzHAvmoem5J4XkVW\r\nX-Yahoo-Profile: stackarchiveorg\r\n\r\n\r\n--------------060401000807090800020507\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\n\r\nI&#39;d suggest you send a patch rather than whole java source file.  A \npatch spotlights your changes whereas changes are hard to find in a \ncomplete source listing mangled by yahoo mail.\n\nDoes your patch work with your proxy?\nSt.Ack\n\n\nAllahbaksh wrote:\n&gt;\n&gt; Hi,\n&gt; Below is my FetchHTTP class. I am unable to crawl the web. We have\n&gt; proxy that requires authentication. Please check whether changes which\n&gt; I have made are at appropriate position.\n&gt; Regards.\n&gt; Allahbaksh\n&gt;\n&gt; /* FetchHTTP.java\n&gt; *\n&gt; * $Id: FetchHTTP.java,v 1.113 2006/08/29 22:47:02 stack-sf Exp $\n&gt; *\n&gt; * Created on Jun 5, 2003\n&gt; *\n&gt; * Copyright (C) 2003 Internet Archive.\n&gt; *\n&gt; * This file is part of the Heritrix web crawler (crawler.archive.org).\n&gt; *\n&gt; * Heritrix is free software; you can redistribute it and/or modify\n&gt; * it under the terms of the GNU Lesser Public License as published by\n&gt; * the Free Software Foundation; either version 2.1 of the License, or\n&gt; * any later version.\n&gt; *\n&gt; * Heritrix is distributed in the hope that it will be useful,\n&gt; * but WITHOUT ANY WARRANTY; without even the implied warranty of\n&gt; * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n&gt; * GNU Lesser Public License for more details.\n&gt; *\n&gt; * You should have received a copy of the GNU Lesser Public License\n&gt; * along with Heritrix; if not, write to the Free Software\n&gt; * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA\n&gt; 02111-1307 USA\n&gt; */\n&gt; package org.archive.crawler.fetcher;\n&gt;\n&gt; import it.unimi.dsi.mg4j.util.MutableString;\n&gt;\n&gt; import java.io.File;\n&gt; import java.io.FileNotFoundException;\n&gt; import java.io.FileOutputStream;\n&gt; import java.io.IOException;\n&gt; import java.io.ObjectInputStream;\n&gt; import java.io.ObjectOutputStream;\n&gt; import java.io.RandomAccessFile;\n&gt; import java.security.KeyManagementException;\n&gt; import java.security.KeyStoreException;\n&gt; import java.security.NoSuchAlgorithmException;\n&gt; import java.util.HashSet;\n&gt; import java.util.Iterator;\n&gt; import java.util.List;\n&gt; import java.util.ListIterator;\n&gt; import java.util.Map;\n&gt; import java.util.Set;\n&gt; import java.util.logging.Level;\n&gt; import java.util.logging.Logger;\n&gt; import java.net.InetAddress;\n&gt; import java.net.UnknownHostException;\n&gt;\n&gt; import javax.management.AttributeNotFoundException;\n&gt; import javax.management.MBeanException;\n&gt; import javax.management.ReflectionException;\n&gt; import javax.net.ssl.SSLContext;\n&gt; import javax.net.ssl.SSLSocketFactory;\n&gt; import javax.net.ssl.TrustManager;\n&gt;\n&gt; import org.apache.commons.httpclient.Cookie;\n&gt; import org.apache.commons.httpclient.Header;\n&gt; import org.apache.commons.httpclient.HostConfiguration;\n&gt; import org.apache.commons.httpclient.HttpClient;\n&gt; import org.apache.commons.httpclient.HttpConnection;\n&gt; import org.apache.commons.httpclient.HttpConnectionManager;\n&gt; import org.apache.commons.httpclient.HttpException;\n&gt; import org.apache.commons.httpclient.HttpMethod;\n&gt; import org.apache.commons.httpclient.HttpMethodBase;\n&gt; import org.apache.commons.httpclient.HttpState;\n&gt; import org.apache.commons.httpclient.HttpStatus;\n&gt; import org.apache.commons.httpclient.HttpVersion;\n&gt; import org.apache.commons.httpclient.NTCredentials;\n&gt; import org.apache.commons.httpclient.auth.AuthChallengeParser;\n&gt; import org.apache.commons.httpclient.auth.AuthScheme;\n&gt; import org.apache.commons.httpclient.auth.AuthScope;\n&gt; import org.apache.commons.httpclient.auth.BasicScheme;\n&gt; import org.apache.commons.httpclient.auth.DigestScheme;\n&gt; import org.apache.commons.httpclient.auth.MalformedChallengeException;\n&gt; import org.apache.commons.httpclient.cookie.CookiePolicy;\n&gt; import org.apache.commons.httpclient.params.HttpClientParams;\n&gt; import org.apache.commons.httpclient.params.HttpConnectionManagerParams;\n&gt; import org.apache.commons.httpclient.params.HttpMethodParams;\n&gt; import org.apache.commons.httpclient.protocol.Protocol;\n&gt; import org.apache.commons.httpclient.protocol.ProtocolSocketFactory;\n&gt; import org.archive.crawler.Heritrix;\n&gt; import org.archive.crawler.datamodel.CoreAttributeConstants;\n&gt; import org.archive.crawler.datamodel.CrawlHost;\n&gt; import org.archive.crawler.datamodel.CrawlOrder;\n&gt; import org.archive.crawler.datamodel.CrawlServer;\n&gt; import org.archive.crawler.datamodel.CrawlURI;\n&gt; import org.archive.crawler.datamodel.CredentialStore;\n&gt; import org.archive.crawler.datamodel.FetchStatusCodes;\n&gt; import org.archive.crawler.datamodel.ServerCache;\n&gt; import org.archive.crawler.datamodel.credential.Credential;\n&gt; import org.archive.crawler.datamodel.credential.CredentialAvatar;\n&gt; import org.archive.crawler.datamodel.credential.Rfc2617Credential;\n&gt; import org.archive.crawler.event.CrawlStatusListener;\n&gt; import org.archive.crawler.framework.Filter;\n&gt; import org.archive.crawler.framework.Processor;\n&gt; import org.archive.crawler.settings.MapType;\n&gt; import org.archive.crawler.settings.SettingsHandler;\n&gt; import org.archive.crawler.settings.SimpleType;\n&gt; import org.archive.crawler.settings.StringList;\n&gt; import org.archive.crawler.settings.Type;\n&gt; import org.archive.httpclient.ConfigurableX509TrustManager;\n&gt; import org.archive.httpclient.HttpRecorderGetMethod;\n&gt; import org.archive.httpclient.HttpRecorderMethod;\n&gt; import org.archive.httpclient.HttpRecorderPostMethod;\n&gt; import org.archive.httpclient.SingleHttpConnectionManager;\n&gt; import org.archive.io.ObjectPlusFilesInputStream;\n&gt; import org.archive.io.RecorderLengthExceededException;\n&gt; import org.archive.io.RecorderTimeoutException;\n&gt; import org.archive.io.RecorderTooMuchHeaderException;\n&gt; import org.archive.util.ArchiveUtils;\n&gt; import org.archive.util.HttpRecorder;\n&gt;\n&gt; import com.sleepycat.bind.serial.SerialBinding;\n&gt; import com.sleepycat.bind.serial.StoredClassCatalog;\n&gt; import com.sleepycat.bind.tuple.StringBinding;\n&gt; import com.sleepycat.collections.StoredSortedMap;\n&gt; import com.sleepycat.je.Database;\n&gt; import com.sleepycat.je.DatabaseConfig;\n&gt; import com.sleepycat.je.DatabaseException;\n&gt; import com.sleepycat.je.Environment;\n&gt;\n&gt; /**\n&gt; * HTTP fetcher that uses &lt;a\n&gt; * href=&quot;http://jakarta.apache.org/commons/httpclient/ \n&gt; &lt;http://jakarta.apache.org/commons/httpclient/&gt;&quot;&gt;Apache Jakarta\n&gt; Commons\n&gt; * HttpClient&lt;/a&gt; library.\n&gt; *\n&gt; * @author Gordon Mohr\n&gt; * @author Igor Ranitovic\n&gt; * @author others\n&gt; * @version $Id: FetchHTTP.java,v 1.113 2006/08/29 22:47:02 stack-sf Exp $\n&gt; */\n&gt; public class FetchHTTP extends Processor\n&gt; implements CoreAttributeConstants, FetchStatusCodes, CrawlStatusListener {\n&gt; // be robust against trivial implementation changes\n&gt; private static final long serialVersionUID =\n&gt; ArchiveUtils.classnameBasedUID(FetchHTTP.class,1);\n&gt;\n&gt; private static Logger logger =\n&gt; Logger.getLogger(FetchHTTP.class.getName());\n&gt; public static final String ATTR_HTTP_PROXY_HOST = &quot;192.168.208.146&quot;;\n&gt; // public static final String ATTR_HTTP_PROXY_HOST = A_HTTP_PROXY_HOST;\n&gt; public static final String ATTR_HTTP_PROXY_PORT = &quot;80&quot;;\n&gt; // public static final String ATTR_HTTP_PROXY_PORT = A_HTTP_PROXY_PORT;\n&gt; public static final String ATTR_TIMEOUT_SECONDS = &quot;timeout-seconds&quot;;\n&gt; public static final String ATTR_SOTIMEOUT_MS = &quot;sotimeout-ms&quot;;\n&gt; public static final String ATTR_MAX_LENGTH_BYTES = &quot;max-length-bytes&quot;;\n&gt; public static final String ATTR_LOAD_COOKIES =\n&gt; &quot;load-cookies-from-file&quot;;\n&gt; public static final String ATTR_SAVE_COOKIES = &quot;save-cookies-to-file&quot;;\n&gt; public static final String ATTR_ACCEPT_HEADERS = &quot;accept-headers&quot;;\n&gt; public static final String ATTR_DEFAULT_ENCODING = &quot;default-encoding&quot;;\n&gt; public static final String ATTR_SHA1_CONTENT = &quot;sha1-content&quot;;\n&gt; public static final String ATTR_FETCH_BANDWIDTH_MAX =\n&gt; &quot;fetch-bandwidth&quot;;\n&gt;\n&gt; /**\n&gt; * SSL trust level setting attribute name.\n&gt; */\n&gt; public static final String ATTR_TRUST = &quot;trust-level&quot;;\n&gt;\n&gt; private static Integer DEFAULT_TIMEOUT_SECONDS = new Integer(1200);\n&gt; private static Integer DEFAULT_SOTIMEOUT_MS = new Integer(20000);\n&gt; private static Long DEFAULT_MAX_LENGTH_BYTES = new Long(0);\n&gt; private static Integer DEFAULT_FETCH_BANDWIDTH_MAX = 0;\n&gt;\n&gt; /**\n&gt; * This is the default value pre-1.4. Needs special handling else\n&gt; * treated as negative number doing math later in processing.\n&gt; */\n&gt; private static long OLD_DEFAULT_MAX_LENGTH_BYTES =\n&gt; 9223372036854775807L;\n&gt;\n&gt; /**\n&gt; * Default character encoding to use for pages that do not specify.\n&gt; */\n&gt; private static String DEFAULT_CONTENT_CHARSET =\n&gt; Heritrix.DEFAULT_ENCODING;\n&gt;\n&gt; /**\n&gt; * Default whether to perform on-the-fly SHA1 hashing of\n&gt; content-bodies.\n&gt; */\n&gt; static Boolean DEFAULT_SHA1_CONTENT = new Boolean(true);\n&gt; public static final String SHA1 = &quot;sha1&quot;;\n&gt;\n&gt; private transient HttpClient http = null;\n&gt;\n&gt; /**\n&gt; * How many &#39;instant retries&#39; of HttpRecoverableExceptions have\n&gt; occurred\n&gt; *\n&gt; * Would like it to be &#39;long&#39;, but longs aren&#39;t atomic\n&gt; */\n&gt; private int recoveryRetries = 0;\n&gt;\n&gt; /**\n&gt; * Count of crawl uris handled.\n&gt; * Would like to be &#39;long&#39;, but longs aren&#39;t atomic\n&gt; */\n&gt; private int curisHandled = 0;\n&gt;\n&gt; /**\n&gt; * Filters to apply mid-fetch, just after receipt of the response\n&gt; * headers before we start to download body.\n&gt; */\n&gt; public final static String MIDFETCH_ATTR_FILTERS = &quot;midfetch-filters&quot;;\n&gt;\n&gt; /**\n&gt; * Instance of midfetchfilters.\n&gt; */\n&gt; private MapType midfetchfilters = null;\n&gt;\n&gt; /**\n&gt; * What to log if midfetch abort.\n&gt; */\n&gt; private static final String MIDFETCH_ABORT_LOG = &quot;midFetchAbort&quot;;\n&gt;\n&gt; public static final String ATTR_SEND_CONNECTION_CLOSE =\n&gt; &quot;send-connection-close&quot;;\n&gt; private static final Header HEADER_SEND_CONNECTION_CLOSE =\n&gt; new Header(&quot;Connection&quot;, &quot;close&quot;);\n&gt; public static final String ATTR_SEND_REFERER = &quot;send-referer&quot;;\n&gt; public static final String ATTR_SEND_RANGE = &quot;send-range&quot;;\n&gt; public static final String REFERER = &quot;Referer&quot;;\n&gt; public static final String RANGE = &quot;Range&quot;;\n&gt; public static final String RANGE_PREFIX = &quot;bytes=0-&quot;;\n&gt; public static final String HTTP_SCHEME = &quot;http&quot;;\n&gt; public static final String HTTPS_SCHEME = &quot;https&quot;;\n&gt;\n&gt; public static final String ATTR_IGNORE_COOKIES = &quot;ignore-cookies&quot;;\n&gt; private static Boolean DEFAULT_IGNORE_COOKIES = new Boolean(false);\n&gt;\n&gt; public static final String ATTR_BDB_COOKIES = &quot;use-bdb-for-cookies&quot;;\n&gt; private static Boolean DEFAULT_BDB_COOKIES = new Boolean(true);\n&gt;\n&gt; public static final String ATTR_LOCAL_ADDRESS = &quot;bind-address&quot;;\n&gt;\n&gt; /**\n&gt; * Database backing cookie map, if using BDB\n&gt; */\n&gt; protected Database cookieDb;\n&gt; /**\n&gt; * Name of cookie BDB Database\n&gt; */\n&gt; public static final String COOKIEDB_NAME = &quot;http_cookies&quot;;\n&gt;\n&gt; static {\n&gt; Protocol.registerProtocol(&quot;http&quot;, new Protocol(&quot;http&quot;,\n&gt; new HeritrixProtocolSocketFactory(), 80));\n&gt; try {\n&gt; Protocol.registerProtocol(&quot;https&quot;,\n&gt; new Protocol(&quot;https&quot;, ((ProtocolSocketFactory)\n&gt; new HeritrixSSLProtocolSocketFactory()), 443));\n&gt; } catch (KeyManagementException e) {\n&gt; e.printStackTrace();\n&gt; } catch (KeyStoreException e) {\n&gt; e.printStackTrace();\n&gt; } catch (NoSuchAlgorithmException e) {\n&gt; e.printStackTrace();\n&gt; }\n&gt; }\n&gt; static final String SERVER_CACHE_KEY = &quot;heritrix.server.cache&quot;;\n&gt; static final String SSL_FACTORY_KEY = &quot;heritrix.ssl.factory&quot;;\n&gt;\n&gt; /***\n&gt; * Socket factory that has the configurable trust manager installed.\n&gt; */\n&gt; private SSLSocketFactory sslfactory = null;\n&gt;\n&gt;\n&gt; /**\n&gt; * Constructor.\n&gt; *\n&gt; * @param name Name of this processor.\n&gt; */\n&gt; public FetchHTTP(String name) {\n&gt; super(name, &quot;HTTP Fetcher&quot;);\n&gt; this.midfetchfilters = (MapType) addElementToDefinition(\n&gt; new MapType(MIDFETCH_ATTR_FILTERS, &quot;Filters applied after&quot; +\n&gt; &quot; receipt of HTTP response headers but before we start\n&gt; to&quot; +\n&gt; &quot; download the body. If any filter returns&quot; +\n&gt; &quot; FALSE, the fetch is aborted. Prerequisites such as&quot; +\n&gt; &quot; robots.txt by-pass filtering (i.e. they cannot be&quot; +\n&gt; &quot; midfetch aborted.&quot;, Filter.class));\n&gt; // see [ 1379040 ] regex for midfetch filter not being stored in crawl\n&gt; order\n&gt; // http://sourceforge.net/support/tracker.php?aid=1379040 \n&gt; &lt;http://sourceforge.net/support/tracker.php?aid=1379040&gt;\n&gt; // this.midfetchfilters.setExpertSetting(true);\n&gt;\n&gt; addElementToDefinition(new SimpleType(ATTR_TIMEOUT_SECONDS,\n&gt; &quot;If the fetch is not completed in this number of seconds,&quot;\n&gt; + &quot; give up (and retry later). For optimal configuration, &quot; +\n&gt; &quot; ensure this value is &gt; &quot; + ATTR_TIMEOUT_SECONDS + &quot;.&quot;,\n&gt; DEFAULT_TIMEOUT_SECONDS));\n&gt; Type e = addElementToDefinition(new SimpleType(ATTR_SOTIMEOUT_MS,\n&gt; &quot;If the socket is unresponsive for this number of\n&gt; milliseconds, &quot; +\n&gt; &quot; give up. Set to zero for no timeout (Not.&quot; +\n&gt; &quot; recommended. Could hang a thread on an unresponsive\n&gt; server).&quot; +\n&gt; &quot; This timeout is used timing out socket opens &quot; +\n&gt; &quot; and for timing out each socket read. Make sure this &quot; +\n&gt; &quot; value is &lt; &quot; + ATTR_TIMEOUT_SECONDS + &quot; for optimal &quot; +\n&gt; &quot; configuration: ensures at least one retry read.&quot;,\n&gt; DEFAULT_SOTIMEOUT_MS));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new\n&gt; SimpleType(ATTR_FETCH_BANDWIDTH_MAX,\n&gt; &quot;The maximum KB/sec to use when fetching data from a\n&gt; server. &quot; +\n&gt; &quot;0 means no maximum. Default: &quot;+ DEFAULT_FETCH_BANDWIDTH_MAX\n&gt; + &quot;.&quot;, DEFAULT_FETCH_BANDWIDTH_MAX));\n&gt; e.setExpertSetting(true);\n&gt; e.setOverrideable(true);\n&gt; addElementToDefinition(new SimpleType(ATTR_MAX_LENGTH_BYTES,\n&gt; &quot;Maximum length in bytes to fetch.&#92;n&quot; +\n&gt; &quot;Fetch is truncated at this length. A value of 0 means no\n&gt; limit.&quot;,\n&gt; DEFAULT_MAX_LENGTH_BYTES));\n&gt; e = addElementToDefinition(new SimpleType(ATTR_IGNORE_COOKIES,\n&gt; &quot;Disable cookie-handling.&quot;, DEFAULT_IGNORE_COOKIES));\n&gt; e.setOverrideable(true);\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_BDB_COOKIES,\n&gt; &quot;Store cookies in BDB-backed map.&quot;, DEFAULT_BDB_COOKIES));\n&gt; e.setExpertSetting(true);\n&gt;\n&gt; e = addElementToDefinition(new SimpleType(ATTR_LOAD_COOKIES,\n&gt; &quot;File to preload cookies from&quot;, &quot;&quot;));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_SAVE_COOKIES,\n&gt; &quot;When crawl finishes save cookies to this file&quot;, &quot;&quot;));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_TRUST,\n&gt; &quot;SSL certificate trust level. Range is from the default\n&gt; &#39;open&#39;&quot;\n&gt; + &quot; (trust all certs including expired, selfsigned, and\n&gt; those for&quot;\n&gt; + &quot; which we do not have a CA) through &#39;loose&#39; (trust all\n&gt; valid&quot;\n&gt; + &quot; certificates including selfsigned), &#39;normal&#39; (all valid&quot;\n&gt; + &quot; certificates not including selfsigned) to &#39;strict&#39;\n&gt; (Cert is&quot;\n&gt; + &quot; valid and DN must match servername)&quot;,\n&gt; ConfigurableX509TrustManager.DEFAULT,\n&gt; ConfigurableX509TrustManager.LEVELS_AS_ARRAY));\n&gt; e.setOverrideable(false);\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new StringList(ATTR_ACCEPT_HEADERS,\n&gt; &quot;Accept Headers to include in each request. Each must be the&quot;\n&gt; + &quot; complete header, e.g., &#39;Accept-Language: en&#39;&quot;));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_HTTP_PROXY_HOST,\n&gt; &quot;Proxy host IP (set only if needed).&quot;, &quot;&quot;));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_HTTP_PROXY_PORT,\n&gt; &quot;Proxy port (set only if needed)&quot;, &quot;&quot;));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_DEFAULT_ENCODING,\n&gt; &quot;The character encoding to use for files that do not have\n&gt; one&quot; +\n&gt; &quot; specified in the HTTP response headers. Default: &quot; +\n&gt; DEFAULT_CONTENT_CHARSET + &quot;.&quot;,\n&gt; DEFAULT_CONTENT_CHARSET));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_SHA1_CONTENT,\n&gt; &quot;Whether or not to perform an on-the-fly SHA1 hash of&quot; +\n&gt; &quot;retrieved content-bodies.&quot;,\n&gt; DEFAULT_SHA1_CONTENT));\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new\n&gt; SimpleType(ATTR_SEND_CONNECTION_CLOSE,\n&gt; &quot;Send &#39;Connection: close&#39; header with every request.&quot;,\n&gt; new Boolean(true)));\n&gt; e.setOverrideable(true);\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_SEND_REFERER,\n&gt; &quot;Send &#39;Referer&#39; header with every request.&#92;n&quot; +\n&gt; &quot;The &#39;Referer&#39; header contans the location the crawler\n&gt; came &quot; +\n&gt; &quot; from, &quot; +\n&gt; &quot;the page the current URI was discovered in. The\n&gt; &#39;Referer&#39; &quot; +\n&gt; &quot;usually is &quot; +\n&gt; &quot;logged on the remote server and can be of assistance to &quot; +\n&gt; &quot;webmasters trying to figure how a crawler got to a &quot; +\n&gt; &quot;particular area on a site.&quot;,\n&gt; new Boolean(true)));\n&gt; e.setOverrideable(true);\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_SEND_RANGE,\n&gt; &quot;Send &#39;Range&#39; header when a limit (&quot; +\n&gt; ATTR_MAX_LENGTH_BYTES +\n&gt; &quot;) on document size.&#92;n&quot; +\n&gt; &quot;Be polite to the HTTP servers and send the &#39;Range&#39;\n&gt; header,&quot; +\n&gt; &quot;stating that you are only interested in the first n\n&gt; bytes. &quot; +\n&gt; &quot;Only pertinent if &quot; + ATTR_MAX_LENGTH_BYTES + &quot; &gt; 0. &quot; +\n&gt; &quot;Sending the &#39;Range&#39; header results in a &quot; +\n&gt; &quot;&#39;206 Partial Content&#39; status response, which is better\n&gt; than &quot; +\n&gt; &quot;just cutting the response mid-download. On rare\n&gt; occasion, &quot; +\n&gt; &quot; sending &#39;Range&#39; will &quot; +\n&gt; &quot;generate &#39;416 Request Range Not Satisfiable&#39; response.&quot;,\n&gt; new Boolean(false)));\n&gt; e.setOverrideable(true);\n&gt; e.setExpertSetting(true);\n&gt; e = addElementToDefinition(new SimpleType(ATTR_LOCAL_ADDRESS,\n&gt; &quot;Local IP address or hostname to use when making\n&gt; connections &quot; +\n&gt; &quot;(binding sockets). When not specified, uses default\n&gt; local&quot; +\n&gt; &quot;address(es).&quot;, &quot;&quot;));\n&gt; e.setExpertSetting(true);\n&gt; System.out.println(&quot;In Constructor&quot;);\n&gt; }\n&gt;\n&gt; protected void innerProcess(final CrawlURI curi)\n&gt; throws InterruptedException {\n&gt; //TODO canFetch\n&gt; System.out.println(&quot;&quot;);\n&gt; if (!canFetch(curi)) {\n&gt; // Cannot fetch this, due to protocol, retries, or other\n&gt; problems\n&gt; return;\n&gt; }\n&gt;\n&gt; this.curisHandled++;\n&gt;\n&gt; // Note begin time\n&gt; curi.putLong(A_FETCH_BEGAN_TIME, System.currentTimeMillis());\n&gt;\n&gt; // Get a reference to the HttpRecorder that is set into this\n&gt; ToeThread.\n&gt; HttpRecorder rec = HttpRecorder.getHttpRecorder();\n&gt;\n&gt; // Shall we get a digest on the content downloaded?\n&gt; boolean sha1Content = ((Boolean)getUncheckedAttribute(curi,\n&gt; ATTR_SHA1_CONTENT)).booleanValue();\n&gt; if(sha1Content) {\n&gt; rec.getRecordedInput().setSha1Digest();\n&gt; } else {\n&gt; // clear\n&gt; rec.getRecordedInput().setDigest(null);\n&gt; }\n&gt;\n&gt; // Below we do two inner classes that add check of midfetch\n&gt; // filters just as we&#39;re about to receive the response body.\n&gt; String curiString = curi.getUURI().toString();\n&gt; HttpMethodBase method = null;\n&gt; if (curi.isPost()) {\n&gt; method = new HttpRecorderPostMethod(curiString, rec) {\n&gt; protected void readResponseBody(HttpState state,\n&gt; HttpConnection conn)\n&gt; throws IOException, HttpException {\n&gt; addResponseContent(this, curi);\n&gt; if (checkMidfetchAbort(curi,\n&gt; this.httpRecorderMethod, conn)) {\n&gt; doAbort(curi, this, MIDFETCH_ABORT_LOG);\n&gt; } else {\n&gt; super.readResponseBody(state, conn);\n&gt; }\n&gt; }\n&gt; };\n&gt; } else {\n&gt; method = new HttpRecorderGetMethod(curiString, rec) {\n&gt; protected void readResponseBody(HttpState state,\n&gt; HttpConnection conn)\n&gt; throws IOException, HttpException {\n&gt; addResponseContent(this, curi);\n&gt; if (checkMidfetchAbort(curi, this.httpRecorderMethod,\n&gt; conn)) {\n&gt; doAbort(curi, this, MIDFETCH_ABORT_LOG);\n&gt; } else {\n&gt; super.readResponseBody(state, conn);\n&gt; }\n&gt; }\n&gt; };\n&gt; }\n&gt;\n&gt; HostConfiguration customConfigOrNull = configureMethod(curi,\n&gt; method);\n&gt;\n&gt; // Set httpRecorder into curi. Subsequent code both here and later\n&gt; // in extractors expects to find the HttpRecorder in the CrawlURI.\n&gt; curi.setHttpRecorder(rec);\n&gt;\n&gt; // Populate credentials. Set config so auth. is not automatic.\n&gt; boolean addedCredentials = populateCredentials(curi, method);\n&gt; method.setDoAuthentication(addedCredentials);\n&gt;\n&gt; try {\n&gt; System.out.println(&quot;Proxy Configured in Try block&quot;);;\n&gt; http.getState().setProxyCredentials(null,null,\n&gt; new\n&gt; NTCredentials(&quot;allahbaksh_asadullah&quot;,&quot;123@naheed&quot;,&quot;192.168.208.146&quot;,&quot;itlinfosys&quot;));\n&gt; this.http.executeMethod(customConfigOrNull, method);\n&gt; } catch (RecorderTooMuchHeaderException ex) {\n&gt; // when too much header material, abort like other truncations\n&gt; doAbort(curi, method, HEADER_TRUNC);\n&gt; } catch (IOException e) {\n&gt; failedExecuteCleanup(method, curi, e);\n&gt; return;\n&gt; } catch (ArrayIndexOutOfBoundsException e) {\n&gt; // For weird windows-only ArrayIndex exceptions in native\n&gt; // code... see\n&gt; // http://forum.java.sun.com/thread.jsp?forum=11&thread=378356 \n&gt; &lt;http://forum.java.sun.com/thread.jsp?forum=11&thread=378356&gt;\n&gt; // treating as if it were an IOException\n&gt; failedExecuteCleanup(method, curi, e);\n&gt; return;\n&gt; }\n&gt;\n&gt; // set softMax on bytes to get (if implied by content-length)\n&gt; long softMax = method.getResponseContentLength();\n&gt;\n&gt; // set hardMax on bytes (if set by operator)\n&gt; long hardMax = getMaxLength(curi);\n&gt;\n&gt; // Get max fetch rate (bytes/ms). It comes in in KB/sec, which\n&gt; // requires nothing to normalize.\n&gt; int maxFetchRate = getMaxFetchRate(curi);\n&gt;\n&gt; try {\n&gt; if (!method.isAborted()) {\n&gt; // Force read-to-end, so that any socket hangs occur here,\n&gt; // not in later modules.\n&gt; rec.getRecordedInput().readFullyOrUntil(softMax,\n&gt; hardMax, 1000 * getTimeout(curi), maxFetchRate);\n&gt; }\n&gt; } catch (RecorderTimeoutException ex) {\n&gt; doAbort(curi, method, TIMER_TRUNC);\n&gt; } catch (RecorderLengthExceededException ex) {\n&gt; doAbort(curi, method, LENGTH_TRUNC);\n&gt; } catch (IOException e) {\n&gt; cleanup(curi, e, &quot;readFully&quot;, S_CONNECT_LOST);\n&gt; return;\n&gt; } catch (ArrayIndexOutOfBoundsException e) {\n&gt; // For weird windows-only ArrayIndex exceptions from\n&gt; native code\n&gt; // see\n&gt; http://forum.java.sun.com/thread.jsp?forum=11&thread=378356 \n&gt; &lt;http://forum.java.sun.com/thread.jsp?forum=11&thread=378356&gt;\n&gt; // treating as if it were an IOException\n&gt; cleanup(curi, e, &quot;readFully&quot;, S_CONNECT_LOST);\n&gt; return;\n&gt; } finally {\n&gt; // ensure recording has stopped\n&gt; rec.closeRecorders();\n&gt; if (!method.isAborted()) {\n&gt; method.releaseConnection();\n&gt; }\n&gt; // Note completion time\n&gt; curi.putLong(A_FETCH_COMPLETED_TIME,\n&gt; System.currentTimeMillis());\n&gt; // Set the response charset into the HttpRecord if available.\n&gt; setCharacterEncoding(rec, method);\n&gt; curi.setContentSize(rec.getRecordedInput().getSize());\n&gt; }\n&gt;\n&gt; curi.setContentDigest(SHA1,\n&gt; rec.getRecordedInput().getDigestValue());\n&gt; if (logger.isLoggable(Level.INFO)) {\n&gt; logger.info((curi.isPost()? &quot;POST&quot;: &quot;GET&quot;) + &quot; &quot; +\n&gt; curi.getUURI().toString() + &quot; &quot; + method.getStatusCode() +\n&gt; &quot; &quot; + rec.getRecordedInput().getSize() + &quot; &quot; +\n&gt; curi.getContentType());\n&gt; }\n&gt;\n&gt; if (curi.isSuccess() && addedCredentials) {\n&gt; // Promote the credentials from the CrawlURI to the\n&gt; CrawlServer\n&gt; // so they are available for all subsequent CrawlURIs on this\n&gt; // server.\n&gt; promoteCredentials(curi);\n&gt; if (logger.isLoggable(Level.FINE)) {\n&gt; // Print out the cookie. Might help with the debugging.\n&gt; Header setCookie = method.getResponseHeader(&quot;set-cookie&quot;);\n&gt; if (setCookie != null) {\n&gt; logger.fine(setCookie.toString().trim());\n&gt; }\n&gt; }\n&gt; } else if (method.getStatusCode() == HttpStatus.SC_UNAUTHORIZED) {\n&gt; // 401 is not &#39;success&#39;.\n&gt; handle401(method, curi);\n&gt; }\n&gt;\n&gt; if (rec.getRecordedInput().isOpen()) {\n&gt; logger.severe(curi.toString() + &quot; RIS still open. Should\n&gt; have&quot; +\n&gt; &quot; been closed by method release: &quot; +\n&gt; Thread.currentThread().getName());\n&gt; try {\n&gt; rec.getRecordedInput().close();\n&gt; } catch (IOException e) {\n&gt; logger.log(Level.SEVERE,&quot;second-chance RIS close\n&gt; failed&quot;,e);\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; protected void doAbort(CrawlURI curi, HttpMethod method,\n&gt; String annotation) {\n&gt; curi.addAnnotation(annotation);\n&gt; curi.getHttpRecorder().close();\n&gt; method.abort();\n&gt; }\n&gt;\n&gt; protected boolean checkMidfetchAbort(CrawlURI curi,\n&gt; HttpRecorderMethod method, HttpConnection conn) {\n&gt; if (curi.isPrerequisite() || filtersAccept(midfetchfilters,\n&gt; curi)) {\n&gt; return false;\n&gt; }\n&gt; method.markContentBegin(conn);\n&gt; return true;\n&gt; }\n&gt;\n&gt; /**\n&gt; * This method populates &lt;code&gt;curi&lt;/code&gt; with response status and\n&gt; * content type.\n&gt; * @param curi CrawlURI to populate.\n&gt; * @param method Method to get response status and headers from.\n&gt; */\n&gt; protected void addResponseContent (HttpMethod method, CrawlURI curi) {\n&gt; curi.setFetchStatus(method.getStatusCode());\n&gt; Header ct = method.getResponseHeader(&quot;content-type&quot;);\n&gt; curi.setContentType((ct == null)? null: ct.getValue());\n&gt; // Save method into curi too. Midfetch filters may want to\n&gt; leverage\n&gt; // info in here.\n&gt; curi.putObject(A_HTTP_TRANSACTION, method);\n&gt; }\n&gt;\n&gt; /**\n&gt; * Set the character encoding based on the result headers or default.\n&gt; *\n&gt; * The HttpClient returns its own default encoding (&quot;ISO-8859-1&quot;)\n&gt; if one\n&gt; * isn&#39;t specified in the Content-Type response header. We give\n&gt; the user\n&gt; * the option of overriding this, so we need to detect the case\n&gt; where the\n&gt; * default is returned.\n&gt; *\n&gt; * Now, it may well be the case that the default returned by\n&gt; HttpClient\n&gt; * and the default defined by the user are the same.\n&gt; *\n&gt; * @param rec Recorder for this request.\n&gt; * @param method Method used for the request.\n&gt; */\n&gt; private void setCharacterEncoding(final HttpRecorder rec,\n&gt; final HttpMethod method) {\n&gt; String encoding = null;\n&gt;\n&gt; try {\n&gt; encoding = ((HttpMethodBase) method).getResponseCharSet();\n&gt; if (encoding == null ||\n&gt; encoding.equals(DEFAULT_CONTENT_CHARSET)) {\n&gt; encoding = (String) getAttribute(ATTR_DEFAULT_ENCODING);\n&gt; }\n&gt; } catch (Exception e) {\n&gt; logger.warning(&quot;Failed get default encoding: &quot; +\n&gt; e.getLocalizedMessage());\n&gt; }\n&gt; rec.setCharacterEncoding(encoding);\n&gt; }\n&gt;\n&gt; /**\n&gt; * Cleanup after a failed method execute.\n&gt; * @param curi CrawlURI we failed on.\n&gt; * @param method Method we failed on.\n&gt; * @param exception Exception we failed with.\n&gt; */\n&gt; private void failedExecuteCleanup(final HttpMethod method,\n&gt; final CrawlURI curi, final Exception exception) {\n&gt; cleanup(curi, exception, &quot;executeMethod&quot;, S_CONNECT_FAILED);\n&gt; method.releaseConnection();\n&gt; }\n&gt;\n&gt; /**\n&gt; * Cleanup after a failed method execute.\n&gt; * @param curi CrawlURI we failed on.\n&gt; * @param exception Exception we failed with.\n&gt; * @param message Message to log with failure.\n&gt; * @param status Status to set on the fetch.\n&gt; */\n&gt; private void cleanup(final CrawlURI curi, final Exception exception,\n&gt; final String message, final int status) {\n&gt; curi.addLocalizedError(this.getName(), exception, message);\n&gt; curi.setFetchStatus(status);\n&gt; curi.getHttpRecorder().close();\n&gt; }\n&gt;\n&gt; /**\n&gt; * Can this processor fetch the given CrawlURI. May set a fetch\n&gt; * status if this processor would usually handle the CrawlURI,\n&gt; * but cannot in this instance.\n&gt; *\n&gt; * @param curi\n&gt; * @return True if processor can fetch.\n&gt; */\n&gt; private boolean canFetch(CrawlURI curi) {\n&gt; if(curi.getFetchStatus()&lt;0) {\n&gt; // already marked as errored, this pass through\n&gt; // skip to end\n&gt;\n&gt; curi.skipToProcessorChain(getController().getPostprocessorChain());\n&gt; return false;\n&gt; }\n&gt; System.out.println(&quot;&quot;);\n&gt; String scheme = curi.getUURI().getScheme();\n&gt; if (!(scheme.equals(&quot;http&quot;) || scheme.equals(&quot;https&quot;))) {\n&gt; // handles only plain http and https\n&gt; return false;\n&gt; }\n&gt; CrawlHost host =\n&gt; getController().getServerCache().getHostFor(curi);\n&gt; // make sure the dns lookup succeeded\n&gt; if (host.getIP() == null && host.hasBeenLookedUp()) {\n&gt; curi.setFetchStatus(S_DOMAIN_PREREQUISITE_FAILURE);\n&gt; return false;\n&gt; }\n&gt; return true;\n&gt; }\n&gt;\n&gt; /**\n&gt; * Configure the HttpMethod setting options and headers.\n&gt; *\n&gt; * @param curi CrawlURI from which we pull configuration.\n&gt; * @param method The Method to configure.\n&gt; */\n&gt; protected HostConfiguration configureMethod(CrawlURI curi,\n&gt; HttpMethod method) {\n&gt; // Don&#39;t auto-follow redirects\n&gt; method.setFollowRedirects(false);\n&gt;\n&gt; // // set soTimeout\n&gt; // method.getParams().setSoTimeout(\n&gt; // ((Integer) getUncheckedAttribute(curi,\n&gt; ATTR_SOTIMEOUT_MS))\n&gt; // .intValue());\n&gt;\n&gt; // Set cookie policy.\n&gt; method.getParams().setCookiePolicy(\n&gt; (((Boolean)getUncheckedAttribute(curi, ATTR_IGNORE_COOKIES)).\n&gt; booleanValue())?\n&gt; CookiePolicy.IGNORE_COOKIES:\n&gt; CookiePolicy.BROWSER_COMPATIBILITY);\n&gt;\n&gt; // Use only HTTP/1.0 (to avoid receiving chunked responses)\n&gt; method.getParams().setVersion(HttpVersion.HTTP_1_0);\n&gt;\n&gt; CrawlOrder order = getSettingsHandler().getOrder();\n&gt; String userAgent = curi.getUserAgent();\n&gt; if (userAgent == null) {\n&gt; userAgent = order.getUserAgent(curi);\n&gt; }\n&gt; method.setRequestHeader(&quot;User-Agent&quot;, userAgent);\n&gt; method.setRequestHeader(&quot;From&quot;, order.getFrom(curi));\n&gt;\n&gt; // Set retry handler.\n&gt; method.getParams().setParameter(HttpMethodParams.RETRY_HANDLER,\n&gt; new HeritrixHttpMethodRetryHandler());\n&gt;\n&gt; final long maxLength = getMaxLength(curi);\n&gt; if(maxLength &gt; 0 &&\n&gt; ((Boolean)getUncheckedAttribute(curi, ATTR_SEND_RANGE)).\n&gt; booleanValue()) {\n&gt; method.addRequestHeader(RANGE,\n&gt; RANGE_PREFIX.concat(Long.toString(maxLength - 1)));\n&gt; }\n&gt;\n&gt; if (((Boolean)getUncheckedAttribute(curi,\n&gt; ATTR_SEND_CONNECTION_CLOSE)).booleanValue()) {\n&gt; method.addRequestHeader(HEADER_SEND_CONNECTION_CLOSE);\n&gt; }\n&gt;\n&gt; if (((Boolean)getUncheckedAttribute(curi,\n&gt; ATTR_SEND_REFERER)).booleanValue()) {\n&gt; // RFC2616 says no referer header if referer is https and\n&gt; the url\n&gt; // is not\n&gt; String via = curi.flattenVia();\n&gt; if (via != null && via.length() &gt; 0 &&\n&gt; !(via.startsWith(HTTPS_SCHEME) &&\n&gt; curi.getUURI().getScheme().equals(HTTP_SCHEME))) {\n&gt; method.setRequestHeader(REFERER, via);\n&gt; }\n&gt; }\n&gt;\n&gt; // TODO: What happens if below method adds a header already\n&gt; // added above: e.g. Connection, Range, or Referer?\n&gt; setAcceptHeaders(curi, method);\n&gt;\n&gt; return configureProxy(curi);\n&gt; }\n&gt;\n&gt; /**\n&gt; * Setup proxy, based on attributes in CrawlURI and settings,\n&gt; * for this CrawlURI only.\n&gt; * @return HostConfiguration customized as necessary, or null if no\n&gt; * customization required\n&gt; */\n&gt; private HostConfiguration configureProxy(CrawlURI curi) {\n&gt; String proxy = (String) getAttributeEither(curi,\n&gt; ATTR_HTTP_PROXY_HOST);\n&gt; int port = -1;\n&gt; if(proxy.length()==0) {\n&gt; proxy = null;\n&gt; } else {\n&gt; String portString = (String)getAttributeEither(curi,\n&gt; ATTR_HTTP_PROXY_PORT);\n&gt; port = portString.length()&gt;0 ?\n&gt; Integer.parseInt(portString) : -1;\n&gt; }\n&gt;\n&gt; //Changes made here.\n&gt; System.out.println(&quot;Initial Changes&quot;);\n&gt; http.getState().setProxyCredentials(null,null,\n&gt; new\n&gt; NTCredentials(&quot;allahbaksh_asadullah&quot;,&quot;123@naheed&quot;,&quot;172.25.232.130&quot;,&quot;itlinfosys&quot;));\n&gt; HostConfiguration config = this.http.getHostConfiguration();\n&gt; if(config.getProxyHost() == proxy && config.getProxyPort() ==\n&gt; port) {\n&gt; // no change,lp\n&gt; return null;\n&gt; }\n&gt; if (proxy != null && proxy.equals(config.getProxyHost())\n&gt; && config.getProxyPort() == port) {\n&gt; // no change\n&gt; return null;\n&gt; }\n&gt; config = new HostConfiguration(config); // copy of config\n&gt; // config.setProxy(&quot;192.168.208.146&quot;,80);\n&gt; //config.setProxy(&quot;10.136.64.194&quot;,80);\n&gt;\n&gt;\n&gt;\n&gt; return config;\n&gt; }\n&gt;\n&gt; /**\n&gt; * Get a value either from inside the CrawlURI instance, or from\n&gt; * settings (module attributes).\n&gt; *\n&gt; * @param curi CrawlURI to consult\n&gt; * @param key key to lookup\n&gt; * @return value from either CrawlURI (preferred) or settings\n&gt; */\n&gt; protected Object getAttributeEither(CrawlURI curi, String key) {\n&gt; Object obj = curi!=null ? curi.getObject(key) : null;\n&gt; if(obj==null) {\n&gt; obj = getUncheckedAttribute(curi, key);\n&gt; }\n&gt; return obj;\n&gt; }\n&gt;\n&gt; /**\n&gt; * Add credentials if any to passed &lt;code&gt;method&lt;/code&gt;.\n&gt; *\n&gt; * Do credential handling. Credentials are in two places. 1.\n&gt; Credentials\n&gt; * that succeeded are added to the CrawlServer (Or rather, avatars for\n&gt; * credentials are whats added because its not safe to keep around\n&gt; * references to credentials). 2. Credentials to be tried are in\n&gt; the curi.\n&gt; * Returns true if found credentials to be tried.\n&gt; *\n&gt; * @param curi Current CrawlURI.\n&gt; * @param method The method to add to.\n&gt; * @return True if prepopulated &lt;code&gt;method&lt;/code&gt; with\n&gt; credentials AND the\n&gt; * credentials came from the &lt;code&gt;curi&lt;/code&gt;, not from the\n&gt; CrawlServer.\n&gt; * The former is special in that if the &lt;code&gt;curi&lt;/curi&gt; credentials\n&gt; * succeed, then the caller needs to promote them from the\n&gt; CrawlURI to the\n&gt; * CrawlServer so they are available for all subsequent CrawlURIs\n&gt; on this\n&gt; * server.\n&gt; */\n&gt; private boolean populateCredentials(CrawlURI curi, HttpMethod\n&gt; method) {\n&gt; // First look at the server avatars. Add any that are to be\n&gt; volunteered\n&gt; // on every request (e.g. RFC2617 credentials). Every time\n&gt; creds will\n&gt; // return true when we call &#39;isEveryTime().\n&gt; CrawlServer server =\n&gt; getController().getServerCache().getServerFor(curi);\n&gt; if (server.hasCredentialAvatars()) {\n&gt; Set avatars = server.getCredentialAvatars();\n&gt; for (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt; CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt; Credential c = ca.getCredential(getSettingsHandler(),\n&gt; curi);\n&gt; if (c.isEveryTime()) {\n&gt; c.populate(curi, this.http, method, ca.getPayload());\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; boolean result = false;\n&gt;\n&gt; // Now look in the curi. The Curi will have credentials\n&gt; loaded either\n&gt; // by the handle401 method if its a rfc2617 or it&#39;ll have been\n&gt; set into\n&gt; // the curi by the preconditionenforcer as this login uri came\n&gt; through.\n&gt; if (curi.hasCredentialAvatars()) {\n&gt; Set avatars = curi.getCredentialAvatars();\n&gt; for (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt; CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt; Credential c = ca.getCredential(getSettingsHandler(),\n&gt; curi);\n&gt; if (c.populate(curi, this.http, method,\n&gt; ca.getPayload())) {\n&gt; result = true;\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; return result;\n&gt; }\n&gt;\n&gt; /**\n&gt; * Promote successful credential to the server.\n&gt; *\n&gt; * @param curi CrawlURI whose credentials we are to promote.\n&gt; */\n&gt; private void promoteCredentials(final CrawlURI curi) {\n&gt; if (!curi.hasCredentialAvatars()) {\n&gt; logger.severe(&quot;No credentials to promote when there should\n&gt; be &quot; +\n&gt; curi);\n&gt; } else {\n&gt; Set avatars = curi.getCredentialAvatars();\n&gt; for (Iterator i = avatars.iterator(); i.hasNext();) {\n&gt; CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt; curi.removeCredentialAvatar(ca);\n&gt; // The server to attach too may not be the server that\n&gt; hosts\n&gt; // this passed curi. It might be of another subdomain.\n&gt; // The avatar needs to be added to the server that is\n&gt; dependent\n&gt; // on this precondition. Find it by name. Get the\n&gt; name from\n&gt; // the credential this avatar represents.\n&gt; Credential c = ca.getCredential(getSettingsHandler(),\n&gt; curi);\n&gt; String cd = null;\n&gt; try {\n&gt; cd = c.getCredentialDomain(curi);\n&gt; }\n&gt; catch (AttributeNotFoundException e) {\n&gt; logger.severe(&quot;Failed to get cred domain for &quot; +\n&gt; curi +\n&gt; &quot; for &quot; + ca + &quot;: &quot; + e.getMessage());\n&gt; }\n&gt; if (cd != null) {\n&gt; CrawlServer cs\n&gt; =\n&gt; getController().getServerCache().getServerFor(cd);\n&gt; if (cs != null) {\n&gt; cs.addCredentialAvatar(ca);\n&gt; }\n&gt; }\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; /**\n&gt; * Server is looking for basic/digest auth credentials (RFC2617).\n&gt; If we have\n&gt; * any, put them into the CrawlURI and have it come around again.\n&gt; Presence\n&gt; * of the credential serves as flag to frontier to requeue\n&gt; promptly. If we\n&gt; * already tried this domain and still got a 401, then our\n&gt; credentials are\n&gt; * bad. Remove them and let this curi die.\n&gt; *\n&gt; * @param method Method that got a 401.\n&gt; * @param curi CrawlURI that got a 401.\n&gt; */\n&gt; protected void handle401(final HttpMethod method, final CrawlURI\n&gt; curi) {\n&gt; AuthScheme authscheme = getAuthScheme(method, curi);\n&gt; if (authscheme == null) {\n&gt; return;\n&gt; }\n&gt; String realm = authscheme.getRealm();\n&gt;\n&gt; // Look to see if this curi had rfc2617 avatars loaded. If\n&gt; so, are\n&gt; // any of them for this realm? If so, then the credential failed\n&gt; // if we got a 401 and it should be let die a natural 401 death.\n&gt; Set curiRfc2617Credentials = getCredentials(getSettingsHandler(),\n&gt; curi, Rfc2617Credential.class);\n&gt; Rfc2617Credential extant = Rfc2617Credential.\n&gt; getByRealm(curiRfc2617Credentials, realm, curi);\n&gt; if (extant != null) {\n&gt; // Then, already tried this credential. Remove ANY rfc2617\n&gt; // credential since presence of a rfc2617 credential serves\n&gt; // as flag to frontier to requeue this curi and let the curi\n&gt; // die a natural death.\n&gt; extant.detachAll(curi);\n&gt; logger.warning(&quot;Auth failed (401) though supplied realm &quot; +\n&gt; realm + &quot; to &quot; + curi.toString());\n&gt; } else {\n&gt; // Look see if we have a credential that corresponds to this\n&gt; // realm in credential store. Filter by type and credential\n&gt; // domain. If not, let this curi die. Else, add it to the\n&gt; // curi and let it come around again. Add in the AuthScheme\n&gt; // we got too. Its needed when we go to run the Auth on\n&gt; // second time around.\n&gt; CredentialStore cs =\n&gt; CredentialStore.getCredentialStore(getSettingsHandler());\n&gt; if (cs == null) {\n&gt; logger.severe(&quot;No credential store for &quot; + curi);\n&gt; } else {\n&gt; CrawlServer server = getController().getServerCache().\n&gt; getServerFor(curi);\n&gt; Set storeRfc2617Credentials = cs.subset(curi,\n&gt; Rfc2617Credential.class, server.getName());\n&gt; if (storeRfc2617Credentials == null ||\n&gt; storeRfc2617Credentials.size() &lt;= 0) {\n&gt; logger.info(&quot;No rfc2617 credentials for &quot; + curi);\n&gt; } else {\n&gt; Rfc2617Credential found = Rfc2617Credential.\n&gt; getByRealm(storeRfc2617Credentials, realm, curi);\n&gt; if (found == null) {\n&gt; logger.info(&quot;No rfc2617 credentials for realm &quot; +\n&gt; realm + &quot; in &quot; + curi);\n&gt; } else {\n&gt; found.attach(curi, authscheme.getRealm());\n&gt; logger.info(&quot;Found credential for realm &quot; + realm +\n&gt; &quot; in store for &quot; + curi.toString());\n&gt; }\n&gt; }\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; /**\n&gt; * @param method Method that got a 401.\n&gt; * @param curi CrawlURI that got a 401.\n&gt; * @return Returns first wholesome authscheme found else null.\n&gt; */\n&gt; protected AuthScheme getAuthScheme(final HttpMethod method,\n&gt; final CrawlURI curi) {\n&gt; Header [] headers = method.getResponseHeaders(&quot;WWW-Authenticate&quot;);\n&gt; if (headers == null || headers.length &lt;= 0) {\n&gt; logger.info(&quot;We got a 401 but no WWW-Authenticate\n&gt; challenge: &quot; +\n&gt; curi.toString());\n&gt; return null;\n&gt; }\n&gt;\n&gt; Map authschemes = null;\n&gt; try {\n&gt; authschemes = AuthChallengeParser.parseChallenges(headers);\n&gt; } catch(MalformedChallengeException e) {\n&gt; logger.info(&quot;Failed challenge parse: &quot; + e.getMessage());\n&gt; }\n&gt; if (authschemes == null || authschemes.size() &lt;= 0) {\n&gt; logger.info(&quot;We got a 401 and WWW-Authenticate challenge&quot; +\n&gt; &quot; but failed parse of the header &quot; + curi.toString());\n&gt; return null;\n&gt; }\n&gt;\n&gt; AuthScheme result = null;\n&gt; // Use the first auth found.\n&gt; for (Iterator i = authschemes.keySet().iterator();\n&gt; result == null && i.hasNext();) {\n&gt; String key = (String)i.next();\n&gt; String challenge = (String)authschemes.get(key);\n&gt; if (key == null || key.length() &lt;= 0 || challenge == null ||\n&gt; challenge.length() &lt;= 0) {\n&gt; logger.warning(&quot;Empty scheme: &quot; + curi.toString() +\n&gt; &quot;: &quot; + headers);\n&gt; }\n&gt; AuthScheme authscheme = null;\n&gt; if (key.equals(&quot;basic&quot;)) {\n&gt; authscheme = new BasicScheme();\n&gt; } else if (key.equals(&quot;digest&quot;)) {\n&gt; authscheme = new DigestScheme();\n&gt; } else {\n&gt; logger.info(&quot;Unsupported scheme: &quot; + key);\n&gt; continue;\n&gt; }\n&gt;\n&gt; try {\n&gt; authscheme.processChallenge(challenge);\n&gt; } catch (MalformedChallengeException e) {\n&gt; logger.info(e.getMessage() + &quot; &quot; + curi + &quot; &quot; + headers);\n&gt; continue;\n&gt; }\n&gt; if (authscheme.isConnectionBased()) {\n&gt; logger.info(&quot;Connection based &quot; + authscheme);\n&gt; continue;\n&gt; }\n&gt;\n&gt; if (authscheme.getRealm() == null ||\n&gt; authscheme.getRealm().length() &lt;= 0) {\n&gt; logger.info(&quot;Empty realm &quot; + authscheme + &quot; for &quot; + curi);\n&gt; continue;\n&gt; }\n&gt; result = authscheme;\n&gt; }\n&gt;\n&gt; return result;\n&gt; }\n&gt;\n&gt; /**\n&gt; * @param handler Settings Handler.\n&gt; * @param curi CrawlURI that got a 401.\n&gt; * @param type Class of credential to get from curi.\n&gt; * @return Set of credentials attached to this curi.\n&gt; */\n&gt; private Set getCredentials(SettingsHandler handler, CrawlURI curi,\n&gt; Class type) {\n&gt; Set result = null;\n&gt;\n&gt; if (curi.hasCredentialAvatars()) {\n&gt; for (Iterator i = curi.getCredentialAvatars().iterator();\n&gt; i.hasNext();) {\n&gt; CredentialAvatar ca = (CredentialAvatar)i.next();\n&gt; if (ca.match(type)) {\n&gt; if (result == null) {\n&gt; result = new HashSet();\n&gt; }\n&gt; result.add(ca.getCredential(handler, curi));\n&gt; }\n&gt; }\n&gt; }\n&gt; return result;\n&gt; }\n&gt;\n&gt; public void initialTasks() {\n&gt; super.initialTasks();\n&gt; this.getController().addCrawlStatusListener(this);\n&gt; configureHttp();\n&gt;\n&gt; // load cookies from a file if specified in the order file.\n&gt; loadCookies();\n&gt;\n&gt; // I tried to get the default KeyManagers but doesn&#39;t work\n&gt; unless you\n&gt; // point at a physical keystore. Passing null seems to do the\n&gt; right\n&gt; // thing so we&#39;ll go w/ that.\n&gt; try {\n&gt; SSLContext context = SSLContext.getInstance(&quot;SSL&quot;);\n&gt; context.init(null, new TrustManager[] {\n&gt; new ConfigurableX509TrustManager((String)\n&gt; getAttribute(ATTR_TRUST))}, null);\n&gt; this.sslfactory = context.getSocketFactory();\n&gt; } catch (Exception e) {\n&gt; logger.log(Level.WARNING, &quot;Failed configure of ssl context &quot;\n&gt; + e.getMessage(), e);\n&gt; }\n&gt; }\n&gt;\n&gt; public void finalTasks() {\n&gt; // At the end save cookies to the file specified in the order\n&gt; file.\n&gt; saveCookies();\n&gt; cleanupHttp();\n&gt; super.finalTasks();\n&gt; }\n&gt;\n&gt; /**\n&gt; * Perform any final cleanup related to the HttpClient instance.\n&gt; */\n&gt; protected void cleanupHttp() {\n&gt; if(cookieDb!=null) {\n&gt; try {\n&gt; cookieDb.close();\n&gt; } catch (DatabaseException e) {\n&gt; // TODO Auto-generated catch block\n&gt; e.printStackTrace();\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; protected void configureHttp() throws RuntimeException {\n&gt; // Get timeout. Use it for socket and for connection timeout.\n&gt; int timeout = (getSoTimeout(null) &gt; 0)? getSoTimeout(null): 0;\n&gt;\n&gt; // HttpConnectionManager cm = new\n&gt; ThreadLocalHttpConnectionManager();\n&gt; HttpConnectionManager cm = new SingleHttpConnectionManager();\n&gt;\n&gt; // TODO: The following settings should be made in the\n&gt; corresponding\n&gt; // HttpConnectionManager, not here.\n&gt; HttpConnectionManagerParams hcmp = cm.getParams();\n&gt; hcmp.setConnectionTimeout(timeout);\n&gt; hcmp.setStaleCheckingEnabled(true);\n&gt; // Minimizes bandwidth usage. Setting to true disables Nagle&#39;s\n&gt; // algorithm. IBM JVMs &lt; 142 give an NPE setting this boolean\n&gt; // on ssl sockets.\n&gt; hcmp.setTcpNoDelay(false);\n&gt; /*\n&gt; * String url = &quot;http://mysite.com/index.html \n&gt; &lt;http://mysite.com/index.html&gt;&quot;;\n&gt; HttpClient client = new HttpClient();\n&gt; HttpMethod method = new GetMethod(url);\n&gt; HostConfiguration hostConfig= client.getHostConfiguration();\n&gt; // Update this to point to NTLM enabled proxy\n&gt; hostConfig.setProxy(&quot;192.168.0.0&quot;, 80);\n&gt;\n&gt; // Authenticate using NTLM\n&gt; client.getState().setProxyCredentials(AuthScope.ANY,\n&gt; new NTCredentials(&quot;userid&quot;,&quot;password&quot;,&quot;&quot;,&quot;&quot;));\n&gt;\n&gt; int statusCode = client.executeMethod(method);\n&gt; System.out.println(&quot;Stat=&quot; +HttpStatus.getStatusText(statusCode));\n&gt; String strOutput = method.getResponseBodyAsString();\n&gt; */\n&gt; this.http = new HttpClient(cm);\n&gt;\n&gt; HttpClientParams hcp = this.http.getParams();\n&gt; // Set default socket timeout.\n&gt; hcp.setSoTimeout(timeout);\n&gt; // Set client to be version 1.0.\n&gt; hcp.setVersion(HttpVersion.HTTP_1_0);\n&gt;\n&gt; String addressStr = null;\n&gt; try {\n&gt; addressStr = (String) getAttribute(ATTR_LOCAL_ADDRESS);\n&gt; } catch (Exception e1) {\n&gt; // If exception, just use default.\n&gt; }\n&gt; if (addressStr != null && addressStr.length() &gt; 0) {\n&gt; try {\n&gt; InetAddress localAddress = InetAddress.getByName(addressStr);\n&gt; this.http.getHostConfiguration().setLocalAddress(localAddress);\n&gt; } catch (UnknownHostException e) {\n&gt; // Convert all to RuntimeException so get an exception out\n&gt; // if initialization fails.\n&gt; throw new RuntimeException(&quot;Unknown host &quot; + addressStr\n&gt; + &quot; in &quot; + ATTR_LOCAL_ADDRESS);\n&gt; }\n&gt; }\n&gt;\n&gt; configureHttpCookies();\n&gt;\n&gt; // Configure how we want the method to act.\n&gt; this.http.getParams().setParameter(\n&gt; HttpMethodParams.SINGLE_COOKIE_HEADER, new Boolean(true));\n&gt; this.http.getParams().setParameter(\n&gt; HttpMethodParams.UNAMBIGUOUS_STATUS_LINE , new\n&gt; Boolean(false));\n&gt; this.http.getParams().setParameter(\n&gt; HttpMethodParams.STRICT_TRANSFER_ENCODING, new\n&gt; Boolean(false));\n&gt; this.http.getParams().setIntParameter(\n&gt; HttpMethodParams.STATUS_LINE_GARBAGE_LIMIT, 10);\n&gt;\n&gt; HostConfiguration configOrNull = configureProxy(null);\n&gt;\n&gt; if(configOrNull!=null) {\n&gt; // global proxy settings are in effect\n&gt; this.http.setHostConfiguration(configOrNull);\n&gt; }\n&gt;\n&gt; // Use our own protocol factory, one that gets IP to use from\n&gt; // heritrix cache (They&#39;re cached in CrawlHost instances).\n&gt; final ServerCache cache = getController().getServerCache();\n&gt; hcmp.setParameter(SERVER_CACHE_KEY, cache);\n&gt; hcmp.setParameter(SSL_FACTORY_KEY, this.sslfactory);\n&gt; }\n&gt;\n&gt; /**\n&gt; * Set the HttpClient HttpState instance to use a BDB-backed\n&gt; * StoredSortedMap for cookie storage, if that option is chosen.\n&gt; */\n&gt; private void configureHttpCookies() {\n&gt; // If Bdb-backed cookies chosen, replace map in HttpState\n&gt; if(((Boolean)getUncheckedAttribute(null, ATTR_BDB_COOKIES)).\n&gt; booleanValue()) {\n&gt; try {\n&gt; Environment env = getController().getBdbEnvironment();\n&gt; StoredClassCatalog classCatalog =\n&gt; getController().getClassCatalog();\n&gt; DatabaseConfig dbConfig = new DatabaseConfig();\n&gt; dbConfig.setTransactional(false);\n&gt; dbConfig.setAllowCreate(true);\n&gt; cookieDb = env.openDatabase(null, COOKIEDB_NAME,\n&gt; dbConfig);\n&gt; StoredSortedMap cookiesMap = new StoredSortedMap(cookieDb,\n&gt; new StringBinding(), new\n&gt; SerialBinding(classCatalog,\n&gt; Cookie.class), true);\n&gt; this.http.getState().setCookiesMap(cookiesMap);\n&gt; } catch (DatabaseException e) {\n&gt; // TODO Auto-generated catch block\n&gt; logger.severe(e.getMessage());\n&gt; e.printStackTrace();\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; /**\n&gt; * @param curi Current CrawlURI. Used to get context.\n&gt; * @return Socket timeout value.\n&gt; */\n&gt; private int getSoTimeout(CrawlURI curi) {\n&gt; Integer res = null;\n&gt; try {\n&gt; res = (Integer) getAttribute(ATTR_SOTIMEOUT_MS, curi);\n&gt; } catch (Exception e) {\n&gt; res = DEFAULT_SOTIMEOUT_MS;\n&gt; }\n&gt; return res.intValue();\n&gt; }\n&gt;\n&gt; /**\n&gt; * @param curi Current CrawlURI. Used to get context.\n&gt; * @return Timeout value for total request.\n&gt; */\n&gt; private int getTimeout(CrawlURI curi) {\n&gt; Integer res;\n&gt; try {\n&gt; res = (Integer) getAttribute(ATTR_TIMEOUT_SECONDS, curi);\n&gt; } catch (Exception e) {\n&gt; res = DEFAULT_TIMEOUT_SECONDS;\n&gt; }\n&gt; return res.intValue();\n&gt; }\n&gt;\n&gt; private int getMaxFetchRate(CrawlURI curi) {\n&gt; Integer res;\n&gt; try {\n&gt; res = (Integer)getAttribute(ATTR_FETCH_BANDWIDTH_MAX, curi);\n&gt; }\n&gt; catch (Exception e) {\n&gt; res = DEFAULT_FETCH_BANDWIDTH_MAX;\n&gt; }\n&gt; return res.intValue();\n&gt; }\n&gt;\n&gt; private long getMaxLength(CrawlURI curi) {\n&gt; Long res;\n&gt; try {\n&gt; res = (Long) getAttribute(ATTR_MAX_LENGTH_BYTES, curi);\n&gt; if (res.longValue() == OLD_DEFAULT_MAX_LENGTH_BYTES) {\n&gt; res = DEFAULT_MAX_LENGTH_BYTES;\n&gt; }\n&gt; } catch (Exception e) {\n&gt; res = DEFAULT_MAX_LENGTH_BYTES;\n&gt; }\n&gt; return res.longValue();\n&gt; }\n&gt;\n&gt; /**\n&gt; * Load cookies from a file before the first fetch.\n&gt; * &lt;p&gt;\n&gt; * The file is a text file in the Netscape&#39;s &#39;cookies.txt&#39; file\n&gt; format.&lt;br&gt;\n&gt; * Example entry of cookies.txt file:&lt;br&gt;\n&gt; * &lt;br&gt;\n&gt; * www.archive.org FALSE / FALSE 1074567117 details-visit\n&gt; texts-cralond&lt;br&gt;\n&gt; * &lt;br&gt;\n&gt; * Each line has 7 tab-separated fields:&lt;br&gt;\n&gt; * &lt;li&gt;1. DOMAIN: The domain that created and have access to the\n&gt; cookie\n&gt; * value.\n&gt; * &lt;li&gt;2. FLAG: A TRUE or FALSE value indicating if hosts within\n&gt; the given\n&gt; * domain can access the cookie value.\n&gt; * &lt;li&gt;3. PATH: The path within the domain that the cookie value\n&gt; is valid\n&gt; * for.\n&gt; * &lt;li&gt;4. SECURE: A TRUE or FALSE value indicating if to use a secure\n&gt; * connection to access the cookie value.\n&gt; * &lt;li&gt;5. EXPIRATION: The expiration time of the cookie value\n&gt; (unix style.)\n&gt; * &lt;li&gt;6. NAME: The name of the cookie value\n&gt; * &lt;li&gt;7. VALUE: The cookie value\n&gt; *\n&gt; * @param cookiesFile file in the Netscape&#39;s &#39;cookies.txt&#39; format.\n&gt; */\n&gt; public void loadCookies(String cookiesFile) {\n&gt; // Do nothing if cookiesFile is not specified.\n&gt; if (cookiesFile == null || cookiesFile.length() &lt;= 0) {\n&gt; return;\n&gt; }\n&gt; RandomAccessFile raf = null;\n&gt; try {\n&gt; raf = new RandomAccessFile(cookiesFile, &quot;r&quot;);\n&gt; String[] cookieParts;\n&gt; String line;\n&gt; Cookie cookie = null;\n&gt; while ((line = raf.readLine()) != null) {\n&gt; // Line that starts with # is commented line,\n&gt; therefore skip it.\n&gt; if (!line.startsWith(&quot;#&quot;)) {\n&gt; cookieParts = line.split(&quot;&#92;&#92;t&quot;);\n&gt; if (cookieParts.length == 7) {\n&gt; // Create cookie with not expiration date (-1\n&gt; value).\n&gt; // TODO: add this as an option.\n&gt; cookie =\n&gt; new Cookie(cookieParts[0], cookieParts[5],\n&gt; cookieParts[6], cookieParts[2], -1,\n&gt;\n&gt; Boolean.valueOf(cookieParts[3]).booleanValue());\n&gt;\n&gt; if (cookieParts[1].toLowerCase().equals(&quot;true&quot;)) {\n&gt; cookie.setDomainAttributeSpecified(true);\n&gt; } else {\n&gt; cookie.setDomainAttributeSpecified(false);\n&gt; }\n&gt; this.http.getState().addCookie(cookie);\n&gt; logger.fine(\n&gt; &quot;Adding cookie: &quot; + cookie.toExternalForm());\n&gt; }\n&gt; }\n&gt; }\n&gt; } catch (FileNotFoundException e) {\n&gt; // We should probably throw FatalConfigurationException.\n&gt; System.out.println(&quot;Could not find file: &quot; + cookiesFile\n&gt; + &quot; (Element: &quot; + ATTR_LOAD_COOKIES + &quot;)&quot;);\n&gt;\n&gt; } catch (IOException e) {\n&gt; // We should probably throw FatalConfigurationException.\n&gt; e.printStackTrace();\n&gt; } finally {\n&gt; try {\n&gt; if (raf != null) {\n&gt; raf.close();\n&gt; }\n&gt; } catch (IOException e) {\n&gt; e.printStackTrace();\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see org.archive.crawler.framework.Processor#report()\n&gt; */\n&gt; public String report() {\n&gt; StringBuffer ret = new StringBuffer();\n&gt; ret.append(&quot;Processor: org.archive.crawler.fetcher.FetchHTTP&#92;n&quot;);\n&gt; ret.append(&quot; Function: Fetch HTTP URIs&#92;n&quot;);\n&gt; ret.append(&quot; CrawlURIs handled: &quot; + this.curisHandled + &quot;&#92;n&quot;);\n&gt; ret.append(&quot; Recovery retries: &quot; + this.recoveryRetries +\n&gt; &quot;&#92;n&#92;n&quot;);\n&gt;\n&gt; return ret.toString();\n&gt; }\n&gt;\n&gt; /**\n&gt; * Load cookies from the file specified in the order file.\n&gt; *\n&gt; * &lt;p&gt;\n&gt; * The file is a text file in the Netscape&#39;s &#39;cookies.txt&#39; file\n&gt; format.&lt;br&gt;\n&gt; * Example entry of cookies.txt file:&lt;br&gt;\n&gt; * &lt;br&gt;\n&gt; * www.archive.org FALSE / FALSE 1074567117 details-visit\n&gt; texts-cralond&lt;br&gt;\n&gt; * &lt;br&gt;\n&gt; * Each line has 7 tab-separated fields:&lt;br&gt;\n&gt; * &lt;li&gt;1. DOMAIN: The domain that created and have access to the\n&gt; cookie\n&gt; * value.\n&gt; * &lt;li&gt;2. FLAG: A TRUE or FALSE value indicating if hosts within\n&gt; the given\n&gt; * domain can access the cookie value.\n&gt; * &lt;li&gt;3. PATH: The path within the domain that the cookie value\n&gt; is valid\n&gt; * for.\n&gt; * &lt;li&gt;4. SECURE: A TRUE or FALSE value indicating if to use a secure\n&gt; * connection to access the cookie value.\n&gt; * &lt;li&gt;5. EXPIRATION: The expiration time of the cookie value\n&gt; (unix style.)\n&gt; * &lt;li&gt;6. NAME: The name of the cookie value\n&gt; * &lt;li&gt;7. VALUE: The cookie value\n&gt; */\n&gt; public void loadCookies() {\n&gt; try {\n&gt; loadCookies((String) getAttribute(ATTR_LOAD_COOKIES));\n&gt; } catch (MBeanException e) {\n&gt; logger.warning(e.getLocalizedMessage());\n&gt; } catch (ReflectionException e) {\n&gt; logger.warning(e.getLocalizedMessage());\n&gt; } catch (AttributeNotFoundException e) {\n&gt; logger.warning(e.getLocalizedMessage());\n&gt; }\n&gt; }\n&gt; /**\n&gt; * Saves cookies to the file specified in the order file.\n&gt; *\n&gt; * Output file is in the Netscape &#39;cookies.txt&#39; format.\n&gt; *\n&gt; */\n&gt; public void saveCookies() {\n&gt; try {\n&gt; saveCookies((String) getAttribute(ATTR_SAVE_COOKIES));\n&gt; } catch (MBeanException e) {\n&gt; logger.warning(e.getLocalizedMessage());\n&gt; } catch (ReflectionException e) {\n&gt; logger.warning(e.getLocalizedMessage());\n&gt; } catch (AttributeNotFoundException e) {\n&gt; logger.warning(e.getLocalizedMessage());\n&gt; }\n&gt; }\n&gt; /**\n&gt; * Saves cookies to a file.\n&gt; *\n&gt; * Output file is in the Netscape &#39;cookies.txt&#39; format.\n&gt; *\n&gt; * @param saveCookiesFile output file.\n&gt; */\n&gt; public void saveCookies(String saveCookiesFile) {\n&gt; // Do nothing if cookiesFile is not specified.\n&gt; if (saveCookiesFile == null || saveCookiesFile.length() &lt;= 0) {\n&gt; return;\n&gt; }\n&gt;\n&gt; FileOutputStream out = null;\n&gt; try {\n&gt; out = new FileOutputStream(new File(saveCookiesFile));\n&gt; Cookie cookies[] = this.http.getState().getCookies();\n&gt; String tab =&quot;&#92;t&quot;;\n&gt; out.write(&quot;# Heritrix Cookie File&#92;n&quot;.getBytes());\n&gt; out.write(\n&gt; &quot;# This file is the Netscape cookies.txt\n&gt; format&#92;n&#92;n&quot;.getBytes());\n&gt; for (int i = 0; i &lt; cookies.length; i++) {\n&gt; MutableString line =\n&gt; new MutableString(1024 * 2 /*Guess an initial size*/);\n&gt; line.append(cookies[i].getDomain());\n&gt; line.append(tab);\n&gt; line.append(\n&gt; cookies[i].isDomainAttributeSpecified() == true\n&gt; ? &quot;TRUE&quot;\n&gt; : &quot;FALSE&quot;);\n&gt; line.append(tab);\n&gt; line.append(cookies[i].getPath());\n&gt; line.append(tab);\n&gt; line.append(\n&gt; cookies[i].getSecure() == true ? &quot;TRUE&quot; : &quot;FALSE&quot;);\n&gt; line.append(tab);\n&gt; line.append(cookies[i].getName());\n&gt; line.append(tab);\n&gt; line.append(cookies[i].getValue());\n&gt; line.append(&quot;&#92;n&quot;);\n&gt; out.write(line.toString().getBytes());\n&gt; }\n&gt; } catch (FileNotFoundException e) {\n&gt; // We should probably throw FatalConfigurationException.\n&gt; System.out.println(&quot;Could not find file: &quot; + saveCookiesFile\n&gt; + &quot; (Element: &quot; + ATTR_SAVE_COOKIES + &quot;)&quot;);\n&gt; } catch (IOException e) {\n&gt; e.printStackTrace();\n&gt; } finally {\n&gt; try {\n&gt; if (out != null) {\n&gt; out.close();\n&gt; }\n&gt; } catch (IOException e) {\n&gt; e.printStackTrace();\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.settings.ModuleType#listUsedFiles(java.util.List)\n&gt; */\n&gt; protected void listUsedFiles(List list) {\n&gt; // List the cookies files\n&gt; // Add seed file\n&gt; try {\n&gt; String tmp = (String)getAttribute(ATTR_LOAD_COOKIES);\n&gt; if(tmp != null && tmp.length() &gt; 0){\n&gt; File file = getSettingsHandler().\n&gt; getPathRelativeToWorkingDirectory(tmp);\n&gt; list.add(file.getAbsolutePath());\n&gt; }\n&gt; tmp = (String)getAttribute(ATTR_SAVE_COOKIES);\n&gt; if(tmp != null && tmp.length() &gt; 0){\n&gt; File file = getSettingsHandler().\n&gt; getPathRelativeToWorkingDirectory(tmp);\n&gt; list.add(file.getAbsolutePath());\n&gt; }\n&gt; } catch (AttributeNotFoundException e) {\n&gt; // TODO Auto-generated catch block\n&gt; e.printStackTrace();\n&gt; } catch (MBeanException e) {\n&gt; // TODO Auto-generated catch block\n&gt; e.printStackTrace();\n&gt; } catch (ReflectionException e) {\n&gt; // TODO Auto-generated catch block\n&gt; e.printStackTrace();\n&gt; }\n&gt; }\n&gt;\n&gt; private void setAcceptHeaders(CrawlURI curi, HttpMethod get) {\n&gt; try {\n&gt; StringList accept_headers = (StringList)\n&gt; getAttribute(ATTR_ACCEPT_HEADERS, curi);\n&gt; if (!accept_headers.isEmpty()) {\n&gt; for (ListIterator i = accept_headers.listIterator();\n&gt; i.hasNext();) {\n&gt; String hdr = (String) i.next();\n&gt; String[] nvp = hdr.split(&quot;: +&quot;);\n&gt; if (nvp.length == 2) {\n&gt; get.setRequestHeader(nvp[0], nvp[1]);\n&gt; }\n&gt; else {\n&gt; logger.warning(&quot;Invalid accept header: &quot; + hdr);\n&gt; }\n&gt; }\n&gt; }\n&gt; }\n&gt; catch (AttributeNotFoundException e) {\n&gt; logger.severe(e.getMessage());\n&gt; }\n&gt; }\n&gt;\n&gt; // custom serialization\n&gt; private void writeObject(ObjectOutputStream stream) throws\n&gt; IOException {\n&gt; stream.defaultWriteObject();\n&gt; // save cookies\n&gt; stream.writeObject(http.getState().getCookies());\n&gt; }\n&gt;\n&gt; private void readObject(ObjectInputStream stream) throws\n&gt; IOException, ClassNotFoundException {\n&gt; stream.defaultReadObject();\n&gt; Cookie cookies[] = (Cookie[]) stream.readObject();\n&gt; ObjectPlusFilesInputStream coistream =\n&gt; (ObjectPlusFilesInputStream)stream;\n&gt; coistream.registerFinishTask( new PostRestore(cookies) );\n&gt; }\n&gt;\n&gt; /**\n&gt; * @return Returns the http instance.\n&gt; */\n&gt; protected HttpClient getHttp() {\n&gt; return this.http;\n&gt; }\n&gt;\n&gt; class PostRestore implements Runnable {\n&gt; Cookie cookies[];\n&gt; public PostRestore(Cookie cookies[]) {\n&gt; this.cookies = cookies;\n&gt; }\n&gt; public void run() {\n&gt; configureHttp();\n&gt; for(int i = 0; i &lt; cookies.length; i++) {\n&gt; getHttp().getState().addCookie(cookies[i]);\n&gt; }\n&gt; }\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.event.CrawlStatusListener#crawlStarted(java.lang.String)\n&gt; */\n&gt; public void crawlStarted(String message) {\n&gt; // TODO Auto-generated method stub\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.event.CrawlStatusListener#crawlStarted(java.lang.String)\n&gt; */\n&gt; public void crawlCheckpoint(File checkpointDir) {\n&gt; // TODO Auto-generated method stub\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.event.CrawlStatusListener#crawlEnding(java.lang.String)\n&gt; */\n&gt; public void crawlEnding(String sExitMessage) {\n&gt; // TODO Auto-generated method stub\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.event.CrawlStatusListener#crawlEnded(java.lang.String)\n&gt; */\n&gt; public void crawlEnded(String sExitMessage) {\n&gt; this.http = null;\n&gt; this.midfetchfilters = null;\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.event.CrawlStatusListener#crawlPausing(java.lang.String)\n&gt; */\n&gt; public void crawlPausing(String statusMessage) {\n&gt; // TODO Auto-generated method stub\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.event.CrawlStatusListener#crawlPaused(java.lang.String)\n&gt; */\n&gt; public void crawlPaused(String statusMessage) {\n&gt; // TODO Auto-generated method stub\n&gt; }\n&gt;\n&gt; /* (non-Javadoc)\n&gt; * @see\n&gt; org.archive.crawler.event.CrawlStatusListener#crawlResuming(java.lang.String)\n&gt; */\n&gt; public void crawlResuming(String statusMessage) {\n&gt; // TODO Auto-generated method stub\n&gt; }\n&gt;\n&gt; }\n&gt;\n&gt;\n&gt;  \n\n\r\n--------------060401000807090800020507\r\nContent-Type: text/html; charset=ISO-8859-1\r\nContent-Transfer-Encoding: 7bit\r\n\r\n&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta content=&quot;text/html;charset=ISO-8859-1&quot; http-equiv=&quot;Content-Type&quot;&gt;\n&lt;/head&gt;\n&lt;body bgcolor=&quot;#ffffff&quot; text=&quot;#000000&quot;&gt;\nI&#39;d suggest you send a patch rather than whole java source file.&nbsp; A\npatch spotlights your changes whereas changes are hard to find in a\ncomplete source listing mangled by yahoo mail.&lt;br&gt;\n&lt;br&gt;\nDoes your patch work with your proxy?&lt;br&gt;\nSt.Ack&lt;br&gt;\n&lt;br&gt;\n&lt;br&gt;\nAllahbaksh wrote:\n&lt;blockquote cite=&quot;mid:ep9qqo+5jpl@...&quot; type=&quot;cite&quot;&gt;&lt;!-- Network content --&gt;\n\n  &lt;div id=&quot;ygrp-text&quot;&gt;\n  &lt;p&gt;Hi,&lt;br&gt;\nBelow is my FetchHTTP class. I am unable to crawl the web. We have&lt;br&gt;\nproxy that requires authentication. Please check whether changes which&lt;br&gt;\nI have made are at appropriate position.&lt;br&gt;\nRegards.&lt;br&gt;\nAllahbaksh&lt;br&gt;\n  &lt;br&gt;\n/* FetchHTTP.java&lt;br&gt;\n*&lt;br&gt;\n* $Id: FetchHTTP.java,&lt;wbr&gt;v 1.113 2006/08/29 22:47:02 stack-sf Exp $&lt;br&gt;\n*&lt;br&gt;\n* Created on Jun 5, 2003&lt;br&gt;\n*&lt;br&gt;\n* Copyright (C) 2003 Internet Archive.&lt;br&gt;\n*&lt;br&gt;\n* This file is part of the Heritrix web crawler (crawler.archive.&lt;wbr&gt;org).&lt;br&gt;\n*&lt;br&gt;\n* Heritrix is free software; you can redistribute it and/or modify&lt;br&gt;\n* it under the terms of the GNU Lesser Public License as published by&lt;br&gt;\n* the Free Software Foundation; either version 2.1 of the License, or&lt;br&gt;\n* any later version.&lt;br&gt;\n*&lt;br&gt;\n* Heritrix is distributed in the hope that it will be useful,&lt;br&gt;\n* but WITHOUT ANY WARRANTY; without even the implied warranty of&lt;br&gt;\n* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the&lt;br&gt;\n* GNU Lesser Public License for more details.&lt;br&gt;\n*&lt;br&gt;\n* You should have received a copy of the GNU Lesser Public License&lt;br&gt;\n* along with Heritrix; if not, write to the Free Software&lt;br&gt;\n* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA &lt;br&gt;\n02111-1307 USA&lt;br&gt;\n*/&lt;br&gt;\npackage org.archive.&lt;wbr&gt;crawler.fetcher;&lt;br&gt;\n  &lt;br&gt;\nimport it.unimi.dsi.&lt;wbr&gt;mg4j.util.&lt;wbr&gt;MutableString;&lt;br&gt;\n  &lt;br&gt;\nimport java.io.File;&lt;br&gt;\nimport java.io.FileNotFoun&lt;wbr&gt;dException;&lt;br&gt;\nimport java.io.FileOutputS&lt;wbr&gt;tream;&lt;br&gt;\nimport java.io.IOException&lt;wbr&gt;;&lt;br&gt;\nimport java.io.ObjectInput&lt;wbr&gt;Stream;&lt;br&gt;\nimport java.io.ObjectOutpu&lt;wbr&gt;tStream;&lt;br&gt;\nimport java.io.RandomAcces&lt;wbr&gt;sFile;&lt;br&gt;\nimport java.security.&lt;wbr&gt;KeyManagementExc&lt;wbr&gt;eption;&lt;br&gt;\nimport java.security.&lt;wbr&gt;KeyStoreExceptio&lt;wbr&gt;n;&lt;br&gt;\nimport java.security.&lt;wbr&gt;NoSuchAlgorithmE&lt;wbr&gt;xception;&lt;br&gt;\nimport java.util.HashSet;&lt;br&gt;\nimport java.util.Iterator;&lt;br&gt;\nimport java.util.List;&lt;br&gt;\nimport java.util.ListItera&lt;wbr&gt;tor;&lt;br&gt;\nimport java.util.Map;&lt;br&gt;\nimport java.util.Set;&lt;br&gt;\nimport java.util.logging.&lt;wbr&gt;Level;&lt;br&gt;\nimport java.util.logging.&lt;wbr&gt;Logger;&lt;br&gt;\nimport java.net.InetAddres&lt;wbr&gt;s;&lt;br&gt;\nimport java.net.UnknownHos&lt;wbr&gt;tException;&lt;br&gt;\n  &lt;br&gt;\nimport javax.management.&lt;wbr&gt;AttributeNotFoun&lt;wbr&gt;dException;&lt;br&gt;\nimport javax.management.&lt;wbr&gt;MBeanException;&lt;br&gt;\nimport javax.management.&lt;wbr&gt;ReflectionExcept&lt;wbr&gt;ion;&lt;br&gt;\nimport javax.net.ssl.&lt;wbr&gt;SSLContext;&lt;br&gt;\nimport javax.net.ssl.&lt;wbr&gt;SSLSocketFactory&lt;wbr&gt;;&lt;br&gt;\nimport javax.net.ssl.&lt;wbr&gt;TrustManager;&lt;br&gt;\n  &lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;Cookie;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;Header;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HostConfiguratio&lt;wbr&gt;n;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpClient;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpConnection;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpConnectionMa&lt;wbr&gt;nager;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpException;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpMethod;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpMethodBase;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpState;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpStatus;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpVersion;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;NTCredentials;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;auth.AuthChallen&lt;wbr&gt;geParser;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;auth.AuthScheme;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;auth.AuthScope;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;auth.BasicScheme&lt;wbr&gt;;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;auth.DigestSchem&lt;wbr&gt;e;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;auth.MalformedCh&lt;wbr&gt;allengeException&lt;wbr&gt;;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;cookie.CookiePol&lt;wbr&gt;icy;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;params.HttpClien&lt;wbr&gt;tParams;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;params.HttpConne&lt;wbr&gt;ctionManagerPara&lt;wbr&gt;ms;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;params.HttpMetho&lt;wbr&gt;dParams;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;protocol.&lt;wbr&gt;Protocol;&lt;br&gt;\nimport org.apache.commons.&lt;wbr&gt;httpclient.&lt;wbr&gt;protocol.&lt;wbr&gt;ProtocolSocketFa&lt;wbr&gt;ctory;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.Heritrix&lt;wbr&gt;;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.CoreAttributeC&lt;wbr&gt;onstants;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.CrawlHost;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.CrawlOrder;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.CrawlServer;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.CrawlURI;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.CredentialStor&lt;wbr&gt;e;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.FetchStatusCod&lt;wbr&gt;es;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.ServerCache;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.credential.&lt;wbr&gt;Credential;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.credential.&lt;wbr&gt;CredentialAvatar&lt;wbr&gt;;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.datamode&lt;wbr&gt;l.credential.&lt;wbr&gt;Rfc2617Credentia&lt;wbr&gt;l;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.event.&lt;wbr&gt;CrawlStatusListe&lt;wbr&gt;ner;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.framewor&lt;wbr&gt;k.Filter;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.framewor&lt;wbr&gt;k.Processor;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.settings&lt;wbr&gt;.MapType;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.settings&lt;wbr&gt;.SettingsHandler&lt;wbr&gt;;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.settings&lt;wbr&gt;.SimpleType;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.settings&lt;wbr&gt;.StringList;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;crawler.settings&lt;wbr&gt;.Type;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;httpclient.&lt;wbr&gt;ConfigurableX509&lt;wbr&gt;TrustManager;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpRecorderGetM&lt;wbr&gt;ethod;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpRecorderMeth&lt;wbr&gt;od;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;httpclient.&lt;wbr&gt;HttpRecorderPost&lt;wbr&gt;Method;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;httpclient.&lt;wbr&gt;SingleHttpConnec&lt;wbr&gt;tionManager;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;io.ObjectPlusFil&lt;wbr&gt;esInputStream;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;io.RecorderLengt&lt;wbr&gt;hExceededExcepti&lt;wbr&gt;on;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;io.RecorderTimeo&lt;wbr&gt;utException;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;io.RecorderTooMu&lt;wbr&gt;chHeaderExceptio&lt;wbr&gt;n;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;util.ArchiveUtil&lt;wbr&gt;s;&lt;br&gt;\nimport org.archive.&lt;wbr&gt;util.HttpRecorde&lt;wbr&gt;r;&lt;br&gt;\n  &lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;bind.serial.&lt;wbr&gt;SerialBinding;&lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;bind.serial.&lt;wbr&gt;StoredClassCatal&lt;wbr&gt;og;&lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;bind.tuple.&lt;wbr&gt;StringBinding;&lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;collections.&lt;wbr&gt;StoredSortedMap;&lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;je.Database;&lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;je.DatabaseConfi&lt;wbr&gt;g;&lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;je.DatabaseExcep&lt;wbr&gt;tion;&lt;br&gt;\nimport com.sleepycat.&lt;wbr&gt;je.Environment;&lt;br&gt;\n  &lt;br&gt;\n/**&lt;br&gt;\n* HTTP fetcher that uses &lt;a&lt;br&gt;\n* href=&quot;&lt;a moz-do-not-send=&quot;true&quot;\n href=&quot;\n(Message over 64 KB, truncated)"}}