{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"VP7xywhz4ycrSDysFLg6MbCb1D28K0eKLL_uxfV9iytX3l2TzMGAF8X_V48SsQkY9pzygeRwqz9dX4F6TgoO5OGqwaZX_QE","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: [archive-crawler] Heritrix 3.2.0 deduplication","postDate":"1423700738","msgId":8676,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDU0REJGMzAyLjIwNTAxMDJAYXJjaGl2ZS5vcmc+","inReplyToHeader":"PDE0MjM2Mjg4OTQuMzkxMDEuWWFob29NYWlsQW5kcm9pZE1vYmlsZUB3ZWIxOTA2MDUubWFpbC5zZzMueWFob28uY29tPg==","referencesHeader":"PDE0MjM2Mjg4OTQuMzkxMDEuWWFob29NYWlsQW5kcm9pZE1vYmlsZUB3ZWIxOTA2MDUubWFpbC5zZzMueWFob28uY29tPg=="},"prevInTopic":8672,"nextInTopic":0,"prevInTime":8675,"nextInTime":8677,"topicId":8664,"numMessagesInTopic":10,"msgSnippet":"On 2/10/15 8:28 PM, Sandip Dev devsandip2511@yahoo.in [archive-crawler] ... Yes, the [BdbContentDigestHistory, ContentDigestHistoryLoader, ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 28408 invoked by uid 102); 12 Feb 2015 00:25:41 -0000\r\nX-Received: from unknown (HELO mtaq2.grp.bf1.yahoo.com) (10.193.84.33)\n  by m5.grp.bf1.yahoo.com with SMTP; 12 Feb 2015 00:25:41 -0000\r\nX-Received: (qmail 18868 invoked from network); 12 Feb 2015 00:25:40 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (98.139.170.167)\n  by mtaq2.grp.bf1.yahoo.com with SMTP; 12 Feb 2015 00:25:40 -0000\r\nX-Received: (qmail 75168 invoked by uid 0); 12 Feb 2015 00:25:40 -0000\r\nX-Received: from 172.56.39.214 (HELO probook.local) (172.56.39.214)\n  by relay03.pair.com with SMTP; 12 Feb 2015 00:25:40 -0000\r\nX-pair-Authenticated: 172.56.39.214\r\nMessage-ID: &lt;54DBF302.2050102@...&gt;\r\nDate: Wed, 11 Feb 2015 16:25:38 -0800\r\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.4.0\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;1423628894.39101.YahooMailAndroidMobile@...&gt;\r\nIn-Reply-To: &lt;1423628894.39101.YahooMailAndroidMobile@...&gt;\r\nContent-Type: text/plain; charset=utf-8; format=flowed\r\nContent-Transfer-Encoding: 8bit\r\nSubject: Re: [archive-crawler] Heritrix 3.2.0 deduplication\r\nX-Yahoo-Group-Post: member; u=137285340; y=Gox2tVSeO9XIhkoGw6rAlo5OwWntbFKBamo17XpcYozB\r\nX-Yahoo-Profile: gojomo\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\n\r\nOn 2/10/15 8:28 PM, Sandip Dev devsandip2511@... [archive-crawler] \nwrote:\n&gt; Hi,\n&gt; So am I correct in assuming when you have the URl agnostic spring beans\n&gt; in our ceawler bean config we do not need FetchHistory processor and\n&gt; Persist Load and Store processors ?\n\nYes, the [BdbContentDigestHistory, ContentDigestHistoryLoader, \nContentDigestHistoryStorer] configuration is an alternative to the \n[FetchHistoryProcessor, PersistStoreProcessor, PersistLoadProcessor] \napproach.\n\n&gt; As I said earlier it seems to me\n&gt; that the Url agnostic revisit seem to happen only when the same Url is\n&gt; encountered in the current crawl however it does not seem to refer\n&gt; hisrorical crawl. Can anyone please help ?\n\nI suspect you need to make sure the second launch is consulting the \nexact same source for historical digest information. If you are \nconfiguring a history-specific BerkeleyDB environment (&#39;historyBdb&#39; \nbean), either:\n\n  (1) use a filesystem path that&#39;s reachable, and the same, for each \nlaunch (which might be an absolute path, or a relative path that \nparents-up from the launch-specific directories); or...\n  (2) use separate/local paths, but then between crawls, manually \ncopy/move the history data forward to where the later crawls will find it.\n\nYou&#39;d probably want (1) for ease of use once everything&#39;s working. But \nsome might prefer (2) to be able to keep the results of each crawl \ndistinct and reversible.\n\nIt looks like the example config on the wiki page may be assuming a (2) \napproach without mentioning the &#39;manually copy forward&#39; step.\n\n- Gordon\n\n\n\n\n&gt; Thanks\n&gt; Sandip\n&gt;\n&gt; Sent from Yahoo Mail on Android\n&gt; &lt;https://overview.mail.yahoo.com/mobile/?.src=Android&gt;\n&gt;\n&gt; From:&quot;Sandip Dev devsandip2511@... [archive-crawler]&quot;\n&gt; &lt;archive-crawler@yahoogroups.com&gt;\n&gt; Date:Sun, Feb 8, 2015 at 12:24 am\n&gt; Subject:Re: [archive-crawler] Heritrix 3.2.0 deduplication\n&gt;\n&gt; I tried using one URL and I tried the *URL-Agnostic Duplication\n&gt; Reduction*  which I believe does not required two different config for\n&gt; Store and Load however it does not seem to read the historical data. The\n&gt; only time it works is when the same URL is encountered during a single\n&gt; crawl. Can anyone please send me a sample crawler-bean.xml for\n&gt; *URL-Agnostic Duplication Reduction *which has worked for them  for\n&gt; historical crawls ?\n&gt;\n&gt; Thanks\n&gt; Sandip\n&gt;\n&gt;\n&gt; On Saturday, 7 February 2015 9:51 AM, &quot;Sandip Dev devsandip2511@...\n&gt; [archive-crawler]&quot; &lt;archive-crawler@yahoogroups.com&gt; wrote:\n&gt;\n&gt;\n&gt; Thanks Gordon will try that. I have a question though when a revisit\n&gt; record is written is the resource physically written to the new archive\n&gt; or is it just a pointer record to a previous archive ? In case it is\n&gt; just a pointer when we playback the new archive via openwayback will it\n&gt; show a &quot;resource not in archive &quot; for the revisit resources if we dont\n&gt; keep the previous archive ?\n&gt; _Sandip_\n&gt; Sent from Yahoo Mail on Android\n&gt; &lt;https://overview.mail.yahoo.com/mobile/?.src=Android&gt;\n&gt; From:&quot;Gordon Mohr gojomo@... [archive-crawler]&quot;\n&gt; &lt;archive-crawler@yahoogroups.com&gt;\n&gt; Date:Sat, Feb 7, 2015 at 4:23 am\n&gt; Subject:Re: [archive-crawler] Heritrix 3.2.0 deduplication\n&gt;\n&gt; It can be helpful in such cases, when learning/testing a feature for the\n&gt; 1st time, to make the crawls toy-sized. That is, crawl just a single URI\n&gt; that you believe would be eligible for duplication-savings. Only when\n&gt; you&#39;re sure the configuration works for the single URI would you then\n&gt; expand to a more realistic crawl.\n&gt;\n&gt; For example, you could try a crawl of just the static resource:\n&gt;\n&gt; http://localhost:8081/artJQWeb/resources/core/main.css\n&gt;\n&gt; In your first crawl, working with no prior state/history, you&#39;d expect\n&gt; this URI to have a normal log-line. In a second crawl that&#39;s set up\n&gt; properly to consult the first crawl&#39;s digest-history, it should appear\n&gt; as a detected duplicate (and then only be written in the WARC as a\n&gt; &#39;revisit&#39; record).\n&gt;\n&gt; Until you have this working for one URI, review your configuration, and\n&gt; make sure the 2nd crawl is properly consulting the history-info from the\n&gt; first.\n&gt;\n&gt; - Gordon\n&gt;\n&gt; On 2/6/15 4:50 AM, Sandip Dev devsandip2511@... [archive-crawler]\n&gt; wrote:\n&gt;  &gt; [Attachment(s) &lt;#TopText&gt; from Sandip Dev included below]\n&gt;  &gt;\n&gt;  &gt; Roger,\n&gt;  &gt; I deleted the state directory and history directory within my job folder\n&gt;  &gt; and then ran the crawl twice .\n&gt;  &gt; I am attaching the crawl log and crawl report for run 1 and 2 in case\n&gt;  &gt; you want to have a look. The crawl logs seems to be very much the same\n&gt;  &gt; for both runs. Do you think it looks ok to you as the warc size remains\n&gt;  &gt; same for both crawl?\n&gt;  &gt;\n&gt;  &gt; I have some other jobs as well which crawled this website so do I need\n&gt;  &gt; to delete the state directory for those jobs as well ?\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; Thanks Sandip\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; On Friday, 6 February 2015 4:59 PM, &quot;Coram, Roger&quot; &lt;Roger.Coram@...&gt;\n&gt;  &gt; wrote:\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt; Hi Sandip,\n&gt;  &gt; If, having remove the legacy beans, you’re seeing “warcRevisit:digest”\n&gt;  &gt; in the log this sounds like it’s already deduplicating from an earlier\n&gt;  &gt; crawl—did you delete Heritrix’s state directory before testing? If not,\n&gt;  &gt; Heritrix will try and deduplicate based on the earlier, stored metadata.\n&gt;  &gt; Roger\n&gt;  &gt; *From:*Sandip Dev [mailto:devsandip2511@...]\n&gt;  &gt; *Sent:* 06 February 2015 11:24\n&gt;  &gt; *To:* Coram, Roger\n&gt;  &gt; *Subject:* Re: [archive-crawler] Heritrix 3.2.0 deduplication\n&gt;  &gt; Hi Roger,\n&gt;  &gt; Thanks a lot for your reply.\n&gt;  &gt; I tried testing this on a small web app I created.\n&gt;  &gt; As suggested I removed the Legacy config&#39;s and only kept the\n&gt;  &gt; URL-Agnostic Duplication Reduction .\n&gt;  &gt; I can also see the warcRevisit:digest records in the crawl log and it\n&gt;  &gt; generated a 312 kb warc file which played back fine.\n&gt;  &gt; I ran the crawl once more without changing anything on the site I was\n&gt;  &gt; expecting the size of the warc to reduce as I had not changed anything\n&gt;  &gt; on the site and as all the contents were already present in the first\n&gt;  &gt; warc file. but the size of the archieve still remained 312 kb.\n&gt;  &gt; Is my assumption not correct ?\n&gt;  &gt; Thanks Sandip\n&gt;  &gt; On Friday, 6 February 2015 4:43 PM, &quot;Sandip Dev devsandip2511@...\n&gt;  &gt; [archive-crawler]&quot; &lt;archive-crawler@yahoogroups.com&gt; wrote:\n&gt;  &gt; Hi Roger,\n&gt;  &gt; Thanks a lot for your reply.\n&gt;  &gt; I tried testing this on a small web app I created.\n&gt;  &gt; As suggested I removed the Legacy config&#39;s and only kept the\n&gt;  &gt; URL-Agnostic Duplication Reduction .\n&gt;  &gt; I can also see the warcRevisit:digest records in the crawl log and it\n&gt;  &gt; generated a 312 kb warc file which played back fine.\n&gt;  &gt; I ran the crawl once more without changing anything on the site I was\n&gt;  &gt; expecting the size of the warc to reduce as I had not changed anything\n&gt;  &gt; on the site and as all the contents were already present in the first\n&gt;  &gt; warc file. but the size of the archieve still remained 312 kb.\n&gt;  &gt; Is my assumption not correct ?\n&gt;  &gt; Thanks Sandip\n&gt;  &gt; On Friday, 6 February 2015 3:55 PM, &quot;&#39;Coram, Roger&#39; Roger.Coram@...\n&gt;  &gt; [archive-crawler]&quot; &lt;archive-crawler@yahoogroups.com&gt; wrote:\n&gt;  &gt; Hi Sandip,\n&gt;  &gt; As far as I’m aware you should definitely be using only one of the\n&gt;  &gt; “URL-Agnostic Duplication Reduction” or the “Legacy…” version. Although\n&gt;  &gt; they largely do the same thing they behave differently (effectively the\n&gt;  &gt; newer version keys on the checksum, the older keys on the URL+checksum).\n&gt;  &gt; At first glance your config. looks correct so it might be the inclusion\n&gt;  &gt; of both the above causing the issue. During a crawl, if you watch your\n&gt;  &gt; crawl.log file, you should see entries with annotations like\n&gt;  &gt; “warcRevisit:digest” in the last field—that indicates deduplication is\n&gt;  &gt; working.\n&gt;  &gt; Roger\n&gt;  &gt; *From:*archive-crawler@yahoogroups.com\n&gt;  &gt; [mailto:archive-crawler@yahoogroups.com]\n&gt;  &gt; *Sent:* 06 February 2015 04:07\n&gt;  &gt; *To:* archive-crawler@yahoogroups.com\n&gt;  &gt; *Subject:* [archive-crawler] Heritrix 3.2.0 deduplication [1 Attachment]\n&gt;  &gt; *[Attachment(s)\n&gt;  &gt; &lt;https://in-mg61.mail.yahoo.com/neo/launch?.rand=b829oo386s801#TopText&gt;\n&gt;  &gt; from Sandip Dev included below]*\n&gt;  &gt; Hi,\n&gt;  &gt; I had already posted my questions earlier regarding the issues that I\n&gt;  &gt; have been facing the deduplication.\n&gt;  &gt; Can you please let me know if this is the correct forum where I should\n&gt;  &gt; post my questions ?\n&gt;  &gt; I am trying to achieve incremental crawling using Heritrix 3.2.0.\n&gt;  &gt; However it seems it is always browsing all the URL&#39;s even if there is no\n&gt;  &gt; change.\n&gt;  &gt; I am attaching crawler-beans.xml for the configuration that I am doing.\n&gt;  &gt; Can anyone suggest what I might be doing wrong ?\n&gt;  &gt; Is the spring bean I have put the configurations for both\n&gt;  &gt;\n&gt;  &gt; * URL-Agnostic Duplication Reduction\n&gt;  &gt; * Legacy Duplication Reduction Configuration\n&gt;  &gt;\n&gt;  &gt; Can I use both the features at the same time in the config ? I see the\n&gt;  &gt; archive size remains the same across multiple crawls even if there is no\n&gt;  &gt; change . How do I check if the the deduplication is working ?\n&gt;  &gt; Any help will be greatly appreciated as I am desperately tryinjg to get\n&gt;  &gt; this work :)\n&gt;  &gt; Thanks\n&gt;  &gt; Sandip Dev\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt; ******************************************************************************************************************\n&gt;  &gt; Experience the British Library online at www.bl.uk &lt;http://www.bl.uk/&gt;\n&gt;  &gt; The British Library’s latest Annual Report and Accounts :\n&gt;  &gt; www.bl.uk/aboutus/annrep/index.html\n&gt;  &gt; &lt;http://www.bl.uk/aboutus/annrep/index.html&gt;\n&gt;  &gt; Help the British Library conserve the world&#39;s knowledge. Adopt a Book.\n&gt;  &gt; www.bl.uk/adoptabook &lt;http://www.bl.uk/adoptabook&gt;\n&gt;  &gt; The Library&#39;s St Pancras site is WiFi - enabled\n&gt;  &gt;\n&gt; *****************************************************************************************************************\n&gt;  &gt; The information contained in this e-mail is confidential and may be\n&gt;  &gt; legally privileged. It is intended for the addressee(s) only. If you are\n&gt;  &gt; not the intended recipient, please delete this e-mail and notify the\n&gt;  &gt; postmaster@... &lt;mailto:postmaster@...&gt;: The contents of this e-mail\n&gt;  &gt; must not be disclosed or copied without the sender&#39;s consent.\n&gt;  &gt; The statements and opinions expressed in this message are those of the\n&gt;  &gt; author and do not necessarily reflect those of the British Library. The\n&gt;  &gt; British Library does not take any responsibility for the views of the\n&gt;  &gt; author.\n&gt;  &gt;\n&gt; *****************************************************************************************************************\n&gt;  &gt;\n&gt;  &gt; Think before you print\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;  &gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt;\n&gt; \n\n"}}