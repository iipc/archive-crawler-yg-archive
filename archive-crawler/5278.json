{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":137285340,"authorName":"Gordon Mohr","from":"Gordon Mohr &lt;gojomo@...&gt;","profile":"gojomo","replyTo":"LIST","senderId":"DqwTQ7KOP3_Bx6Zy059PvwYblafEM0A4yCpxzlbztthvdMm138MotAq0LUdjX9Sro3vBSH1ipude1__sQjZL8Ry7IUZigRE","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [archive-crawler] Re: Content type specific crawling?","postDate":"1212523062","msgId":5278,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDQ4NDVBMjM2LjYwNzA5QGFyY2hpdmUub3JnPg==","inReplyToHeader":"PGcyM3BwNCtnZmt0QGVHcm91cHMuY29tPg==","referencesHeader":"PGcyM3BwNCtnZmt0QGVHcm91cHMuY29tPg=="},"prevInTopic":5277,"nextInTopic":5282,"prevInTime":5277,"nextInTime":5279,"topicId":5248,"numMessagesInTopic":9,"msgSnippet":"So if I may summarize: - You have a set of decide rules set up on the writer processor - these rules begin with a REJECT-all rule, then have a ","rawEmail":"Return-Path: &lt;gojomo@...&gt;\r\nX-Sender: gojomo@...\r\nX-Apparently-To: archive-crawler@yahoogroups.com\r\nX-Received: (qmail 48143 invoked from network); 3 Jun 2008 19:57:37 -0000\r\nX-Received: from unknown (66.218.67.94)\n  by m43.grp.scd.yahoo.com with QMQP; 3 Jun 2008 19:57:37 -0000\r\nX-Received: from unknown (HELO relay03.pair.com) (209.68.5.17)\n  by mta15.grp.scd.yahoo.com with SMTP; 3 Jun 2008 19:57:37 -0000\r\nX-Received: (qmail 97378 invoked from network); 3 Jun 2008 19:57:36 -0000\r\nX-Received: from unknown (HELO ?192.168.1.15?) (unknown)\n  by unknown with SMTP; 3 Jun 2008 19:57:36 -0000\r\nX-pair-Authenticated: 67.180.197.118\r\nMessage-ID: &lt;4845A236.60709@...&gt;\r\nDate: Tue, 03 Jun 2008 12:57:42 -0700\r\nUser-Agent: Thunderbird 2.0.0.14 (Windows/20080421)\r\nMIME-Version: 1.0\r\nTo: archive-crawler@yahoogroups.com\r\nReferences: &lt;g23pp4+gfkt@...&gt;\r\nIn-Reply-To: &lt;g23pp4+gfkt@...&gt;\r\nContent-Type: text/plain; charset=ISO-8859-1; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:12:0:0:0\r\nFrom: Gordon Mohr &lt;gojomo@...&gt;\r\nSubject: Re: [archive-crawler] Re: Content type specific crawling?\r\nX-Yahoo-Group-Post: member; u=137285340; y=Q5BU0MXlAfToDI67VEpWzKPxbqx3hjXDom3r2iGPxFms\r\nX-Yahoo-Profile: gojomo\r\n\r\nSo if I may summarize:\n\n- You have a set of decide rules set up on the writer processor\n- these rules begin with a REJECT-all rule, then have a \nContentTypeMatchesRegexpDecideRule set to only ACCEPT XML content\n- you still see HTML being written by that writer processor\n\nIs that correct?\n\nSome ideas:\n- are you using a non-standard writer that isn&#39;t consulting \nshouldProcess correctly?\n- do you have multiple writer processors active, and another processor \nis writing?\n- the content is actually coming back as XHTML, and has &#39;xml&#39; in its \ncontent-type?\n- the writer-processor&#39;s decide-rules are somehow not-enabled?\n- an error in the regex is accepting everything?\n\nIf it&#39;s none of these, feel free to send an excerpt of your \nconfiguration for the writer processor (from your heritrix1 order.xml or \nheritrix2.0 global.sheet) for analysis.\n\n- Gordon @ IA\n\nlpeterus wrote:\n&gt; I had a default REJECT in the scope to start with but not in the\n&gt; mid-fetch or writer processor phase. I also tried adding those in as\n&gt; the first rule for them but with no success. The\n&gt; ContentTypeMatchesRegexpDecideRule is configured as accept with the\n&gt; regex for xml content type. I got the idea from a previous post but\n&gt; configured to just accept html content. \n&gt; http://tech.groups.yahoo.com/group/archive-crawler/message/2824\n&gt; \n&gt; -Shawn\n&gt; \n&gt; --- In archive-crawler@yahoogroups.com, Gordon Mohr &lt;gojomo@...&gt; wrote:\n&gt;&gt; Do you have a REJECT rule first that applies to everything, then the \n&gt;&gt; ContentTypeMatchesRegExpDecideRule to ACCEPT the right kind of content?\n&gt;&gt;\n&gt;&gt; Otherwise, the decision of the decide-rules will be ambiguous (PASS), \n&gt;&gt; and in the case of Processor decide-rules, anything but a REJECT allows \n&gt;&gt; the Processor to run. (See Processor.process().)\n&gt;&gt;\n&gt;&gt; - Gordon @ IA\n&gt;&gt;\n&gt;&gt; lpeterus wrote:\n&gt;&gt;&gt; Hi!\n&gt;&gt;&gt;\n&gt;&gt;&gt; Thanks for the reply. I did have a ContentTypeMatchesRegExpDecideRule\n&gt;&gt;&gt; under the writer processor section with the following regex\n&gt;&gt;&gt; (?i)application/xml.*\n&gt;&gt;&gt; But it still seems to be writing the text/html pages from dynamic\n&gt;&gt;&gt; URLs. Did I use the wrong type of expression? Thanks.\n&gt;&gt;&gt;\n&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, &quot;ermhes82&quot; &lt;ermhes@&gt; wrote:\n&gt;&gt;&gt;&gt; You can prevent this if you include in decide-rules a \n&gt;&gt;&gt;&gt; ContentTypeMatchesRegExpDecideRule o \n&gt;&gt;&gt;&gt; ContentTypeNotMatchesRegExpDecideRule.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; Mario.\n&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt; --- In archive-crawler@yahoogroups.com, &quot;lpeterus&quot; &lt;lpeterus@&gt; wrote:\n&gt;&gt;&gt;&gt;&gt; Hi all!\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; I have a question about how to exclude dynamic URLs. I&#39;m using\n&gt; regular\n&gt;&gt;&gt;&gt;&gt; expression rules based on suggestions from previous posting. I would\n&gt;&gt;&gt;&gt;&gt; like to filter out everything except for text/xml and\n&gt; application/xml\n&gt;&gt;&gt;&gt;&gt; files. The filters are applied on the scope, midfetch, and the arc\n&gt;&gt;&gt;&gt;&gt; writer processor. \n&gt;&gt;&gt;&gt;&gt; So far everything works ok, except it still dowloads some text/html\n&gt;&gt;&gt;&gt;&gt; files even though html is part of the rejection regexp in the scope.\n&gt;&gt;&gt;&gt;&gt; Turns out all of the text/html downloads are dynamic URLs like\n&gt; this, \n&gt;&gt;&gt;&gt;&gt; http://somewebpage/subdir/?C=D;O=A \n&gt;&gt;&gt;&gt;&gt; My question then is how do I prevent these text/html from being\n&gt;&gt;&gt;&gt;&gt; written into the arc file. Thanks!\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;&gt;&gt; Shawn\n&gt;&gt;&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; ------------------------------------\n&gt;&gt;&gt;\n&gt;&gt;&gt; Yahoo! Groups Links\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt; \n&gt; \n&gt; \n&gt; ------------------------------------\n&gt; \n&gt; Yahoo! Groups Links\n&gt; \n&gt; \n&gt; \n\n"}}